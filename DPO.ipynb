{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model vocab size: 30522\n",
      "Model embedding size: 256\n",
      "Configured max length: 512\n",
      "Batch 1:\n",
      "Input shape to LanguageModelTransformer: torch.Size([2, 512])\n",
      "Max position index: 511, Position Embedding Size: 512\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([2])) must be the same as input size (torch.Size([31254528]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1235\u001b[0m\n\u001b[0;32m   1233\u001b[0m label_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1234\u001b[0m input_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 1235\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mexpert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(expert_model\u001b[38;5;241m.\u001b[39mtransformer_dpo\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n",
      "Cell \u001b[1;32mIn[32], line 947\u001b[0m, in \u001b[0;36mExpert.train_dpo\u001b[1;34m(self, train_loader, optimizer, label_column, input_columns, config, save_path)\u001b[0m\n\u001b[0;32m    944\u001b[0m     combined_input_ids \u001b[38;5;241m=\u001b[39m combined_input_ids[:, :max_length]\n\u001b[0;32m    946\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_dpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_question\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_chosen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_rejected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    950\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 854\u001b[0m, in \u001b[0;36mExpert.DPO.forward\u001b[1;34m(self, input_ids_question, input_ids_chosen, input_ids_rejected, labels, attention_mask)\u001b[0m\n\u001b[0;32m    852\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(output, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m output\n\u001b[0;32m    853\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m--> 854\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, loss\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\functional.py:3193\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3190\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m-> 3193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([2])) must be the same as input size (torch.Size([31254528]))"
     ]
    }
   ],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "class ExpertConfig:\n",
    "    def __init__(self, seq_len=512, head_dim=64, block_size=64, sparsity_factor=4,\n",
    "                 input_dim=512, num_experts=3, vocab_size=30522, embed_size=256,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0.1,\n",
    "                 max_length=512,  # Updated to match seq_len for consistency\n",
    "                 rank=16, device='cpu',\n",
    "                 mamba_model_path='D:\\\\EXPERT_WEIGHTS\\\\mamba_model_weights.pth',\n",
    "                 context_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\context_encoder.pth',\n",
    "                 language_model_path='D:\\\\EXPERT_WEIGHTS\\\\language_model.pth',\n",
    "                 question_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\question_encoder.pth',\n",
    "                 dpo_model_path='D:\\\\EXPERT_WEIGHTS\\\\dpo_model_weights.pth',\n",
    "                 model_name='bert-base-uncased', embedding_dim=768,\n",
    "                 alpha=1, quantization_bits=8, tokenizer_name='bert-base-uncased',\n",
    "                 d_model=512, d_state=2048, d_conv=3, expansion_factor=2):\n",
    "\n",
    "        # Common hyperparameters\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        self.input_dim = input_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length  # Ensure this is properly reflected in model components\n",
    "        self.rank = rank\n",
    "\n",
    "        # Model paths and device\n",
    "        self.mamba_model_path = mamba_model_path\n",
    "        self.context_encoder_path = context_encoder_path\n",
    "        self.language_model_path = language_model_path\n",
    "        self.question_encoder_path = question_encoder_path\n",
    "        self.dpo_model_path = dpo_model_path\n",
    "        self.device = device\n",
    "\n",
    "        # Unique hyperparameters\n",
    "        self.num_experts = num_experts\n",
    "        self.model_name = model_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        # PDFs (unchanged)\n",
    "        self.pdf_file_paths = [\n",
    "            r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "            r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf'\n",
    "        ]\n",
    "        \n",
    "        # Preserving original dataset loading functionality\n",
    "        self.rag_dataset = Expert.TransformerRAG.create_dataset_from_pdfs(self.pdf_file_paths)\n",
    "\n",
    "    def validate(self):\n",
    "        assert self.seq_len % self.block_size == 0, \"seq_len must be divisible by block_size\"\n",
    "        assert self.max_length >= self.seq_len, \"max_length should be equal to or greater than seq_len\"\n",
    "\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    # Load model weights function\n",
    "    def load_model_weights(model, model_path):\n",
    "        #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        checkpoint = torch.load(model_path, map_location=config.device)\n",
    "\n",
    "        if isinstance(checkpoint, dict):\n",
    "            # Check for 'state_dict' or 'model_state_dict' keys\n",
    "            if 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            elif 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                # If no known key is found, try loading it as a raw state dictionary\n",
    "                try:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                except RuntimeError as e:\n",
    "                    raise ValueError(f\"Error loading state dict: {e}\")\n",
    "        elif isinstance(checkpoint, nn.Module):\n",
    "            # If the checkpoint is a model object, assign it directly\n",
    "            model = checkpoint\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    # Flash2_Attention\n",
    "    class FlashAttention2(nn.Module):\n",
    "        def __init__(self, sequence_length, head_dimension, block_size):\n",
    "            super(Expert.FlashAttention2, self).__init__()\n",
    "            self.block_size = block_size\n",
    "            # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "            assert sequence_length % block_size == 0\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            # Partitioning of inputs\n",
    "            Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "            # Efficient computation of the attention mechanism\n",
    "            outputs = []\n",
    "            for i, Q_block in enumerate(Q_blocks):\n",
    "                output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "                outputs.append(output_block)\n",
    "\n",
    "            # Concatenating the processed blocks\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "            return output\n",
    "\n",
    "        def partition_inputs(self, Q, K, V):\n",
    "            # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "            Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "            K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "            V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "            return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "        def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "            # Process each block efficiently as per FLASH2's optimized method\n",
    "            # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "            output_blocks = []\n",
    "            for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "                attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "                attention_scores = self.online_softmax(attention_scores)\n",
    "                output_block = torch.matmul(attention_scores, V_block)\n",
    "                output_blocks.append(output_block)\n",
    "\n",
    "            # Summing up the results from each block\n",
    "            output_block_sum = sum(output_blocks)\n",
    "            return output_block_sum\n",
    "\n",
    "        def online_softmax(self, scores, chunk_size=128):\n",
    "            # Apply softmax in chunks for large sequences\n",
    "            softmaxed_scores = []\n",
    "            for i in range(0, scores.size(0), chunk_size):\n",
    "                chunk = scores[i:i + chunk_size, :]\n",
    "                softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "                softmaxed_scores.append(softmaxed_chunk)\n",
    "            return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "    # SparseFlash2_Attention\n",
    "    class SparseFlash2Attention(nn.Module):\n",
    "        def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "            super().__init__()\n",
    "            self.flash_attention = Expert.FlashAttention2(seq_len, head_dim, blk_size)\n",
    "            self.seq_len = seq_len\n",
    "            self.head_dim = head_dim\n",
    "            self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "            self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        def generate_sparsity_mask(self):\n",
    "            mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "            step = self.sparsity_factor\n",
    "            for i in range(0, self.seq_len, step):\n",
    "                mask[i:i + step, :] = 1\n",
    "            return mask.bool()\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "            # Reshape output to be 3D for batch matrix multiplication\n",
    "            output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "            sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "            # Apply the sparsity mask to the output\n",
    "            sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "            output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "            # Reshape the output back to 2D\n",
    "            output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "            return output\n",
    "\n",
    "\n",
    "    # MAMBA\n",
    "    # RoPE\n",
    "    class RotaryPositionalEncoding(nn.Module):\n",
    "        def __init__(self, dim, max_len=5000):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "            t = torch.arange(max_len).type_as(inv_freq)\n",
    "            freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "            self.register_buffer('sin', freqs.sin())\n",
    "            self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "        def forward(self, x):\n",
    "            n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "            sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "            # Apply RoPE to even and odd indices separately\n",
    "            x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "    # SWIGLU\n",
    "    class SwiGLU(nn.Module):\n",
    "        def __init__(self, dim_in, dim_out):\n",
    "            super(Expert.SwiGLU, self).__init__()\n",
    "            self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "            self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "        def forward(self, x):\n",
    "            gate = torch.sigmoid(self.fc2(x))\n",
    "            return self.fc1(x) * gate\n",
    "\n",
    "    class SimplifiedMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "            \n",
    "            self.feedforward = nn.Sequential(\n",
    "                nn.Linear(self.d_model, self.d_state),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(self.d_state, self.d_model)\n",
    "            )\n",
    "            self.input_embedding = nn.Linear(self.d_model, self.d_model)\n",
    "            self.convs = nn.Sequential(*[nn.Conv1d(self.d_model, self.d_model, kernel_size=self.d_conv, padding=(self.d_conv // 2)) for _ in range(self.num_layers)])\n",
    "            self.swiglu = Expert.SwiGLU(self.d_model, self.d_model)\n",
    "            self.output_projection = nn.Linear(self.d_model, self.d_model * self.expansion_factor)\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "            nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "            nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "            nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "        def forward(self, inputs, attention_mask=None):\n",
    "            print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "            # Apply the attention mask if provided\n",
    "            if attention_mask is not None:\n",
    "                inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "            projected_inputs = self.input_embedding(inputs)\n",
    "            print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            for conv in self.convs:\n",
    "                projected_inputs = conv(projected_inputs)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = self.swiglu(projected_inputs)\n",
    "            print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "            output = self.output_projection(projected_inputs)\n",
    "            print(\"output shape:\", output.shape)\n",
    "\n",
    "            return output\n",
    "\n",
    "    class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.vocab_size = config.vocab_size\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "            self.pos_encoder = Expert.RotaryPositionalEncoding(self.d_model)\n",
    "            self.simplified_mamba = Expert.SimplifiedMAMBA(config)\n",
    "            self.output_projection = nn.Linear(self.d_model * 2, self.vocab_size)  # Adjust if needed\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = 1.0\n",
    "\n",
    "            nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "        def forward(self, input_values, attention_mask):\n",
    "            embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "            embedded = self.pos_encoder(embedded)\n",
    "            simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "            logits = self.output_projection(simplified_mamba_output)\n",
    "            return logits\n",
    "\n",
    "\n",
    "    class SwitchRouter(nn.Module):\n",
    "        CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "        class SwitchGate(nn.Module):\n",
    "            def __init__(self, input_dim, num_experts):\n",
    "                super(Expert.SwitchRouter.SwitchGate, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "                self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "                return gate_scores\n",
    "\n",
    "        def __init__(self, config):\n",
    "            super(Expert.SwitchRouter, self).__init__()\n",
    "            self.config = config\n",
    "            self.device = config.device\n",
    "            self.router = self.SwitchGate(config.input_dim, config.num_experts).to(config.device)\n",
    "            self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "            self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.transformer_dpo = Expert.DPO(self.lmt, config.device).to(config.device)\n",
    "            self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "            self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "            self.input_embedding = nn.Linear(config.input_dim, config.input_dim)\n",
    "\n",
    "        def forward(self, x, attention_mask, context_texts, question_text):\n",
    "            x = x.to(self.device)\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.float()\n",
    "\n",
    "            x = self.input_embedding(x)\n",
    "            gate_scores = self.router(x)\n",
    "            expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "            final_output = torch.zeros_like(x)\n",
    "\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                mask = expert_indices == i\n",
    "                if mask.any():\n",
    "                    selected_inputs = x[mask]\n",
    "                    selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                    if isinstance(expert, Expert.TransformerRAG):\n",
    "                        expert_output = expert.forward(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                    else:\n",
    "                        expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                    final_output[mask] = expert_output\n",
    "\n",
    "            aux_loss = self.auxiliary_loss(gate_scores)\n",
    "            return final_output, aux_loss\n",
    "\n",
    "        def auxiliary_loss(self, gate_scores):\n",
    "            expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "            loss_balancing = torch.std(expert_load)\n",
    "            return loss_balancing\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "            capacity_factor_tensor = torch.tensor([Expert.SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "            capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "            expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "            for idx in range(len(expert_indices)):\n",
    "                selected_expert = expert_indices[idx]\n",
    "                if expert_counts[selected_expert] < capacities[0]:\n",
    "                    expert_counts[selected_expert] += 1\n",
    "                else:\n",
    "                    available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                    if len(available_experts) > 0:\n",
    "                        alternative_expert = available_experts[0]\n",
    "                        expert_indices[idx] = alternative_expert\n",
    "                        expert_counts[alternative_expert] += 1\n",
    "                    else:\n",
    "                        print(\"No available experts to reroute. Handling overflow.\")\n",
    "            return expert_indices\n",
    "    \n",
    "    class CustomDPRContextEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.CustomDPRContextEncoder, self).__init__()  \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            context_embeddings = self.embedding_layer(pooled_output)\n",
    "            return context_embeddings\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.DPRQuestionEncoder, self).__init__() \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, **kwargs):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            embeddings = self.embedding_layer(pooled_output)\n",
    "            return embeddings\n",
    "\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(Expert.TransformerRAG, self).__init__()\n",
    "            self.config = config\n",
    "            self.context_encoder = Expert.CustomDPRContextEncoder(config.embedding_dim).to(config.device)\n",
    "            self.language_model = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.question_encoder = Expert.DPRQuestionEncoder(config.embedding_dim).to(config.device)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "            \n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "                \n",
    "                aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = context['input_ids'].to(config.device)\n",
    "                    context_attention_mask = context['attention_mask'].to(config.device)\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "                \n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "            \n",
    "            question_input_ids = question_input_ids.to(config.device).long()\n",
    "            question_attention_mask = question_attention_mask.to(config.device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "            \n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=config.device))\n",
    "            \n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(config.device) for k, v in tokenized_combined_input.items()}\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            \n",
    "            return response\n",
    "\n",
    "        @staticmethod\n",
    "        def extract_text_from_pdf(file_path):\n",
    "            text = \"\"\n",
    "            with fitz.open(file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            return text\n",
    "\n",
    "        @staticmethod\n",
    "        def split_into_chunks(text, chunk_size):\n",
    "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        @staticmethod\n",
    "        def preprocess_text(text, max_length=512):\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            chunk_size = max_length - 50\n",
    "            text_chunks = Expert.TransformerRAG.split_into_chunks(text, chunk_size)\n",
    "            processed_chunks = []\n",
    "            for chunk in text_chunks:\n",
    "                tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "                processed_chunk = {\n",
    "                    'input_ids': tokenized_output['input_ids'],\n",
    "                    'attention_mask': tokenized_output['attention_mask']\n",
    "                }\n",
    "                processed_chunks.append(processed_chunk)\n",
    "            return processed_chunks\n",
    "\n",
    "        @staticmethod\n",
    "        def create_dataset_from_pdfs(pdf_file_paths):\n",
    "            dataset = []\n",
    "            for file_path in pdf_file_paths:\n",
    "                text = Expert.TransformerRAG.extract_text_from_pdf(file_path)\n",
    "                processed_text = Expert.TransformerRAG.preprocess_text(text)\n",
    "                dataset.append(processed_text)\n",
    "            return dataset\n",
    "\n",
    "        def retrieve_contexts(self, dataset, query_embedding, top_k=5):\n",
    "            similarity_scores = []\n",
    "            for context in dataset:\n",
    "                context_input_ids = context['input_ids'].to(self.config.device)\n",
    "                context_attention_mask = context['attention_mask'].to(self.config.device)\n",
    "                # Use the class's context_encoder\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "                similarity_scores.append(similarity.squeeze().item())\n",
    "            top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "            top_contexts = [dataset[i] for i in top_k_indices]\n",
    "            return top_contexts\n",
    "\n",
    "        def rag_retrieve_and_generate(self, dataset, query):\n",
    "            # Tokenize the query using the class's tokenizer\n",
    "            tokenized_query = self.tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
    "            input_ids = tokenized_query['input_ids']\n",
    "            attention_mask = tokenized_query['attention_mask']\n",
    "            # Use the class's question_encoder\n",
    "            encoded_query = self.question_encoder(input_ids, attention_mask)\n",
    "            relevant_contexts = self.retrieve_contexts(dataset, encoded_query)\n",
    "            # Assuming generate_response is a method of LanguageModelTransformer that accepts tokenized contexts and generates a response\n",
    "            response = self.language_model.generate_response(relevant_contexts)\n",
    "            return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "            super(Expert.LORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "\n",
    "            # Original weight and bias of the linear layer\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # LORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "            super(Expert.QLORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "            self.quantization_bits = quantization_bits\n",
    "\n",
    "            # Original weight and bias\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # QLORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, embed_size, heads):\n",
    "            super(Expert.MultiHeadAttention, self).__init__()\n",
    "            self.embed_size = embed_size\n",
    "            self.heads = heads\n",
    "            self.head_dim = embed_size // heads\n",
    "\n",
    "            assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "            ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        def forward(self, values, keys, query, mask):\n",
    "            N = query.shape[0]\n",
    "            value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "            # Split the embedding into self.heads different pieces\n",
    "            values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "            queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            # Einsum does the matrix multiplication for query*keys for each training example\n",
    "            # with every other training example, then sum it up\n",
    "            attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "                N, query_len, self.heads * self.head_dim\n",
    "            )\n",
    "\n",
    "            out = self.fc_out(out)\n",
    "            return out\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "            super(Expert.TransformerBlock, self).__init__()\n",
    "            self.attention = Expert.MultiHeadAttention(embed_size, heads)\n",
    "            self.norm1 = nn.LayerNorm(embed_size)\n",
    "            self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                Expert.LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, value, key, query, mask):\n",
    "            attention = self.attention(value, key, query, mask)\n",
    "            x = self.dropout(self.norm1(attention + query))\n",
    "            forward = self.feed_forward(x)\n",
    "            out = self.dropout(self.norm2(forward + x))\n",
    "            return out\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "            super(Expert.LanguageModelDecoder, self).__init__()\n",
    "            self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "            # Adding BatchNorm layers\n",
    "            self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "            self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    Expert.TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # QLORA layers\n",
    "            self.qlora_feed_forward = nn.Sequential(\n",
    "                Expert.QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "            self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "            self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, trg_mask):\n",
    "            N, seq_length = x.shape\n",
    "            if seq_length > self.position_embedding.num_embeddings:\n",
    "                raise ValueError(f\"Sequence length {seq_length} exceeds maximum allowed {self.position_embedding.num_embeddings}\")\n",
    "            positions = torch.arange(0, seq_length).expand(N, seq_length).to(config.device)\n",
    "            x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "            print(f\"Max position index: {positions.max().item()}, Position Embedding Size: {self.position_embedding.num_embeddings}\")\n",
    "\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn1(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, trg_mask)\n",
    "                if self.use_qlora:\n",
    "                    x = self.qlora_feed_forward(x)\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            out = self.fc_out(x)\n",
    "            #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "            return out\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.use_qlora = use_qlora\n",
    "\n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.vocab_size = config.vocab_size  # Use vocab_size from ExpertConfig\n",
    "            self.embed_size = config.embed_size  # Use embed_size from ExpertConfig\n",
    "            self.num_layers = config.num_layers  # Use num_layers from ExpertConfig\n",
    "            self.forward_expansion = config.forward_expansion  # Use forward_expansion from ExpertConfig\n",
    "            self.heads = config.heads  # Use heads from ExpertConfig\n",
    "            self.dropout = config.dropout  # Use dropout from ExpertConfig\n",
    "            self.max_length = config.max_length  # Use max_length from ExpertConfig\n",
    "            self.rank = config.rank  # Use rank from ExpertConfig\n",
    "            self.tokenizer_name = config.tokenizer_name  # Use tokenizer_name from ExpertConfig\n",
    "\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "            self.decoder = Expert.LanguageModelDecoder(\n",
    "                self.vocab_size,\n",
    "                self.embed_size,\n",
    "                self.num_layers,\n",
    "                self.heads,\n",
    "                self.forward_expansion,\n",
    "                self.dropout,\n",
    "                self.max_length,\n",
    "                self.rank,\n",
    "            )\n",
    "\n",
    "        def forward(self, trg, attention_mask=None):  # Remove attention_mask here since it's not used\n",
    "            print(f\"Input shape to LanguageModelTransformer: {trg.shape}\")\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            out = self.decoder(trg, trg_mask)  # Do not pass attention_mask here\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len).to(trg.device)\n",
    "            return trg_mask\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = [self.tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            return response\n",
    "\n",
    "\n",
    "    class DPO(nn.Module):\n",
    "        def __init__(self, language_model, device):\n",
    "            super(Expert.DPO, self).__init__()\n",
    "            self.language_model = language_model\n",
    "            self.device = device\n",
    "        \n",
    "        def forward(self, input_ids_question, input_ids_chosen=None, input_ids_rejected=None, labels=None, attention_mask=None):\n",
    "            # Concatenate for DPO-specific data or use regular input handling\n",
    "            combined_input_ids = torch.cat([input_ids_question, input_ids_chosen, input_ids_rejected], dim=1) if input_ids_chosen is not None and input_ids_rejected is not None else input_ids_question\n",
    "            \n",
    "            # Use the vocab_size from the language model\n",
    "            combined_input_ids = torch.clamp(combined_input_ids, 0, self.language_model.vocab_size - 1)\n",
    "            \n",
    "            # Pass through the language model\n",
    "            output = self.language_model(combined_input_ids) #, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate loss if labels are provided\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                logits = output.logits if hasattr(output, 'logits') else output\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.float().view(-1))\n",
    "            \n",
    "            return output, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, config: ExpertConfig):\n",
    "        super().__init__()\n",
    "        # Validate configuration\n",
    "        config.validate()\n",
    "\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(\n",
    "            config.seq_len, \n",
    "            config.head_dim, \n",
    "            config.block_size, \n",
    "            config.sparsity_factor\n",
    "        )\n",
    "\n",
    "        # Inside the Expert class definition\n",
    "        self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "\n",
    "        self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "\n",
    "        # Corrected instantiation of SimplifiedLanguageModelMAMBA\n",
    "        self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "\n",
    "        # Now initialize DPO with the language model transformer\n",
    "        self.transformer_dpo = Expert.DPO(self.lmt, config.device).to(config.device)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(config.input_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = Expert.SwitchRouter(config)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = Expert.QLORALayer(config.input_dim, config.input_dim, config.rank)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, aux_loss = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "        \"\"\"\n",
    "        Calculate a new alpha value based on the current loss.\n",
    "        \"\"\"\n",
    "        if current_loss >= initial_loss:\n",
    "            return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "        loss_ratio = current_loss / initial_loss\n",
    "        alpha_range = initial_alpha - final_alpha\n",
    "        new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "        return new_alpha  \n",
    "    \n",
    "\n",
    "    def train_dpo(self, train_loader, optimizer, label_column, input_columns, config, save_path):\n",
    "        self.transformer_dpo.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            print(f\"Batch {i+1}:\")\n",
    "            input_ids_question = batch['input_ids_question'].to(config.device)\n",
    "            input_ids_chosen = batch['input_ids_chosen'].to(config.device)\n",
    "            input_ids_rejected = batch['input_ids_rejected'].to(config.device)\n",
    "            labels = batch['labels'].to(config.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Combine the input ids\n",
    "            combined_input_ids = torch.cat([input_ids_question, input_ids_chosen, input_ids_rejected], dim=1)\n",
    "            \n",
    "            # Truncate the combined_input_ids to the maximum sequence length\n",
    "            max_length = config.max_length  # 512\n",
    "            if combined_input_ids.size(1) > max_length:\n",
    "                combined_input_ids = combined_input_ids[:, :max_length]\n",
    "\n",
    "            # Forward pass\n",
    "            logits, loss = self.transformer_dpo(input_ids_question=combined_input_ids, input_ids_chosen=None, input_ids_rejected=None, labels=labels)\n",
    "\n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            else:\n",
    "                print(f\"No loss to backpropagate for batch {i+1}\")\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(self.transformer_dpo.state_dict(), save_path)\n",
    "        return average_loss\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_language_model_rag(self, model, save_path, train_loader, device, num_epochs=5, lr=1e-8, weight_decay=1e-4):\n",
    "        # Enable QLORA during training\n",
    "        model.decoder.toggle_qlora(True)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "\n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                targets = batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        # Toggle QLORA off after training\n",
    "        model.decoder.toggle_qlora(False)\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"Training Complete\")\n",
    "        return model, average_loss\n",
    "\n",
    "    def train_dpr_encoders(self, train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs , context_save_path, question_save_path):\n",
    "        loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(train_data[\"queries\"])):\n",
    "                query = train_data[\"queries\"][i]\n",
    "                context = train_data[\"contexts\"][i]\n",
    "\n",
    "                # Ensure query is a string\n",
    "                if not isinstance(query, str):\n",
    "                    raise ValueError(\"Query must be a string.\")\n",
    "                tokenized_query = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                # Ensure context is a string\n",
    "                if isinstance(context, dict):\n",
    "                    # If context is a dictionary, extract the text content. This is a placeholder and might need adjustment.\n",
    "                    context_text = context.get(\"text\", \"\")\n",
    "                elif isinstance(context, str):\n",
    "                    context_text = context\n",
    "                else:\n",
    "                    raise ValueError(\"Context must be a string or a dictionary containing a text field.\")\n",
    "                tokenized_context = tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                question_embeddings = question_encoder(input_ids=tokenized_query['input_ids'], attention_mask=tokenized_query['attention_mask'])\n",
    "                context_embeddings = context_encoder(input_ids=tokenized_context['input_ids'], attention_mask=tokenized_context['attention_mask'])\n",
    "\n",
    "                # The labels tensor should have the same first dimension size as the input tensors\n",
    "                labels = torch.tensor([1.0] * question_embeddings.size(0), dtype=torch.float).to(question_embeddings.device)\n",
    "\n",
    "                loss = loss_function(question_embeddings, context_embeddings, labels)\n",
    "\n",
    "                optimizer_context.zero_grad()\n",
    "                optimizer_question.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_context.step()\n",
    "                optimizer_question.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_data['queries'])}\")\n",
    "        average_loss = total_loss / len(train_data['queries'])\n",
    "        torch.save(context_encoder.state_dict(), context_save_path)\n",
    "        torch.save(question_encoder.state_dict(), question_save_path)\n",
    "        return (context_encoder, question_encoder), average_loss\n",
    "       \n",
    "    def train_language_model_transformer(self, train_loader, device, vocab_size, save_path):\n",
    "        model = self.lmt\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "        num_epochs = 5\n",
    "        \n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            model.decoder.toggle_qlora(True)\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs, targets = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        return model, average_loss\n",
    "\n",
    "\n",
    "    def train_expert(self, train_loader, train_data, optimizer, main_loss_function, aux_loss_weight, device,save_path, accumulation_steps=4, num_epochs=5):\n",
    "            self.train()  # Set the model to training mode\n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0\n",
    "                optimizer.zero_grad()  # Initialize gradients to zero\n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "                    inputs, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "\n",
    "                    # Calculate start and end indices for current batch in train_data\n",
    "                    start_idx = batch_idx * batch['input_ids'].size(0)\n",
    "                    end_idx = start_idx + batch['input_ids'].size(0)\n",
    "\n",
    "                    # Extract current_queries and current_contexts for the batch\n",
    "                    current_queries = train_data['queries'][start_idx:end_idx]\n",
    "                    current_contexts = train_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "                    # Call to the model forward function\n",
    "                    outputs, aux_loss = self(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "                    # Calculate loss and accumulate\n",
    "                    main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                    loss = (main_loss + aux_loss_weight * aux_loss) / accumulation_steps\n",
    "                    loss.backward()\n",
    "\n",
    "                    if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    total_loss += loss.item() * accumulation_steps  # Scale back up\n",
    "\n",
    "                average_loss = total_loss / len(train_loader)\n",
    "                print(f'End of Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            torch.save(self.state_dict(), save_path)\n",
    "            return self, average_loss\n",
    "\n",
    "\n",
    "##################################\n",
    "# Training transformer_with_dpo\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def preprocess_dpo_data(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    vocab_size = len(tokenizer.vocab)  # Make sure this matches your embedding layer's vocab size\n",
    "\n",
    "    # Define max sequence length\n",
    "    max_seq_length = 512\n",
    "\n",
    "    # Tokenize 'question', 'chosen', and 'rejected' fields\n",
    "    tokenized_questions = tokenizer(examples['question'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_chosen = tokenizer(examples['chosen'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_rejected = tokenizer(examples['rejected'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "\n",
    "    # Prepare the labels: 1 for 'chosen' and 0 for 'rejected'\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(len(examples['question']))]\n",
    "\n",
    "    return {\n",
    "        'input_ids_question': tokenized_questions['input_ids'],\n",
    "        'attention_mask_question': tokenized_questions['attention_mask'],\n",
    "        'input_ids_chosen': tokenized_chosen['input_ids'],\n",
    "        'attention_mask_chosen': tokenized_chosen['attention_mask'],\n",
    "        'input_ids_rejected': tokenized_rejected['input_ids'],\n",
    "        'attention_mask_rejected': tokenized_rejected['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Load the DPO dataset from Hugging Face\n",
    "dpo_dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "\n",
    "# Apply the preprocessing to the dataset\n",
    "dpo_dataset = dpo_dataset.map(preprocess_dpo_data, batched=True)\n",
    "\n",
    "\n",
    "# You can convert to PyTorch tensors after processing\n",
    "dpo_dataset.set_format(type='torch', columns=['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(dpo_dataset['train'], batch_size=2, shuffle=True)\n",
    "\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "expert_model = Expert(config)\n",
    "\n",
    "model_vocab_size = expert_model.lmt.decoder.word_embedding.num_embeddings\n",
    "print(f\"Model vocab size: {model_vocab_size}\")\n",
    "print(f\"Model embedding size: {expert_model.lmt.decoder.word_embedding.embedding_dim}\")\n",
    "print(f\"Configured max length: {config.max_length}\")\n",
    "\n",
    "optimizer = AdamW(expert_model.parameters(), lr=1e-5)\n",
    "save_path = 'D:/EXPERT_WEIGHTS/dpo_model.pth'\n",
    "\n",
    "# Train the DPO model\n",
    "label_column = 'labels'\n",
    "input_columns = ['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
    "avg_loss = expert_model.train_dpo(train_loader, optimizer, label_column, input_columns, config, save_path)\n",
    "\n",
    "# Save the model\n",
    "torch.save(expert_model.transformer_dpo.state_dict(), save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model vocab size: 30522\n",
      "Model embedding size: 256\n",
      "Configured max length: 512\n",
      "Batch 1:\n",
      "Max index in input_ids_question: 20714\n",
      "Max index in input_ids_chosen: 22414\n",
      "Max index in input_ids_rejected: 22414\n",
      "Max index in combined_input_ids: 20714\n",
      "Input shape to LanguageModelTransformer: torch.Size([2, 512])\n",
      "Max position index: 511, Position Embedding Size: 512\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1240\u001b[0m\n\u001b[0;32m   1238\u001b[0m label_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1239\u001b[0m input_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 1240\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mexpert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m   1243\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(expert_model\u001b[38;5;241m.\u001b[39mtransformer_dpo\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n",
      "Cell \u001b[1;32mIn[33], line 959\u001b[0m, in \u001b[0;36mExpert.train_dpo\u001b[1;34m(self, train_loader, optimizer, label_column, input_columns, config, save_path)\u001b[0m\n\u001b[0;32m    954\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_dpo(combined_input_ids, labels)  \u001b[38;5;66;03m# Use combined_input_ids here\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;66;03m# logits, loss = self.transformer_dpo(input_ids_question, input_ids_chosen, input_ids_rejected, labels)\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \n\u001b[0;32m    958\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n\u001b[0;32m    960\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    961\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "class ExpertConfig:\n",
    "    def __init__(self, seq_len=512, head_dim=64, block_size=64, sparsity_factor=4,\n",
    "                 input_dim=512, num_experts=3, vocab_size=30522, embed_size=256,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0.1,\n",
    "                 max_length=512,  # Updated to match seq_len for consistency\n",
    "                 rank=16, device='cpu',\n",
    "                 mamba_model_path='D:\\\\EXPERT_WEIGHTS\\\\mamba_model_weights.pth',\n",
    "                 context_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\context_encoder.pth',\n",
    "                 language_model_path='D:\\\\EXPERT_WEIGHTS\\\\language_model.pth',\n",
    "                 question_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\question_encoder.pth',\n",
    "                 dpo_model_path='D:\\\\EXPERT_WEIGHTS\\\\dpo_model_weights.pth',\n",
    "                 model_name='bert-base-uncased', embedding_dim=768,\n",
    "                 alpha=1, quantization_bits=8, tokenizer_name='bert-base-uncased',\n",
    "                 d_model=512, d_state=2048, d_conv=3, expansion_factor=2):\n",
    "\n",
    "        # Common hyperparameters\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        self.input_dim = input_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length  # Ensure this is properly reflected in model components\n",
    "        self.rank = rank\n",
    "\n",
    "        # Model paths and device\n",
    "        self.mamba_model_path = mamba_model_path\n",
    "        self.context_encoder_path = context_encoder_path\n",
    "        self.language_model_path = language_model_path\n",
    "        self.question_encoder_path = question_encoder_path\n",
    "        self.dpo_model_path = dpo_model_path\n",
    "        self.device = device\n",
    "\n",
    "        # Unique hyperparameters\n",
    "        self.num_experts = num_experts\n",
    "        self.model_name = model_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        # PDFs (unchanged)\n",
    "        self.pdf_file_paths = [\n",
    "            r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "            r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf'\n",
    "        ]\n",
    "        \n",
    "        # Preserving original dataset loading functionality\n",
    "        self.rag_dataset = Expert.TransformerRAG.create_dataset_from_pdfs(self.pdf_file_paths)\n",
    "\n",
    "    def validate(self):\n",
    "        assert self.seq_len % self.block_size == 0, \"seq_len must be divisible by block_size\"\n",
    "        assert self.max_length >= self.seq_len, \"max_length should be equal to or greater than seq_len\"\n",
    "\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    # Load model weights function\n",
    "    def load_model_weights(model, model_path):\n",
    "        #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        checkpoint = torch.load(model_path, map_location=config.device)\n",
    "\n",
    "        if isinstance(checkpoint, dict):\n",
    "            # Check for 'state_dict' or 'model_state_dict' keys\n",
    "            if 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            elif 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                # If no known key is found, try loading it as a raw state dictionary\n",
    "                try:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                except RuntimeError as e:\n",
    "                    raise ValueError(f\"Error loading state dict: {e}\")\n",
    "        elif isinstance(checkpoint, nn.Module):\n",
    "            # If the checkpoint is a model object, assign it directly\n",
    "            model = checkpoint\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    # Flash2_Attention\n",
    "    class FlashAttention2(nn.Module):\n",
    "        def __init__(self, sequence_length, head_dimension, block_size):\n",
    "            super(Expert.FlashAttention2, self).__init__()\n",
    "            self.block_size = block_size\n",
    "            # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "            assert sequence_length % block_size == 0\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            # Partitioning of inputs\n",
    "            Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "            # Efficient computation of the attention mechanism\n",
    "            outputs = []\n",
    "            for i, Q_block in enumerate(Q_blocks):\n",
    "                output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "                outputs.append(output_block)\n",
    "\n",
    "            # Concatenating the processed blocks\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "            return output\n",
    "\n",
    "        def partition_inputs(self, Q, K, V):\n",
    "            # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "            Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "            K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "            V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "            return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "        def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "            # Process each block efficiently as per FLASH2's optimized method\n",
    "            # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "            output_blocks = []\n",
    "            for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "                attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "                attention_scores = self.online_softmax(attention_scores)\n",
    "                output_block = torch.matmul(attention_scores, V_block)\n",
    "                output_blocks.append(output_block)\n",
    "\n",
    "            # Summing up the results from each block\n",
    "            output_block_sum = sum(output_blocks)\n",
    "            return output_block_sum\n",
    "\n",
    "        def online_softmax(self, scores, chunk_size=128):\n",
    "            # Apply softmax in chunks for large sequences\n",
    "            softmaxed_scores = []\n",
    "            for i in range(0, scores.size(0), chunk_size):\n",
    "                chunk = scores[i:i + chunk_size, :]\n",
    "                softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "                softmaxed_scores.append(softmaxed_chunk)\n",
    "            return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "    # SparseFlash2_Attention\n",
    "    class SparseFlash2Attention(nn.Module):\n",
    "        def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "            super().__init__()\n",
    "            self.flash_attention = Expert.FlashAttention2(seq_len, head_dim, blk_size)\n",
    "            self.seq_len = seq_len\n",
    "            self.head_dim = head_dim\n",
    "            self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "            self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        def generate_sparsity_mask(self):\n",
    "            mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "            step = self.sparsity_factor\n",
    "            for i in range(0, self.seq_len, step):\n",
    "                mask[i:i + step, :] = 1\n",
    "            return mask.bool()\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "            # Reshape output to be 3D for batch matrix multiplication\n",
    "            output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "            sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "            # Apply the sparsity mask to the output\n",
    "            sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "            output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "            # Reshape the output back to 2D\n",
    "            output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "            return output\n",
    "\n",
    "\n",
    "    # MAMBA\n",
    "    # RoPE\n",
    "    class RotaryPositionalEncoding(nn.Module):\n",
    "        def __init__(self, dim, max_len=5000):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "            t = torch.arange(max_len).type_as(inv_freq)\n",
    "            freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "            self.register_buffer('sin', freqs.sin())\n",
    "            self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "        def forward(self, x):\n",
    "            n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "            sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "            # Apply RoPE to even and odd indices separately\n",
    "            x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "    # SWIGLU\n",
    "    class SwiGLU(nn.Module):\n",
    "        def __init__(self, dim_in, dim_out):\n",
    "            super(Expert.SwiGLU, self).__init__()\n",
    "            self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "            self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "        def forward(self, x):\n",
    "            gate = torch.sigmoid(self.fc2(x))\n",
    "            return self.fc1(x) * gate\n",
    "\n",
    "    class SimplifiedMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "            \n",
    "            self.feedforward = nn.Sequential(\n",
    "                nn.Linear(self.d_model, self.d_state),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(self.d_state, self.d_model)\n",
    "            )\n",
    "            self.input_embedding = nn.Linear(self.d_model, self.d_model)\n",
    "            self.convs = nn.Sequential(*[nn.Conv1d(self.d_model, self.d_model, kernel_size=self.d_conv, padding=(self.d_conv // 2)) for _ in range(self.num_layers)])\n",
    "            self.swiglu = Expert.SwiGLU(self.d_model, self.d_model)\n",
    "            self.output_projection = nn.Linear(self.d_model, self.d_model * self.expansion_factor)\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "            nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "            nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "            nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "        def forward(self, inputs, attention_mask=None):\n",
    "            print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "            # Apply the attention mask if provided\n",
    "            if attention_mask is not None:\n",
    "                inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "            projected_inputs = self.input_embedding(inputs)\n",
    "            print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            for conv in self.convs:\n",
    "                projected_inputs = conv(projected_inputs)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = self.swiglu(projected_inputs)\n",
    "            print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "            output = self.output_projection(projected_inputs)\n",
    "            print(\"output shape:\", output.shape)\n",
    "\n",
    "            return output\n",
    "\n",
    "    class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.vocab_size = config.vocab_size\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "            self.pos_encoder = Expert.RotaryPositionalEncoding(self.d_model)\n",
    "            self.simplified_mamba = Expert.SimplifiedMAMBA(config)\n",
    "            self.output_projection = nn.Linear(self.d_model * 2, self.vocab_size)  # Adjust if needed\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = 1.0\n",
    "\n",
    "            nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "        def forward(self, input_values, attention_mask):\n",
    "            embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "            embedded = self.pos_encoder(embedded)\n",
    "            simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "            logits = self.output_projection(simplified_mamba_output)\n",
    "            return logits\n",
    "\n",
    "\n",
    "    class SwitchRouter(nn.Module):\n",
    "        CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "        class SwitchGate(nn.Module):\n",
    "            def __init__(self, input_dim, num_experts):\n",
    "                super(Expert.SwitchRouter.SwitchGate, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "                self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "                return gate_scores\n",
    "\n",
    "        def __init__(self, config):\n",
    "            super(Expert.SwitchRouter, self).__init__()\n",
    "            self.config = config\n",
    "            self.device = config.device\n",
    "            self.router = self.SwitchGate(config.input_dim, config.num_experts).to(config.device)\n",
    "            self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "            self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.transformer_dpo = Expert.DPO(self.lmt, config.device).to(config.device)\n",
    "            self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "            self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "            self.input_embedding = nn.Linear(config.input_dim, config.input_dim)\n",
    "\n",
    "        def forward(self, x, attention_mask, context_texts, question_text):\n",
    "            x = x.to(self.device)\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.float()\n",
    "\n",
    "            x = self.input_embedding(x)\n",
    "            gate_scores = self.router(x)\n",
    "            expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "            final_output = torch.zeros_like(x)\n",
    "\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                mask = expert_indices == i\n",
    "                if mask.any():\n",
    "                    selected_inputs = x[mask]\n",
    "                    selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                    if isinstance(expert, Expert.TransformerRAG):\n",
    "                        expert_output = expert.forward(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                    else:\n",
    "                        expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                    final_output[mask] = expert_output\n",
    "\n",
    "            aux_loss = self.auxiliary_loss(gate_scores)\n",
    "            return final_output, aux_loss\n",
    "\n",
    "        def auxiliary_loss(self, gate_scores):\n",
    "            expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "            loss_balancing = torch.std(expert_load)\n",
    "            return loss_balancing\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "            capacity_factor_tensor = torch.tensor([Expert.SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "            capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "            expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "            for idx in range(len(expert_indices)):\n",
    "                selected_expert = expert_indices[idx]\n",
    "                if expert_counts[selected_expert] < capacities[0]:\n",
    "                    expert_counts[selected_expert] += 1\n",
    "                else:\n",
    "                    available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                    if len(available_experts) > 0:\n",
    "                        alternative_expert = available_experts[0]\n",
    "                        expert_indices[idx] = alternative_expert\n",
    "                        expert_counts[alternative_expert] += 1\n",
    "                    else:\n",
    "                        print(\"No available experts to reroute. Handling overflow.\")\n",
    "            return expert_indices\n",
    "    \n",
    "    class CustomDPRContextEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.CustomDPRContextEncoder, self).__init__()  \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            context_embeddings = self.embedding_layer(pooled_output)\n",
    "            return context_embeddings\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.DPRQuestionEncoder, self).__init__() \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, **kwargs):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            embeddings = self.embedding_layer(pooled_output)\n",
    "            return embeddings\n",
    "\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(Expert.TransformerRAG, self).__init__()\n",
    "            self.config = config\n",
    "            self.context_encoder = Expert.CustomDPRContextEncoder(config.embedding_dim).to(config.device)\n",
    "            self.language_model = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.question_encoder = Expert.DPRQuestionEncoder(config.embedding_dim).to(config.device)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "            \n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "                \n",
    "                aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = context['input_ids'].to(config.device)\n",
    "                    context_attention_mask = context['attention_mask'].to(config.device)\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "                \n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "            \n",
    "            question_input_ids = question_input_ids.to(config.device).long()\n",
    "            question_attention_mask = question_attention_mask.to(config.device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "            \n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=config.device))\n",
    "            \n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(config.device) for k, v in tokenized_combined_input.items()}\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            \n",
    "            return response\n",
    "\n",
    "        @staticmethod\n",
    "        def extract_text_from_pdf(file_path):\n",
    "            text = \"\"\n",
    "            with fitz.open(file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            return text\n",
    "\n",
    "        @staticmethod\n",
    "        def split_into_chunks(text, chunk_size):\n",
    "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        @staticmethod\n",
    "        def preprocess_text(text, max_length=512):\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            chunk_size = max_length - 50\n",
    "            text_chunks = Expert.TransformerRAG.split_into_chunks(text, chunk_size)\n",
    "            processed_chunks = []\n",
    "            for chunk in text_chunks:\n",
    "                tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "                processed_chunk = {\n",
    "                    'input_ids': tokenized_output['input_ids'],\n",
    "                    'attention_mask': tokenized_output['attention_mask']\n",
    "                }\n",
    "                processed_chunks.append(processed_chunk)\n",
    "            return processed_chunks\n",
    "\n",
    "        @staticmethod\n",
    "        def create_dataset_from_pdfs(pdf_file_paths):\n",
    "            dataset = []\n",
    "            for file_path in pdf_file_paths:\n",
    "                text = Expert.TransformerRAG.extract_text_from_pdf(file_path)\n",
    "                processed_text = Expert.TransformerRAG.preprocess_text(text)\n",
    "                dataset.append(processed_text)\n",
    "            return dataset\n",
    "\n",
    "        def retrieve_contexts(self, dataset, query_embedding, top_k=5):\n",
    "            similarity_scores = []\n",
    "            for context in dataset:\n",
    "                context_input_ids = context['input_ids'].to(self.config.device)\n",
    "                context_attention_mask = context['attention_mask'].to(self.config.device)\n",
    "                # Use the class's context_encoder\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "                similarity_scores.append(similarity.squeeze().item())\n",
    "            top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "            top_contexts = [dataset[i] for i in top_k_indices]\n",
    "            return top_contexts\n",
    "\n",
    "        def rag_retrieve_and_generate(self, dataset, query):\n",
    "            # Tokenize the query using the class's tokenizer\n",
    "            tokenized_query = self.tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
    "            input_ids = tokenized_query['input_ids']\n",
    "            attention_mask = tokenized_query['attention_mask']\n",
    "            # Use the class's question_encoder\n",
    "            encoded_query = self.question_encoder(input_ids, attention_mask)\n",
    "            relevant_contexts = self.retrieve_contexts(dataset, encoded_query)\n",
    "            # Assuming generate_response is a method of LanguageModelTransformer that accepts tokenized contexts and generates a response\n",
    "            response = self.language_model.generate_response(relevant_contexts)\n",
    "            return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "            super(Expert.LORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "\n",
    "            # Original weight and bias of the linear layer\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # LORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "            super(Expert.QLORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "            self.quantization_bits = quantization_bits\n",
    "\n",
    "            # Original weight and bias\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # QLORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, embed_size, heads):\n",
    "            super(Expert.MultiHeadAttention, self).__init__()\n",
    "            self.embed_size = embed_size\n",
    "            self.heads = heads\n",
    "            self.head_dim = embed_size // heads\n",
    "\n",
    "            assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "            ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        def forward(self, values, keys, query, mask):\n",
    "            N = query.shape[0]\n",
    "            value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "            # Split the embedding into self.heads different pieces\n",
    "            values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "            queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            # Einsum does the matrix multiplication for query*keys for each training example\n",
    "            # with every other training example, then sum it up\n",
    "            attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "                N, query_len, self.heads * self.head_dim\n",
    "            )\n",
    "\n",
    "            out = self.fc_out(out)\n",
    "            return out\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "            super(Expert.TransformerBlock, self).__init__()\n",
    "            self.attention = Expert.MultiHeadAttention(embed_size, heads)\n",
    "            self.norm1 = nn.LayerNorm(embed_size)\n",
    "            self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                Expert.LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, value, key, query, mask):\n",
    "            attention = self.attention(value, key, query, mask)\n",
    "            x = self.dropout(self.norm1(attention + query))\n",
    "            forward = self.feed_forward(x)\n",
    "            out = self.dropout(self.norm2(forward + x))\n",
    "            return out\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "            super(Expert.LanguageModelDecoder, self).__init__()\n",
    "            self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "            # Adding BatchNorm layers\n",
    "            self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "            self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    Expert.TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # QLORA layers\n",
    "            self.qlora_feed_forward = nn.Sequential(\n",
    "                Expert.QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "            self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "            self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, trg_mask):\n",
    "            N, seq_length = x.shape\n",
    "            if seq_length > self.position_embedding.num_embeddings:\n",
    "                raise ValueError(f\"Sequence length {seq_length} exceeds maximum allowed {self.position_embedding.num_embeddings}\")\n",
    "            positions = torch.arange(0, seq_length).expand(N, seq_length).to(config.device)\n",
    "            x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "            print(f\"Max position index: {positions.max().item()}, Position Embedding Size: {self.position_embedding.num_embeddings}\")\n",
    "\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn1(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, trg_mask)\n",
    "                if self.use_qlora:\n",
    "                    x = self.qlora_feed_forward(x)\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            out = self.fc_out(x)\n",
    "            #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "            return out\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.use_qlora = use_qlora\n",
    "\n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.vocab_size = config.vocab_size  # Use vocab_size from ExpertConfig\n",
    "            self.embed_size = config.embed_size  # Use embed_size from ExpertConfig\n",
    "            self.num_layers = config.num_layers  # Use num_layers from ExpertConfig\n",
    "            self.forward_expansion = config.forward_expansion  # Use forward_expansion from ExpertConfig\n",
    "            self.heads = config.heads  # Use heads from ExpertConfig\n",
    "            self.dropout = config.dropout  # Use dropout from ExpertConfig\n",
    "            self.max_length = config.max_length  # Use max_length from ExpertConfig\n",
    "            self.rank = config.rank  # Use rank from ExpertConfig\n",
    "            self.tokenizer_name = config.tokenizer_name  # Use tokenizer_name from ExpertConfig\n",
    "\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "            self.decoder = Expert.LanguageModelDecoder(\n",
    "                self.vocab_size,\n",
    "                self.embed_size,\n",
    "                self.num_layers,\n",
    "                self.heads,\n",
    "                self.forward_expansion,\n",
    "                self.dropout,\n",
    "                self.max_length,\n",
    "                self.rank,\n",
    "            )\n",
    "\n",
    "        def forward(self, trg, attention_mask=None):  # Remove attention_mask here since it's not used\n",
    "            print(f\"Input shape to LanguageModelTransformer: {trg.shape}\")\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            out = self.decoder(trg, trg_mask)  # Do not pass attention_mask here\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len).to(trg.device)\n",
    "            return trg_mask\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = [self.tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            return response\n",
    "\n",
    "\n",
    "    class DPO(nn.Module):\n",
    "        def __init__(self, language_model, device):\n",
    "            super(Expert.DPO, self).__init__()\n",
    "            self.language_model = language_model\n",
    "            self.device = device\n",
    "        \n",
    "        def forward(self, input_ids_question, input_ids_chosen=None, input_ids_rejected=None, labels=None, attention_mask=None):\n",
    "            # Concatenate for DPO-specific data or use regular input handling\n",
    "            combined_input_ids = torch.cat([input_ids_question, input_ids_chosen, input_ids_rejected], dim=1) if input_ids_chosen is not None and input_ids_rejected is not None else input_ids_question\n",
    "            \n",
    "            # Use the vocab_size from the language model\n",
    "            combined_input_ids = torch.clamp(combined_input_ids, 0, self.language_model.vocab_size - 1)\n",
    "            \n",
    "            # Pass through the language model\n",
    "            output = self.language_model(combined_input_ids) #, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate loss if labels are provided\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                logits = output.logits if hasattr(output, 'logits') else output\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.float().view(-1))\n",
    "            \n",
    "            return output, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, config: ExpertConfig):\n",
    "        super().__init__()\n",
    "        # Validate configuration\n",
    "        config.validate()\n",
    "\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(\n",
    "            config.seq_len, \n",
    "            config.head_dim, \n",
    "            config.block_size, \n",
    "            config.sparsity_factor\n",
    "        )\n",
    "\n",
    "        # Inside the Expert class definition\n",
    "        self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "\n",
    "        self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "\n",
    "        # Corrected instantiation of SimplifiedLanguageModelMAMBA\n",
    "        self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "\n",
    "        # Now initialize DPO with the language model transformer\n",
    "        self.transformer_dpo = Expert.DPO(self.lmt, config.device).to(config.device)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(config.input_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = Expert.SwitchRouter(config)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = Expert.QLORALayer(config.input_dim, config.input_dim, config.rank)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, aux_loss = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "        \"\"\"\n",
    "        Calculate a new alpha value based on the current loss.\n",
    "        \"\"\"\n",
    "        if current_loss >= initial_loss:\n",
    "            return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "        loss_ratio = current_loss / initial_loss\n",
    "        alpha_range = initial_alpha - final_alpha\n",
    "        new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "        return new_alpha  \n",
    "    \n",
    "    def train_dpo(self, train_loader, optimizer, label_column, input_columns, config, save_path):\n",
    "        self.transformer_dpo.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):  # Include i here to use it in your print statements\n",
    "            print(f\"Batch {i+1}:\")\n",
    "            input_ids_question = batch['input_ids_question'].to(config.device)\n",
    "            input_ids_chosen = batch['input_ids_chosen'].to(config.device)\n",
    "            input_ids_rejected = batch['input_ids_rejected'].to(config.device)\n",
    "            labels = batch['labels'].to(config.device)\n",
    "\n",
    "            # Ensure none of the input ids exceed the vocab size\n",
    "            assert input_ids_question.max() < config.vocab_size, \"Question IDs exceed vocab size\"\n",
    "            assert input_ids_chosen.max() < config.vocab_size, \"Chosen IDs exceed vocab size\"\n",
    "            assert input_ids_rejected.max() < config.vocab_size, \"Rejected IDs exceed vocab size\"\n",
    "\n",
    "            # Print maximum index for each part of the input\n",
    "            print(\"Max index in input_ids_question:\", input_ids_question.max().item())\n",
    "            print(\"Max index in input_ids_chosen:\", input_ids_chosen.max().item())\n",
    "            print(\"Max index in input_ids_rejected:\", input_ids_rejected.max().item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Combine the input ids and ensure they do not exceed max_seq_length\n",
    "            combined_input_ids = torch.cat([input_ids_question, input_ids_chosen, input_ids_rejected], dim=1)\n",
    "            max_seq_length = config.max_length  # Ensure this uses the correct attribute from your config\n",
    "            if combined_input_ids.size(1) > max_seq_length:\n",
    "                combined_input_ids = combined_input_ids[:, :max_seq_length]          \n",
    "            print(f\"Max index in combined_input_ids: {combined_input_ids.max().item()}\")  # Add this debug line\n",
    "\n",
    "            # Forward pass\n",
    "            logits, loss = self.transformer_dpo(combined_input_ids, labels)  # Use combined_input_ids here\n",
    "\n",
    "            # logits, loss = self.transformer_dpo(input_ids_question, input_ids_chosen, input_ids_rejected, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(self.transformer_dpo.state_dict(), save_path)\n",
    "        return average_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_language_model_rag(self, model, save_path, train_loader, device, num_epochs=5, lr=1e-8, weight_decay=1e-4):\n",
    "        # Enable QLORA during training\n",
    "        model.decoder.toggle_qlora(True)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "\n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                targets = batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        # Toggle QLORA off after training\n",
    "        model.decoder.toggle_qlora(False)\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"Training Complete\")\n",
    "        return model, average_loss\n",
    "\n",
    "    def train_dpr_encoders(self, train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs , context_save_path, question_save_path):\n",
    "        loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(train_data[\"queries\"])):\n",
    "                query = train_data[\"queries\"][i]\n",
    "                context = train_data[\"contexts\"][i]\n",
    "\n",
    "                # Ensure query is a string\n",
    "                if not isinstance(query, str):\n",
    "                    raise ValueError(\"Query must be a string.\")\n",
    "                tokenized_query = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                # Ensure context is a string\n",
    "                if isinstance(context, dict):\n",
    "                    # If context is a dictionary, extract the text content. This is a placeholder and might need adjustment.\n",
    "                    context_text = context.get(\"text\", \"\")\n",
    "                elif isinstance(context, str):\n",
    "                    context_text = context\n",
    "                else:\n",
    "                    raise ValueError(\"Context must be a string or a dictionary containing a text field.\")\n",
    "                tokenized_context = tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                question_embeddings = question_encoder(input_ids=tokenized_query['input_ids'], attention_mask=tokenized_query['attention_mask'])\n",
    "                context_embeddings = context_encoder(input_ids=tokenized_context['input_ids'], attention_mask=tokenized_context['attention_mask'])\n",
    "\n",
    "                # The labels tensor should have the same first dimension size as the input tensors\n",
    "                labels = torch.tensor([1.0] * question_embeddings.size(0), dtype=torch.float).to(question_embeddings.device)\n",
    "\n",
    "                loss = loss_function(question_embeddings, context_embeddings, labels)\n",
    "\n",
    "                optimizer_context.zero_grad()\n",
    "                optimizer_question.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_context.step()\n",
    "                optimizer_question.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_data['queries'])}\")\n",
    "        average_loss = total_loss / len(train_data['queries'])\n",
    "        torch.save(context_encoder.state_dict(), context_save_path)\n",
    "        torch.save(question_encoder.state_dict(), question_save_path)\n",
    "        return (context_encoder, question_encoder), average_loss\n",
    "       \n",
    "    def train_language_model_transformer(self, train_loader, device, vocab_size, save_path):\n",
    "        model = self.lmt\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "        num_epochs = 5\n",
    "        \n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            model.decoder.toggle_qlora(True)\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs, targets = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        return model, average_loss\n",
    "\n",
    "\n",
    "    def train_expert(self, train_loader, train_data, optimizer, main_loss_function, aux_loss_weight, device,save_path, accumulation_steps=4, num_epochs=5):\n",
    "            self.train()  # Set the model to training mode\n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0\n",
    "                optimizer.zero_grad()  # Initialize gradients to zero\n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "                    inputs, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "\n",
    "                    # Calculate start and end indices for current batch in train_data\n",
    "                    start_idx = batch_idx * batch['input_ids'].size(0)\n",
    "                    end_idx = start_idx + batch['input_ids'].size(0)\n",
    "\n",
    "                    # Extract current_queries and current_contexts for the batch\n",
    "                    current_queries = train_data['queries'][start_idx:end_idx]\n",
    "                    current_contexts = train_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "                    # Call to the model forward function\n",
    "                    outputs, aux_loss = self(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "                    # Calculate loss and accumulate\n",
    "                    main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                    loss = (main_loss + aux_loss_weight * aux_loss) / accumulation_steps\n",
    "                    loss.backward()\n",
    "\n",
    "                    if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    total_loss += loss.item() * accumulation_steps  # Scale back up\n",
    "\n",
    "                average_loss = total_loss / len(train_loader)\n",
    "                print(f'End of Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            torch.save(self.state_dict(), save_path)\n",
    "            return self, average_loss\n",
    "\n",
    "\n",
    "##################################\n",
    "# Training transformer_with_dpo\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def preprocess_dpo_data(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    vocab_size = len(tokenizer.vocab)  # Make sure this matches your embedding layer's vocab size\n",
    "\n",
    "    # Define max sequence length\n",
    "    max_seq_length = 512\n",
    "\n",
    "    # Tokenize 'question', 'chosen', and 'rejected' fields\n",
    "    tokenized_questions = tokenizer(examples['question'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_chosen = tokenizer(examples['chosen'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_rejected = tokenizer(examples['rejected'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "\n",
    "    # Prepare the labels: 1 for 'chosen' and 0 for 'rejected'\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(len(examples['question']))]\n",
    "\n",
    "    return {\n",
    "        'input_ids_question': tokenized_questions['input_ids'],\n",
    "        'attention_mask_question': tokenized_questions['attention_mask'],\n",
    "        'input_ids_chosen': tokenized_chosen['input_ids'],\n",
    "        'attention_mask_chosen': tokenized_chosen['attention_mask'],\n",
    "        'input_ids_rejected': tokenized_rejected['input_ids'],\n",
    "        'attention_mask_rejected': tokenized_rejected['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Load the DPO dataset from Hugging Face\n",
    "dpo_dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "\n",
    "# Apply the preprocessing to the dataset\n",
    "dpo_dataset = dpo_dataset.map(preprocess_dpo_data, batched=True)\n",
    "\n",
    "\n",
    "# You can convert to PyTorch tensors after processing\n",
    "dpo_dataset.set_format(type='torch', columns=['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(dpo_dataset['train'], batch_size=2, shuffle=True)\n",
    "\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "expert_model = Expert(config)\n",
    "\n",
    "model_vocab_size = expert_model.lmt.decoder.word_embedding.num_embeddings\n",
    "print(f\"Model vocab size: {model_vocab_size}\")\n",
    "print(f\"Model embedding size: {expert_model.lmt.decoder.word_embedding.embedding_dim}\")\n",
    "print(f\"Configured max length: {config.max_length}\")\n",
    "\n",
    "optimizer = AdamW(expert_model.parameters(), lr=1e-5)\n",
    "save_path = 'D:/EXPERT_WEIGHTS/dpo_model.pth'\n",
    "\n",
    "# Train the DPO model\n",
    "label_column = 'labels'\n",
    "input_columns = ['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
    "avg_loss = expert_model.train_dpo(train_loader, optimizer, label_column, input_columns, config, save_path)\n",
    "\n",
    "# Save the model\n",
    "torch.save(expert_model.transformer_dpo.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model vocab size: 30522\n",
      "Model embedding size: 256\n",
      "Configured max length: 512\n",
      "Batch 1:\n",
      "Input shape to LanguageModelTransformer: torch.Size([2, 510])\n",
      "Max position index: 509, Position Embedding Size: 512\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([2])) must be the same as input size (torch.Size([31132440]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1237\u001b[0m\n\u001b[0;32m   1235\u001b[0m label_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1236\u001b[0m input_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 1237\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mexpert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(expert_model\u001b[38;5;241m.\u001b[39mtransformer_dpo\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n",
      "Cell \u001b[1;32mIn[37], line 952\u001b[0m, in \u001b[0;36mExpert.train_dpo\u001b[1;34m(self, train_loader, optimizer, label_column, input_columns, config, save_path)\u001b[0m\n\u001b[0;32m    949\u001b[0m combined_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([input_ids_question, input_ids_chosen, input_ids_rejected], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 952\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_dpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_chosen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_rejected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[37], line 855\u001b[0m, in \u001b[0;36mExpert.DPO.forward\u001b[1;34m(self, input_ids_question, input_ids_chosen, input_ids_rejected, labels)\u001b[0m\n\u001b[0;32m    853\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust based on your model's output and labels shape\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m--> 855\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\functional.py:3193\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3190\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m-> 3193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([2])) must be the same as input size (torch.Size([31132440]))"
     ]
    }
   ],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "class ExpertConfig:\n",
    "    def __init__(self, seq_len=512, head_dim=64, block_size=64, sparsity_factor=4,\n",
    "                 input_dim=512, num_experts=3, vocab_size=30522, embed_size=256,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0.1,\n",
    "                 max_length=512,  # Updated to match seq_len for consistency\n",
    "                 rank=16, device='cpu',\n",
    "                 mamba_model_path='D:\\\\EXPERT_WEIGHTS\\\\mamba_model_weights.pth',\n",
    "                 context_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\context_encoder.pth',\n",
    "                 language_model_path='D:\\\\EXPERT_WEIGHTS\\\\language_model.pth',\n",
    "                 question_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\question_encoder.pth',\n",
    "                 dpo_model_path='D:\\\\EXPERT_WEIGHTS\\\\dpo_model_weights.pth',\n",
    "                 model_name='bert-base-uncased', embedding_dim=768,\n",
    "                 alpha=1, quantization_bits=8, tokenizer_name='bert-base-uncased',\n",
    "                 d_model=512, d_state=2048, d_conv=3, expansion_factor=2):\n",
    "\n",
    "        # Common hyperparameters\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        self.input_dim = input_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length  # Ensure this is properly reflected in model components\n",
    "        self.rank = rank\n",
    "\n",
    "        # Model paths and device\n",
    "        self.mamba_model_path = mamba_model_path\n",
    "        self.context_encoder_path = context_encoder_path\n",
    "        self.language_model_path = language_model_path\n",
    "        self.question_encoder_path = question_encoder_path\n",
    "        self.dpo_model_path = dpo_model_path\n",
    "        self.device = device\n",
    "\n",
    "        # Unique hyperparameters\n",
    "        self.num_experts = num_experts\n",
    "        self.model_name = model_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        # PDFs (unchanged)\n",
    "        self.pdf_file_paths = [\n",
    "            r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "            r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf'\n",
    "        ]\n",
    "        \n",
    "        # Preserving original dataset loading functionality\n",
    "        self.rag_dataset = Expert.TransformerRAG.create_dataset_from_pdfs(self.pdf_file_paths)\n",
    "\n",
    "    def validate(self):\n",
    "        assert self.seq_len % self.block_size == 0, \"seq_len must be divisible by block_size\"\n",
    "        assert self.max_length >= self.seq_len, \"max_length should be equal to or greater than seq_len\"\n",
    "\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    # Load model weights function\n",
    "    def load_model_weights(model, model_path):\n",
    "        #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        checkpoint = torch.load(model_path, map_location=config.device)\n",
    "\n",
    "        if isinstance(checkpoint, dict):\n",
    "            # Check for 'state_dict' or 'model_state_dict' keys\n",
    "            if 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            elif 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                # If no known key is found, try loading it as a raw state dictionary\n",
    "                try:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                except RuntimeError as e:\n",
    "                    raise ValueError(f\"Error loading state dict: {e}\")\n",
    "        elif isinstance(checkpoint, nn.Module):\n",
    "            # If the checkpoint is a model object, assign it directly\n",
    "            model = checkpoint\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    # Flash2_Attention\n",
    "    class FlashAttention2(nn.Module):\n",
    "        def __init__(self, sequence_length, head_dimension, block_size):\n",
    "            super(Expert.FlashAttention2, self).__init__()\n",
    "            self.block_size = block_size\n",
    "            # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "            assert sequence_length % block_size == 0\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            # Partitioning of inputs\n",
    "            Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "            # Efficient computation of the attention mechanism\n",
    "            outputs = []\n",
    "            for i, Q_block in enumerate(Q_blocks):\n",
    "                output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "                outputs.append(output_block)\n",
    "\n",
    "            # Concatenating the processed blocks\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "            return output\n",
    "\n",
    "        def partition_inputs(self, Q, K, V):\n",
    "            # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "            Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "            K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "            V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "            return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "        def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "            # Process each block efficiently as per FLASH2's optimized method\n",
    "            # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "            output_blocks = []\n",
    "            for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "                attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "                attention_scores = self.online_softmax(attention_scores)\n",
    "                output_block = torch.matmul(attention_scores, V_block)\n",
    "                output_blocks.append(output_block)\n",
    "\n",
    "            # Summing up the results from each block\n",
    "            output_block_sum = sum(output_blocks)\n",
    "            return output_block_sum\n",
    "\n",
    "        def online_softmax(self, scores, chunk_size=128):\n",
    "            # Apply softmax in chunks for large sequences\n",
    "            softmaxed_scores = []\n",
    "            for i in range(0, scores.size(0), chunk_size):\n",
    "                chunk = scores[i:i + chunk_size, :]\n",
    "                softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "                softmaxed_scores.append(softmaxed_chunk)\n",
    "            return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "    # SparseFlash2_Attention\n",
    "    class SparseFlash2Attention(nn.Module):\n",
    "        def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "            super().__init__()\n",
    "            self.flash_attention = Expert.FlashAttention2(seq_len, head_dim, blk_size)\n",
    "            self.seq_len = seq_len\n",
    "            self.head_dim = head_dim\n",
    "            self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "            self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        def generate_sparsity_mask(self):\n",
    "            mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "            step = self.sparsity_factor\n",
    "            for i in range(0, self.seq_len, step):\n",
    "                mask[i:i + step, :] = 1\n",
    "            return mask.bool()\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "            # Reshape output to be 3D for batch matrix multiplication\n",
    "            output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "            sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "            # Apply the sparsity mask to the output\n",
    "            sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "            output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "            # Reshape the output back to 2D\n",
    "            output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "            return output\n",
    "\n",
    "\n",
    "    # MAMBA\n",
    "    # RoPE\n",
    "    class RotaryPositionalEncoding(nn.Module):\n",
    "        def __init__(self, dim, max_len=5000):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "            t = torch.arange(max_len).type_as(inv_freq)\n",
    "            freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "            self.register_buffer('sin', freqs.sin())\n",
    "            self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "        def forward(self, x):\n",
    "            n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "            sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "            # Apply RoPE to even and odd indices separately\n",
    "            x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "    # SWIGLU\n",
    "    class SwiGLU(nn.Module):\n",
    "        def __init__(self, dim_in, dim_out):\n",
    "            super(Expert.SwiGLU, self).__init__()\n",
    "            self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "            self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "        def forward(self, x):\n",
    "            gate = torch.sigmoid(self.fc2(x))\n",
    "            return self.fc1(x) * gate\n",
    "\n",
    "    class SimplifiedMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "            \n",
    "            self.feedforward = nn.Sequential(\n",
    "                nn.Linear(self.d_model, self.d_state),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(self.d_state, self.d_model)\n",
    "            )\n",
    "            self.input_embedding = nn.Linear(self.d_model, self.d_model)\n",
    "            self.convs = nn.Sequential(*[nn.Conv1d(self.d_model, self.d_model, kernel_size=self.d_conv, padding=(self.d_conv // 2)) for _ in range(self.num_layers)])\n",
    "            self.swiglu = Expert.SwiGLU(self.d_model, self.d_model)\n",
    "            self.output_projection = nn.Linear(self.d_model, self.d_model * self.expansion_factor)\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "            nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "            nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "            nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "        def forward(self, inputs, attention_mask=None):\n",
    "            print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "            # Apply the attention mask if provided\n",
    "            if attention_mask is not None:\n",
    "                inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "            projected_inputs = self.input_embedding(inputs)\n",
    "            print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            for conv in self.convs:\n",
    "                projected_inputs = conv(projected_inputs)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = self.swiglu(projected_inputs)\n",
    "            print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "            output = self.output_projection(projected_inputs)\n",
    "            print(\"output shape:\", output.shape)\n",
    "\n",
    "            return output\n",
    "\n",
    "    class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.vocab_size = config.vocab_size\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "            self.pos_encoder = Expert.RotaryPositionalEncoding(self.d_model)\n",
    "            self.simplified_mamba = Expert.SimplifiedMAMBA(config)\n",
    "            self.output_projection = nn.Linear(self.d_model * 2, self.vocab_size)  # Adjust if needed\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = 1.0\n",
    "\n",
    "            nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "        def forward(self, input_values, attention_mask):\n",
    "            embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "            embedded = self.pos_encoder(embedded)\n",
    "            simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "            logits = self.output_projection(simplified_mamba_output)\n",
    "            return logits\n",
    "\n",
    "\n",
    "    class SwitchRouter(nn.Module):\n",
    "        CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "        class SwitchGate(nn.Module):\n",
    "            def __init__(self, input_dim, num_experts):\n",
    "                super(Expert.SwitchRouter.SwitchGate, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "                self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "                return gate_scores\n",
    "\n",
    "        def __init__(self, config):\n",
    "            super(Expert.SwitchRouter, self).__init__()\n",
    "            self.config = config\n",
    "            self.device = config.device\n",
    "            self.router = self.SwitchGate(config.input_dim, config.num_experts).to(config.device)\n",
    "            self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "            self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.transformer_dpo = Expert.DPO(self.lmt, config.device).to(config.device)\n",
    "            self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "            self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "            self.input_embedding = nn.Linear(config.input_dim, config.input_dim)\n",
    "\n",
    "        def forward(self, x, attention_mask, context_texts, question_text):\n",
    "            x = x.to(self.device)\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.float()\n",
    "\n",
    "            x = self.input_embedding(x)\n",
    "            gate_scores = self.router(x)\n",
    "            expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "            final_output = torch.zeros_like(x)\n",
    "\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                mask = expert_indices == i\n",
    "                if mask.any():\n",
    "                    selected_inputs = x[mask]\n",
    "                    selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                    if isinstance(expert, Expert.TransformerRAG):\n",
    "                        expert_output = expert.forward(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                    else:\n",
    "                        expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                    final_output[mask] = expert_output\n",
    "\n",
    "            aux_loss = self.auxiliary_loss(gate_scores)\n",
    "            return final_output, aux_loss\n",
    "\n",
    "        def auxiliary_loss(self, gate_scores):\n",
    "            expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "            loss_balancing = torch.std(expert_load)\n",
    "            return loss_balancing\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "            capacity_factor_tensor = torch.tensor([Expert.SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "            capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "            expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "            for idx in range(len(expert_indices)):\n",
    "                selected_expert = expert_indices[idx]\n",
    "                if expert_counts[selected_expert] < capacities[0]:\n",
    "                    expert_counts[selected_expert] += 1\n",
    "                else:\n",
    "                    available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                    if len(available_experts) > 0:\n",
    "                        alternative_expert = available_experts[0]\n",
    "                        expert_indices[idx] = alternative_expert\n",
    "                        expert_counts[alternative_expert] += 1\n",
    "                    else:\n",
    "                        print(\"No available experts to reroute. Handling overflow.\")\n",
    "            return expert_indices\n",
    "    \n",
    "    class CustomDPRContextEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.CustomDPRContextEncoder, self).__init__()  \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            context_embeddings = self.embedding_layer(pooled_output)\n",
    "            return context_embeddings\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.DPRQuestionEncoder, self).__init__() \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, **kwargs):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            embeddings = self.embedding_layer(pooled_output)\n",
    "            return embeddings\n",
    "\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(Expert.TransformerRAG, self).__init__()\n",
    "            self.config = config\n",
    "            self.context_encoder = Expert.CustomDPRContextEncoder(config.embedding_dim).to(config.device)\n",
    "            self.language_model = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.question_encoder = Expert.DPRQuestionEncoder(config.embedding_dim).to(config.device)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "            \n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "                \n",
    "                aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = context['input_ids'].to(config.device)\n",
    "                    context_attention_mask = context['attention_mask'].to(config.device)\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "                \n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "            \n",
    "            question_input_ids = question_input_ids.to(config.device).long()\n",
    "            question_attention_mask = question_attention_mask.to(config.device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "            \n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=config.device))\n",
    "            \n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(config.device) for k, v in tokenized_combined_input.items()}\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            \n",
    "            return response\n",
    "\n",
    "        @staticmethod\n",
    "        def extract_text_from_pdf(file_path):\n",
    "            text = \"\"\n",
    "            with fitz.open(file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            return text\n",
    "\n",
    "        @staticmethod\n",
    "        def split_into_chunks(text, chunk_size):\n",
    "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        @staticmethod\n",
    "        def preprocess_text(text, max_length=512):\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            chunk_size = max_length - 50\n",
    "            text_chunks = Expert.TransformerRAG.split_into_chunks(text, chunk_size)\n",
    "            processed_chunks = []\n",
    "            for chunk in text_chunks:\n",
    "                tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "                processed_chunk = {\n",
    "                    'input_ids': tokenized_output['input_ids'],\n",
    "                    'attention_mask': tokenized_output['attention_mask']\n",
    "                }\n",
    "                processed_chunks.append(processed_chunk)\n",
    "            return processed_chunks\n",
    "\n",
    "        @staticmethod\n",
    "        def create_dataset_from_pdfs(pdf_file_paths):\n",
    "            dataset = []\n",
    "            for file_path in pdf_file_paths:\n",
    "                text = Expert.TransformerRAG.extract_text_from_pdf(file_path)\n",
    "                processed_text = Expert.TransformerRAG.preprocess_text(text)\n",
    "                dataset.append(processed_text)\n",
    "            return dataset\n",
    "\n",
    "        def retrieve_contexts(self, dataset, query_embedding, top_k=5):\n",
    "            similarity_scores = []\n",
    "            for context in dataset:\n",
    "                context_input_ids = context['input_ids'].to(self.config.device)\n",
    "                context_attention_mask = context['attention_mask'].to(self.config.device)\n",
    "                # Use the class's context_encoder\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "                similarity_scores.append(similarity.squeeze().item())\n",
    "            top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "            top_contexts = [dataset[i] for i in top_k_indices]\n",
    "            return top_contexts\n",
    "\n",
    "        def rag_retrieve_and_generate(self, dataset, query):\n",
    "            # Tokenize the query using the class's tokenizer\n",
    "            tokenized_query = self.tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
    "            input_ids = tokenized_query['input_ids']\n",
    "            attention_mask = tokenized_query['attention_mask']\n",
    "            # Use the class's question_encoder\n",
    "            encoded_query = self.question_encoder(input_ids, attention_mask)\n",
    "            relevant_contexts = self.retrieve_contexts(dataset, encoded_query)\n",
    "            # Assuming generate_response is a method of LanguageModelTransformer that accepts tokenized contexts and generates a response\n",
    "            response = self.language_model.generate_response(relevant_contexts)\n",
    "            return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "            super(Expert.LORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "\n",
    "            # Original weight and bias of the linear layer\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # LORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "            super(Expert.QLORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "            self.quantization_bits = quantization_bits\n",
    "\n",
    "            # Original weight and bias\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # QLORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, embed_size, heads):\n",
    "            super(Expert.MultiHeadAttention, self).__init__()\n",
    "            self.embed_size = embed_size\n",
    "            self.heads = heads\n",
    "            self.head_dim = embed_size // heads\n",
    "\n",
    "            assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "            ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        def forward(self, values, keys, query, mask):\n",
    "            N = query.shape[0]\n",
    "            value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "            # Split the embedding into self.heads different pieces\n",
    "            values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "            queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            # Einsum does the matrix multiplication for query*keys for each training example\n",
    "            # with every other training example, then sum it up\n",
    "            attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "                N, query_len, self.heads * self.head_dim\n",
    "            )\n",
    "\n",
    "            out = self.fc_out(out)\n",
    "            return out\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "            super(Expert.TransformerBlock, self).__init__()\n",
    "            self.attention = Expert.MultiHeadAttention(embed_size, heads)\n",
    "            self.norm1 = nn.LayerNorm(embed_size)\n",
    "            self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                Expert.LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, value, key, query, mask):\n",
    "            attention = self.attention(value, key, query, mask)\n",
    "            x = self.dropout(self.norm1(attention + query))\n",
    "            forward = self.feed_forward(x)\n",
    "            out = self.dropout(self.norm2(forward + x))\n",
    "            return out\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "            super(Expert.LanguageModelDecoder, self).__init__()\n",
    "            self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "            # Adding BatchNorm layers\n",
    "            self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "            self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    Expert.TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # QLORA layers\n",
    "            self.qlora_feed_forward = nn.Sequential(\n",
    "                Expert.QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "            self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "            self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, trg_mask):\n",
    "            N, seq_length = x.shape\n",
    "            if seq_length > self.position_embedding.num_embeddings:\n",
    "                raise ValueError(f\"Sequence length {seq_length} exceeds maximum allowed {self.position_embedding.num_embeddings}\")\n",
    "            positions = torch.arange(0, seq_length).expand(N, seq_length).to(config.device)\n",
    "            x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "            print(f\"Max position index: {positions.max().item()}, Position Embedding Size: {self.position_embedding.num_embeddings}\")\n",
    "\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn1(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, trg_mask)\n",
    "                if self.use_qlora:\n",
    "                    x = self.qlora_feed_forward(x)\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            out = self.fc_out(x)\n",
    "            #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "            return out\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.use_qlora = use_qlora\n",
    "\n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.vocab_size = config.vocab_size  # Use vocab_size from ExpertConfig\n",
    "            self.embed_size = config.embed_size  # Use embed_size from ExpertConfig\n",
    "            self.num_layers = config.num_layers  # Use num_layers from ExpertConfig\n",
    "            self.forward_expansion = config.forward_expansion  # Use forward_expansion from ExpertConfig\n",
    "            self.heads = config.heads  # Use heads from ExpertConfig\n",
    "            self.dropout = config.dropout  # Use dropout from ExpertConfig\n",
    "            self.max_length = config.max_length  # Use max_length from ExpertConfig\n",
    "            self.rank = config.rank  # Use rank from ExpertConfig\n",
    "            self.tokenizer_name = config.tokenizer_name  # Use tokenizer_name from ExpertConfig\n",
    "\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "            self.decoder = Expert.LanguageModelDecoder(\n",
    "                self.vocab_size,\n",
    "                self.embed_size,\n",
    "                self.num_layers,\n",
    "                self.heads,\n",
    "                self.forward_expansion,\n",
    "                self.dropout,\n",
    "                self.max_length,\n",
    "                self.rank,\n",
    "            )\n",
    "\n",
    "        def forward(self, trg, attention_mask=None):  # Remove attention_mask here since it's not used\n",
    "            print(f\"Input shape to LanguageModelTransformer: {trg.shape}\")\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            out = self.decoder(trg, trg_mask)  # Do not pass attention_mask here\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len).to(trg.device)\n",
    "            return trg_mask\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = [self.tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            return response\n",
    "\n",
    "\n",
    "    class DPO(nn.Module):\n",
    "        def __init__(self, language_model, device):\n",
    "            super(Expert.DPO, self).__init__()\n",
    "            self.language_model = language_model\n",
    "            self.device = device\n",
    "        \n",
    "        def forward(self, input_ids_question, input_ids_chosen=None, input_ids_rejected=None, labels=None):\n",
    "            # Concatenate for DPO-specific data or use regular input handling\n",
    "            combined_input_ids = torch.cat([input_ids_question, input_ids_chosen, input_ids_rejected], dim=1) if input_ids_chosen is not None and input_ids_rejected is not None else input_ids_question\n",
    "            \n",
    "            # Ensure the inputs do not exceed the language model's vocab size\n",
    "            combined_input_ids = torch.clamp(combined_input_ids, 0, self.language_model.vocab_size - 1)\n",
    "            \n",
    "            # Pass through the language model\n",
    "            logits = self.language_model(combined_input_ids)  # Adjust based on your model's output\n",
    "            \n",
    "            # Calculate loss if labels are provided\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                # Adjust the shape of logits if necessary to match labels\n",
    "                logits = logits.view(-1)  # Adjust based on your model's output and labels shape\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels.float().view(-1))\n",
    "            \n",
    "            return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, config: ExpertConfig):\n",
    "        super().__init__()\n",
    "        # Validate configuration\n",
    "        config.validate()\n",
    "\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(\n",
    "            config.seq_len, \n",
    "            config.head_dim, \n",
    "            config.block_size, \n",
    "            config.sparsity_factor\n",
    "        )\n",
    "\n",
    "        # Inside the Expert class definition\n",
    "        self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "\n",
    "        self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "\n",
    "        # Corrected instantiation of SimplifiedLanguageModelMAMBA\n",
    "        self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "\n",
    "        # Now initialize DPO with the language model transformer\n",
    "        self.transformer_dpo = Expert.DPO(self.lmt, config.device).to(config.device)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(config.input_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = Expert.SwitchRouter(config)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = Expert.QLORALayer(config.input_dim, config.input_dim, config.rank)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, aux_loss = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "        \"\"\"\n",
    "        Calculate a new alpha value based on the current loss.\n",
    "        \"\"\"\n",
    "        if current_loss >= initial_loss:\n",
    "            return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "        loss_ratio = current_loss / initial_loss\n",
    "        alpha_range = initial_alpha - final_alpha\n",
    "        new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "        return new_alpha  \n",
    "    \n",
    "    def train_dpo(self, train_loader, optimizer, label_column, input_columns, config, save_path):\n",
    "        self.transformer_dpo.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            print(f\"Batch {i+1}:\")\n",
    "\n",
    "            input_ids_question = batch['input_ids_question'].to(config.device)\n",
    "            input_ids_chosen = batch['input_ids_chosen'].to(config.device)\n",
    "            input_ids_rejected = batch['input_ids_rejected'].to(config.device)\n",
    "            \n",
    "            # Adjust labels here to ensure they're in the correct shape\n",
    "            labels = batch[label_column].view(-1).to(config.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Trim inputs before concatenation\n",
    "            max_length_per_input = config.max_length // 3  # Divide max_length by the number of inputs\n",
    "            input_ids_question = input_ids_question[:, :max_length_per_input]\n",
    "            input_ids_chosen = input_ids_chosen[:, :max_length_per_input]\n",
    "            input_ids_rejected = input_ids_rejected[:, :max_length_per_input]\n",
    "\n",
    "            # Combine the input ids\n",
    "            combined_input_ids = torch.cat([input_ids_question, input_ids_chosen, input_ids_rejected], dim=1)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, loss = self.transformer_dpo(input_ids_question, input_ids_chosen, input_ids_rejected, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(self.transformer_dpo.state_dict(), save_path)\n",
    "        return average_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_language_model_rag(self, model, save_path, train_loader, device, num_epochs=5, lr=1e-8, weight_decay=1e-4):\n",
    "        # Enable QLORA during training\n",
    "        model.decoder.toggle_qlora(True)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "\n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                targets = batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        # Toggle QLORA off after training\n",
    "        model.decoder.toggle_qlora(False)\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"Training Complete\")\n",
    "        return model, average_loss\n",
    "\n",
    "    def train_dpr_encoders(self, train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs , context_save_path, question_save_path):\n",
    "        loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(train_data[\"queries\"])):\n",
    "                query = train_data[\"queries\"][i]\n",
    "                context = train_data[\"contexts\"][i]\n",
    "\n",
    "                # Ensure query is a string\n",
    "                if not isinstance(query, str):\n",
    "                    raise ValueError(\"Query must be a string.\")\n",
    "                tokenized_query = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                # Ensure context is a string\n",
    "                if isinstance(context, dict):\n",
    "                    # If context is a dictionary, extract the text content. This is a placeholder and might need adjustment.\n",
    "                    context_text = context.get(\"text\", \"\")\n",
    "                elif isinstance(context, str):\n",
    "                    context_text = context\n",
    "                else:\n",
    "                    raise ValueError(\"Context must be a string or a dictionary containing a text field.\")\n",
    "                tokenized_context = tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                question_embeddings = question_encoder(input_ids=tokenized_query['input_ids'], attention_mask=tokenized_query['attention_mask'])\n",
    "                context_embeddings = context_encoder(input_ids=tokenized_context['input_ids'], attention_mask=tokenized_context['attention_mask'])\n",
    "\n",
    "                # The labels tensor should have the same first dimension size as the input tensors\n",
    "                labels = torch.tensor([1.0] * question_embeddings.size(0), dtype=torch.float).to(question_embeddings.device)\n",
    "\n",
    "                loss = loss_function(question_embeddings, context_embeddings, labels)\n",
    "\n",
    "                optimizer_context.zero_grad()\n",
    "                optimizer_question.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_context.step()\n",
    "                optimizer_question.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_data['queries'])}\")\n",
    "        average_loss = total_loss / len(train_data['queries'])\n",
    "        torch.save(context_encoder.state_dict(), context_save_path)\n",
    "        torch.save(question_encoder.state_dict(), question_save_path)\n",
    "        return (context_encoder, question_encoder), average_loss\n",
    "       \n",
    "    def train_language_model_transformer(self, train_loader, device, vocab_size, save_path):\n",
    "        model = self.lmt\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "        num_epochs = 5\n",
    "        \n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            model.decoder.toggle_qlora(True)\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs, targets = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        return model, average_loss\n",
    "\n",
    "\n",
    "    def train_expert(self, train_loader, train_data, optimizer, main_loss_function, aux_loss_weight, device,save_path, accumulation_steps=4, num_epochs=5):\n",
    "            self.train()  # Set the model to training mode\n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0\n",
    "                optimizer.zero_grad()  # Initialize gradients to zero\n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "                    inputs, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "\n",
    "                    # Calculate start and end indices for current batch in train_data\n",
    "                    start_idx = batch_idx * batch['input_ids'].size(0)\n",
    "                    end_idx = start_idx + batch['input_ids'].size(0)\n",
    "\n",
    "                    # Extract current_queries and current_contexts for the batch\n",
    "                    current_queries = train_data['queries'][start_idx:end_idx]\n",
    "                    current_contexts = train_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "                    # Call to the model forward function\n",
    "                    outputs, aux_loss = self(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "                    # Calculate loss and accumulate\n",
    "                    main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                    loss = (main_loss + aux_loss_weight * aux_loss) / accumulation_steps\n",
    "                    loss.backward()\n",
    "\n",
    "                    if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    total_loss += loss.item() * accumulation_steps  # Scale back up\n",
    "\n",
    "                average_loss = total_loss / len(train_loader)\n",
    "                print(f'End of Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            torch.save(self.state_dict(), save_path)\n",
    "            return self, average_loss\n",
    "\n",
    "\n",
    "##################################\n",
    "# Training transformer_with_dpo\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def preprocess_dpo_data(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    vocab_size = len(tokenizer.vocab)  # Make sure this matches your embedding layer's vocab size\n",
    "\n",
    "    # Define max sequence length\n",
    "    max_seq_length = 512\n",
    "\n",
    "    # Tokenize 'question', 'chosen', and 'rejected' fields\n",
    "    tokenized_questions = tokenizer(examples['question'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_chosen = tokenizer(examples['chosen'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_rejected = tokenizer(examples['rejected'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "\n",
    "    # Prepare the labels: 1 for 'chosen' and 0 for 'rejected'\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(len(examples['question']))]\n",
    "\n",
    "    return {\n",
    "        'input_ids_question': tokenized_questions['input_ids'],\n",
    "        'attention_mask_question': tokenized_questions['attention_mask'],\n",
    "        'input_ids_chosen': tokenized_chosen['input_ids'],\n",
    "        'attention_mask_chosen': tokenized_chosen['attention_mask'],\n",
    "        'input_ids_rejected': tokenized_rejected['input_ids'],\n",
    "        'attention_mask_rejected': tokenized_rejected['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Load the DPO dataset from Hugging Face\n",
    "dpo_dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "\n",
    "# Apply the preprocessing to the dataset\n",
    "dpo_dataset = dpo_dataset.map(preprocess_dpo_data, batched=True)\n",
    "\n",
    "\n",
    "# You can convert to PyTorch tensors after processing\n",
    "dpo_dataset.set_format(type='torch', columns=['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(dpo_dataset['train'], batch_size=2, shuffle=True)\n",
    "\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "expert_model = Expert(config)\n",
    "\n",
    "model_vocab_size = expert_model.lmt.decoder.word_embedding.num_embeddings\n",
    "print(f\"Model vocab size: {model_vocab_size}\")\n",
    "print(f\"Model embedding size: {expert_model.lmt.decoder.word_embedding.embedding_dim}\")\n",
    "print(f\"Configured max length: {config.max_length}\")\n",
    "\n",
    "optimizer = AdamW(expert_model.parameters(), lr=1e-5)\n",
    "save_path = 'D:/EXPERT_WEIGHTS/dpo_model.pth'\n",
    "\n",
    "# Train the DPO model\n",
    "label_column = 'labels'\n",
    "input_columns = ['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
    "avg_loss = expert_model.train_dpo(train_loader, optimizer, label_column, input_columns, config, save_path)\n",
    "\n",
    "# Save the model\n",
    "torch.save(expert_model.transformer_dpo.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
