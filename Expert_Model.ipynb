{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# 2. Sub-Model Weights\n",
    "# 1. Transformer with DPO:\n",
    "tran_dpo = r'C:\\Users\\robbi\\IEEMM\\language_model_weights.pth'\n",
    "# 2. MAMBA:\n",
    "mamba_model_path = r'C:\\Users\\robbi\\IEEMM\\mamba_model_weights.pth'\n",
    "# 3. Transformer and RAG:\n",
    "context_encoder = r'C:\\Users\\robbi\\IEEMM\\context_encoder.pth'\n",
    "language_model = r'C:\\Users\\robbi\\IEEMM\\language_model.pth'\n",
    "question_encoder = r'C:\\Users\\robbi\\IEEMM\\question_encoder.pth'\n",
    "\n",
    "# Load model weights function\n",
    "def load_model_weights(model, model_path):\n",
    "    #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "    if isinstance(checkpoint, dict):\n",
    "        # Check for 'state_dict' or 'model_state_dict' keys\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        elif 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            # If no known key is found, try loading it as a raw state dictionary\n",
    "            try:\n",
    "                model.load_state_dict(checkpoint)\n",
    "            except RuntimeError as e:\n",
    "                raise ValueError(f\"Error loading state dict: {e}\")\n",
    "    elif isinstance(checkpoint, nn.Module):\n",
    "        # If the checkpoint is a model object, assign it directly\n",
    "        model = checkpoint\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# 3. MAMBA\n",
    "# RoPE\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "        self.register_buffer('sin', freqs.sin())\n",
    "        self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "        sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "        # Apply RoPE to even and odd indices separately\n",
    "        x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "# SWIGLU\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(SwiGLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "        self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = torch.sigmoid(self.fc2(x))\n",
    "        return self.fc1(x) * gate\n",
    "\n",
    "class SimplifiedMAMBA(nn.Module):\n",
    "    # Adjusted to include SwiGLU blocks\n",
    "    def __init__(self, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_state),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_state, d_model)\n",
    "        )\n",
    "        self.input_embedding = nn.Linear(d_model, d_model)\n",
    "        self.convs = nn.Sequential(*[nn.Conv1d(d_model, d_model, kernel_size=d_conv, padding=(d_conv // 2)) for _ in range(num_layers)])\n",
    "        self.swiglu = SwiGLU(d_model, d_model)\n",
    "        self.output_projection = nn.Linear(d_model, d_model * expansion_factor)  # Adjusted to match the output of SwiGLU\n",
    "\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "        nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "        nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "    def forward(self, inputs, attention_mask=None):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "        # Apply the attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "        projected_inputs = self.input_embedding(inputs)\n",
    "        print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            projected_inputs = conv(projected_inputs)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = self.swiglu(projected_inputs)\n",
    "        print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "        output = self.output_projection(projected_inputs)\n",
    "        print(\"output shape:\", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "    # Including rotary positional encodings if required\n",
    "    def __init__(self, vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = RotaryPositionalEncoding(d_model)\n",
    "        self.simplified_mamba = SimplifiedMAMBA(num_layers, d_model, d_state, d_conv, expansion_factor)\n",
    "        self.output_projection = nn.Linear(d_model*2, vocab_size)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = 1.0\n",
    "\n",
    "        nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "        logits = self.output_projection(simplified_mamba_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Linear warmup with cosine decay\n",
    "    scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "# 4. RAG\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    # Split the text into chunks of chunk_size\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def preprocess_text(text, max_length=512):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Split the text into smaller chunks to maintain context\n",
    "    # The chunk size is slightly less than max_length to account for special tokens\n",
    "    chunk_size = max_length - 50  # Adjust this value based on the model's requirements\n",
    "    text_chunks = split_into_chunks(text, chunk_size)\n",
    "\n",
    "    # Process each chunk\n",
    "    processed_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        processed_chunk = {\n",
    "            'input_ids': tokenized_output['input_ids'],\n",
    "            'attention_mask': tokenized_output['attention_mask']\n",
    "        }\n",
    "        processed_chunks.append(processed_chunk)\n",
    "\n",
    "    return processed_chunks\n",
    "\n",
    "def create_dataset_from_pdfs(pdf_file_paths):\n",
    "    dataset = []\n",
    "    for file_path in pdf_file_paths:\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        processed_text = preprocess_text(text)\n",
    "        dataset.append(processed_text)\n",
    "    return dataset\n",
    "\n",
    "def retrieve_contexts(dataset, query_embedding, top_k=5):\n",
    "    # Assume dataset is a list of dictionaries with each dictionary containing 'input_ids' and 'attention_mask'\n",
    "    # for a particular context and that each context has been processed through a DPRContextEncoder to get embeddings\n",
    "\n",
    "    # Placeholder for storing similarity scores\n",
    "    similarity_scores = []\n",
    "\n",
    "    # Iterate over each context in the dataset\n",
    "    for context in dataset:\n",
    "        context_input_ids = context['input_ids']\n",
    "        context_attention_mask = context['attention_mask']\n",
    "\n",
    "        # Assuming context_encoder is an instance of CustomDPRContextEncoder that's already trained\n",
    "        # and available in your scope\n",
    "        context_embedding = context_encoder(context_input_ids, context_attention_mask)\n",
    "\n",
    "        # Compute similarity (e.g., using dot product)\n",
    "        similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "\n",
    "        similarity_scores.append(similarity.squeeze().item())\n",
    "\n",
    "    # Sort contexts based on similarity scores and retrieve top_k indices\n",
    "    top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "\n",
    "    # Retrieve top_k contexts\n",
    "    top_contexts = [dataset[i] for i in top_k_indices]\n",
    "\n",
    "    return top_contexts\n",
    "\n",
    "def rag_retrieve_and_generate(dataset, query):\n",
    "    # Instantiate the question encoder\n",
    "    question_encoder = DPRQuestionEncoder()\n",
    "\n",
    "    # Encode the query\n",
    "    encoded_query = question_encoder(query)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    # This involves finding the most similar documents in the dataset\n",
    "    # For simplicity, this is represented as a function 'retrieve_contexts'\n",
    "    relevant_contexts = retrieve_contexts(dataset, encoded_query)\n",
    "\n",
    "    # Language model for generation\n",
    "    language_model = LanguageModelTransformer()\n",
    "\n",
    "    # Generate a response based on the retrieved contexts\n",
    "    # This step may involve further formatting or preprocessing\n",
    "    response = language_model.generate_response(relevant_contexts)\n",
    "\n",
    "    return response\n",
    "\n",
    "# pdfs\n",
    "pdf_file_paths = [r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "                  r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf']\n",
    "\n",
    "rag_dataset = create_dataset_from_pdfs(pdf_file_paths)\n",
    "\n",
    "class CustomDPRContextEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(CustomDPRContextEncoder, self).__init__()\n",
    "        # Transformer-based model, e.g., BERT\n",
    "        self.bert_model = BertModel.from_pretrained(model_name)\n",
    "        # Additional layer to produce fixed-size embeddings\n",
    "        self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Generate outputs from the BERT model\n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the pooled output for creating embeddings\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # Pass through the embedding layer\n",
    "        context_embeddings = self.embedding_layer(pooled_output)\n",
    "        return context_embeddings\n",
    "\n",
    "class DPRQuestionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(DPRQuestionEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.embedding_layer = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        embeddings = self.embedding_layer(pooled_output)\n",
    "        return embeddings\n",
    "\n",
    "# TransformerRAG class\n",
    "class TransformerRAG(nn.Module):\n",
    "    def __init__(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n",
    "        super(TransformerRAG, self).__init__()\n",
    "        self.context_encoder = CustomDPRContextEncoder().to(device)\n",
    "        self.language_model = LanguageModelTransformer(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_size=256, \n",
    "            num_layers=6, \n",
    "            forward_expansion=4, \n",
    "            heads=8, \n",
    "            dropout=0, \n",
    "            max_length=100,  # Set to 512 to match the tokenization max_length\n",
    "            rank=16\n",
    "        ).to(device)\n",
    "        self.language_model = load_model_weights(self.language_model, language_model_path)\n",
    "        self.question_encoder = DPRQuestionEncoder().to(device)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "        if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "            raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "\n",
    "        # Process each context_text\n",
    "        aggregated_context_embeddings = []\n",
    "        for context_list in context_texts:\n",
    "            if not all(isinstance(context, dict) for context in context_list):\n",
    "                raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "\n",
    "            # Create a tensor of zeros with the correct shape on the GPU\n",
    "            aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "            for context in context_list:\n",
    "                context_input_ids = context['input_ids'].to(device)\n",
    "                context_attention_mask = context['attention_mask'].to(device)\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "\n",
    "            aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "\n",
    "        question_input_ids = question_input_ids.to(device).long()\n",
    "        question_attention_mask = question_attention_mask.to(device).long()\n",
    "        question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "\n",
    "        cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "        similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "        most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=device))\n",
    "\n",
    "        combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "        tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        tokenized_combined_input = {k: v.to(device) for k, v in tokenized_combined_input.items()}  # Move to GPU\n",
    "        response_logits = self.language_model(**tokenized_combined_input)\n",
    "        probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "        predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "        predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "        response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "\n",
    "        return response\n",
    "\n",
    "class LORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "        super(LORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weight and bias of the linear layer\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # LORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"LORALayer Input Shape:\", x.shape)\n",
    "        \n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        \n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "class QLORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "        super(QLORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "\n",
    "        # Original weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # QLORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def quantize(self, x, num_bits):\n",
    "        # Implement a simple quantization method\n",
    "        scale = x.abs().max()\n",
    "        x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "        return x_quantized, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "        B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "        lora_adjustment = self.dropout(lora_adjustment)\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def update_alpha(self, new_alpha):\n",
    "        \"\"\"\n",
    "        Update the alpha scaling factor.\n",
    "        \"\"\"\n",
    "        self.alpha = new_alpha\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does the matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, then sum it up\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class LanguageModelDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "        super(LanguageModelDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Adding BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # QLORA layers\n",
    "        self.qlora_feed_forward = nn.Sequential(\n",
    "            QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "        self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length) #.to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, trg_mask)\n",
    "            if self.use_qlora:\n",
    "                x = self.qlora_feed_forward(x)\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "        return out\n",
    "\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.use_qlora = use_qlora\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "        self.decoder = LanguageModelDecoder(\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            rank,\n",
    "        )\n",
    "\n",
    "    def forward(self, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        out = self.decoder(trg, trg_mask)\n",
    "        return out\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ).to(trg.device)\n",
    "\n",
    "        return trg_mask\n",
    "    \n",
    "    # Function to enable or disable QLORA layers (for fine-tuning purposes)\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "    def generate_response(self, input_ids, attention_mask):\n",
    "        # Assuming you have a forward method that returns logits\n",
    "        logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # For simplicity, using greedy decoding here. You might want to use beam search or sampling.\n",
    "        predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "        \n",
    "        # Convert predicted token ids to tokens\n",
    "        predicted_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "        \n",
    "        # Join tokens to form the response string. This is a very basic way to generate text and might not produce the best results.\n",
    "        response = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Flash2_Attention\n",
    "\n",
    "class FlashAttention2(nn.Module):\n",
    "    def __init__(self, sequence_length, head_dimension, block_size):\n",
    "        super(FlashAttention2, self).__init__()\n",
    "        self.block_size = block_size\n",
    "        # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "        assert sequence_length % block_size == 0\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # Partitioning of inputs\n",
    "        Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "        # Efficient computation of the attention mechanism\n",
    "        outputs = []\n",
    "        for i, Q_block in enumerate(Q_blocks):\n",
    "            output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "            outputs.append(output_block)\n",
    "\n",
    "        # Concatenating the processed blocks\n",
    "        output = torch.cat(outputs, dim=0)\n",
    "        return output\n",
    "\n",
    "    def partition_inputs(self, Q, K, V):\n",
    "        # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "        Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "        K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "        V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "        return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "    def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "        # Process each block efficiently as per FLASH2's optimized method\n",
    "        # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "        output_blocks = []\n",
    "        for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "            attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "            attention_scores = self.online_softmax(attention_scores)\n",
    "            output_block = torch.matmul(attention_scores, V_block)\n",
    "            output_blocks.append(output_block)\n",
    "\n",
    "        # Summing up the results from each block\n",
    "        output_block_sum = sum(output_blocks)\n",
    "        return output_block_sum\n",
    "\n",
    "    def online_softmax(self, scores, chunk_size=128):\n",
    "        # Apply softmax in chunks for large sequences\n",
    "        softmaxed_scores = []\n",
    "        for i in range(0, scores.size(0), chunk_size):\n",
    "            chunk = scores[i:i + chunk_size, :]\n",
    "            softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "            softmaxed_scores.append(softmaxed_chunk)\n",
    "        return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "# SparseFlash2_Attention\n",
    "\n",
    "class SparseFlash2Attention(nn.Module):\n",
    "    def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "        super().__init__()\n",
    "        self.flash_attention = FlashAttention2(seq_len, head_dim, blk_size)\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "\n",
    "    def generate_sparsity_mask(self):\n",
    "        mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "        step = self.sparsity_factor\n",
    "        for i in range(0, self.seq_len, step):\n",
    "            mask[i:i + step, :] = 1\n",
    "        return mask.bool()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "        # Reshape output to be 3D for batch matrix multiplication\n",
    "        output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "        sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "        # Apply the sparsity mask to the output\n",
    "        sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "        output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "        # Reshape the output back to 2D\n",
    "        output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Auxiliary loss function\n",
    "def auxiliary_loss(gate_scores, expert_capacity):\n",
    "    expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "    loss_balancing = torch.std(expert_load)\n",
    "    return loss_balancing\n",
    "\n",
    "# Routing function\n",
    "CAPACITY_FACTOR = 1\n",
    "\n",
    "def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "    capacity_factor_tensor = torch.tensor([CAPACITY_FACTOR], dtype=torch.float32)\n",
    "    capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "    expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "    for idx in range(len(expert_indices)):\n",
    "        selected_expert = expert_indices[idx]\n",
    "        if expert_counts[selected_expert] < capacities[0]:\n",
    "            expert_counts[selected_expert] += 1\n",
    "        else:\n",
    "            available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "            if len(available_experts) > 0:\n",
    "                alternative_expert = available_experts[0]\n",
    "                expert_indices[idx] = alternative_expert\n",
    "                expert_counts[alternative_expert] += 1\n",
    "            else:\n",
    "                print(\"No available experts to reroute. Handling overflow.\")\n",
    "    return expert_indices\n",
    "\n",
    "# SwitchGate \n",
    "class SwitchGate(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(SwitchGate, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "        self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "        return gate_scores\n",
    "\n",
    "# SwitchRouter \n",
    "class SwitchRouter(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank):\n",
    "        super(SwitchRouter, self).__init__()\n",
    "        self.router = SwitchGate(input_dim, num_experts)\n",
    "        self.transformer_rag = TransformerRAG(context_encoder_path, \n",
    "                                              language_model_path, \n",
    "                                              question_encoder_path, \n",
    "                                              vocab_size).to(device)\n",
    "        self.transformer_dpo = LanguageModelTransformer(vocab_size, \n",
    "                                                        embed_size, \n",
    "                                                        num_layers, \n",
    "                                                        forward_expansion, \n",
    "                                                        heads, dropout, \n",
    "                                                        max_length, \n",
    "                                                        rank).to(device)\n",
    "        self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "        self.mamba = SimplifiedLanguageModelMAMBA(vocab_size=VOCAB_SIZE, \n",
    "                                     num_layers=NUM_LAYERS, \n",
    "                                     d_model=D_MODEL, \n",
    "                                     d_state=D_STATE, \n",
    "                                     d_conv=D_CONV, \n",
    "                                     expansion_factor=EXPANSION_FACTOR).to(device)\n",
    "        self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "        self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "        self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        x = self.input_embedding(x.float())\n",
    "        gate_scores = self.router(x)\n",
    "        expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "        expert_indices = route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "        final_output = torch.zeros_like(x)\n",
    "        aux_loss = 0\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_indices == i\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                selected_attention_mask = attention_mask[mask]\n",
    "\n",
    "                if isinstance(expert, TransformerRAG):\n",
    "                    # Now passing the required arguments to TransformerRAG\n",
    "                    expert_output = self.transformer_rag(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                else:\n",
    "                    # Process as usual for other experts\n",
    "                    expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        # Compute auxiliary loss for load balancing\n",
    "        aux_loss += auxiliary_loss(gate_scores, expert_capacity=torch.tensor([CAPACITY_FACTOR] * len(self.experts)))\n",
    "\n",
    "        return final_output, aux_loss\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, sparsity_factor, seq_len, head_dim, blk_size, input_dim, num_experts, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank):\n",
    "        super(Expert, self).__init__()\n",
    "\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = SparseFlash2Attention(seq_len, head_dim, blk_size, sparsity_factor)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = SwitchRouter(input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank)\n",
    "\n",
    "        # Sub-models are part of SwitchRouter\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = QLORALayer(input_dim, input_dim, rank)  # Adjust dimensions as needed\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, _ = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert v2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchRouter(nn.Module):\n",
    "    CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "    class SwitchGate(nn.Module):\n",
    "        def __init__(self, input_dim, num_experts):\n",
    "            super(SwitchRouter.SwitchGate, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "            self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "            return gate_scores\n",
    "\n",
    "    def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, d_model, d_state, d_conv, expansion_factor, device):\n",
    "        super(SwitchRouter, self).__init__()\n",
    "        self.device = device\n",
    "        self.router = self.SwitchGate(input_dim, num_experts).to(self.device)\n",
    "        self.transformer_rag = TransformerRAG(context_encoder_path, language_model_path, question_encoder_path, vocab_size).to(self.device)\n",
    "        self.transformer_dpo = LanguageModelTransformer(vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank).to(self.device)\n",
    "        self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "        self.mamba = SimplifiedLanguageModelMAMBA(vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor).to(self.device)\n",
    "        self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "        self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "        self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        x = self.input_embedding(x.float()).to(self.device)\n",
    "        gate_scores = self.router(x)\n",
    "        expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "        expert_indices = self.route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_indices == i\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                selected_attention_mask = attention_mask[mask]\n",
    "\n",
    "                # Handle expert processing\n",
    "                if isinstance(expert, TransformerRAG):\n",
    "                    expert_output = self.transformer_rag(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                else:\n",
    "                    expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        # Compute auxiliary loss for load balancing\n",
    "        aux_loss = self.auxiliary_loss(gate_scores)\n",
    "\n",
    "        return final_output, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def auxiliary_loss(gate_scores):\n",
    "        expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "        loss_balancing = torch.std(expert_load)\n",
    "        return loss_balancing\n",
    "\n",
    "    @staticmethod\n",
    "    def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "        capacity_factor_tensor = torch.tensor([SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "        capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "        expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "\n",
    "        for idx in range(len(expert_indices)):\n",
    "            selected_expert = expert_indices[idx]\n",
    "            if expert_counts[selected_expert] < capacities[0]:\n",
    "                expert_counts[selected_expert] += 1\n",
    "            else:\n",
    "                available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                if len(available_experts) > 0:\n",
    "                    alternative_expert = available_experts[0]\n",
    "                    expert_indices[idx] = alternative_expert\n",
    "                    expert_counts[alternative_expert] += 1\n",
    "                else:\n",
    "                    print(\"No available experts to reroute. Handling overflow.\")\n",
    "\n",
    "        return expert_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchRouter(nn.Module):\n",
    "    CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "    class SwitchGate(nn.Module):\n",
    "        def __init__(self, input_dim, num_experts):\n",
    "            super(SwitchRouter.SwitchGate, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "            self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "            return gate_scores\n",
    "\n",
    "    def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, d_model, d_state, d_conv, expansion_factor, device):\n",
    "        super(SwitchRouter, self).__init__()\n",
    "        self.device = device\n",
    "        self.router = self.SwitchGate(input_dim, num_experts).to(device)\n",
    "        self.transformer_rag = TransformerRAG(context_encoder_path, language_model_path, question_encoder_path, vocab_size).to(device)\n",
    "        self.transformer_dpo = LanguageModelTransformer(vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank).to(device)\n",
    "        self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "        self.mamba = SimplifiedLanguageModelMAMBA(vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor).to(device)\n",
    "        self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "        self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "        self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        x = x.to(self.device)\n",
    "        if x.dtype != torch.float32:\n",
    "            x = x.float()\n",
    "\n",
    "        x = self.input_embedding(x)\n",
    "        gate_scores = self.router(x)\n",
    "        expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "        expert_indices = self.route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_indices == i\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                # Handling expert processing...\n",
    "                if isinstance(expert, TransformerRAG):\n",
    "                    expert_output = expert(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                else:\n",
    "                    expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        aux_loss = self.auxiliary_loss(gate_scores)\n",
    "        return final_output, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def auxiliary_loss(gate_scores):\n",
    "        expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "        loss_balancing = torch.std(expert_load)\n",
    "        return loss_balancing\n",
    "\n",
    "    @staticmethod\n",
    "    def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "        capacity_factor_tensor = torch.tensor([SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "        capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "        expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "        for idx in range(len(expert_indices)):\n",
    "            selected_expert = expert_indices[idx]\n",
    "            if expert_counts[selected_expert] < capacities[0]:\n",
    "                expert_counts[selected_expert] += 1\n",
    "            else:\n",
    "                available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                if len(available_experts) > 0:\n",
    "                    alternative_expert = available_experts[0]\n",
    "                    expert_indices[idx] = alternative_expert\n",
    "                    expert_counts[alternative_expert] += 1\n",
    "                else:\n",
    "                    print(\"No available experts to reroute. Handling overflow.\")\n",
    "        return expert_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming all other necessary imports are done, like models for TransformerRAG, etc.\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    # Flash2_Attention\n",
    "    class FlashAttention2(nn.Module):\n",
    "        def __init__(self, sequence_length, head_dimension, block_size):\n",
    "            super(FlashAttention2, self).__init__()\n",
    "            self.block_size = block_size\n",
    "            # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "            assert sequence_length % block_size == 0\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            # Partitioning of inputs\n",
    "            Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "            # Efficient computation of the attention mechanism\n",
    "            outputs = []\n",
    "            for i, Q_block in enumerate(Q_blocks):\n",
    "                output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "                outputs.append(output_block)\n",
    "\n",
    "            # Concatenating the processed blocks\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "            return output\n",
    "\n",
    "        def partition_inputs(self, Q, K, V):\n",
    "            # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "            Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "            K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "            V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "            return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "        def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "            # Process each block efficiently as per FLASH2's optimized method\n",
    "            # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "            output_blocks = []\n",
    "            for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "                attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "                attention_scores = self.online_softmax(attention_scores)\n",
    "                output_block = torch.matmul(attention_scores, V_block)\n",
    "                output_blocks.append(output_block)\n",
    "\n",
    "            # Summing up the results from each block\n",
    "            output_block_sum = sum(output_blocks)\n",
    "            return output_block_sum\n",
    "\n",
    "        def online_softmax(self, scores, chunk_size=128):\n",
    "            # Apply softmax in chunks for large sequences\n",
    "            softmaxed_scores = []\n",
    "            for i in range(0, scores.size(0), chunk_size):\n",
    "                chunk = scores[i:i + chunk_size, :]\n",
    "                softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "                softmaxed_scores.append(softmaxed_chunk)\n",
    "            return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "    # SparseFlash2_Attention\n",
    "    class SparseFlash2Attention(nn.Module):\n",
    "        def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "            super().__init__()\n",
    "            self.flash_attention = FlashAttention2(seq_len, head_dim, blk_size)\n",
    "            self.seq_len = seq_len\n",
    "            self.head_dim = head_dim\n",
    "            self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "            self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        def generate_sparsity_mask(self):\n",
    "            mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "            step = self.sparsity_factor\n",
    "            for i in range(0, self.seq_len, step):\n",
    "                mask[i:i + step, :] = 1\n",
    "            return mask.bool()\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "            # Reshape output to be 3D for batch matrix multiplication\n",
    "            output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "            sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "            # Apply the sparsity mask to the output\n",
    "            sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "            output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "            # Reshape the output back to 2D\n",
    "            output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "            return output\n",
    "\n",
    "    class SwitchRouter(nn.Module):\n",
    "        CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "        class SwitchGate(nn.Module):\n",
    "            def __init__(self, input_dim, num_experts):\n",
    "                super(SwitchRouter.SwitchGate, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "                self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "                return gate_scores\n",
    "\n",
    "        def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, d_model, d_state, d_conv, expansion_factor, device):\n",
    "            super(SwitchRouter, self).__init__()\n",
    "            self.device = device\n",
    "            self.router = self.SwitchGate(input_dim, num_experts).to(device)\n",
    "            self.transformer_rag = TransformerRAG(context_encoder_path, language_model_path, question_encoder_path, vocab_size).to(device)\n",
    "            self.transformer_dpo = LanguageModelTransformer(vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank).to(device)\n",
    "            self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "            self.mamba = SimplifiedLanguageModelMAMBA(vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor).to(device)\n",
    "            self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "            self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "            self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "        def forward(self, x, attention_mask, context_texts, question_text):\n",
    "            x = x.to(self.device)\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.float()\n",
    "\n",
    "            x = self.input_embedding(x)\n",
    "            gate_scores = self.router(x)\n",
    "            expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "            expert_indices = self.route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "            final_output = torch.zeros_like(x)\n",
    "\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                mask = expert_indices == i\n",
    "                if mask.any():\n",
    "                    selected_inputs = x[mask]\n",
    "                    selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                    # Handling expert processing...\n",
    "                    if isinstance(expert, TransformerRAG):\n",
    "                        expert_output = expert(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                    else:\n",
    "                        expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                    final_output[mask] = expert_output\n",
    "\n",
    "            aux_loss = self.auxiliary_loss(gate_scores)\n",
    "            return final_output, aux_loss\n",
    "\n",
    "        @staticmethod\n",
    "        def auxiliary_loss(gate_scores):\n",
    "            expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "            loss_balancing = torch.std(expert_load)\n",
    "            return loss_balancing\n",
    "\n",
    "        @staticmethod\n",
    "        def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "            capacity_factor_tensor = torch.tensor([SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "            capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "            expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "            for idx in range(len(expert_indices)):\n",
    "                selected_expert = expert_indices[idx]\n",
    "                if expert_counts[selected_expert] < capacities[0]:\n",
    "                    expert_counts[selected_expert] += 1\n",
    "                else:\n",
    "                    available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                    if len(available_experts) > 0:\n",
    "                        alternative_expert = available_experts[0]\n",
    "                        expert_indices[idx] = alternative_expert\n",
    "                        expert_counts[alternative_expert] += 1\n",
    "                    else:\n",
    "                        print(\"No available experts to reroute. Handling overflow.\")\n",
    "            return expert_indices\n",
    "\n",
    "    class CustomDPRContextEncoder(nn.Module):\n",
    "        def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "            super(CustomDPRContextEncoder, self).__init__()\n",
    "            # Transformer-based model, e.g., BERT\n",
    "            self.bert_model = BertModel.from_pretrained(model_name)\n",
    "            # Additional layer to produce fixed-size embeddings\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            # Generate outputs from the BERT model\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use the pooled output for creating embeddings\n",
    "            pooled_output = outputs.pooler_output\n",
    "            # Pass through the embedding layer\n",
    "            context_embeddings = self.embedding_layer(pooled_output)\n",
    "            return context_embeddings\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "            super(DPRQuestionEncoder, self).__init__()\n",
    "            self.bert = BertModel.from_pretrained(model_name)\n",
    "            self.embedding_layer = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, **kwargs):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            embeddings = self.embedding_layer(pooled_output)\n",
    "            return embeddings\n",
    "\n",
    "    # TransformerRAG class\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n",
    "            super(TransformerRAG, self).__init__()\n",
    "            self.context_encoder = CustomDPRContextEncoder().to(device)\n",
    "            self.language_model = LanguageModelTransformer(\n",
    "                vocab_size=vocab_size,\n",
    "                embed_size=256, \n",
    "                num_layers=6, \n",
    "                forward_expansion=4, \n",
    "                heads=8, \n",
    "                dropout=0, \n",
    "                max_length=100,  # Set to 512 to match the tokenization max_length\n",
    "                rank=16\n",
    "            ).to(device)\n",
    "            self.language_model = load_model_weights(self.language_model, language_model_path)\n",
    "            self.question_encoder = DPRQuestionEncoder().to(device)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "\n",
    "            # Process each context_text\n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "\n",
    "                # Create a tensor of zeros with the correct shape on the GPU\n",
    "                aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = context['input_ids'].to(device)\n",
    "                    context_attention_mask = context['attention_mask'].to(device)\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "\n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "\n",
    "            question_input_ids = question_input_ids.to(device).long()\n",
    "            question_attention_mask = question_attention_mask.to(device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "\n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=device))\n",
    "\n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(device) for k, v in tokenized_combined_input.items()}  # Move to GPU\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "\n",
    "            return response\n",
    "\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "            super(LORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "\n",
    "            # Original weight and bias of the linear layer\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # LORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "            super(QLORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "            self.quantization_bits = quantization_bits\n",
    "\n",
    "            # Original weight and bias\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # QLORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, embed_size, heads):\n",
    "            super(MultiHeadAttention, self).__init__()\n",
    "            self.embed_size = embed_size\n",
    "            self.heads = heads\n",
    "            self.head_dim = embed_size // heads\n",
    "\n",
    "            assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "            ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        def forward(self, values, keys, query, mask):\n",
    "            N = query.shape[0]\n",
    "            value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "            # Split the embedding into self.heads different pieces\n",
    "            values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "            queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            # Einsum does the matrix multiplication for query*keys for each training example\n",
    "            # with every other training example, then sum it up\n",
    "            attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "                N, query_len, self.heads * self.head_dim\n",
    "            )\n",
    "\n",
    "            out = self.fc_out(out)\n",
    "            return out\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "            super(TransformerBlock, self).__init__()\n",
    "            self.attention = MultiHeadAttention(embed_size, heads)\n",
    "            self.norm1 = nn.LayerNorm(embed_size)\n",
    "            self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, value, key, query, mask):\n",
    "            attention = self.attention(value, key, query, mask)\n",
    "            x = self.dropout(self.norm1(attention + query))\n",
    "            forward = self.feed_forward(x)\n",
    "            out = self.dropout(self.norm2(forward + x))\n",
    "            return out\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "            super(LanguageModelDecoder, self).__init__()\n",
    "            self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "            # Adding BatchNorm layers\n",
    "            self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "            self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # QLORA layers\n",
    "            self.qlora_feed_forward = nn.Sequential(\n",
    "                QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "            self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "            self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, trg_mask):\n",
    "            N, seq_length = x.shape\n",
    "            positions = torch.arange(0, seq_length).expand(N, seq_length) #.to(x.device)\n",
    "            x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn1(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, trg_mask)\n",
    "                if self.use_qlora:\n",
    "                    x = self.qlora_feed_forward(x)\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            out = self.fc_out(x)\n",
    "            #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "            return out\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.use_qlora = use_qlora\n",
    "\n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "            super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "            self.decoder = LanguageModelDecoder(\n",
    "                vocab_size,\n",
    "                embed_size,\n",
    "                num_layers,\n",
    "                heads,\n",
    "                forward_expansion,\n",
    "                dropout,\n",
    "                max_length,\n",
    "                rank,\n",
    "            )\n",
    "\n",
    "        def forward(self, trg):\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            out = self.decoder(trg, trg_mask)\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "                N, 1, trg_len, trg_len\n",
    "            ).to(trg.device)\n",
    "\n",
    "            return trg_mask\n",
    "        \n",
    "        # Function to enable or disable QLORA layers (for fine-tuning purposes)\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            # Assuming you have a forward method that returns logits\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Convert logits to probabilities\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # For simplicity, using greedy decoding here. You might want to use beam search or sampling.\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            \n",
    "            # Convert predicted token ids to tokens\n",
    "            predicted_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            \n",
    "            # Join tokens to form the response string. This is a very basic way to generate text and might not produce the best results.\n",
    "            response = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            \n",
    "            return response\n",
    "\n",
    "\n",
    "    def __init__(self, sparsity_factor, seq_len, head_dim, blk_size, input_dim, num_experts, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, device):\n",
    "        super().__init__()\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(seq_len, head_dim, blk_size, sparsity_factor)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = Expert.SwitchRouter(input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, device)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = Expert.QLORALayer(input_dim, input_dim, rank)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, _ = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of the Expert class\n",
    "# expert = Expert(sparsity_factor, seq_len, head_dim, blk_size, input_dim, num_experts, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expert: {\n",
    "1. SparseFlash2_attention\n",
    "2. LayerNorm + Dropout\n",
    "3. Internal Switch Routing Between Sub-models:\n",
    "a) Sub model 1: Transformer with DPO (Direct Preference Optimization)\n",
    "b) Sub model 2: Transformer with RAG(Retrieval Augmented Generation)\n",
    "c) Sub Model 3: MAMBA (Linear-Time Sequence Modelling with Selective State Spaces)\n",
    "4. QLORA\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class ExpertConfig:\n",
    "    def __init__(self, seq_len=512, head_dim=64, block_size=64, sparsity_factor=4,\n",
    "                 input_dim=512, num_experts=3, vocab_size=30000, embed_size=256,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0.1,\n",
    "                 max_length=100, rank=16, device='cuda',\n",
    "                 mamba_model_path='path/to/mamba_model',\n",
    "                 context_encoder_path='path/to/context_encoder',\n",
    "                 language_model_path='path/to/language_model',\n",
    "                 question_encoder_path='path/to/question_encoder',\n",
    "                 dpo_model_path='path/to/dpo_model'):\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        self.input_dim = input_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length\n",
    "        self.rank = rank\n",
    "        self.device = device\n",
    "\n",
    "        # Model paths\n",
    "        self.mamba_model_path = mamba_model_path\n",
    "        self.context_encoder_path = context_encoder_path\n",
    "        self.language_model_path = language_model_path\n",
    "        self.question_encoder_path = question_encoder_path\n",
    "        self.dpo_model_path = dpo_model_path\n",
    "\n",
    "    def validate(self):\n",
    "        # Add validation checks here if needed\n",
    "        assert self.seq_len % self.block_size == 0, \"seq_len must be divisible by block_size\"\n",
    "        # Additional validations can be added as required\n",
    "\n",
    "class Expert(nn.Module):\n",
    "\n",
    "    # Flash2_Attention\n",
    "    class FlashAttention2(nn.Module):\n",
    "        def __init__(self, sequence_length, head_dimension, block_size):\n",
    "            super(FlashAttention2, self).__init__()\n",
    "            self.block_size = block_size\n",
    "            # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "            assert sequence_length % block_size == 0\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            # Partitioning of inputs\n",
    "            Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "            # Efficient computation of the attention mechanism\n",
    "            outputs = []\n",
    "            for i, Q_block in enumerate(Q_blocks):\n",
    "                output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "                outputs.append(output_block)\n",
    "\n",
    "            # Concatenating the processed blocks\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "            return output\n",
    "\n",
    "        def partition_inputs(self, Q, K, V):\n",
    "            # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "            Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "            K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "            V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "            return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "        def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "            # Process each block efficiently as per FLASH2's optimized method\n",
    "            # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "            output_blocks = []\n",
    "            for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "                attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "                attention_scores = self.online_softmax(attention_scores)\n",
    "                output_block = torch.matmul(attention_scores, V_block)\n",
    "                output_blocks.append(output_block)\n",
    "\n",
    "            # Summing up the results from each block\n",
    "            output_block_sum = sum(output_blocks)\n",
    "            return output_block_sum\n",
    "\n",
    "        def online_softmax(self, scores, chunk_size=128):\n",
    "            # Apply softmax in chunks for large sequences\n",
    "            softmaxed_scores = []\n",
    "            for i in range(0, scores.size(0), chunk_size):\n",
    "                chunk = scores[i:i + chunk_size, :]\n",
    "                softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "                softmaxed_scores.append(softmaxed_chunk)\n",
    "            return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "    # SparseFlash2_Attention\n",
    "    class SparseFlash2Attention(nn.Module):\n",
    "        def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "            super().__init__()\n",
    "            self.flash_attention = FlashAttention2(seq_len, head_dim, blk_size)\n",
    "            self.seq_len = seq_len\n",
    "            self.head_dim = head_dim\n",
    "            self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "            self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        def generate_sparsity_mask(self):\n",
    "            mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "            step = self.sparsity_factor\n",
    "            for i in range(0, self.seq_len, step):\n",
    "                mask[i:i + step, :] = 1\n",
    "            return mask.bool()\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "            # Reshape output to be 3D for batch matrix multiplication\n",
    "            output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "            sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "            # Apply the sparsity mask to the output\n",
    "            sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "            output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "            # Reshape the output back to 2D\n",
    "            output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "            return output\n",
    "\n",
    "    class SwitchRouter(nn.Module):\n",
    "        CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "        class SwitchGate(nn.Module):\n",
    "            def __init__(self, input_dim, num_experts):\n",
    "                super(SwitchRouter.SwitchGate, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "                self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "                return gate_scores\n",
    "\n",
    "        def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, d_model, d_state, d_conv, expansion_factor, device):\n",
    "            super(SwitchRouter, self).__init__()\n",
    "            self.device = device\n",
    "            self.router = self.SwitchGate(input_dim, num_experts).to(device)\n",
    "            self.transformer_rag = TransformerRAG(context_encoder_path, language_model_path, question_encoder_path, vocab_size).to(device)\n",
    "            self.transformer_dpo = LanguageModelTransformer(vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank).to(device)\n",
    "            self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "            self.mamba = SimplifiedLanguageModelMAMBA(vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor).to(device)\n",
    "            self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "            self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "            self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "        def forward(self, x, attention_mask, context_texts, question_text):\n",
    "            x = x.to(self.device)\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.float()\n",
    "\n",
    "            x = self.input_embedding(x)\n",
    "            gate_scores = self.router(x)\n",
    "            expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "            expert_indices = self.route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "            final_output = torch.zeros_like(x)\n",
    "\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                mask = expert_indices == i\n",
    "                if mask.any():\n",
    "                    selected_inputs = x[mask]\n",
    "                    selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                    # Handling expert processing...\n",
    "                    if isinstance(expert, TransformerRAG):\n",
    "                        expert_output = expert(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                    else:\n",
    "                        expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                    final_output[mask] = expert_output\n",
    "\n",
    "            aux_loss = self.auxiliary_loss(gate_scores)\n",
    "            return final_output, aux_loss\n",
    "\n",
    "        @staticmethod\n",
    "        def auxiliary_loss(gate_scores):\n",
    "            expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "            loss_balancing = torch.std(expert_load)\n",
    "            return loss_balancing\n",
    "\n",
    "        @staticmethod\n",
    "        def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "            capacity_factor_tensor = torch.tensor([SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "            capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "            expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "            for idx in range(len(expert_indices)):\n",
    "                selected_expert = expert_indices[idx]\n",
    "                if expert_counts[selected_expert] < capacities[0]:\n",
    "                    expert_counts[selected_expert] += 1\n",
    "                else:\n",
    "                    available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                    if len(available_experts) > 0:\n",
    "                        alternative_expert = available_experts[0]\n",
    "                        expert_indices[idx] = alternative_expert\n",
    "                        expert_counts[alternative_expert] += 1\n",
    "                    else:\n",
    "                        print(\"No available experts to reroute. Handling overflow.\")\n",
    "            return expert_indices\n",
    "\n",
    "    class CustomDPRContextEncoder(nn.Module):\n",
    "        def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "            super(CustomDPRContextEncoder, self).__init__()\n",
    "            # Transformer-based model, e.g., BERT\n",
    "            self.bert_model = BertModel.from_pretrained(model_name)\n",
    "            # Additional layer to produce fixed-size embeddings\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            # Generate outputs from the BERT model\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use the pooled output for creating embeddings\n",
    "            pooled_output = outputs.pooler_output\n",
    "            # Pass through the embedding layer\n",
    "            context_embeddings = self.embedding_layer(pooled_output)\n",
    "            return context_embeddings\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "            super(DPRQuestionEncoder, self).__init__()\n",
    "            self.bert = BertModel.from_pretrained(model_name)\n",
    "            self.embedding_layer = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, **kwargs):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            embeddings = self.embedding_layer(pooled_output)\n",
    "            return embeddings\n",
    "\n",
    "    # TransformerRAG class\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n",
    "            super(TransformerRAG, self).__init__()\n",
    "            self.context_encoder = CustomDPRContextEncoder().to(device)\n",
    "            self.language_model = LanguageModelTransformer(\n",
    "                vocab_size=vocab_size,\n",
    "                embed_size=256, \n",
    "                num_layers=6, \n",
    "                forward_expansion=4, \n",
    "                heads=8, \n",
    "                dropout=0, \n",
    "                max_length=100,  # Set to 512 to match the tokenization max_length\n",
    "                rank=16\n",
    "            ).to(device)\n",
    "            self.language_model = load_model_weights(self.language_model, language_model_path)\n",
    "            self.question_encoder = DPRQuestionEncoder().to(device)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "\n",
    "            # Process each context_text\n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "\n",
    "                # Create a tensor of zeros with the correct shape on the GPU\n",
    "                aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = context['input_ids'].to(device)\n",
    "                    context_attention_mask = context['attention_mask'].to(device)\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "\n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "\n",
    "            question_input_ids = question_input_ids.to(device).long()\n",
    "            question_attention_mask = question_attention_mask.to(device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "\n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=device))\n",
    "\n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(device) for k, v in tokenized_combined_input.items()}  # Move to GPU\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "\n",
    "            return response\n",
    "\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "            super(LORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "\n",
    "            # Original weight and bias of the linear layer\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # LORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "            super(QLORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "            self.quantization_bits = quantization_bits\n",
    "\n",
    "            # Original weight and bias\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # QLORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, embed_size, heads):\n",
    "            super(MultiHeadAttention, self).__init__()\n",
    "            self.embed_size = embed_size\n",
    "            self.heads = heads\n",
    "            self.head_dim = embed_size // heads\n",
    "\n",
    "            assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "            ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        def forward(self, values, keys, query, mask):\n",
    "            N = query.shape[0]\n",
    "            value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "            # Split the embedding into self.heads different pieces\n",
    "            values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "            queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            # Einsum does the matrix multiplication for query*keys for each training example\n",
    "            # with every other training example, then sum it up\n",
    "            attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "                N, query_len, self.heads * self.head_dim\n",
    "            )\n",
    "\n",
    "            out = self.fc_out(out)\n",
    "            return out\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "            super(TransformerBlock, self).__init__()\n",
    "            self.attention = MultiHeadAttention(embed_size, heads)\n",
    "            self.norm1 = nn.LayerNorm(embed_size)\n",
    "            self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, value, key, query, mask):\n",
    "            attention = self.attention(value, key, query, mask)\n",
    "            x = self.dropout(self.norm1(attention + query))\n",
    "            forward = self.feed_forward(x)\n",
    "            out = self.dropout(self.norm2(forward + x))\n",
    "            return out\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "            super(LanguageModelDecoder, self).__init__()\n",
    "            self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "            # Adding BatchNorm layers\n",
    "            self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "            self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # QLORA layers\n",
    "            self.qlora_feed_forward = nn.Sequential(\n",
    "                QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "            self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "            self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, trg_mask):\n",
    "            N, seq_length = x.shape\n",
    "            positions = torch.arange(0, seq_length).expand(N, seq_length) #.to(x.device)\n",
    "            x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn1(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, trg_mask)\n",
    "                if self.use_qlora:\n",
    "                    x = self.qlora_feed_forward(x)\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            out = self.fc_out(x)\n",
    "            #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "            return out\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.use_qlora = use_qlora\n",
    "\n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "            super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "            self.decoder = LanguageModelDecoder(\n",
    "                vocab_size,\n",
    "                embed_size,\n",
    "                num_layers,\n",
    "                heads,\n",
    "                forward_expansion,\n",
    "                dropout,\n",
    "                max_length,\n",
    "                rank,\n",
    "            )\n",
    "\n",
    "        def forward(self, trg):\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            out = self.decoder(trg, trg_mask)\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "                N, 1, trg_len, trg_len\n",
    "            ).to(trg.device)\n",
    "\n",
    "            return trg_mask\n",
    "        \n",
    "        # Function to enable or disable QLORA layers (for fine-tuning purposes)\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            # Assuming you have a forward method that returns logits\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Convert logits to probabilities\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # For simplicity, using greedy decoding here. You might want to use beam search or sampling.\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            \n",
    "            # Convert predicted token ids to tokens\n",
    "            predicted_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            \n",
    "            # Join tokens to form the response string. This is a very basic way to generate text and might not produce the best results.\n",
    "            response = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            \n",
    "            return response\n",
    "\n",
    "    class DPO(nn.Module):\n",
    "        def __init__(self, model, device):\n",
    "            super(DPO, self).__init__()\n",
    "            self.model = model\n",
    "            self.device = device\n",
    "\n",
    "        def forward(self, input_ids, labels):\n",
    "            # Forward pass through the existing model\n",
    "            logits = self.model(input_ids)\n",
    "\n",
    "            # Compute binary cross-entropy loss\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, labels.float())\n",
    "            return loss\n",
    "\n",
    "        def train_dpo(self, train_loader, optimizer):\n",
    "            self.model.train()\n",
    "            for input_ids, labels in train_loader:\n",
    "                input_ids, labels = input_ids.to(self.device), labels.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = self.forward(input_ids, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "    def __init__(self, config: ExpertConfig):\n",
    "        super().__init__()\n",
    "        # Validate configuration\n",
    "        config.validate()\n",
    "\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(\n",
    "            config.seq_len, \n",
    "            config.head_dim, \n",
    "            config.block_size, \n",
    "            config.sparsity_factor\n",
    "        )\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(config.input_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = Expert.SwitchRouter(\n",
    "            config.input_dim, \n",
    "            config.num_experts, \n",
    "            config.mamba_model_path,\n",
    "            config.context_encoder_path, \n",
    "            config.language_model_path,\n",
    "            config.question_encoder_path, \n",
    "            config.dpo_model_path,\n",
    "            config.vocab_size, \n",
    "            config.embed_size, \n",
    "            config.num_layers,\n",
    "            config.forward_expansion, \n",
    "            config.heads, config.dropout,\n",
    "            config.max_length, \n",
    "            config.rank, \n",
    "            config.device\n",
    "        )\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = Expert.QLORALayer(config.input_dim, config.input_dim, config.rank)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, aux_loss = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x, aux_loss\n",
    "\n",
    "# Example usage\n",
    "config = ExpertConfig()\n",
    "expert_model = Expert(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
