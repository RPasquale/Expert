{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# 2. Sub-Model Weights\n",
    "# 1. Transformer with DPO:\n",
    "tran_dpo = r'C:\\Users\\robbi\\IEEMM\\language_model_weights.pth'\n",
    "# 2. MAMBA:\n",
    "mamba_model_path = r'C:\\Users\\robbi\\IEEMM\\mamba_model_weights.pth'\n",
    "# 3. Transformer and RAG:\n",
    "context_encoder = r'C:\\Users\\robbi\\IEEMM\\context_encoder.pth'\n",
    "language_model = r'C:\\Users\\robbi\\IEEMM\\language_model.pth'\n",
    "question_encoder = r'C:\\Users\\robbi\\IEEMM\\question_encoder.pth'\n",
    "\n",
    "# Load model weights function\n",
    "def load_model_weights(model, model_path):\n",
    "    #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "    if isinstance(checkpoint, dict):\n",
    "        # Check for 'state_dict' or 'model_state_dict' keys\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        elif 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            # If no known key is found, try loading it as a raw state dictionary\n",
    "            try:\n",
    "                model.load_state_dict(checkpoint)\n",
    "            except RuntimeError as e:\n",
    "                raise ValueError(f\"Error loading state dict: {e}\")\n",
    "    elif isinstance(checkpoint, nn.Module):\n",
    "        # If the checkpoint is a model object, assign it directly\n",
    "        model = checkpoint\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# 3. MAMBA\n",
    "# RoPE\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "        self.register_buffer('sin', freqs.sin())\n",
    "        self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "        sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "        # Apply RoPE to even and odd indices separately\n",
    "        x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "# SWIGLU\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(SwiGLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "        self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = torch.sigmoid(self.fc2(x))\n",
    "        return self.fc1(x) * gate\n",
    "\n",
    "class SimplifiedMAMBA(nn.Module):\n",
    "    # Adjusted to include SwiGLU blocks\n",
    "    def __init__(self, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_state),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_state, d_model)\n",
    "        )\n",
    "        self.input_embedding = nn.Linear(d_model, d_model)\n",
    "        self.convs = nn.Sequential(*[nn.Conv1d(d_model, d_model, kernel_size=d_conv, padding=(d_conv // 2)) for _ in range(num_layers)])\n",
    "        self.swiglu = SwiGLU(d_model, d_model)\n",
    "        self.output_projection = nn.Linear(d_model, d_model * expansion_factor)  # Adjusted to match the output of SwiGLU\n",
    "\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "        nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "        nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "    def forward(self, inputs, attention_mask=None):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "        # Apply the attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "        projected_inputs = self.input_embedding(inputs)\n",
    "        print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            projected_inputs = conv(projected_inputs)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = self.swiglu(projected_inputs)\n",
    "        print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "        output = self.output_projection(projected_inputs)\n",
    "        print(\"output shape:\", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "    # Including rotary positional encodings if required\n",
    "    def __init__(self, vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = RotaryPositionalEncoding(d_model)\n",
    "        self.simplified_mamba = SimplifiedMAMBA(num_layers, d_model, d_state, d_conv, expansion_factor)\n",
    "        self.output_projection = nn.Linear(d_model*2, vocab_size)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = 1.0\n",
    "\n",
    "        nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "        logits = self.output_projection(simplified_mamba_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Linear warmup with cosine decay\n",
    "    scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "# 4. RAG\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    # Split the text into chunks of chunk_size\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def preprocess_text(text, max_length=512):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Split the text into smaller chunks to maintain context\n",
    "    # The chunk size is slightly less than max_length to account for special tokens\n",
    "    chunk_size = max_length - 50  # Adjust this value based on the model's requirements\n",
    "    text_chunks = split_into_chunks(text, chunk_size)\n",
    "\n",
    "    # Process each chunk\n",
    "    processed_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        processed_chunk = {\n",
    "            'input_ids': tokenized_output['input_ids'],\n",
    "            'attention_mask': tokenized_output['attention_mask']\n",
    "        }\n",
    "        processed_chunks.append(processed_chunk)\n",
    "\n",
    "    return processed_chunks\n",
    "\n",
    "def create_dataset_from_pdfs(pdf_file_paths):\n",
    "    dataset = []\n",
    "    for file_path in pdf_file_paths:\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        processed_text = preprocess_text(text)\n",
    "        dataset.append(processed_text)\n",
    "    return dataset\n",
    "\n",
    "def retrieve_contexts(dataset, query_embedding, top_k=5):\n",
    "    # Assume dataset is a list of dictionaries with each dictionary containing 'input_ids' and 'attention_mask'\n",
    "    # for a particular context and that each context has been processed through a DPRContextEncoder to get embeddings\n",
    "\n",
    "    # Placeholder for storing similarity scores\n",
    "    similarity_scores = []\n",
    "\n",
    "    # Iterate over each context in the dataset\n",
    "    for context in dataset:\n",
    "        context_input_ids = context['input_ids']\n",
    "        context_attention_mask = context['attention_mask']\n",
    "\n",
    "        # Assuming context_encoder is an instance of CustomDPRContextEncoder that's already trained\n",
    "        # and available in your scope\n",
    "        context_embedding = context_encoder(context_input_ids, context_attention_mask)\n",
    "\n",
    "        # Compute similarity (e.g., using dot product)\n",
    "        similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "\n",
    "        similarity_scores.append(similarity.squeeze().item())\n",
    "\n",
    "    # Sort contexts based on similarity scores and retrieve top_k indices\n",
    "    top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "\n",
    "    # Retrieve top_k contexts\n",
    "    top_contexts = [dataset[i] for i in top_k_indices]\n",
    "\n",
    "    return top_contexts\n",
    "\n",
    "def rag_retrieve_and_generate(dataset, query):\n",
    "    # Instantiate the question encoder\n",
    "    question_encoder = DPRQuestionEncoder()\n",
    "\n",
    "    # Encode the query\n",
    "    encoded_query = question_encoder(query)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    # This involves finding the most similar documents in the dataset\n",
    "    # For simplicity, this is represented as a function 'retrieve_contexts'\n",
    "    relevant_contexts = retrieve_contexts(dataset, encoded_query)\n",
    "\n",
    "    # Language model for generation\n",
    "    language_model = LanguageModelTransformer()\n",
    "\n",
    "    # Generate a response based on the retrieved contexts\n",
    "    # This step may involve further formatting or preprocessing\n",
    "    response = language_model.generate_response(relevant_contexts)\n",
    "\n",
    "    return response\n",
    "\n",
    "# pdfs\n",
    "pdf_file_paths = [r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "                  r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf']\n",
    "\n",
    "rag_dataset = create_dataset_from_pdfs(pdf_file_paths)\n",
    "\n",
    "class CustomDPRContextEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(CustomDPRContextEncoder, self).__init__()\n",
    "        # Transformer-based model, e.g., BERT\n",
    "        self.bert_model = BertModel.from_pretrained(model_name)\n",
    "        # Additional layer to produce fixed-size embeddings\n",
    "        self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Generate outputs from the BERT model\n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the pooled output for creating embeddings\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # Pass through the embedding layer\n",
    "        context_embeddings = self.embedding_layer(pooled_output)\n",
    "        return context_embeddings\n",
    "\n",
    "class DPRQuestionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(DPRQuestionEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.embedding_layer = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        embeddings = self.embedding_layer(pooled_output)\n",
    "        return embeddings\n",
    "\n",
    "# TransformerRAG class\n",
    "class TransformerRAG(nn.Module):\n",
    "    def __init__(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n",
    "        super(TransformerRAG, self).__init__()\n",
    "        self.context_encoder = CustomDPRContextEncoder().to(device)\n",
    "        self.language_model = LanguageModelTransformer(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_size=256, \n",
    "            num_layers=6, \n",
    "            forward_expansion=4, \n",
    "            heads=8, \n",
    "            dropout=0, \n",
    "            max_length=100,  # Set to 512 to match the tokenization max_length\n",
    "            rank=16\n",
    "        ).to(device)\n",
    "        self.language_model = load_model_weights(self.language_model, language_model_path)\n",
    "        self.question_encoder = DPRQuestionEncoder().to(device)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "        if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "            raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "\n",
    "        # Process each context_text\n",
    "        aggregated_context_embeddings = []\n",
    "        for context_list in context_texts:\n",
    "            if not all(isinstance(context, dict) for context in context_list):\n",
    "                raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "\n",
    "            # Create a tensor of zeros with the correct shape on the GPU\n",
    "            aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "            for context in context_list:\n",
    "                context_input_ids = context['input_ids'].to(device)\n",
    "                context_attention_mask = context['attention_mask'].to(device)\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "\n",
    "            aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "\n",
    "        question_input_ids = question_input_ids.to(device).long()\n",
    "        question_attention_mask = question_attention_mask.to(device).long()\n",
    "        question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "\n",
    "        cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "        similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "        most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=device))\n",
    "\n",
    "        combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "        tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        tokenized_combined_input = {k: v.to(device) for k, v in tokenized_combined_input.items()}  # Move to GPU\n",
    "        response_logits = self.language_model(**tokenized_combined_input)\n",
    "        probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "        predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "        predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "        response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "\n",
    "        return response\n",
    "\n",
    "class LORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "        super(LORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weight and bias of the linear layer\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # LORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"LORALayer Input Shape:\", x.shape)\n",
    "        \n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        \n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "class QLORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "        super(QLORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "\n",
    "        # Original weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # QLORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def quantize(self, x, num_bits):\n",
    "        # Implement a simple quantization method\n",
    "        scale = x.abs().max()\n",
    "        x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "        return x_quantized, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "        B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "        lora_adjustment = self.dropout(lora_adjustment)\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def update_alpha(self, new_alpha):\n",
    "        \"\"\"\n",
    "        Update the alpha scaling factor.\n",
    "        \"\"\"\n",
    "        self.alpha = new_alpha\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does the matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, then sum it up\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class LanguageModelDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "        super(LanguageModelDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Adding BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # QLORA layers\n",
    "        self.qlora_feed_forward = nn.Sequential(\n",
    "            QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "        self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length) #.to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, trg_mask)\n",
    "            if self.use_qlora:\n",
    "                x = self.qlora_feed_forward(x)\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "        return out\n",
    "\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.use_qlora = use_qlora\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "        self.decoder = LanguageModelDecoder(\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            rank,\n",
    "        )\n",
    "\n",
    "    def forward(self, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        out = self.decoder(trg, trg_mask)\n",
    "        return out\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ).to(trg.device)\n",
    "\n",
    "        return trg_mask\n",
    "    \n",
    "    # Function to enable or disable QLORA layers (for fine-tuning purposes)\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "    def generate_response(self, input_ids, attention_mask):\n",
    "        # Assuming you have a forward method that returns logits\n",
    "        logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # For simplicity, using greedy decoding here. You might want to use beam search or sampling.\n",
    "        predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "        \n",
    "        # Convert predicted token ids to tokens\n",
    "        predicted_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "        \n",
    "        # Join tokens to form the response string. This is a very basic way to generate text and might not produce the best results.\n",
    "        response = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Flash2_Attention\n",
    "\n",
    "class FlashAttention2(nn.Module):\n",
    "    def __init__(self, sequence_length, head_dimension, block_size):\n",
    "        super(FlashAttention2, self).__init__()\n",
    "        self.block_size = block_size\n",
    "        # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "        assert sequence_length % block_size == 0\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # Partitioning of inputs\n",
    "        Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "        # Efficient computation of the attention mechanism\n",
    "        outputs = []\n",
    "        for i, Q_block in enumerate(Q_blocks):\n",
    "            output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "            outputs.append(output_block)\n",
    "\n",
    "        # Concatenating the processed blocks\n",
    "        output = torch.cat(outputs, dim=0)\n",
    "        return output\n",
    "\n",
    "    def partition_inputs(self, Q, K, V):\n",
    "        # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "        Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "        K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "        V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "        return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "    def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "        # Process each block efficiently as per FLASH2's optimized method\n",
    "        # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "        output_blocks = []\n",
    "        for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "            attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "            attention_scores = self.online_softmax(attention_scores)\n",
    "            output_block = torch.matmul(attention_scores, V_block)\n",
    "            output_blocks.append(output_block)\n",
    "\n",
    "        # Summing up the results from each block\n",
    "        output_block_sum = sum(output_blocks)\n",
    "        return output_block_sum\n",
    "\n",
    "    def online_softmax(self, scores, chunk_size=128):\n",
    "        # Apply softmax in chunks for large sequences\n",
    "        softmaxed_scores = []\n",
    "        for i in range(0, scores.size(0), chunk_size):\n",
    "            chunk = scores[i:i + chunk_size, :]\n",
    "            softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "            softmaxed_scores.append(softmaxed_chunk)\n",
    "        return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "# SparseFlash2_Attention\n",
    "\n",
    "class SparseFlash2Attention(nn.Module):\n",
    "    def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "        super().__init__()\n",
    "        self.flash_attention = FlashAttention2(seq_len, head_dim, blk_size)\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "\n",
    "    def generate_sparsity_mask(self):\n",
    "        mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "        step = self.sparsity_factor\n",
    "        for i in range(0, self.seq_len, step):\n",
    "            mask[i:i + step, :] = 1\n",
    "        return mask.bool()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "        # Reshape output to be 3D for batch matrix multiplication\n",
    "        output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "        sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "        # Apply the sparsity mask to the output\n",
    "        sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "        output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "        # Reshape the output back to 2D\n",
    "        output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Auxiliary loss function\n",
    "def auxiliary_loss(gate_scores, expert_capacity):\n",
    "    expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "    loss_balancing = torch.std(expert_load)\n",
    "    return loss_balancing\n",
    "\n",
    "# Routing function\n",
    "CAPACITY_FACTOR = 1\n",
    "\n",
    "def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "    capacity_factor_tensor = torch.tensor([CAPACITY_FACTOR], dtype=torch.float32)\n",
    "    capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "    expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "    for idx in range(len(expert_indices)):\n",
    "        selected_expert = expert_indices[idx]\n",
    "        if expert_counts[selected_expert] < capacities[0]:\n",
    "            expert_counts[selected_expert] += 1\n",
    "        else:\n",
    "            available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "            if len(available_experts) > 0:\n",
    "                alternative_expert = available_experts[0]\n",
    "                expert_indices[idx] = alternative_expert\n",
    "                expert_counts[alternative_expert] += 1\n",
    "            else:\n",
    "                print(\"No available experts to reroute. Handling overflow.\")\n",
    "    return expert_indices\n",
    "\n",
    "# SwitchGate \n",
    "class SwitchGate(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(SwitchGate, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "        self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "        return gate_scores\n",
    "\n",
    "# SwitchRouter \n",
    "class SwitchRouter(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank):\n",
    "        super(SwitchRouter, self).__init__()\n",
    "        self.router = SwitchGate(input_dim, num_experts)\n",
    "        self.transformer_rag = TransformerRAG(context_encoder_path, \n",
    "                                              language_model_path, \n",
    "                                              question_encoder_path, \n",
    "                                              vocab_size).to(device)\n",
    "        self.transformer_dpo = LanguageModelTransformer(vocab_size, \n",
    "                                                        embed_size, \n",
    "                                                        num_layers, \n",
    "                                                        forward_expansion, \n",
    "                                                        heads, dropout, \n",
    "                                                        max_length, \n",
    "                                                        rank).to(device)\n",
    "        self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "        self.mamba = SimplifiedLanguageModelMAMBA(vocab_size=VOCAB_SIZE, \n",
    "                                     num_layers=NUM_LAYERS, \n",
    "                                     d_model=D_MODEL, \n",
    "                                     d_state=D_STATE, \n",
    "                                     d_conv=D_CONV, \n",
    "                                     expansion_factor=EXPANSION_FACTOR).to(device)\n",
    "        self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "        self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "        self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        x = self.input_embedding(x.float())\n",
    "        gate_scores = self.router(x)\n",
    "        expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "        expert_indices = route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "        final_output = torch.zeros_like(x)\n",
    "        aux_loss = 0\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_indices == i\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                selected_attention_mask = attention_mask[mask]\n",
    "\n",
    "                if isinstance(expert, TransformerRAG):\n",
    "                    # Now passing the required arguments to TransformerRAG\n",
    "                    expert_output = self.transformer_rag(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                else:\n",
    "                    # Process as usual for other experts\n",
    "                    expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        # Compute auxiliary loss for load balancing\n",
    "        aux_loss += auxiliary_loss(gate_scores, expert_capacity=torch.tensor([CAPACITY_FACTOR] * len(self.experts)))\n",
    "\n",
    "        return final_output, aux_loss\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, sparsity_factor, seq_len, head_dim, blk_size, input_dim, num_experts, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank):\n",
    "        super(Expert, self).__init__()\n",
    "\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = SparseFlash2Attention(seq_len, head_dim, blk_size, sparsity_factor)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = SwitchRouter(input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank)\n",
    "\n",
    "        # Sub-models are part of SwitchRouter\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = QLORALayer(input_dim, input_dim, rank)  # Adjust dimensions as needed\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, _ = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert v2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchRouter(nn.Module):\n",
    "    CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "    class SwitchGate(nn.Module):\n",
    "        def __init__(self, input_dim, num_experts):\n",
    "            super(SwitchRouter.SwitchGate, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "            self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "            return gate_scores\n",
    "\n",
    "    def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, d_model, d_state, d_conv, expansion_factor, device):\n",
    "        super(SwitchRouter, self).__init__()\n",
    "        self.device = device\n",
    "        self.router = self.SwitchGate(input_dim, num_experts).to(self.device)\n",
    "        self.transformer_rag = TransformerRAG(context_encoder_path, language_model_path, question_encoder_path, vocab_size).to(self.device)\n",
    "        self.transformer_dpo = LanguageModelTransformer(vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank).to(self.device)\n",
    "        self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "        self.mamba = SimplifiedLanguageModelMAMBA(vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor).to(self.device)\n",
    "        self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "        self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "        self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        x = self.input_embedding(x.float()).to(self.device)\n",
    "        gate_scores = self.router(x)\n",
    "        expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "        expert_indices = self.route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_indices == i\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                selected_attention_mask = attention_mask[mask]\n",
    "\n",
    "                # Handle expert processing\n",
    "                if isinstance(expert, TransformerRAG):\n",
    "                    expert_output = self.transformer_rag(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                else:\n",
    "                    expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        # Compute auxiliary loss for load balancing\n",
    "        aux_loss = self.auxiliary_loss(gate_scores)\n",
    "\n",
    "        return final_output, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def auxiliary_loss(gate_scores):\n",
    "        expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "        loss_balancing = torch.std(expert_load)\n",
    "        return loss_balancing\n",
    "\n",
    "    @staticmethod\n",
    "    def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "        capacity_factor_tensor = torch.tensor([SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "        capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "        expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "\n",
    "        for idx in range(len(expert_indices)):\n",
    "            selected_expert = expert_indices[idx]\n",
    "            if expert_counts[selected_expert] < capacities[0]:\n",
    "                expert_counts[selected_expert] += 1\n",
    "            else:\n",
    "                available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                if len(available_experts) > 0:\n",
    "                    alternative_expert = available_experts[0]\n",
    "                    expert_indices[idx] = alternative_expert\n",
    "                    expert_counts[alternative_expert] += 1\n",
    "                else:\n",
    "                    print(\"No available experts to reroute. Handling overflow.\")\n",
    "\n",
    "        return expert_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchRouter(nn.Module):\n",
    "    CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "    class SwitchGate(nn.Module):\n",
    "        def __init__(self, input_dim, num_experts):\n",
    "            super(SwitchRouter.SwitchGate, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "            self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "            return gate_scores\n",
    "\n",
    "    def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, d_model, d_state, d_conv, expansion_factor, device):\n",
    "        super(SwitchRouter, self).__init__()\n",
    "        self.device = device\n",
    "        self.router = self.SwitchGate(input_dim, num_experts).to(device)\n",
    "        self.transformer_rag = TransformerRAG(context_encoder_path, language_model_path, question_encoder_path, vocab_size).to(device)\n",
    "        self.transformer_dpo = LanguageModelTransformer(vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank).to(device)\n",
    "        self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "        self.mamba = SimplifiedLanguageModelMAMBA(vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor).to(device)\n",
    "        self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "        self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "        self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        x = x.to(self.device)\n",
    "        if x.dtype != torch.float32:\n",
    "            x = x.float()\n",
    "\n",
    "        x = self.input_embedding(x)\n",
    "        gate_scores = self.router(x)\n",
    "        expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "        expert_indices = self.route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_indices == i\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                # Handling expert processing...\n",
    "                if isinstance(expert, TransformerRAG):\n",
    "                    expert_output = expert(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                else:\n",
    "                    expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        aux_loss = self.auxiliary_loss(gate_scores)\n",
    "        return final_output, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def auxiliary_loss(gate_scores):\n",
    "        expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "        loss_balancing = torch.std(expert_load)\n",
    "        return loss_balancing\n",
    "\n",
    "    @staticmethod\n",
    "    def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "        capacity_factor_tensor = torch.tensor([SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "        capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "        expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "        for idx in range(len(expert_indices)):\n",
    "            selected_expert = expert_indices[idx]\n",
    "            if expert_counts[selected_expert] < capacities[0]:\n",
    "                expert_counts[selected_expert] += 1\n",
    "            else:\n",
    "                available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                if len(available_experts) > 0:\n",
    "                    alternative_expert = available_experts[0]\n",
    "                    expert_indices[idx] = alternative_expert\n",
    "                    expert_counts[alternative_expert] += 1\n",
    "                else:\n",
    "                    print(\"No available experts to reroute. Handling overflow.\")\n",
    "        return expert_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming all other necessary imports are done, like models for TransformerRAG, etc.\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    # Flash2_Attention\n",
    "    class FlashAttention2(nn.Module):\n",
    "        def __init__(self, sequence_length, head_dimension, block_size):\n",
    "            super(FlashAttention2, self).__init__()\n",
    "            self.block_size = block_size\n",
    "            # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "            assert sequence_length % block_size == 0\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            # Partitioning of inputs\n",
    "            Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "            # Efficient computation of the attention mechanism\n",
    "            outputs = []\n",
    "            for i, Q_block in enumerate(Q_blocks):\n",
    "                output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "                outputs.append(output_block)\n",
    "\n",
    "            # Concatenating the processed blocks\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "            return output\n",
    "\n",
    "        def partition_inputs(self, Q, K, V):\n",
    "            # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "            Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "            K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "            V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "            return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "        def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "            # Process each block efficiently as per FLASH2's optimized method\n",
    "            # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "            output_blocks = []\n",
    "            for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "                attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "                attention_scores = self.online_softmax(attention_scores)\n",
    "                output_block = torch.matmul(attention_scores, V_block)\n",
    "                output_blocks.append(output_block)\n",
    "\n",
    "            # Summing up the results from each block\n",
    "            output_block_sum = sum(output_blocks)\n",
    "            return output_block_sum\n",
    "\n",
    "        def online_softmax(self, scores, chunk_size=128):\n",
    "            # Apply softmax in chunks for large sequences\n",
    "            softmaxed_scores = []\n",
    "            for i in range(0, scores.size(0), chunk_size):\n",
    "                chunk = scores[i:i + chunk_size, :]\n",
    "                softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "                softmaxed_scores.append(softmaxed_chunk)\n",
    "            return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "    # SparseFlash2_Attention\n",
    "    class SparseFlash2Attention(nn.Module):\n",
    "        def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "            super().__init__()\n",
    "            self.flash_attention = FlashAttention2(seq_len, head_dim, blk_size)\n",
    "            self.seq_len = seq_len\n",
    "            self.head_dim = head_dim\n",
    "            self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "            self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        def generate_sparsity_mask(self):\n",
    "            mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "            step = self.sparsity_factor\n",
    "            for i in range(0, self.seq_len, step):\n",
    "                mask[i:i + step, :] = 1\n",
    "            return mask.bool()\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "            # Reshape output to be 3D for batch matrix multiplication\n",
    "            output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "            sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "            # Apply the sparsity mask to the output\n",
    "            sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "            output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "            # Reshape the output back to 2D\n",
    "            output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "            return output\n",
    "\n",
    "    class SwitchRouter(nn.Module):\n",
    "        CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "        class SwitchGate(nn.Module):\n",
    "            def __init__(self, input_dim, num_experts):\n",
    "                super(SwitchRouter.SwitchGate, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "                self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "                return gate_scores\n",
    "\n",
    "        def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, d_model, d_state, d_conv, expansion_factor, device):\n",
    "            super(SwitchRouter, self).__init__()\n",
    "            self.device = device\n",
    "            self.router = self.SwitchGate(input_dim, num_experts).to(device)\n",
    "            self.transformer_rag = TransformerRAG(context_encoder_path, language_model_path, question_encoder_path, vocab_size).to(device)\n",
    "            self.transformer_dpo = LanguageModelTransformer(vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank).to(device)\n",
    "            self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "            self.mamba = SimplifiedLanguageModelMAMBA(vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor).to(device)\n",
    "            self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "            self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "            self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "        def forward(self, x, attention_mask, context_texts, question_text):\n",
    "            x = x.to(self.device)\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.float()\n",
    "\n",
    "            x = self.input_embedding(x)\n",
    "            gate_scores = self.router(x)\n",
    "            expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "            expert_indices = self.route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "            final_output = torch.zeros_like(x)\n",
    "\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                mask = expert_indices == i\n",
    "                if mask.any():\n",
    "                    selected_inputs = x[mask]\n",
    "                    selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                    # Handling expert processing...\n",
    "                    if isinstance(expert, TransformerRAG):\n",
    "                        expert_output = expert(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                    else:\n",
    "                        expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                    final_output[mask] = expert_output\n",
    "\n",
    "            aux_loss = self.auxiliary_loss(gate_scores)\n",
    "            return final_output, aux_loss\n",
    "\n",
    "        @staticmethod\n",
    "        def auxiliary_loss(gate_scores):\n",
    "            expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "            loss_balancing = torch.std(expert_load)\n",
    "            return loss_balancing\n",
    "\n",
    "        @staticmethod\n",
    "        def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "            capacity_factor_tensor = torch.tensor([SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "            capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "            expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "            for idx in range(len(expert_indices)):\n",
    "                selected_expert = expert_indices[idx]\n",
    "                if expert_counts[selected_expert] < capacities[0]:\n",
    "                    expert_counts[selected_expert] += 1\n",
    "                else:\n",
    "                    available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                    if len(available_experts) > 0:\n",
    "                        alternative_expert = available_experts[0]\n",
    "                        expert_indices[idx] = alternative_expert\n",
    "                        expert_counts[alternative_expert] += 1\n",
    "                    else:\n",
    "                        print(\"No available experts to reroute. Handling overflow.\")\n",
    "            return expert_indices\n",
    "\n",
    "    class CustomDPRContextEncoder(nn.Module):\n",
    "        def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "            super(CustomDPRContextEncoder, self).__init__()\n",
    "            # Transformer-based model, e.g., BERT\n",
    "            self.bert_model = BertModel.from_pretrained(model_name)\n",
    "            # Additional layer to produce fixed-size embeddings\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            # Generate outputs from the BERT model\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use the pooled output for creating embeddings\n",
    "            pooled_output = outputs.pooler_output\n",
    "            # Pass through the embedding layer\n",
    "            context_embeddings = self.embedding_layer(pooled_output)\n",
    "            return context_embeddings\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "            super(DPRQuestionEncoder, self).__init__()\n",
    "            self.bert = BertModel.from_pretrained(model_name)\n",
    "            self.embedding_layer = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, **kwargs):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            embeddings = self.embedding_layer(pooled_output)\n",
    "            return embeddings\n",
    "\n",
    "    # TransformerRAG class\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n",
    "            super(TransformerRAG, self).__init__()\n",
    "            self.context_encoder = CustomDPRContextEncoder().to(device)\n",
    "            self.language_model = LanguageModelTransformer(\n",
    "                vocab_size=vocab_size,\n",
    "                embed_size=256, \n",
    "                num_layers=6, \n",
    "                forward_expansion=4, \n",
    "                heads=8, \n",
    "                dropout=0, \n",
    "                max_length=100,  # Set to 512 to match the tokenization max_length\n",
    "                rank=16\n",
    "            ).to(device)\n",
    "            self.language_model = load_model_weights(self.language_model, language_model_path)\n",
    "            self.question_encoder = DPRQuestionEncoder().to(device)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "\n",
    "            # Process each context_text\n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "\n",
    "                # Create a tensor of zeros with the correct shape on the GPU\n",
    "                aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = context['input_ids'].to(device)\n",
    "                    context_attention_mask = context['attention_mask'].to(device)\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "\n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "\n",
    "            question_input_ids = question_input_ids.to(device).long()\n",
    "            question_attention_mask = question_attention_mask.to(device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "\n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=device))\n",
    "\n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(device) for k, v in tokenized_combined_input.items()}  # Move to GPU\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "\n",
    "            return response\n",
    "\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "            super(LORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "\n",
    "            # Original weight and bias of the linear layer\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # LORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "            super(QLORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "            self.quantization_bits = quantization_bits\n",
    "\n",
    "            # Original weight and bias\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # QLORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, embed_size, heads):\n",
    "            super(MultiHeadAttention, self).__init__()\n",
    "            self.embed_size = embed_size\n",
    "            self.heads = heads\n",
    "            self.head_dim = embed_size // heads\n",
    "\n",
    "            assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "            ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        def forward(self, values, keys, query, mask):\n",
    "            N = query.shape[0]\n",
    "            value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "            # Split the embedding into self.heads different pieces\n",
    "            values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "            queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            # Einsum does the matrix multiplication for query*keys for each training example\n",
    "            # with every other training example, then sum it up\n",
    "            attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "                N, query_len, self.heads * self.head_dim\n",
    "            )\n",
    "\n",
    "            out = self.fc_out(out)\n",
    "            return out\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "            super(TransformerBlock, self).__init__()\n",
    "            self.attention = MultiHeadAttention(embed_size, heads)\n",
    "            self.norm1 = nn.LayerNorm(embed_size)\n",
    "            self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, value, key, query, mask):\n",
    "            attention = self.attention(value, key, query, mask)\n",
    "            x = self.dropout(self.norm1(attention + query))\n",
    "            forward = self.feed_forward(x)\n",
    "            out = self.dropout(self.norm2(forward + x))\n",
    "            return out\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "            super(LanguageModelDecoder, self).__init__()\n",
    "            self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "            # Adding BatchNorm layers\n",
    "            self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "            self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # QLORA layers\n",
    "            self.qlora_feed_forward = nn.Sequential(\n",
    "                QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "            self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "            self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, trg_mask):\n",
    "            N, seq_length = x.shape\n",
    "            positions = torch.arange(0, seq_length).expand(N, seq_length) #.to(x.device)\n",
    "            x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn1(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, trg_mask)\n",
    "                if self.use_qlora:\n",
    "                    x = self.qlora_feed_forward(x)\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            out = self.fc_out(x)\n",
    "            #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "            return out\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.use_qlora = use_qlora\n",
    "\n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "            super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "            self.decoder = LanguageModelDecoder(\n",
    "                vocab_size,\n",
    "                embed_size,\n",
    "                num_layers,\n",
    "                heads,\n",
    "                forward_expansion,\n",
    "                dropout,\n",
    "                max_length,\n",
    "                rank,\n",
    "            )\n",
    "\n",
    "        def forward(self, trg):\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            out = self.decoder(trg, trg_mask)\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "                N, 1, trg_len, trg_len\n",
    "            ).to(trg.device)\n",
    "\n",
    "            return trg_mask\n",
    "        \n",
    "        # Function to enable or disable QLORA layers (for fine-tuning purposes)\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            # Assuming you have a forward method that returns logits\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Convert logits to probabilities\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # For simplicity, using greedy decoding here. You might want to use beam search or sampling.\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            \n",
    "            # Convert predicted token ids to tokens\n",
    "            predicted_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            \n",
    "            # Join tokens to form the response string. This is a very basic way to generate text and might not produce the best results.\n",
    "            response = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            \n",
    "            return response\n",
    "\n",
    "\n",
    "    def __init__(self, sparsity_factor, seq_len, head_dim, blk_size, input_dim, num_experts, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, device):\n",
    "        super().__init__()\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(seq_len, head_dim, blk_size, sparsity_factor)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = Expert.SwitchRouter(input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, device)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = Expert.QLORALayer(input_dim, input_dim, rank)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, _ = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of the Expert class\n",
    "# expert = Expert(sparsity_factor, seq_len, head_dim, blk_size, input_dim, num_experts, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expert: {\n",
    "1. SparseFlash2_attention\n",
    "2. LayerNorm + Dropout\n",
    "3. Internal Switch Routing Between Sub-models:\n",
    "a) Sub model 1: Transformer with DPO (Direct Preference Optimization)\n",
    "b) Sub model 2: Transformer with RAG(Retrieval Augmented Generation)\n",
    "c) Sub Model 3: MAMBA (Linear-Time Sequence Modelling with Selective State Spaces)\n",
    "4. QLORAc\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "class ExpertConfig:\n",
    "    def __init__(self, seq_len=512, head_dim=64, block_size=64, sparsity_factor=4,\n",
    "                 input_dim=512, num_experts=3, vocab_size=30522, embed_size=256,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0.1,\n",
    "                 max_length=100, rank=16, device='cpu',\n",
    "                 mamba_model_path='D:\\\\EXPERT_WEIGHTS\\\\mamba_model_weights.pth',\n",
    "                 context_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\context_encoder.pth',\n",
    "                 language_model_path='D:\\\\EXPERT_WEIGHTS\\\\language_model.pth',\n",
    "                 question_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\question_encoder.pth',\n",
    "                 dpo_model_path='D:\\\\EXPERT_WEIGHTS\\\\dpo_model_weights.pth',\n",
    "                 model_name='bert-base-uncased', embedding_dim=768,\n",
    "                 alpha=1, quantization_bits=8, tokenizer_name='bert-base-uncased',\n",
    "                 d_model=512, d_state=2048  , d_conv=3, expansion_factor=2):\n",
    "\n",
    "        # Common hyperparameters\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        self.input_dim = input_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length\n",
    "        self.rank = rank\n",
    "\n",
    "        # Model paths and device\n",
    "        self.mamba_model_path = mamba_model_path\n",
    "        self.context_encoder_path = context_encoder_path\n",
    "        self.language_model_path = language_model_path\n",
    "        self.question_encoder_path = question_encoder_path\n",
    "        self.dpo_model_path = dpo_model_path\n",
    "        self.device = device\n",
    "\n",
    "        # Unique hyperparameters\n",
    "        self.num_experts = num_experts\n",
    "        self.model_name = model_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        \n",
    "        # pdfs\n",
    "        self.pdf_file_paths = [r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "                        r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "                        r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "                        r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "                        r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf']\n",
    "        \n",
    "        self.rag_dataset = Expert.TransformerRAG.create_dataset_from_pdfs(self.pdf_file_paths)\n",
    "\n",
    "\n",
    "\n",
    "    def validate(self):\n",
    "        # Add validation checks here if needed\n",
    "        assert self.seq_len % self.block_size == 0, \"seq_len must be divisible by block_size\"\n",
    "        # Additional validations can be added as required\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    # Load model weights function\n",
    "    def load_model_weights(model, model_path):\n",
    "        #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        checkpoint = torch.load(model_path, map_location=config.device)\n",
    "\n",
    "        if isinstance(checkpoint, dict):\n",
    "            # Check for 'state_dict' or 'model_state_dict' keys\n",
    "            if 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            elif 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                # If no known key is found, try loading it as a raw state dictionary\n",
    "                try:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                except RuntimeError as e:\n",
    "                    raise ValueError(f\"Error loading state dict: {e}\")\n",
    "        elif isinstance(checkpoint, nn.Module):\n",
    "            # If the checkpoint is a model object, assign it directly\n",
    "            model = checkpoint\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    # Flash2_Attention\n",
    "    class FlashAttention2(nn.Module):\n",
    "        def __init__(self, sequence_length, head_dimension, block_size):\n",
    "            super(Expert.FlashAttention2, self).__init__()\n",
    "            self.block_size = block_size\n",
    "            # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "            assert sequence_length % block_size == 0\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            # Partitioning of inputs\n",
    "            Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "            # Efficient computation of the attention mechanism\n",
    "            outputs = []\n",
    "            for i, Q_block in enumerate(Q_blocks):\n",
    "                output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "                outputs.append(output_block)\n",
    "\n",
    "            # Concatenating the processed blocks\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "            return output\n",
    "\n",
    "        def partition_inputs(self, Q, K, V):\n",
    "            # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "            Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "            K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "            V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "            return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "        def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "            # Process each block efficiently as per FLASH2's optimized method\n",
    "            # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "            output_blocks = []\n",
    "            for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "                attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "                attention_scores = self.online_softmax(attention_scores)\n",
    "                output_block = torch.matmul(attention_scores, V_block)\n",
    "                output_blocks.append(output_block)\n",
    "\n",
    "            # Summing up the results from each block\n",
    "            output_block_sum = sum(output_blocks)\n",
    "            return output_block_sum\n",
    "\n",
    "        def online_softmax(self, scores, chunk_size=128):\n",
    "            # Apply softmax in chunks for large sequences\n",
    "            softmaxed_scores = []\n",
    "            for i in range(0, scores.size(0), chunk_size):\n",
    "                chunk = scores[i:i + chunk_size, :]\n",
    "                softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "                softmaxed_scores.append(softmaxed_chunk)\n",
    "            return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "    # SparseFlash2_Attention\n",
    "    class SparseFlash2Attention(nn.Module):\n",
    "        def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "            super().__init__()\n",
    "            self.flash_attention = Expert.FlashAttention2(seq_len, head_dim, blk_size)\n",
    "            self.seq_len = seq_len\n",
    "            self.head_dim = head_dim\n",
    "            self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "            self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        def generate_sparsity_mask(self):\n",
    "            mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "            step = self.sparsity_factor\n",
    "            for i in range(0, self.seq_len, step):\n",
    "                mask[i:i + step, :] = 1\n",
    "            return mask.bool()\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "            # Reshape output to be 3D for batch matrix multiplication\n",
    "            output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "            sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "            # Apply the sparsity mask to the output\n",
    "            sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "            output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "            # Reshape the output back to 2D\n",
    "            output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "            return output\n",
    "\n",
    "\n",
    "    # MAMBA\n",
    "    # RoPE\n",
    "    class RotaryPositionalEncoding(nn.Module):\n",
    "        def __init__(self, dim, max_len=5000):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "            t = torch.arange(max_len).type_as(inv_freq)\n",
    "            freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "            self.register_buffer('sin', freqs.sin())\n",
    "            self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "        def forward(self, x):\n",
    "            n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "            sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "            # Apply RoPE to even and odd indices separately\n",
    "            x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "    # SWIGLU\n",
    "    class SwiGLU(nn.Module):\n",
    "        def __init__(self, dim_in, dim_out):\n",
    "            super(Expert.SwiGLU, self).__init__()\n",
    "            self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "            self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "        def forward(self, x):\n",
    "            gate = torch.sigmoid(self.fc2(x))\n",
    "            return self.fc1(x) * gate\n",
    "\n",
    "    class SimplifiedMAMBA(nn.Module):\n",
    "        # Adjusted to include SwiGLU blocks\n",
    "        def __init__(self, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "            super().__init__()\n",
    "\n",
    "            self.num_layers = num_layers\n",
    "            self.d_model = d_model\n",
    "            self.d_state = d_state\n",
    "            self.d_conv = d_conv\n",
    "            self.expansion_factor = expansion_factor\n",
    "            self.feedforward = nn.Sequential(\n",
    "                nn.Linear(d_model, d_state),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(d_state, d_model)\n",
    "            )\n",
    "            self.input_embedding = nn.Linear(d_model, d_model)\n",
    "            self.convs = nn.Sequential(*[nn.Conv1d(d_model, d_model, kernel_size=d_conv, padding=(d_conv // 2)) for _ in range(num_layers)])\n",
    "            self.swiglu = Expert.SwiGLU(d_model, d_model)\n",
    "            self.output_projection = nn.Linear(d_model, d_model * expansion_factor)  # Adjusted to match the output of SwiGLU\n",
    "\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "            nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "            nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "            nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "        def forward(self, inputs, attention_mask=None):\n",
    "            print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "            # Apply the attention mask if provided\n",
    "            if attention_mask is not None:\n",
    "                inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "            projected_inputs = self.input_embedding(inputs)\n",
    "            print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            for conv in self.convs:\n",
    "                projected_inputs = conv(projected_inputs)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = self.swiglu(projected_inputs)\n",
    "            print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "            output = self.output_projection(projected_inputs)\n",
    "            print(\"output shape:\", output.shape)\n",
    "\n",
    "            return output\n",
    "\n",
    "    class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "        # Including rotary positional encodings if required\n",
    "        def __init__(self, vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "            super().__init__()\n",
    "\n",
    "            self.vocab_size = vocab_size\n",
    "            self.num_layers = num_layers\n",
    "            self.d_model = d_model\n",
    "            self.d_state = d_state\n",
    "            self.d_conv = d_conv\n",
    "            self.expansion_factor = expansion_factor\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "            self.pos_encoder = Expert.RotaryPositionalEncoding(d_model)\n",
    "            self.simplified_mamba = Expert.SimplifiedMAMBA(num_layers, d_model, d_state, d_conv, expansion_factor)\n",
    "            self.output_projection = nn.Linear(d_model*2, vocab_size)\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = 1.0\n",
    "\n",
    "            nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "        def forward(self, input_values, attention_mask):\n",
    "            embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "            embedded = self.pos_encoder(embedded)\n",
    "            simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "            logits = self.output_projection(simplified_mamba_output)\n",
    "            return logits\n",
    "\n",
    "\n",
    "    class SwitchRouter(nn.Module):\n",
    "        CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "        class SwitchGate(nn.Module):\n",
    "            def __init__(self, input_dim, num_experts):\n",
    "                super(Expert.SwitchRouter.SwitchGate, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "                self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "                return gate_scores\n",
    "\n",
    "        def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank, d_model, d_state, d_conv, expansion_factor, device):\n",
    "            super(Expert.SwitchRouter, self).__init__()\n",
    "            self.device = device\n",
    "            self.router = self.SwitchGate(input_dim, num_experts).to(device)\n",
    "            self.transformer_rag = Expert.TransformerRAG(context_encoder_path, language_model_path, question_encoder_path, vocab_size).to(device)\n",
    "            self.lmt = Expert.LanguageModelTransformer(vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank).to(device)\n",
    "            self.transformer_dpo = Expert.DPO(self.lmt, device).to(device)\n",
    "            #self.lmt = Expert.load_model_weights(self.transformer_dpo, config.dpo_model_path)\n",
    "            self.mamba = Expert.SimplifiedLanguageModelMAMBA(vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor).to(device)\n",
    "            #self.mamba = Expert.load_model_weights(self.mamba, config.mamba_model_path)\n",
    "            self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "            self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "        def forward(self, x, attention_mask, context_texts, question_text):\n",
    "            x = x.to(self.device)\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.float()\n",
    "\n",
    "            x = self.input_embedding(x)\n",
    "            gate_scores = self.router(x)\n",
    "            expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "            expert_indices = self.route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "            final_output = torch.zeros_like(x)\n",
    "\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                mask = expert_indices == i\n",
    "                if mask.any():\n",
    "                    selected_inputs = x[mask]\n",
    "                    selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                    if isinstance(expert, TransformerRAG):\n",
    "                        expert_output = expert(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                    else:\n",
    "                        expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                    final_output[mask] = expert_output\n",
    "\n",
    "            aux_loss = self.auxiliary_loss(gate_scores)\n",
    "            return final_output, aux_loss\n",
    "\n",
    "        @staticmethod\n",
    "        def auxiliary_loss(gate_scores):\n",
    "            expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "            loss_balancing = torch.std(expert_load)\n",
    "            return loss_balancing\n",
    "\n",
    "        @staticmethod\n",
    "        def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "            capacity_factor_tensor = torch.tensor([SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "            capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "            expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "            for idx in range(len(expert_indices)):\n",
    "                selected_expert = expert_indices[idx]\n",
    "                if expert_counts[selected_expert] < capacities[0]:\n",
    "                    expert_counts[selected_expert] += 1\n",
    "                else:\n",
    "                    available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                    if len(available_experts) > 0:\n",
    "                        alternative_expert = available_experts[0]\n",
    "                        expert_indices[idx] = alternative_expert\n",
    "                        expert_counts[alternative_expert] += 1\n",
    "                    else:\n",
    "                        print(\"No available experts to reroute. Handling overflow.\")\n",
    "            return expert_indices\n",
    "\n",
    "    class CustomDPRContextEncoder(nn.Module):\n",
    "        def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "            super(Expert.CustomDPRContextEncoder, self).__init__()\n",
    "            # Transformer-based model, e.g., BERT\n",
    "            self.bert_model = BertModel.from_pretrained(model_name)\n",
    "            # Additional layer to produce fixed-size embeddings\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            # Generate outputs from the BERT model\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use the pooled output for creating embeddings\n",
    "            pooled_output = outputs.pooler_output\n",
    "            # Pass through the embedding layer\n",
    "            context_embeddings = self.embedding_layer(pooled_output)\n",
    "            return context_embeddings\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "            super(Expert.DPRQuestionEncoder, self).__init__()\n",
    "            self.bert = BertModel.from_pretrained(model_name)\n",
    "            self.embedding_layer = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, **kwargs):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            embeddings = self.embedding_layer(pooled_output)\n",
    "            return embeddings\n",
    "\n",
    "    # TransformerRAG class\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n",
    "            super(Expert.TransformerRAG, self).__init__()\n",
    "            self.context_encoder = Expert.CustomDPRContextEncoder().to(config.device)\n",
    "            self.language_model = Expert.LanguageModelTransformer(\n",
    "                vocab_size=vocab_size,\n",
    "                embed_size=256, \n",
    "                num_layers=6, \n",
    "                forward_expansion=4, \n",
    "                heads=8, \n",
    "                dropout=0, \n",
    "                max_length=100,  # Set to 512 to match the tokenization max_length\n",
    "                rank=16\n",
    "            ).to(config.device)\n",
    "            #self.language_model = Expert.load_model_weights(self.language_model, config.language_model_path)\n",
    "            self.question_encoder = Expert.DPRQuestionEncoder().to(config.device)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "\n",
    "            # Process each context_text\n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "\n",
    "                # Create a tensor of zeros with the correct shape on the GPU\n",
    "                aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = context['input_ids'].to(config.device)\n",
    "                    context_attention_mask = context['attention_mask'].to(config.device)\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "\n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "\n",
    "            question_input_ids = question_input_ids.to(config.device).long()\n",
    "            question_attention_mask = question_attention_mask.to(config.device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "\n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=config.device))\n",
    "\n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(config.device) for k, v in tokenized_combined_input.items()}  # Move to GPU\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "\n",
    "            return response\n",
    "\n",
    "        # 4. RAG\n",
    "        @staticmethod\n",
    "        def extract_text_from_pdf(file_path):\n",
    "            text = \"\"\n",
    "            with fitz.open(file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            return text\n",
    "        \n",
    "        @staticmethod\n",
    "        def split_into_chunks(text, chunk_size):\n",
    "            # Split the text into chunks of chunk_size\n",
    "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        \n",
    "        @staticmethod\n",
    "        def preprocess_text(text, max_length=512):\n",
    "            # Initialize tokenizer\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            # Split the text into smaller chunks to maintain context\n",
    "            # The chunk size is slightly less than max_length to account for special tokens\n",
    "            chunk_size = max_length - 50  # Adjust this value based on the model's requirements\n",
    "            text_chunks = Expert.TransformerRAG.split_into_chunks(text, chunk_size)\n",
    "\n",
    "            # Process each chunk\n",
    "            processed_chunks = []\n",
    "            for chunk in text_chunks:\n",
    "                tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "                processed_chunk = {\n",
    "                    'input_ids': tokenized_output['input_ids'],\n",
    "                    'attention_mask': tokenized_output['attention_mask']\n",
    "                }\n",
    "                processed_chunks.append(processed_chunk)\n",
    "\n",
    "            return processed_chunks\n",
    "        \n",
    "        @staticmethod\n",
    "        def create_dataset_from_pdfs(pdf_file_paths):\n",
    "            dataset = []\n",
    "            for file_path in pdf_file_paths:\n",
    "                text = Expert.TransformerRAG.extract_text_from_pdf(file_path)\n",
    "                processed_text = Expert.TransformerRAG.preprocess_text(text)\n",
    "                dataset.append(processed_text)\n",
    "            return dataset\n",
    "\n",
    "        def retrieve_contexts(self, dataset, query_embedding, top_k=5):\n",
    "            # Assume dataset is a list of dictionaries with each dictionary containing 'input_ids' and 'attention_mask'\n",
    "            # for a particular context and that each context has been processed through a DPRContextEncoder to get embeddings\n",
    "\n",
    "            # Placeholder for storing similarity scores\n",
    "            similarity_scores = []\n",
    "\n",
    "            # Iterate over each context in the dataset\n",
    "            for context in dataset:\n",
    "                context_input_ids = context['input_ids']\n",
    "                context_attention_mask = context['attention_mask']\n",
    "\n",
    "                # Assuming context_encoder is an instance of CustomDPRContextEncoder that's already trained\n",
    "                # and available in your scope\n",
    "                context_embedding = context_encoder(context_input_ids, context_attention_mask)\n",
    "\n",
    "                # Compute similarity (e.g., using dot product)\n",
    "                similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "\n",
    "                similarity_scores.append(similarity.squeeze().item())\n",
    "\n",
    "            # Sort contexts based on similarity scores and retrieve top_k indices\n",
    "            top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "\n",
    "            # Retrieve top_k contexts\n",
    "            top_contexts = [dataset[i] for i in top_k_indices]\n",
    "\n",
    "            return top_contexts\n",
    "\n",
    "        def rag_retrieve_and_generate(dataset, query):\n",
    "            # Instantiate the question encoder\n",
    "            question_encoder = DPRQuestionEncoder()\n",
    "\n",
    "            # Encode the query\n",
    "            encoded_query = question_encoder(query)\n",
    "\n",
    "            # Retrieve relevant context\n",
    "            # This involves finding the most similar documents in the dataset\n",
    "            # For simplicity, this is represented as a function 'retrieve_contexts'\n",
    "            relevant_contexts = retrieve_contexts(dataset, encoded_query)\n",
    "\n",
    "            # Language model for generation\n",
    "            language_model = LanguageModelTransformer()\n",
    "\n",
    "            # Generate a response based on the retrieved contexts\n",
    "            # This step may involve further formatting or preprocessing\n",
    "            response = language_model.generate_response(relevant_contexts)\n",
    "\n",
    "            return response\n",
    "\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "            super(Expert.LORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "\n",
    "            # Original weight and bias of the linear layer\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # LORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "            super(Expert.QLORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "            self.quantization_bits = quantization_bits\n",
    "\n",
    "            # Original weight and bias\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # QLORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, embed_size, heads):\n",
    "            super(Expert.MultiHeadAttention, self).__init__()\n",
    "            self.embed_size = embed_size\n",
    "            self.heads = heads\n",
    "            self.head_dim = embed_size // heads\n",
    "\n",
    "            assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "            ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        def forward(self, values, keys, query, mask):\n",
    "            N = query.shape[0]\n",
    "            value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "            # Split the embedding into self.heads different pieces\n",
    "            values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "            queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            # Einsum does the matrix multiplication for query*keys for each training example\n",
    "            # with every other training example, then sum it up\n",
    "            attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "                N, query_len, self.heads * self.head_dim\n",
    "            )\n",
    "\n",
    "            out = self.fc_out(out)\n",
    "            return out\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "            super(Expert.TransformerBlock, self).__init__()\n",
    "            self.attention = Expert.MultiHeadAttention(embed_size, heads)\n",
    "            self.norm1 = nn.LayerNorm(embed_size)\n",
    "            self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                Expert.LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, value, key, query, mask):\n",
    "            attention = self.attention(value, key, query, mask)\n",
    "            x = self.dropout(self.norm1(attention + query))\n",
    "            forward = self.feed_forward(x)\n",
    "            out = self.dropout(self.norm2(forward + x))\n",
    "            return out\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "            super(Expert.LanguageModelDecoder, self).__init__()\n",
    "            self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "            # Adding BatchNorm layers\n",
    "            self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "            self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    Expert.TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # QLORA layers\n",
    "            self.qlora_feed_forward = nn.Sequential(\n",
    "                Expert.QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "            self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "            self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, trg_mask):\n",
    "            N, seq_length = x.shape\n",
    "            positions = torch.arange(0, seq_length).expand(N, seq_length).to(config.device)\n",
    "            x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "            print(f\"Max position index: {positions.max().item()}, Position Embedding Size: {self.position_embedding.num_embeddings}\")\n",
    "\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn1(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, trg_mask)\n",
    "                if self.use_qlora:\n",
    "                    x = self.qlora_feed_forward(x)\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            out = self.fc_out(x)\n",
    "            #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "            return out\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.use_qlora = use_qlora\n",
    "\n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16, tokenizer_name='bert-base-uncased'):\n",
    "            super(Expert.LanguageModelTransformer, self).__init__()\n",
    "\n",
    "            self.decoder = Expert.LanguageModelDecoder(\n",
    "                vocab_size,\n",
    "                embed_size,\n",
    "                num_layers,\n",
    "                heads,\n",
    "                forward_expansion,\n",
    "                dropout,\n",
    "                max_length,\n",
    "                rank,\n",
    "            )\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "\n",
    "        def forward(self, trg):\n",
    "            print(f\"Input shape to LanguageModelTransformer: {trg.shape}\")\n",
    "\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            out = self.decoder(trg, trg_mask)\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "                N, 1, trg_len, trg_len\n",
    "            ).to(trg.device)\n",
    "\n",
    "            return trg_mask\n",
    "        \n",
    "        # Function to enable or disable QLORA layers (for fine-tuning purposes)\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = [self.tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            return response\n",
    "\n",
    "    class DPO(nn.Module):\n",
    "        def __init__(self, language_model_transformer, device):\n",
    "            super().__init__()\n",
    "            self.language_model_transformer = language_model_transformer\n",
    "            self.device = device\n",
    "\n",
    "        def forward(self, input_ids_question, input_ids_chosen, input_ids_rejected, labels):\n",
    "            # Concatenate or otherwise combine the inputs from the DPO dataset\n",
    "            # Here we assume concatenation for simplicity. Adjust as necessary for your model.\n",
    "            combined_input_ids = torch.cat((input_ids_question, input_ids_chosen, input_ids_rejected), dim=1)\n",
    "\n",
    "            # Send the combined inputs through the language model transformer\n",
    "            logits = self.language_model_transformer(combined_input_ids)\n",
    "\n",
    "            # Calculate loss if labels are provided\n",
    "            if labels is not None:\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, labels.float())\n",
    "                return logits, loss\n",
    "            return logits\n",
    "\n",
    "\n",
    "    def __init__(self, config: ExpertConfig):\n",
    "        super().__init__()\n",
    "        # Validate configuration\n",
    "        config.validate()\n",
    "\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(\n",
    "            config.seq_len, \n",
    "            config.head_dim, \n",
    "            config.block_size, \n",
    "            config.sparsity_factor\n",
    "        )\n",
    "\n",
    "                # Initialize your sub-models here\n",
    "        self.lmt = Expert.LanguageModelTransformer(\n",
    "            config.vocab_size, \n",
    "            config.embed_size, \n",
    "            config.num_layers, \n",
    "            config.forward_expansion, \n",
    "            config.heads, \n",
    "            config.dropout, \n",
    "            config.max_length, \n",
    "            config.rank\n",
    "        ).to(config.device)\n",
    "        \n",
    "        # Now initialize DPO with the language model transformer\n",
    "        self.transformer_dpo = Expert.DPO(self.lmt, config.device)\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(config.input_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = Expert.SwitchRouter(\n",
    "            config.input_dim, \n",
    "            config.num_experts, \n",
    "            config.mamba_model_path,\n",
    "            config.context_encoder_path, \n",
    "            config.language_model_path,\n",
    "            config.question_encoder_path, \n",
    "            config.dpo_model_path,\n",
    "            config.vocab_size, \n",
    "            config.embed_size, \n",
    "            config.num_layers,\n",
    "            config.forward_expansion, \n",
    "            config.heads, \n",
    "            config.dropout,\n",
    "            config.max_length, \n",
    "            config.rank,\n",
    "            config.d_model,  \n",
    "            config.d_state,  \n",
    "            config.d_conv,   \n",
    "            config.expansion_factor,  \n",
    "            config.device    \n",
    "        )\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = Expert.QLORALayer(config.input_dim, config.input_dim, config.rank)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, aux_loss = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "        \"\"\"\n",
    "        Calculate a new alpha value based on the current loss.\n",
    "        \"\"\"\n",
    "        if current_loss >= initial_loss:\n",
    "            return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "        loss_ratio = current_loss / initial_loss\n",
    "        alpha_range = initial_alpha - final_alpha\n",
    "        new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "        return new_alpha  \n",
    "    \n",
    "    def train_dpo(self, train_loader, optimizer, label_column, input_columns, config, save_path):\n",
    "        self.transformer_dpo.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):  # Include i here to use it in your print statements\n",
    "            print(f\"Batch {i+1}:\")\n",
    "            print(f\"input_ids_question shape: {batch['input_ids_question'].shape}\")\n",
    "            print(f\"input_ids_chosen shape: {batch['input_ids_chosen'].shape}\")\n",
    "            print(f\"input_ids_rejected shape: {batch['input_ids_rejected'].shape}\")\n",
    "            print(f\"Labels shape: {batch['labels'].shape}\")\n",
    "            input_ids_question = batch['input_ids_question'].to(config.device)\n",
    "            input_ids_chosen = batch['input_ids_chosen'].to(config.device)\n",
    "            input_ids_rejected = batch['input_ids_rejected'].to(config.device)\n",
    "            labels = batch['labels'].to(config.device)\n",
    "\n",
    "            assert input_ids_question.max() < config.vocab_size, \"Question IDs exceed vocab size\"\n",
    "            assert input_ids_chosen.max() < config.vocab_size, \"Chosen IDs exceed vocab size\"\n",
    "            assert input_ids_rejected.max() < config.vocab_size, \"Rejected IDs exceed vocab size\"\n",
    "\n",
    "            print(f\"input_ids_question shape: {input_ids_question.shape}\")\n",
    "            print(f\"input_ids_chosen shape: {input_ids_chosen.shape}\")\n",
    "            print(f\"input_ids_rejected shape: {input_ids_rejected.shape}\")\n",
    "\n",
    "            print(\"Max index in input_ids_question:\", torch.max(input_ids_question).item())\n",
    "            print(\"Max index in input_ids_chosen:\", torch.max(input_ids_chosen).item())\n",
    "            print(\"Max index in input_ids_rejected:\", torch.max(input_ids_rejected).item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits, loss = self.transformer_dpo(input_ids_question, input_ids_chosen, input_ids_rejected, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(self.transformer_dpo.state_dict(), save_path)\n",
    "        return average_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_language_model_rag(self, model, save_path, train_loader, device, num_epochs=5, lr=1e-8, weight_decay=1e-4):\n",
    "        # Enable QLORA during training\n",
    "        model.decoder.toggle_qlora(True)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "\n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                targets = batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        # Toggle QLORA off after training\n",
    "        model.decoder.toggle_qlora(False)\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"Training Complete\")\n",
    "        return model, average_loss\n",
    "\n",
    "    def train_dpr_encoders(self, train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs , context_save_path, question_save_path):\n",
    "        loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(train_data[\"queries\"])):\n",
    "                query = train_data[\"queries\"][i]\n",
    "                context = train_data[\"contexts\"][i]\n",
    "\n",
    "                # Ensure query is a string\n",
    "                if not isinstance(query, str):\n",
    "                    raise ValueError(\"Query must be a string.\")\n",
    "                tokenized_query = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                # Ensure context is a string\n",
    "                if isinstance(context, dict):\n",
    "                    # If context is a dictionary, extract the text content. This is a placeholder and might need adjustment.\n",
    "                    context_text = context.get(\"text\", \"\")\n",
    "                elif isinstance(context, str):\n",
    "                    context_text = context\n",
    "                else:\n",
    "                    raise ValueError(\"Context must be a string or a dictionary containing a text field.\")\n",
    "                tokenized_context = tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                question_embeddings = question_encoder(input_ids=tokenized_query['input_ids'], attention_mask=tokenized_query['attention_mask'])\n",
    "                context_embeddings = context_encoder(input_ids=tokenized_context['input_ids'], attention_mask=tokenized_context['attention_mask'])\n",
    "\n",
    "                # The labels tensor should have the same first dimension size as the input tensors\n",
    "                labels = torch.tensor([1.0] * question_embeddings.size(0), dtype=torch.float).to(question_embeddings.device)\n",
    "\n",
    "                loss = loss_function(question_embeddings, context_embeddings, labels)\n",
    "\n",
    "                optimizer_context.zero_grad()\n",
    "                optimizer_question.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_context.step()\n",
    "                optimizer_question.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_data['queries'])}\")\n",
    "        average_loss = total_loss / len(train_data['queries'])\n",
    "        torch.save(context_encoder.state_dict(), context_save_path)\n",
    "        torch.save(question_encoder.state_dict(), question_save_path)\n",
    "        return (context_encoder, question_encoder), average_loss\n",
    "       \n",
    "    def train_language_model_transformer(self, train_loader, device, vocab_size, save_path):\n",
    "        model = self.lmt\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "        num_epochs = 5\n",
    "        \n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            model.decoder.toggle_qlora(True)\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs, targets = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        return model, average_loss\n",
    "\n",
    "\n",
    "    def train_expert(self, train_loader, train_data, optimizer, main_loss_function, aux_loss_weight, device,save_path, accumulation_steps=4, num_epochs=5):\n",
    "            self.train()  # Set the model to training mode\n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0\n",
    "                optimizer.zero_grad()  # Initialize gradients to zero\n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "                    inputs, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "\n",
    "                    # Calculate start and end indices for current batch in train_data\n",
    "                    start_idx = batch_idx * batch['input_ids'].size(0)\n",
    "                    end_idx = start_idx + batch['input_ids'].size(0)\n",
    "\n",
    "                    # Extract current_queries and current_contexts for the batch\n",
    "                    current_queries = train_data['queries'][start_idx:end_idx]\n",
    "                    current_contexts = train_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "                    # Call to the model forward function\n",
    "                    outputs, aux_loss = self(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "                    # Calculate loss and accumulate\n",
    "                    main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                    loss = (main_loss + aux_loss_weight * aux_loss) / accumulation_steps\n",
    "                    loss.backward()\n",
    "\n",
    "                    if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    total_loss += loss.item() * accumulation_steps  # Scale back up\n",
    "\n",
    "                average_loss = total_loss / len(train_loader)\n",
    "                print(f'End of Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            torch.save(self.state_dict(), save_path)\n",
    "            return self, average_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model vocab size: 30522\n",
      "Model embedding size: 256\n",
      "Configured max length: 100\n",
      "Batch 1:\n",
      "input_ids_question shape: torch.Size([2, 512])\n",
      "input_ids_chosen shape: torch.Size([2, 512])\n",
      "input_ids_rejected shape: torch.Size([2, 512])\n",
      "Labels shape: torch.Size([2])\n",
      "input_ids_question shape: torch.Size([2, 512])\n",
      "input_ids_chosen shape: torch.Size([2, 512])\n",
      "input_ids_rejected shape: torch.Size([2, 512])\n",
      "Max index in input_ids_question: 22507\n",
      "Max index in input_ids_chosen: 29486\n",
      "Max index in input_ids_rejected: 28415\n",
      "Input shape to LanguageModelTransformer: torch.Size([2, 1536])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m label_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     52\u001b[0m input_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 53\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mexpert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     56\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(expert_model\u001b[38;5;241m.\u001b[39mtransformer_dpo\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n",
      "Cell \u001b[1;32mIn[15], line 1013\u001b[0m, in \u001b[0;36mExpert.train_dpo\u001b[1;34m(self, train_loader, optimizer, label_column, input_columns, config, save_path)\u001b[0m\n\u001b[0;32m   1010\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m-> 1013\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_dpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_chosen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_rejected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 888\u001b[0m, in \u001b[0;36mExpert.DPO.forward\u001b[1;34m(self, input_ids_question, input_ids_chosen, input_ids_rejected, labels)\u001b[0m\n\u001b[0;32m    885\u001b[0m combined_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((input_ids_question, input_ids_chosen, input_ids_rejected), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    887\u001b[0m \u001b[38;5;66;03m# Send the combined inputs through the language model transformer\u001b[39;00m\n\u001b[1;32m--> 888\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;66;03m# Calculate loss if labels are provided\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 853\u001b[0m, in \u001b[0;36mExpert.LanguageModelTransformer.forward\u001b[1;34m(self, trg)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape to LanguageModelTransformer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrg\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    852\u001b[0m trg_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_trg_mask(trg)\n\u001b[1;32m--> 853\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 804\u001b[0m, in \u001b[0;36mExpert.LanguageModelDecoder.forward\u001b[1;34m(self, x, trg_mask)\u001b[0m\n\u001b[0;32m    802\u001b[0m N, seq_length \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    803\u001b[0m positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, seq_length)\u001b[38;5;241m.\u001b[39mexpand(N, seq_length)\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 804\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax position index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpositions\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Position Embedding Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding\u001b[38;5;241m.\u001b[39mnum_embeddings\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# Transpose for BatchNorm, apply batch normalization, and then transpose back\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def preprocess_dpo_data(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    print(f\"Tokenizer vocab size: {len(tokenizer.vocab)}\")\n",
    "\n",
    "    # Tokenize 'question', 'chosen', and 'rejected' fields\n",
    "    tokenized_questions = tokenizer(examples['question'], padding='max_length', truncation=True)\n",
    "    tokenized_chosen = tokenizer(examples['chosen'], padding='max_length', truncation=True)\n",
    "    tokenized_rejected = tokenizer(examples['rejected'], padding='max_length', truncation=True)\n",
    "\n",
    "    # Prepare the labels: 1 for 'chosen' and 0 for 'rejected'\n",
    "    # The labels should be interleaved since we have one 'chosen' and one 'rejected' per question\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(len(examples['question']))]\n",
    "\n",
    "    return {\n",
    "        'input_ids_question': tokenized_questions['input_ids'],\n",
    "        'attention_mask_question': tokenized_questions['attention_mask'],\n",
    "        'input_ids_chosen': tokenized_chosen['input_ids'],\n",
    "        'attention_mask_chosen': tokenized_chosen['attention_mask'],\n",
    "        'input_ids_rejected': tokenized_rejected['input_ids'],\n",
    "        'attention_mask_rejected': tokenized_rejected['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "# Load the DPO dataset from Hugging Face\n",
    "dpo_dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "\n",
    "# Apply the preprocessing to the dataset\n",
    "dpo_dataset = dpo_dataset.map(preprocess_dpo_data, batched=True)\n",
    "\n",
    "# You can convert to PyTorch tensors after processing\n",
    "dpo_dataset.set_format(type='torch', columns=['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(dpo_dataset['train'], batch_size=2, shuffle=True)\n",
    "\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "expert_model = Expert(config)\n",
    "\n",
    "model_vocab_size = expert_model.lmt.decoder.word_embedding.num_embeddings\n",
    "print(f\"Model vocab size: {model_vocab_size}\")\n",
    "print(f\"Model embedding size: {expert_model.lmt.decoder.word_embedding.embedding_dim}\")\n",
    "print(f\"Configured max length: {config.max_length}\")\n",
    "\n",
    "optimizer = AdamW(expert_model.parameters(), lr=1e-5)\n",
    "save_path = 'D:/EXPERT_WEIGHTS/dpo_model.pth'\n",
    "\n",
    "# Train the DPO model\n",
    "label_column = 'labels'\n",
    "input_columns = ['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
    "avg_loss = expert_model.train_dpo(train_loader, optimizer, label_column, input_columns, config, save_path)\n",
    "\n",
    "# Save the model\n",
    "torch.save(expert_model.transformer_dpo.state_dict(), save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48d1ab0399e48eaae73786005fa66c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 4 named input_ids expected length 1000 but got length 3000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 41\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: input_ids,\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: attention_mask,\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: labels\n\u001b[0;32m     38\u001b[0m     }\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing to the dataset\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m dpo_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdpo_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_dpo_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Create a DataLoader\u001b[39;00m\n\u001b[0;32m     44\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(dpo_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\dataset_dict.py:853\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 853\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\dataset_dict.py:854\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    853\u001b[0m     {\n\u001b[1;32m--> 854\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    874\u001b[0m     }\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3091\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   3092\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3093\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3094\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3095\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3096\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3097\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3098\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3099\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:3493\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3491\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(batch))\n\u001b[0;32m   3492\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3493\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3494\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_writer.py:558\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    556\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[0;32m    557\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m--> 558\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\pyarrow\\table.pxi:3674\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\pyarrow\\table.pxi:2837\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Column 4 named input_ids expected length 1000 but got length 3000"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "\n",
    "# Set the environment variable for CUDA\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Assuming ExpertConfig and Expert classes are already defined\n",
    "config = ExpertConfig()\n",
    "expert_model = Expert(config)\n",
    "\n",
    "# Load the DPO dataset from Hugging Face\n",
    "dpo_dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "# Process and prepare the DPO dataset for training\n",
    "def preprocess_dpo_data(examples):\n",
    "    # Tokenize the 'question', 'chosen', and 'rejected' fields\n",
    "    questions = tokenizer(examples['question'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "    chosen_answers = tokenizer(examples['chosen'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "    rejected_answers = tokenizer(examples['rejected'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Here, create labels based on your specific needs. This is just a placeholder.\n",
    "    # For instance, you could assign 1 to chosen and 0 to rejected, or vice versa.\n",
    "    # Ensure that labels are a one-dimensional tensor with the same length as the number of examples\n",
    "    labels = torch.tensor([1] * len(examples['question']), dtype=torch.float)\n",
    "\n",
    "    # Return processed examples without concatenating the input_ids and attention_masks\n",
    "    return {\n",
    "        'input_ids_question': questions['input_ids'],\n",
    "        'attention_mask_question': questions['attention_mask'],\n",
    "        'input_ids_chosen': chosen_answers['input_ids'],\n",
    "        'attention_mask_chosen': chosen_answers['attention_mask'],\n",
    "        'input_ids_rejected': rejected_answers['input_ids'],\n",
    "        'attention_mask_rejected': rejected_answers['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Apply the preprocessing to the dataset\n",
    "dpo_dataset = dpo_dataset.map(preprocess_dpo_data, batched=True)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(dpo_dataset['train'], batch_size=8, shuffle=True)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(expert_model.parameters(), lr=1e-5)  # Update this if your model has a different parameter structure\n",
    "\n",
    "# Define the path where the model will be saved\n",
    "save_path = 'D:/EXPERT_WEIGHTS/dpo_model.pth'\n",
    "\n",
    "# Train the DPO model using the updated train_dpo method\n",
    "model, avg_loss = expert_model.train_dpo(train_loader, optimizer, save_path)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Expert and DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "class ExpertConfig:\n",
    "    def __init__(self, seq_len=512, head_dim=64, block_size=64, sparsity_factor=4,\n",
    "                 input_dim=512, num_experts=3, vocab_size=30522, embed_size=256,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0.1,\n",
    "                 max_length=512,  # Updated to match seq_len for consistency\n",
    "                 rank=16, device='cpu',\n",
    "                 mamba_model_path='D:\\\\EXPERT_WEIGHTS\\\\mamba_model_weights.pth',\n",
    "                 rag_model_path='D:\\\\EXPERT_WEIGHTS\\\\rag_model_weights.pth',\n",
    "                 context_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\context_encoder.pth',\n",
    "                 language_model_path='D:\\\\EXPERT_WEIGHTS\\\\language_model.pth',\n",
    "                 question_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\question_encoder.pth',\n",
    "                 dpo_model_path='D:\\\\EXPERT_WEIGHTS\\\\dpo_model_weights.pth',\n",
    "                 model_name='bert-base-uncased', embedding_dim=768,\n",
    "                 alpha=1, quantization_bits=8, tokenizer_name='bert-base-uncased',\n",
    "                 d_model=512, d_state=2048, d_conv=3, expansion_factor=2):\n",
    "\n",
    "        # Common hyperparameters\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        self.input_dim = input_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length  # Ensure this is properly reflected in model components\n",
    "        self.rank = rank\n",
    "\n",
    "        # Model paths and device\n",
    "        self.mamba_model_path = mamba_model_path\n",
    "        self.context_encoder_path = context_encoder_path\n",
    "        self.language_model_path = language_model_path\n",
    "        self.question_encoder_path = question_encoder_path\n",
    "        self.dpo_model_path = dpo_model_path\n",
    "        self.device = device\n",
    "\n",
    "        # Unique hyperparameters\n",
    "        self.num_experts = num_experts\n",
    "        self.model_name = model_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        # PDFs (unchanged)\n",
    "        self.pdf_file_paths = [\n",
    "            r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "            r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf'\n",
    "        ]\n",
    "        \n",
    "        # Preserving original dataset loading functionality\n",
    "        self.rag_dataset = Expert.TransformerRAG.create_dataset_from_pdfs(self.pdf_file_paths)\n",
    "\n",
    "    def validate(self):\n",
    "        assert self.seq_len % self.block_size == 0, \"seq_len must be divisible by block_size\"\n",
    "        assert self.max_length >= self.seq_len, \"max_length should be equal to or greater than seq_len\"\n",
    "\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    # Load model weights function\n",
    "    def load_model_weights(model, model_path):\n",
    "        #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        checkpoint = torch.load(model_path, map_location=config.device)\n",
    "\n",
    "        if isinstance(checkpoint, dict):\n",
    "            # Check for 'state_dict' or 'model_state_dict' keys\n",
    "            if 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            elif 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                # If no known key is found, try loading it as a raw state dictionary\n",
    "                try:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                except RuntimeError as e:\n",
    "                    raise ValueError(f\"Error loading state dict: {e}\")\n",
    "        elif isinstance(checkpoint, nn.Module):\n",
    "            # If the checkpoint is a model object, assign it directly\n",
    "            model = checkpoint\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    ###############################\n",
    "    # Flash2_Attention\n",
    "    class FlashAttention2(nn.Module):\n",
    "        def __init__(self, sequence_length, head_dimension, block_size):\n",
    "            super(Expert.FlashAttention2, self).__init__()\n",
    "            self.block_size = block_size\n",
    "            # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "            assert sequence_length % block_size == 0\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            # Partitioning of inputs\n",
    "            Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "            # Efficient computation of the attention mechanism\n",
    "            outputs = []\n",
    "            for i, Q_block in enumerate(Q_blocks):\n",
    "                output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "                outputs.append(output_block)\n",
    "\n",
    "            # Concatenating the processed blocks\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "            return output\n",
    "\n",
    "        def partition_inputs(self, Q, K, V):\n",
    "            # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "            Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "            K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "            V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "            return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "        def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "            # Process each block efficiently as per FLASH2's optimized method\n",
    "            # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "            output_blocks = []\n",
    "            for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "                attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "                attention_scores = self.online_softmax(attention_scores)\n",
    "                output_block = torch.matmul(attention_scores, V_block)\n",
    "                output_blocks.append(output_block)\n",
    "\n",
    "            # Summing up the results from each block\n",
    "            output_block_sum = sum(output_blocks)\n",
    "            return output_block_sum\n",
    "\n",
    "        def online_softmax(self, scores, chunk_size=128):\n",
    "            # Apply softmax in chunks for large sequences\n",
    "            softmaxed_scores = []\n",
    "            for i in range(0, scores.size(0), chunk_size):\n",
    "                chunk = scores[i:i + chunk_size, :]\n",
    "                softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "                softmaxed_scores.append(softmaxed_chunk)\n",
    "            return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "    ###############################\n",
    "    # SparseFlash2_Attention\n",
    "    class SparseFlash2Attention(nn.Module):\n",
    "        def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "            super().__init__()\n",
    "            self.flash_attention = Expert.FlashAttention2(seq_len, head_dim, blk_size)\n",
    "            self.seq_len = seq_len\n",
    "            self.head_dim = head_dim\n",
    "            self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "            self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        def generate_sparsity_mask(self):\n",
    "            mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "            step = self.sparsity_factor\n",
    "            for i in range(0, self.seq_len, step):\n",
    "                mask[i:i + step, :] = 1\n",
    "            return mask.bool()\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "            # Reshape output to be 3D for batch matrix multiplication\n",
    "            output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "            sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "            # Apply the sparsity mask to the output\n",
    "            sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "            output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "            # Reshape the output back to 2D\n",
    "            output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "            return output\n",
    "\n",
    "    ###############################\n",
    "    # MAMBA\n",
    "    # RoPE\n",
    "    class RotaryPositionalEncoding(nn.Module):\n",
    "        def __init__(self, dim, max_len=5000):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "            t = torch.arange(max_len).type_as(inv_freq)\n",
    "            freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "            self.register_buffer('sin', freqs.sin())\n",
    "            self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "        def forward(self, x):\n",
    "            n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "            sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "            # Apply RoPE to even and odd indices separately\n",
    "            x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "    # SWIGLU\n",
    "    class SwiGLU(nn.Module):\n",
    "        def __init__(self, dim_in, dim_out):\n",
    "            super(Expert.SwiGLU, self).__init__()\n",
    "            self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "            self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "        def forward(self, x):\n",
    "            gate = torch.sigmoid(self.fc2(x))\n",
    "            return self.fc1(x) * gate\n",
    "\n",
    "    class SimplifiedMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "            \n",
    "            self.feedforward = nn.Sequential(\n",
    "                nn.Linear(self.d_model, self.d_state),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(self.d_state, self.d_model)\n",
    "            )\n",
    "            self.input_embedding = nn.Linear(self.d_model, self.d_model)\n",
    "            self.convs = nn.Sequential(*[nn.Conv1d(self.d_model, self.d_model, kernel_size=self.d_conv, padding=(self.d_conv // 2)) for _ in range(self.num_layers)])\n",
    "            self.swiglu = Expert.SwiGLU(self.d_model, self.d_model)\n",
    "            self.output_projection = nn.Linear(self.d_model, self.d_model * self.expansion_factor)\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "            nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "            nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "            nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "        def forward(self, inputs, attention_mask=None):\n",
    "            print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "            # Apply the attention mask if provided\n",
    "            if attention_mask is not None:\n",
    "                inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "            projected_inputs = self.input_embedding(inputs)\n",
    "            print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            for conv in self.convs:\n",
    "                projected_inputs = conv(projected_inputs)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = self.swiglu(projected_inputs)\n",
    "            print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "            output = self.output_projection(projected_inputs)\n",
    "            print(\"output shape:\", output.shape)\n",
    "\n",
    "            return output\n",
    "\n",
    "    class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.vocab_size = config.vocab_size\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "            self.pos_encoder = Expert.RotaryPositionalEncoding(self.d_model)\n",
    "            self.simplified_mamba = Expert.SimplifiedMAMBA(config)\n",
    "            self.output_projection = nn.Linear(self.d_model * 2, self.vocab_size)  # Adjust if needed\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = 1.0\n",
    "\n",
    "            nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "        def forward(self, input_values, attention_mask):\n",
    "            embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "            embedded = self.pos_encoder(embedded)\n",
    "            simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "            logits = self.output_projection(simplified_mamba_output)\n",
    "            return logits\n",
    "\n",
    "\n",
    "    class SwitchRouter(nn.Module):\n",
    "        CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "        class SwitchGate(nn.Module):\n",
    "            def __init__(self, input_dim, num_experts):\n",
    "                super(Expert.SwitchRouter.SwitchGate, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "                self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "                return gate_scores\n",
    "\n",
    "        def __init__(self, config):\n",
    "            super(Expert.SwitchRouter, self).__init__()\n",
    "            self.config = config\n",
    "            self.device = config.device\n",
    "            self.router = self.SwitchGate(config.input_dim, config.num_experts).to(config.device)\n",
    "            self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "            self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.transformer_dpo = Expert.DPO(self.lmt, config.device,config.embed_size).to(config.device)\n",
    "            self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "            self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "            self.input_embedding = nn.Linear(config.input_dim, config.input_dim)\n",
    "\n",
    "        def forward(self, x, attention_mask, context_texts, question_text):\n",
    "            x = x.to(self.device)\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.float()\n",
    "\n",
    "            x = self.input_embedding(x)\n",
    "            gate_scores = self.router(x)\n",
    "            expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "            final_output = torch.zeros_like(x)\n",
    "\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                mask = expert_indices == i\n",
    "                if mask.any():\n",
    "                    selected_inputs = x[mask]\n",
    "                    selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                    if isinstance(expert, Expert.TransformerRAG):\n",
    "                        expert_output = expert.forward(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                    else:\n",
    "                        expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                    final_output[mask] = expert_output\n",
    "\n",
    "            aux_loss = self.auxiliary_loss(gate_scores)\n",
    "            return final_output, aux_loss\n",
    "\n",
    "        def auxiliary_loss(self, gate_scores):\n",
    "            expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "            loss_balancing = torch.std(expert_load)\n",
    "            return loss_balancing\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "            capacity_factor_tensor = torch.tensor([Expert.SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "            capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "            expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "            for idx in range(len(expert_indices)):\n",
    "                selected_expert = expert_indices[idx]\n",
    "                if expert_counts[selected_expert] < capacities[0]:\n",
    "                    expert_counts[selected_expert] += 1\n",
    "                else:\n",
    "                    available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                    if len(available_experts) > 0:\n",
    "                        alternative_expert = available_experts[0]\n",
    "                        expert_indices[idx] = alternative_expert\n",
    "                        expert_counts[alternative_expert] += 1\n",
    "                    else:\n",
    "                        print(\"No available experts to reroute. Handling overflow.\")\n",
    "            return expert_indices\n",
    "    \n",
    "    ###############################\n",
    "    # RAG\n",
    "    class CustomDPRContextEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.CustomDPRContextEncoder, self).__init__()  \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            context_embeddings = self.embedding_layer(pooled_output)\n",
    "            return context_embeddings\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.DPRQuestionEncoder, self).__init__() \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, **kwargs):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            embeddings = self.embedding_layer(pooled_output)\n",
    "            return embeddings\n",
    "\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(Expert.TransformerRAG, self).__init__()\n",
    "            self.config = config\n",
    "            self.context_encoder = Expert.CustomDPRContextEncoder(config.embedding_dim).to(config.device)\n",
    "            self.language_model = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.question_encoder = Expert.DPRQuestionEncoder(config.embedding_dim).to(config.device)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "            \n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "                \n",
    "                aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = context['input_ids'].to(config.device)\n",
    "                    context_attention_mask = context['attention_mask'].to(config.device)\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "                \n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "            \n",
    "            question_input_ids = question_input_ids.to(config.device).long()\n",
    "            question_attention_mask = question_attention_mask.to(config.device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "            \n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=config.device))\n",
    "            \n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(config.device) for k, v in tokenized_combined_input.items()}\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            \n",
    "            return response\n",
    "\n",
    "        @staticmethod\n",
    "        def extract_text_from_pdf(file_path):\n",
    "            text = \"\"\n",
    "            with fitz.open(file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            return text\n",
    "\n",
    "        @staticmethod\n",
    "        def split_into_chunks(text, chunk_size):\n",
    "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        @staticmethod\n",
    "        def preprocess_text(text, max_length=512):\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            chunk_size = max_length - 50\n",
    "            text_chunks = Expert.TransformerRAG.split_into_chunks(text, chunk_size)\n",
    "            processed_chunks = []\n",
    "            for chunk in text_chunks:\n",
    "                tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "                processed_chunk = {\n",
    "                    'input_ids': tokenized_output['input_ids'],\n",
    "                    'attention_mask': tokenized_output['attention_mask']\n",
    "                }\n",
    "                processed_chunks.append(processed_chunk)\n",
    "            return processed_chunks\n",
    "\n",
    "        @staticmethod\n",
    "        def create_dataset_from_pdfs(pdf_file_paths):\n",
    "            dataset = []\n",
    "            for file_path in pdf_file_paths:\n",
    "                text = Expert.TransformerRAG.extract_text_from_pdf(file_path)\n",
    "                processed_text = Expert.TransformerRAG.preprocess_text(text)\n",
    "                dataset.append(processed_text)\n",
    "            return dataset\n",
    "\n",
    "        def retrieve_contexts(self, dataset, query_embedding, top_k=5):\n",
    "            similarity_scores = []\n",
    "            for context in dataset:\n",
    "                context_input_ids = context['input_ids'].to(self.config.device)\n",
    "                context_attention_mask = context['attention_mask'].to(self.config.device)\n",
    "                # Use the class's context_encoder\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "                similarity_scores.append(similarity.squeeze().item())\n",
    "            top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "            top_contexts = [dataset[i] for i in top_k_indices]\n",
    "            return top_contexts\n",
    "\n",
    "        def rag_retrieve_and_generate(self, dataset, query):\n",
    "            # Tokenize the query using the class's tokenizer\n",
    "            tokenized_query = self.tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
    "            input_ids = tokenized_query['input_ids']\n",
    "            attention_mask = tokenized_query['attention_mask']\n",
    "            # Use the class's question_encoder\n",
    "            encoded_query = self.question_encoder(input_ids, attention_mask)\n",
    "            relevant_contexts = self.retrieve_contexts(dataset, encoded_query)\n",
    "            # Assuming generate_response is a method of LanguageModelTransformer that accepts tokenized contexts and generates a response\n",
    "            response = self.language_model.generate_response(relevant_contexts)\n",
    "            return response\n",
    "\n",
    "    ###############################\n",
    "    # LORA\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "            super(Expert.LORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "\n",
    "            # Original weight and bias of the linear layer\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # LORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # QLORA\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "            super(Expert.QLORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "            self.quantization_bits = quantization_bits\n",
    "\n",
    "            # Original weight and bias\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # QLORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    ###############################\n",
    "    # Language Model Transformer\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, embed_size, heads):\n",
    "            super(Expert.MultiHeadAttention, self).__init__()\n",
    "            self.embed_size = embed_size\n",
    "            self.heads = heads\n",
    "            self.head_dim = embed_size // heads\n",
    "\n",
    "            assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "            ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        def forward(self, values, keys, query, mask):\n",
    "            N = query.shape[0]\n",
    "            value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "            # Split the embedding into self.heads different pieces\n",
    "            values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "            queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            # Einsum does the matrix multiplication for query*keys for each training example\n",
    "            # with every other training example, then sum it up\n",
    "            attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "                N, query_len, self.heads * self.head_dim\n",
    "            )\n",
    "\n",
    "            out = self.fc_out(out)\n",
    "            return out\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "            super(Expert.TransformerBlock, self).__init__()\n",
    "            self.attention = Expert.MultiHeadAttention(embed_size, heads)\n",
    "            self.norm1 = nn.LayerNorm(embed_size)\n",
    "            self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                Expert.LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, value, key, query, mask):\n",
    "            attention = self.attention(value, key, query, mask)\n",
    "            x = self.dropout(self.norm1(attention + query))\n",
    "            forward = self.feed_forward(x)\n",
    "            out = self.dropout(self.norm2(forward + x))\n",
    "            return out\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "            super(Expert.LanguageModelDecoder, self).__init__()\n",
    "            self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "            # Adding BatchNorm layers\n",
    "            self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "            self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    Expert.TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # QLORA layers\n",
    "            self.qlora_feed_forward = nn.Sequential(\n",
    "                Expert.QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "            self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "            self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, trg_mask):\n",
    "            N, seq_length = x.shape\n",
    "            if seq_length > self.position_embedding.num_embeddings:\n",
    "                raise ValueError(f\"Sequence length {seq_length} exceeds maximum allowed {self.position_embedding.num_embeddings}\")\n",
    "            positions = torch.arange(0, seq_length).expand(N, seq_length).to(config.device)\n",
    "            x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "            print(f\"Max position index: {positions.max().item()}, Position Embedding Size: {self.position_embedding.num_embeddings}\")\n",
    "\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn1(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, trg_mask)\n",
    "                if self.use_qlora:\n",
    "                    x = self.qlora_feed_forward(x)\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            out = self.fc_out(x)\n",
    "            #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "            return out\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.use_qlora = use_qlora\n",
    "\n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.vocab_size = config.vocab_size  # Use vocab_size from ExpertConfig\n",
    "            self.embed_size = config.embed_size  # Use embed_size from ExpertConfig\n",
    "            self.num_layers = config.num_layers  # Use num_layers from ExpertConfig\n",
    "            self.forward_expansion = config.forward_expansion  # Use forward_expansion from ExpertConfig\n",
    "            self.heads = config.heads  # Use heads from ExpertConfig\n",
    "            self.dropout = config.dropout  # Use dropout from ExpertConfig\n",
    "            self.max_length = config.max_length  # Use max_length from ExpertConfig\n",
    "            self.rank = config.rank  # Use rank from ExpertConfig\n",
    "            self.tokenizer_name = config.tokenizer_name  # Use tokenizer_name from ExpertConfig\n",
    "\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "            self.decoder = Expert.LanguageModelDecoder(\n",
    "                self.vocab_size,\n",
    "                self.embed_size,\n",
    "                self.num_layers,\n",
    "                self.heads,\n",
    "                self.forward_expansion,\n",
    "                self.dropout,\n",
    "                self.max_length,\n",
    "                self.rank,\n",
    "            )\n",
    "\n",
    "        def forward(self, trg, attention_mask=None):  # Remove attention_mask here since it's not used\n",
    "            print(f\"Input shape to LanguageModelTransformer: {trg.shape}\")\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            out = self.decoder(trg, trg_mask)  # Do not pass attention_mask here\n",
    "            print(f\"Language Model Transformer out shape: {out.shape}\")\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len).to(trg.device)\n",
    "            return trg_mask\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = [self.tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            return response\n",
    "\n",
    "    ###############################\n",
    "    # DPO\n",
    "    class DPO(nn.Module):\n",
    "        def __init__(self, language_model, device, embed_size):\n",
    "            super(Expert.DPO, self).__init__()\n",
    "            self.language_model = language_model\n",
    "            self.device = device\n",
    "            # Assuming embed_size is accessible and correct\n",
    "            self.projection = nn.Linear(language_model.vocab_size, embed_size)  # Project from vocab_size to embed_size\n",
    "            self.classifier = nn.Linear(embed_size, 2)  # Assuming embed_size is accessible\n",
    "\n",
    "        def forward(self, input_ids_question, input_ids_chosen=None, input_ids_rejected=None, labels=None):\n",
    "            combined_input_ids = torch.cat((input_ids_question, input_ids_chosen, input_ids_rejected), dim=1)\n",
    "\n",
    "            # Assuming combined_input_ids has shape [batch_size, sequence_length]\n",
    "            logits = self.language_model(combined_input_ids)  # Output shape: [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "            # Project logits to embedding space before pooling\n",
    "            projected_logits = self.projection(logits)  # New shape: [batch_size, sequence_length, embed_size]\n",
    "            \n",
    "            # Apply global mean pooling across the sequence length dimension\n",
    "            pooled_logits = projected_logits.mean(dim=1)  # New shape: [batch_size, embed_size]\n",
    "\n",
    "            # Pass the pooled representation through the classifier\n",
    "            predictions = self.classifier(pooled_logits)  # New shape: [batch_size, 2]\n",
    "\n",
    "            # Calculate loss if labels are provided\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(predictions, labels)\n",
    "\n",
    "            return predictions, loss\n",
    "\n",
    "\n",
    "    def __init__(self, config: ExpertConfig):\n",
    "        super().__init__()\n",
    "        # Validate configuration\n",
    "        config.validate()\n",
    "\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(\n",
    "            config.seq_len, \n",
    "            config.head_dim, \n",
    "            config.block_size, \n",
    "            config.sparsity_factor\n",
    "        )\n",
    "\n",
    "        # Inside the Expert class definition\n",
    "        self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "\n",
    "        self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "\n",
    "        # Corrected instantiation of SimplifiedLanguageModelMAMBA\n",
    "        self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "\n",
    "        # Now initialize DPO with the language model transformer\n",
    "        self.transformer_dpo = Expert.DPO(self.lmt, config.device, config.embed_size).to(config.device)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(config.input_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = Expert.SwitchRouter(config)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = Expert.QLORALayer(config.input_dim, config.input_dim, config.rank)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, aux_loss = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "        \"\"\"\n",
    "        Calculate a new alpha value based on the current loss.\n",
    "        \"\"\"\n",
    "        if current_loss >= initial_loss:\n",
    "            return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "        loss_ratio = current_loss / initial_loss\n",
    "        alpha_range = initial_alpha - final_alpha\n",
    "        new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "        return new_alpha  \n",
    "    \n",
    "\n",
    "    ###############################\n",
    "    # TRAINING METHODS\n",
    "\n",
    "    # DPO Training\n",
    "    def train_dpo(self, train_loader, optimizer, config, save_path):\n",
    "            self.train()  # Set the model to training mode\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                input_ids_question = batch['input_ids_question'].to(config.device)\n",
    "                input_ids_chosen = batch['input_ids_chosen'].to(config.device)\n",
    "                input_ids_rejected = batch['input_ids_rejected'].to(config.device)\n",
    "                labels = batch['labels'].to(config.device)\n",
    "                print(f\"train_dpo input_ids_question: {input_ids_question.shape}\")\n",
    "                print(f\"train_dpo input_ids_chosen: {input_ids_chosen.shape}\")\n",
    "                print(f\"train_dpo input_ids_rejected: {input_ids_rejected.shape}\")\n",
    "                print(f\"train_dpo labels: {labels.shape}\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                logit, loss = self.transformer_dpo(input_ids_question, input_ids_chosen, input_ids_rejected, labels)\n",
    "                print(f\"Logits shape: {logit.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            print(f\"Training complete. Average Loss: {average_loss}\")\n",
    "            \n",
    "            # Save the model\n",
    "            torch.save(self.transformer_dpo.state_dict(), save_path)\n",
    "\n",
    "            return average_loss\n",
    "\n",
    "    # RAG Training\n",
    "    def train_language_model_rag(self, model, save_path, train_loader, device, num_epochs=5, lr=1e-8, weight_decay=1e-4):\n",
    "        # Enable QLORA during training\n",
    "        model.decoder.toggle_qlora(True)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "\n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                targets = batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        # Toggle QLORA off after training\n",
    "        model.decoder.toggle_qlora(False)\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"Training Complete\")\n",
    "        return model, average_loss\n",
    "\n",
    "    # DPR Training\n",
    "    def train_dpr_encoders(self, train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs , context_save_path, question_save_path):\n",
    "        loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(train_data[\"queries\"])):\n",
    "                query = train_data[\"queries\"][i]\n",
    "                context = train_data[\"contexts\"][i]\n",
    "\n",
    "                # Ensure query is a string\n",
    "                if not isinstance(query, str):\n",
    "                    raise ValueError(\"Query must be a string.\")\n",
    "                tokenized_query = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                # Ensure context is a string\n",
    "                if isinstance(context, dict):\n",
    "                    # If context is a dictionary, extract the text content. This is a placeholder and might need adjustment.\n",
    "                    context_text = context.get(\"text\", \"\")\n",
    "                elif isinstance(context, str):\n",
    "                    context_text = context\n",
    "                else:\n",
    "                    raise ValueError(\"Context must be a string or a dictionary containing a text field.\")\n",
    "                tokenized_context = tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                question_embeddings = question_encoder(input_ids=tokenized_query['input_ids'], attention_mask=tokenized_query['attention_mask'])\n",
    "                context_embeddings = context_encoder(input_ids=tokenized_context['input_ids'], attention_mask=tokenized_context['attention_mask'])\n",
    "\n",
    "                # The labels tensor should have the same first dimension size as the input tensors\n",
    "                labels = torch.tensor([1.0] * question_embeddings.size(0), dtype=torch.float).to(question_embeddings.device)\n",
    "\n",
    "                loss = loss_function(question_embeddings, context_embeddings, labels)\n",
    "\n",
    "                optimizer_context.zero_grad()\n",
    "                optimizer_question.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_context.step()\n",
    "                optimizer_question.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_data['queries'])}\")\n",
    "        average_loss = total_loss / len(train_data['queries'])\n",
    "        torch.save(context_encoder.state_dict(), context_save_path)\n",
    "        torch.save(question_encoder.state_dict(), question_save_path)\n",
    "        return (context_encoder, question_encoder), average_loss\n",
    "       \n",
    "    # LMT Training\n",
    "    def train_language_model_transformer(self, train_loader, device, vocab_size, save_path):\n",
    "        model = self.lmt\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "        num_epochs = 5\n",
    "        \n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            model.decoder.toggle_qlora(True)\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs, targets = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        return model, average_loss\n",
    "\n",
    "    # Full Expert Training\n",
    "\n",
    "    def train_expert(self, train_loader, train_data, optimizer, main_loss_function, aux_loss_weight, device,save_path, accumulation_steps=4, num_epochs=5):\n",
    "            self.train()  # Set the model to training mode\n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0\n",
    "                optimizer.zero_grad()  # Initialize gradients to zero\n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "                    inputs, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "\n",
    "                    # Calculate start and end indices for current batch in train_data\n",
    "                    start_idx = batch_idx * batch['input_ids'].size(0)\n",
    "                    end_idx = start_idx + batch['input_ids'].size(0)\n",
    "\n",
    "                    # Extract current_queries and current_contexts for the batch\n",
    "                    current_queries = train_data['queries'][start_idx:end_idx]\n",
    "                    current_contexts = train_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "                    # Call to the model forward function\n",
    "                    outputs, aux_loss = self(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "                    # Calculate loss and accumulate\n",
    "                    main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                    loss = (main_loss + aux_loss_weight * aux_loss) / accumulation_steps\n",
    "                    loss.backward()\n",
    "\n",
    "                    if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    total_loss += loss.item() * accumulation_steps  # Scale back up\n",
    "\n",
    "                average_loss = total_loss / len(train_loader)\n",
    "                print(f'End of Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            torch.save(self.state_dict(), save_path)\n",
    "            return self, average_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training transformer_with_dpo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Training transformer_with_dpo\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def preprocess_dpo_data(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    vocab_size = len(tokenizer.vocab)  # Make sure this matches your embedding layer's vocab size\n",
    "\n",
    "    # Define max sequence length\n",
    "    max_seq_length = 512 // 3\n",
    "\n",
    "    # Tokenize 'question', 'chosen', and 'rejected' fields\n",
    "    tokenized_questions = tokenizer(examples['question'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_chosen = tokenizer(examples['chosen'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_rejected = tokenizer(examples['rejected'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "\n",
    "    # Prepare the labels: 1 for 'chosen' and 0 for 'rejected'\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(len(examples['question']))]\n",
    "\n",
    "    return {\n",
    "        'input_ids_question': tokenized_questions['input_ids'],\n",
    "        'attention_mask_question': tokenized_questions['attention_mask'],\n",
    "        'input_ids_chosen': tokenized_chosen['input_ids'],\n",
    "        'attention_mask_chosen': tokenized_chosen['attention_mask'],\n",
    "        'input_ids_rejected': tokenized_rejected['input_ids'],\n",
    "        'attention_mask_rejected': tokenized_rejected['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Load the DPO dataset from Hugging Face\n",
    "dpo_dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "\n",
    "# Apply the preprocessing to the dataset\n",
    "dpo_dataset = dpo_dataset.map(preprocess_dpo_data, batched=True)\n",
    "\n",
    "\n",
    "# You can convert to PyTorch tensors after processing\n",
    "dpo_dataset.set_format(type='torch', columns=['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(dpo_dataset['train'], batch_size=2, shuffle=True)\n",
    "\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "expert_model = Expert(config)\n",
    "\n",
    "model_vocab_size = expert_model.lmt.decoder.word_embedding.num_embeddings\n",
    "print(f\"Model vocab size: {model_vocab_size}\")\n",
    "print(f\"Model embedding size: {expert_model.lmt.decoder.word_embedding.embedding_dim}\")\n",
    "print(f\"Configured max length: {config.max_length}\")\n",
    "\n",
    "optimizer = AdamW(expert_model.parameters(), lr=1e-5)\n",
    "save_path = 'D:/EXPERT_WEIGHTS/dpo_model.pth'\n",
    "\n",
    "# Train the DPO model\n",
    "label_column = 'labels'\n",
    "input_columns = ['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
    "# Assuming expert_model is an instance of Expert\n",
    "avg_loss = expert_model.train_dpo(train_loader, optimizer, config, save_path)\n",
    "\n",
    "# Save the model\n",
    "torch.save(expert_model.transformer_dpo.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LMT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feeb04c71a8345c09557831db66d4a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3b947d7c314c9c95271f6023513a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m expert_system \u001b[38;5;241m=\u001b[39m Expert(config)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Load Wikipedia dataset and preprocess\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwikipedia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m20220301.en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain[:1\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Using 1% of the data for demonstration\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[0;32m     19\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mtokenizer_name)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\load.py:2153\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   2150\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2153\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2162\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2164\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2165\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\builder.py:942\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m try_from_hf_gcs:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_prepared_from_hf_gcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    943\u001b[0m         downloaded_from_gcs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (DatasetNotOnHfGcsError, MissingFilesOnHfGcsError):\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\builder.py:992\u001b[0m, in \u001b[0;36mDatasetBuilder._download_prepared_from_hf_gcs\u001b[1;34m(self, download_config)\u001b[0m\n\u001b[0;32m    990\u001b[0m reader \u001b[38;5;241m=\u001b[39m ArrowReader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo)\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# use reader instructions to download the right files\u001b[39;00m\n\u001b[1;32m--> 992\u001b[0m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_from_hf_gcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_data_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    993\u001b[0m downloaded_info \u001b[38;5;241m=\u001b[39m DatasetInfo\u001b[38;5;241m.\u001b[39mfrom_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir)\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mupdate(downloaded_info)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_reader.py:305\u001b[0m, in \u001b[0;36mBaseReader.download_from_hf_gcs\u001b[1;34m(self, download_config, relative_data_dir)\u001b[0m\n\u001b[0;32m    303\u001b[0m             file_to_download \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(Path(file_instruction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mrelative_to(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path))\n\u001b[0;32m    304\u001b[0m             remote_prepared_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(remote_cache_dir, file_to_download)\n\u001b[1;32m--> 305\u001b[0m             downloaded_prepared_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[43mremote_prepared_filename\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m             shutil\u001b[38;5;241m.\u001b[39mmove(downloaded_prepared_filename, file_instruction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\utils\\file_utils.py:182\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(url_or_filename)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_etag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_etag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_url_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_url_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m url_or_filename\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\utils\\file_utils.py:644\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, use_auth_token, ignore_url_params, storage_options, download_desc)\u001b[0m\n\u001b[0;32m    642\u001b[0m         fsspec_get(url, temp_file, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, desc\u001b[38;5;241m=\u001b[39mdownload_desc)\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    656\u001b[0m shutil\u001b[38;5;241m.\u001b[39mmove(temp_file\u001b[38;5;241m.\u001b[39mname, cache_path)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\utils\\file_utils.py:419\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries, desc)\u001b[0m\n\u001b[0;32m    410\u001b[0m total \u001b[38;5;241m=\u001b[39m resume_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(content_length) \u001b[38;5;28;01mif\u001b[39;00m content_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m    412\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    413\u001b[0m     unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m    418\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[1;32m--> 419\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Assuming the Expert class and ExpertConfig class have already been defined\n",
    "\n",
    "# Initialize configuration\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "\n",
    "# Initialize Expert system\n",
    "expert_system = Expert(config)\n",
    "\n",
    "# Load Wikipedia dataset and preprocess\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:1%]\")  # Using 1% of the data for demonstration\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=config.max_length)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])  # Remove original text to only keep tokenized versions\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(tokenized_datasets, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define save path for the trained model\n",
    "save_path = 'D:/EXPERT_WEIGHTS/lmt_expert_trained.pth'\n",
    "\n",
    "\n",
    "# Train the LMT sub-model within the Expert system\n",
    "trained_model, average_loss = expert_system.train_language_model_transformer(\n",
    "    train_loader=train_loader, \n",
    "    device=config.device, \n",
    "    vocab_size=config.vocab_size, \n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "print(f\"Training complete. Model saved to {save_path}. Average Loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transformer-With_RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ddf6e590774102a57fc9a2a774258f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'ExpertConfig' object has no attribute 'rag_model_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 94\u001b[0m\n\u001b[0;32m     32\u001b[0m train_rag_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m# Queries for DPO.pdf\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m     ]\n\u001b[0;32m     87\u001b[0m }\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Train the LMT_Rag sub-model within the Expert system\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Train th model\u001b[39;00m\n\u001b[0;32m     92\u001b[0m model, average_loss \u001b[38;5;241m=\u001b[39m expert_system\u001b[38;5;241m.\u001b[39mtrain_language_model_rag(\n\u001b[0;32m     93\u001b[0m     expert_system\u001b[38;5;241m.\u001b[39mtransformer_rag,  \u001b[38;5;66;03m# Assuming transformer_rag is your RAG model within Expert\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag_model_path\u001b[49m,\n\u001b[0;32m     95\u001b[0m     train_loader,\n\u001b[0;32m     96\u001b[0m     device \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m     97\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     98\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,  \u001b[38;5;66;03m# Adjust learning rate as needed\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m\n\u001b[0;32m    100\u001b[0m )\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Train DPR encoders\u001b[39;00m\n\u001b[0;32m    103\u001b[0m (context_encoder, question_encoder), average_loss \u001b[38;5;241m=\u001b[39m expert_system\u001b[38;5;241m.\u001b[39mtrain_dpr_encoders(\n\u001b[0;32m    104\u001b[0m     train_rag_data,\n\u001b[0;32m    105\u001b[0m     expert_system\u001b[38;5;241m.\u001b[39mcontext_encoder,  \u001b[38;5;66;03m# Assuming you have these initialized within Expert\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     question_save_path\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mquestion_encoder_path\n\u001b[0;32m    112\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ExpertConfig' object has no attribute 'rag_model_path'"
     ]
    }
   ],
   "source": [
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "\n",
    "# Initialize Expert system\n",
    "expert_system = Expert(config)\n",
    "sequence_length = 30 \n",
    "# Load dataset\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=sequence_length)\n",
    "    \n",
    "    # Shift input_ids to create labels and truncate the last token\n",
    "    labels = [seq[1:] + [tokenizer.pad_token_id] for seq in tokenized_output['input_ids']]\n",
    "    tokenized_output['labels'] = labels\n",
    "    \n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=64, shuffle=True)\n",
    "\n",
    "# Sample training data structure (You need to construct this with real data)\n",
    "train_rag_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        dataset['train'][0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        dataset['train'][0],\n",
    "        dataset['train'][0],\n",
    "        dataset['train'][0],\n",
    "        # Contexts from MAMBA.pdf\n",
    "        dataset['train'][1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        dataset['train'][1],\n",
    "        dataset['train'][1],\n",
    "        dataset['train'][1],\n",
    "        # Contexts from QLORA.pdf\n",
    "        dataset['train'][2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        dataset['train'][2],\n",
    "        dataset['train'][2],\n",
    "        dataset['train'][2],\n",
    "        # Contexts from RAG.pdf\n",
    "        dataset['train'][3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        dataset['train'][3],\n",
    "        dataset['train'][3],\n",
    "        dataset['train'][3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        dataset['train'][4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        dataset['train'][4],\n",
    "        dataset['train'][4],\n",
    "        dataset['train'][4]\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Train the LMT_Rag sub-model within the Expert system\n",
    "# Train th model\n",
    "model, average_loss = expert_system.train_language_model_rag(\n",
    "    expert_system.transformer_rag,  # Assuming transformer_rag is your RAG model within Expert\n",
    "    config.rag_model_path,\n",
    "    train_loader,\n",
    "    device = config.device,\n",
    "    num_epochs=5,\n",
    "    lr=1e-5,  # Adjust learning rate as needed\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Train DPR encoders\n",
    "(context_encoder, question_encoder), average_loss = expert_system.train_dpr_encoders(\n",
    "    train_rag_data,\n",
    "    expert_system.context_encoder,  # Assuming you have these initialized within Expert\n",
    "    expert_system.question_encoder,\n",
    "    optimizer_context = AdamW(expert_system.context_encoder.parameters(), lr=1e-5),  \n",
    "    optimizer_question = AdamW(expert_system.question_encoder.parameters(), lr=1e-5),\n",
    "    epochs=5,\n",
    "    context_save_path=config.context_encoder_path,\n",
    "    question_save_path=config.question_encoder_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
