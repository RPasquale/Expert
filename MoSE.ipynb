{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trie built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Load Wikipedia dataset and preprocess\\ndataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\\n\\n# Access a portion of the dataset for inspection\\n#print(dataset[\\'train\\'][0])\\n\\ndef generate_attention_mask(token_ids):\\n    \"\"\"Generate an attention mask for the given token IDs.\"\"\"\\n    return [1 if token_id != 0 else 0 for token_id in token_ids]\\n\\ndef tokenize_function(examples):\\n    # Initialize lists for token IDs, attention masks, and labels\\n    token_ids, attention_masks, labels = [], [], []\\n\\n    for text in examples[\"text\"]:\\n        # Tokenize text and ensure at least one token ID is generated\\n        ids = wordpiece_tokenizer.tokenize(text) or [0]  # Replace [0] with your tokenizer\\'s pad token ID\\n        # Generate attention mask for the tokenized text\\n        mask = [1] * len(ids)\\n\\n        # Check for length of ids and pad/truncate as necessary\\n        if len(ids) < config.max_length:\\n            # Pad\\n            pad_length = config.max_length - len(ids)\\n            ids += [0] * pad_length  # Assuming 0 is the padding ID\\n            mask += [0] * pad_length\\n        else:\\n            # Truncate\\n            ids = ids[:config.max_length]\\n            mask = mask[:config.max_length]\\n\\n        token_ids.append(ids)\\n        attention_masks.append(mask)\\n        labels.append(ids)  # For simplicity, using the same IDs as labels; adjust as needed for your model\\n\\n    return {\"input_ids\": token_ids, \"attention_mask\": attention_masks, \"labels\": labels}\\n\\n\\n\\n# Apply the custom tokenize function to the dataset\\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\\ntokenized_datasets.set_format(type=\\'torch\\', columns=[\\'input_ids\\', \\'attention_mask\\', \\'labels\\'])\\n\\ntrain_loader = DataLoader(tokenized_datasets, batch_size=8, shuffle=True)\\n\\nmain_loss_function = torch.nn.CrossEntropyLoss()\\naux_loss_weight = 0.1  # Adjust based on the significance of the auxiliary loss in your training\\n\\noptimizer = torch.optim.AdamW(expert_model.parameters(), lr=1e-5, weight_decay=1e-4)\\nsave_path = \\'D:\\\\EXPERT_WEIGHTS\\\\expert_model_weights.pth\\'\\n# Rag data\\ntrain_rag_data = {\\n    \"queries\": [\\n        # Queries for DPO.pdf\\n        \"What is Direct Preference Optimization (DPO)?\",\\n        \"How does Direct Preference Optimization work?\",\\n        \"How can I implement Direct Preference Optimization in my organization?\",\\n        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\\n        # Queries for MAMBA.pdf\\n        \"What is MAMBA?\",\\n        \"How does MAMBA function?\",\\n        \"How can I build a system based on MAMBA technology?\",\\n        \"Why does MAMBA enhance the performance of its application area?\",\\n        # Queries for QLORA.pdf\\n        \"What is QLORA?\",\\n        \"How does QLORA operate?\",\\n        \"How can I develop a project using QLORA?\",\\n        \"Why does QLORA improve the capabilities of its relevant field?\",\\n        # Queries for RAG.pdf\\n        \"What is Retrieval Augmented Generation (RAG)?\",\\n        \"How does Retrieval Augmented Generation work?\",\\n        \"How can I build a Retrieval Augmented Generation model?\",\\n        \"Why does Retrieval Augmented Generation enhance language model performance?\",\\n        # Queries for SWITCH_TRANSFORMER.pdf\\n        \"What is the Switch Transformer model?\",\\n        \"How does the Switch Transformer model operate?\",\\n        \"How can I construct a Switch Transformer model?\",\\n        \"Why does the Switch Transformer model improve language processing tasks?\"\\n    ],\\n    \"contexts\": [\\n        # Contexts from DPO.pdf\\n        config.rag_dataset[0],  # Assuming dataset[0] is the processed content of DPO.pdf\\n        config.rag_dataset[0],\\n        config.rag_dataset[0],        \\n        config.rag_dataset[0],        \\n        # Contexts from MAMBA.pdf\\n        config.rag_dataset[1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\\n        config.rag_dataset[1], \\n        config.rag_dataset[1], \\n        config.rag_dataset[1], \\n        # Contexts from QLORA.pdf\\n        config.rag_dataset[2],  # Assuming dataset[2] is the processed content of QLORA.pdf\\n        config.rag_dataset[2],\\n        config.rag_dataset[2],\\n        config.rag_dataset[2],\\n        # Contexts from RAG.pdf\\n        config.rag_dataset[3],  # Assuming dataset[3] is the processed content of RAG.pdf\\n        config.rag_dataset[3],\\n        config.rag_dataset[3],\\n        config.rag_dataset[3],\\n        # Contexts from SWITCH_TRANSFORMER.pdf\\n        config.rag_dataset[4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\\n        config.rag_dataset[4],\\n        config.rag_dataset[4],\\n        config.rag_dataset[4],\\n    ]\\n}\\n# Train the model\\ntrained_expert_model, average_loss = expert_model.train_expert(\\n    train_loader=train_loader,\\n    train_data=train_rag_data,\\n    optimizer=optimizer,\\n    main_loss_function=main_loss_function,\\n    aux_loss_weight=aux_loss_weight,\\n    device=config.device,\\n    save_path=save_path,\\n    accumulation_steps=4,  # Adjust based on your preference\\n    num_epochs=5  # Adjust based on your training needs\\n)\\n\\nprint(f\"Training completed. Average loss: {average_loss}\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the HF_HOME environment variable to a new cache directory on the D drive\n",
    "#os.environ['HF_HOME'] = 'D:/hf_datasets_cache'\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Test on larger text\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExpertConfig:\n",
    "    def __init__(self,wordpiece_vocab,wordpiece_tokenizer, cls_token_id=1770, \n",
    "                 sep_token_id=1771, pad_token_id=0, \n",
    "                 seq_len=512, head_dim=64, block_size=64, sparsity_factor=4,\n",
    "                 input_dim=512, num_experts=3, vocab_size=30522, embed_size=256,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0.1,\n",
    "                 max_length=512,  # Updated to match seq_len for consistency\n",
    "                 rank=16, device='cpu',\n",
    "                 mamba_model_path='D:\\\\EXPERT_WEIGHTS\\\\mamba_model_weights.pth',\n",
    "                 rag_model_path='D:\\\\EXPERT_WEIGHTS\\\\rag_model_weights.pth',\n",
    "                 context_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\context_encoder.pth',\n",
    "                 language_model_path='D:\\\\EXPERT_WEIGHTS\\\\language_model.pth',\n",
    "                 question_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\question_encoder.pth',\n",
    "                 dpo_model_path='D:\\\\EXPERT_WEIGHTS\\\\dpo_model_weights.pth',\n",
    "                 model_name='bert-base-uncased', embedding_dim=768,\n",
    "                 alpha=1, quantization_bits=8, tokenizer_name='bert-base-uncased',                 \n",
    "                  freq_threshold=100, d_model=512, d_state=2048, d_conv=3, expansion_factor=2, \n",
    "                 clip_gradient = 1.0, mamba_learning_rate = 5e-4, weight_decay = 0.1,\n",
    "                 warmup_steps = 10, total_mamba_steps = 100\n",
    "                ):\n",
    "\n",
    "        # Common hyperparameters\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.wordpiece_vocab = wordpiece_vocab\n",
    "        self.wordpiece_tokenizer = wordpiece_tokenizer        \n",
    "        self.seq_len = seq_len\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.cls_token_id = cls_token_id\n",
    "        self.sep_token_id = sep_token_id\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        self.input_dim = input_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length  \n",
    "        self.rank = rank\n",
    "\n",
    "        # Model paths and device\n",
    "        self.mamba_model_path = mamba_model_path\n",
    "        self.rag_model_path = rag_model_path\n",
    "        self.context_encoder_path = context_encoder_path\n",
    "        self.language_model_path = language_model_path\n",
    "        self.question_encoder_path = question_encoder_path\n",
    "        self.dpo_model_path = dpo_model_path\n",
    "        self.device = device\n",
    "\n",
    "        # Unique hyperparameters\n",
    "        self.num_experts = num_experts\n",
    "        self.model_name = model_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.clip_gradient = clip_gradient\n",
    "        self.mamba_learning_rate = mamba_learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_mamba_steps = total_mamba_steps\n",
    "\n",
    "        # PDFs (unchanged)\n",
    "        self.pdf_file_paths = [\n",
    "            #r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "            r'C:\\Users\\robbi\\OneDrive\\AI_Papers_Research\\DPO.pdf',\n",
    "            #r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "            r'C:\\Users\\robbi\\OneDrive\\AI_Papers_Research\\DPO.pdf',\n",
    "\n",
    "            #r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "            r'C:\\Users\\robbi\\OneDrive\\AI_Papers_Research\\DPO.pdf',\n",
    "\n",
    "            #r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "            r'C:\\Users\\robbi\\OneDrive\\AI_Papers_Research\\DPO.pdf',\n",
    "\n",
    "            #r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf'\n",
    "            r'C:\\Users\\robbi\\OneDrive\\AI_Papers_Research\\DPO.pdf',\n",
    "\n",
    "        ]\n",
    "        \n",
    "        # Preserving original dataset loading functionality\n",
    "        self.rag_dataset = Expert.TransformerRAG.create_dataset_from_pdfs(self.pdf_file_paths, self.wordpiece_tokenizer)\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def adapt_vocab_for_wordpiece(ssp_vocab):\n",
    "        adapted_vocab = {}\n",
    "        for token, id_or_freq in ssp_vocab.items():\n",
    "            if not token.startswith(\" \") and not token.endswith(\"</w>\"):\n",
    "                adapted_token = \"##\" + token.replace(\"</w>\", \"\")\n",
    "            else:\n",
    "                adapted_token = token.replace(\"</w>\", \"\")\n",
    "            adapted_vocab[adapted_token] = id_or_freq\n",
    "        return adapted_vocab\n",
    "\n",
    "        \n",
    "    def validate(self):\n",
    "        assert self.seq_len % self.block_size == 0, \"seq_len must be divisible by block_size\"\n",
    "        assert self.max_length >= self.seq_len, \"max_length should be equal to or greater than seq_len\"\n",
    "\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    # Load model weights function\n",
    "    def load_model_weights(model, model_path):\n",
    "        #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        checkpoint = torch.load(model_path, map_location=config.device)\n",
    "\n",
    "        if isinstance(checkpoint, dict):\n",
    "            # Check for 'state_dict' or 'model_state_dict' keys\n",
    "            if 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            elif 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                # If no known key is found, try loading it as a raw state dictionary\n",
    "                try:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                except RuntimeError as e:\n",
    "                    raise ValueError(f\"Error loading state dict: {e}\")\n",
    "        elif isinstance(checkpoint, nn.Module):\n",
    "            # If the checkpoint is a model object, assign it directly\n",
    "            model = checkpoint\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    ###############################\n",
    "    # Flash2_Attention\n",
    "    class FlashAttention2(nn.Module):\n",
    "        def __init__(self, sequence_length, head_dimension, block_size):\n",
    "            super(Expert.FlashAttention2, self).__init__()\n",
    "            self.block_size = block_size\n",
    "            # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "            assert sequence_length % block_size == 0\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            # Partitioning of inputs\n",
    "            Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "            # Efficient computation of the attention mechanism\n",
    "            outputs = []\n",
    "            for i, Q_block in enumerate(Q_blocks):\n",
    "                output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "                outputs.append(output_block)\n",
    "\n",
    "            # Concatenating the processed blocks\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "            return output\n",
    "\n",
    "        def partition_inputs(self, Q, K, V):\n",
    "            # Calculate the number of chunks, ensuring it's at least 1\n",
    "            num_chunks_Q = max(1, Q.size(0) // self.block_size)\n",
    "            num_chunks_K = max(1, K.size(0) // self.block_size)\n",
    "            num_chunks_V = max(1, V.size(0) // self.block_size)\n",
    "            \n",
    "            # Chunk the inputs\n",
    "            Q_blocks = Q.chunk(chunks=num_chunks_Q, dim=0)\n",
    "            K_blocks = K.chunk(chunks=num_chunks_K, dim=0)\n",
    "            V_blocks = V.chunk(chunks=num_chunks_V, dim=0)\n",
    "            \n",
    "            return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "        def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "            # Process each block efficiently as per FLASH2's optimized method\n",
    "            # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "            output_blocks = []\n",
    "            for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "                print(f\"Q_block: {Q_block.shape} , K_block.transpose(-2, -1): {K_block.transpose(-2, -1).shape}\")\n",
    "                attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "                print(f\"attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1)): {attention_scores.shape}\")\n",
    "                attention_scores = attention_scores.float()\n",
    "                attention_scores = self.online_softmax(attention_scores)\n",
    "                output_block = torch.matmul(attention_scores, V_block.float())\n",
    "                print(f\"output_block shape:, {output_block.shape}\") \n",
    "                output_blocks.append(output_block)\n",
    "\n",
    "            # Summing up the results from each block\n",
    "            output_block_sum = sum(output_blocks)\n",
    "            return output_block_sum\n",
    "\n",
    "        def online_softmax(self, scores, chunk_size=128):\n",
    "            # Apply softmax in chunks for large sequences\n",
    "            softmaxed_scores = []\n",
    "            for i in range(0, scores.size(0), chunk_size):\n",
    "                chunk = scores[i:i + chunk_size, :]\n",
    "                softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "                softmaxed_scores.append(softmaxed_chunk)\n",
    "            return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "    ###############################\n",
    "    # SparseFlash2_Attention\n",
    "    class SparseFlash2Attention(nn.Module):\n",
    "        def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "            super().__init__()\n",
    "            self.flash_attention = Expert.FlashAttention2(seq_len, head_dim, blk_size)\n",
    "            self.seq_len = seq_len\n",
    "            self.head_dim = head_dim\n",
    "            self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "            self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        def generate_sparsity_mask(self):\n",
    "            mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "            step = self.sparsity_factor\n",
    "            for i in range(0, self.seq_len, step):\n",
    "                mask[i:i + step, :] = 1\n",
    "            return mask.bool()\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "            print(f\"Initial output shape: {output.shape}\")\n",
    "\n",
    "            # Reshape output to be 3D for batch matrix multiplication\n",
    "            output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "            print(f\"output.unsqueeze(0) shape: {output.shape}\")\n",
    "\n",
    "            sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "            print(f\"Sparsity mask shape: {sparsity_mask.shape}\")\n",
    "\n",
    "            # Apply the sparsity mask to the output\n",
    "            sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "            print(f\"sparsity mask sparsity_mask.unsqueeze(0) shape: {sparsity_mask.shape}\")\n",
    "            output = torch.bmm(sparsity_mask.float(), output.float().transpose(1, 2))  \n",
    "            print(f\"output from torch.bmm shape: {output.shape}\")\n",
    "            output = output.transpose(1, 2)\n",
    "            # Reshape the output back to 2D\n",
    "            output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "            return output\n",
    "\n",
    "\n",
    "    ###############################\n",
    "    # MAMBA\n",
    "    # RoPE\n",
    "    class RotaryPositionalEncoding(nn.Module):\n",
    "        def __init__(self, dim, max_len=5000):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "            t = torch.arange(max_len).type_as(inv_freq)\n",
    "            freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "            self.register_buffer('sin', freqs.sin())\n",
    "            self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "        def forward(self, x):\n",
    "            n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "            sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "            # Apply RoPE to even and odd indices separately\n",
    "            x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "    # SWIGLU\n",
    "    class SwiGLU(nn.Module):\n",
    "        def __init__(self, dim_in, dim_out):\n",
    "            super(Expert.SwiGLU, self).__init__()\n",
    "            self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "            self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "        def forward(self, x):\n",
    "            gate = torch.sigmoid(self.fc2(x))\n",
    "            return self.fc1(x) * gate\n",
    "\n",
    "    class SimplifiedMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "            \n",
    "            self.feedforward = nn.Sequential(\n",
    "                nn.Linear(self.d_model, self.d_state),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(self.d_state, self.d_model)\n",
    "            )\n",
    "            self.input_embedding = nn.Linear(self.d_model, self.d_model)\n",
    "            self.convs = nn.Sequential(*[nn.Conv1d(self.d_model, self.d_model, kernel_size=self.d_conv, padding=(self.d_conv // 2)) for _ in range(self.num_layers)])\n",
    "            self.swiglu = Expert.SwiGLU(self.d_model, self.d_model)\n",
    "            self.output_projection = nn.Linear(self.d_model, self.d_model * self.expansion_factor)\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "            nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "            nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "            nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "        def forward(self, inputs, attention_mask=None):\n",
    "            print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "            # Apply the attention mask if provided\n",
    "            if attention_mask is not None:\n",
    "                inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "            projected_inputs = self.input_embedding(inputs)\n",
    "            print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            for conv in self.convs:\n",
    "                projected_inputs = conv(projected_inputs)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = self.swiglu(projected_inputs)\n",
    "            print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "            output = self.output_projection(projected_inputs)\n",
    "            print(\"output shape:\", output.shape)\n",
    "\n",
    "            return output\n",
    "\n",
    "    class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.vocab_size = config.vocab_size\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "            self.pos_encoder = Expert.RotaryPositionalEncoding(self.d_model)\n",
    "            self.simplified_mamba = Expert.SimplifiedMAMBA(config)\n",
    "            self.output_projection = nn.Linear(self.d_model * 2, self.vocab_size)  # Adjust if needed\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = 1.0\n",
    "\n",
    "            nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "        def forward(self, input_values, attention_mask):\n",
    "            print(f\"SimplifiedLanguageModelMAMBA fwd input_values: {input_values.shape}\")\n",
    "            print(f\"SimplifiedLanguageModelMAMBA fwd input_values: {input_values.dtype}\")\n",
    "            print(f\"Max input_id before embedding: {input_values.max().item()}\")\n",
    "            if input_values.max() >= vocab_size:\n",
    "                print(\"Detected out-of-range value after adjustment.\")\n",
    "\n",
    "            if input_values.dtype != torch.long:\n",
    "              input_values = input_values.long() \n",
    "            embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "            embedded = self.pos_encoder(embedded)\n",
    "            simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "            logits = self.output_projection(simplified_mamba_output)\n",
    "            return logits\n",
    "\n",
    "    ###############################\n",
    "    # Switch Router\n",
    "\n",
    "    class SwitchRouter(nn.Module):\n",
    "        CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "        class SwitchGate(nn.Module):\n",
    "            def __init__(self, input_dim, num_experts):\n",
    "                super(Expert.SwitchRouter.SwitchGate, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "                self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "                return gate_scores\n",
    "\n",
    "        def __init__(self, config):\n",
    "            super(Expert.SwitchRouter, self).__init__()\n",
    "            self.config = config\n",
    "            self.device = config.device\n",
    "            self.router = self.SwitchGate(config.input_dim, config.num_experts).to(config.device)\n",
    "            self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "            self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.transformer_dpo = Expert.DPO(self.lmt, config.device,config.embed_size).to(config.device)\n",
    "            self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "            self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "            self.input_embedding = nn.Linear(config.input_dim, config.input_dim)\n",
    "\n",
    "        def forward(self, x, attention_mask, context_texts, question_text):\n",
    "            x = x.to(self.device)\n",
    "            x = self.input_embedding(x)\n",
    "            gate_scores = self.router(x)\n",
    "            expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "            final_output = torch.zeros_like(x)\n",
    "\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                mask = expert_indices == i\n",
    "                if mask.any():\n",
    "                    selected_inputs = x[mask]\n",
    "                    selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                    if isinstance(expert, Expert.TransformerRAG):\n",
    "                        expert_output = expert.forward(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                    elif isinstance(expert, Expert.DPO):  # Check if the expert is an instance of DPO\n",
    "                        # Use the forward_expert method for DPO within the routing process\n",
    "                        expert_output = expert.forward_expert(selected_inputs, selected_attention_mask, context_texts, question_text)\n",
    "                    else:\n",
    "                        # For other experts, continue using the standard forward method\n",
    "                        expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                    final_output[mask] = expert_output\n",
    "\n",
    "            aux_loss = self.auxiliary_loss(gate_scores)\n",
    "            return final_output, aux_loss\n",
    "\n",
    "        def auxiliary_loss(self, gate_scores):\n",
    "            expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "            loss_balancing = torch.std(expert_load)\n",
    "            return loss_balancing\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "            capacity_factor_tensor = torch.tensor([Expert.SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "            capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "            expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "            for idx in range(len(expert_indices)):\n",
    "                selected_expert = expert_indices[idx]\n",
    "                if expert_counts[selected_expert] < capacities[0]:\n",
    "                    expert_counts[selected_expert] += 1\n",
    "                else:\n",
    "                    available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                    if len(available_experts) > 0:\n",
    "                        alternative_expert = available_experts[0]\n",
    "                        expert_indices[idx] = alternative_expert\n",
    "                        expert_counts[alternative_expert] += 1\n",
    "                    else:\n",
    "                        print(\"No available experts to reroute. Handling overflow.\")\n",
    "            return expert_indices\n",
    "    \n",
    "    ###############################\n",
    "    # RAG\n",
    "    \n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self, d_model, max_len=10000):\n",
    "            super(Expert.PositionalEncoding, self).__init__()\n",
    "            self.d_model = d_model\n",
    "            self.max_len = max_len\n",
    "\n",
    "            # Create positional encodings\n",
    "            pe = torch.zeros(max_len, d_model)\n",
    "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "            # Add a batch dimension (B x T x C)\n",
    "            pe = pe.unsqueeze(0)\n",
    "            self.register_buffer('pe', pe)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # x: Tensor of shape [Batch Size, Sequence Length, Embedding Dimension]\n",
    "            # Adjust positional encoding to match the input size and device\n",
    "            pe = self.pe[:, :x.size(1)]\n",
    "            # Assuming x is on the correct device, pe will be automatically aligned to the same device\n",
    "            return pe\n",
    "\n",
    "\n",
    "    class AdaptiveDropoutLayer(nn.Module):\n",
    "        def __init__(self, init_dropout_rate=0.1):\n",
    "            super(Expert.AdaptiveDropoutLayer, self).__init__()\n",
    "            # Use logit transformation for stability\n",
    "            self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())\n",
    "\n",
    "        def forward(self, x):\n",
    "            p = torch.sigmoid(self.log_alpha)\n",
    "            # Convert p from a tensor to a float\n",
    "            p_value = p.item()  # This extracts the scalar value as a Python float\n",
    "            return nn.functional.dropout(x, p=p_value, training=self.training)\n",
    "\n",
    "\n",
    "    class MultiHeadLinformerAttention(nn.Module):\n",
    "        def __init__(self, embed_dim, num_heads, k=None):\n",
    "            super().__init__()  # Ensure this is called first\n",
    "            self.embed_dim = embed_dim\n",
    "            self.num_heads = num_heads\n",
    "            self.k = k if k is not None else embed_dim // num_heads  # Projection dimension per head\n",
    "\n",
    "            self.key_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "            self.value_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "            self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "        def forward(self, query, attention_mask=None):\n",
    "            batch_size, seq_len, _ = query.size()\n",
    "            \n",
    "            # Project keys and values\n",
    "            keys = self.key_projections(query)\n",
    "            values = self.value_projections(query)\n",
    "            \n",
    "            # Reshape into [batch_size, num_heads, seq_len, k]\n",
    "            keys = keys.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "            values = values.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "            \n",
    "            # Calculate attention (scaled dot-product attention)\n",
    "            # Scaling by the square root of the depth of the key vectors to prevent large values in the dot product\n",
    "            # which could push the softmax function into regions where it has extremely small gradients\n",
    "            keys = keys / (self.k ** 0.5)\n",
    "            attention_scores = torch.softmax(torch.matmul(keys, values.transpose(-2, -1)), dim=-1)\n",
    "            if attention_mask is not None:\n",
    "                attention_scores = attention_scores.masked_fill(~attention_mask.bool(), float('-inf'))\n",
    "                # Recalculate softmax for masked scores\n",
    "                attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "            # Apply attention to values\n",
    "            out = torch.matmul(attention_scores, values)\n",
    "            \n",
    "            # Concatenate heads and project back to original embedding dimension\n",
    "            out = out.transpose(1, 2).reshape(batch_size, seq_len, self.num_heads * self.k)\n",
    "            out = self.out_projection(out)\n",
    "            \n",
    "            return out\n",
    "\n",
    "\n",
    "    class AdaptiveEmbeddingLayer(nn.Module):\n",
    "        def __init__(self, vocab,  vocab_size, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "            super(Expert.AdaptiveEmbeddingLayer, self).__init__()\n",
    "            self.vocab = vocab\n",
    "            self.vocab_size = vocab_size\n",
    "            self.freq_threshold = freq_threshold\n",
    "            self.large_embed_dim = large_embed_dim\n",
    "            self.small_embed_dim = small_embed_dim\n",
    "            self.max_seq_len = max_seq_len\n",
    "\n",
    "            self.split_vocab(vocab, freq_threshold)  \n",
    "\n",
    "            self.frequent_embeddings = nn.Embedding(num_embeddings=len(self.frequent_vocab), embedding_dim=large_embed_dim)\n",
    "            self.infrequent_embeddings = nn.Embedding(num_embeddings=len(self.infrequent_vocab), embedding_dim=small_embed_dim)\n",
    "            self.infrequent_projection = nn.Linear(small_embed_dim, large_embed_dim)\n",
    "            self.positional_embeddings = Expert.PositionalEncoding(large_embed_dim, max_seq_len)\n",
    "\n",
    "\n",
    "        def split_vocab(self, vocab, freq_threshold):\n",
    "            token_counts = [(token, count) for token, count in vocab.items()]\n",
    "            token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "            split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "            \n",
    "            self.frequent_vocab = dict(token_counts[:split_point])\n",
    "            self.infrequent_vocab = dict(token_counts[split_point:])\n",
    "\n",
    "        def forward(self, token_ids):\n",
    "            device = token_ids.device\n",
    "            seq_len = token_ids.size(1)\n",
    "            batch_size = token_ids.size(0)  # Obtain batch size from token_ids tensor\n",
    "\n",
    "            # Initialize embeddings tensor\n",
    "            embeddings = torch.zeros(token_ids.shape[0], seq_len, self.large_embed_dim, device=device)\n",
    "\n",
    "            # Map token_ids to indices for frequent and infrequent vocab\n",
    "            frequent_indices = torch.zeros_like(token_ids)\n",
    "            infrequent_indices = torch.zeros_like(token_ids)\n",
    "            \n",
    "            for token_id, index in self.vocab.items():\n",
    "                mask = token_ids == token_id\n",
    "                if token_id in self.frequent_vocab:\n",
    "                    # Map to index in frequent_vocab\n",
    "                    frequent_indices[mask] = self.frequent_vocab[token_id]\n",
    "                elif token_id in self.infrequent_vocab:\n",
    "                    # Map to index in infrequent_vocab\n",
    "                    infrequent_indices[mask] = self.infrequent_vocab[token_id]\n",
    "\n",
    "            # Create masks for frequent and infrequent tokens\n",
    "            frequent_mask = frequent_indices > 0\n",
    "            infrequent_mask = infrequent_indices > 0\n",
    "\n",
    "            # Embed frequent tokens\n",
    "            if frequent_mask.any():\n",
    "                frequent_embeddings = self.frequent_embeddings(frequent_indices[frequent_mask])\n",
    "                embeddings[frequent_mask] = frequent_embeddings\n",
    "\n",
    "            # Embed and project infrequent tokens\n",
    "            if infrequent_mask.any():\n",
    "                infrequent_embeddings = self.infrequent_embeddings(infrequent_indices[infrequent_mask])\n",
    "                infrequent_embeddings_projected = self.infrequent_projection(infrequent_embeddings)\n",
    "                embeddings[infrequent_mask] = infrequent_embeddings_projected\n",
    "\n",
    "            # Apply positional embeddings\n",
    "            position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            position_embeddings = self.positional_embeddings(position_ids)  # Generate for seq_len\n",
    "\n",
    "            # Ensure positional embeddings are broadcastable to the embeddings tensor\n",
    "            if position_embeddings.size(0) != batch_size:\n",
    "                position_embeddings = position_embeddings.expand(batch_size, -1, -1)\n",
    "\n",
    "            print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "            print(f\"Positional embeddings shape: {position_embeddings.shape}\")\n",
    "            embeddings += position_embeddings\n",
    "\n",
    "            return embeddings\n",
    "\n",
    "\n",
    "\n",
    "    class DPRContextEncoder(nn.Module):\n",
    "        def __init__(self, config):  # Accept an ExpertConfig instance\n",
    "            super().__init__()\n",
    "            self.wordpiece_tokenizer = config.wordpiece_tokenizer\n",
    "            # Use attributes from config directly\n",
    "            self.embedding_layer = Expert.AdaptiveEmbeddingLayer(\n",
    "                config.wordpiece_vocab,\n",
    "                config.vocab_size,\n",
    "                config.freq_threshold,\n",
    "                config.embedding_dim,\n",
    "                config.embedding_dim // 4,  \n",
    "                max_seq_len=config.max_length  # Use max_length from config\n",
    "            )\n",
    "            self.attention_layer = Expert.MultiHeadLinformerAttention(\n",
    "                config.embedding_dim, \n",
    "                num_heads=config.heads\n",
    "            )\n",
    "            self.dropout = Expert.AdaptiveDropoutLayer(init_dropout_rate=config.dropout)  \n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            embeddings = self.embedding_layer(input_ids)\n",
    "            attention_output = self.attention_layer(embeddings, attention_mask=attention_mask)\n",
    "            attention_output = self.dropout(attention_output)\n",
    "\n",
    "            # Mean pooling across the sequence length dimension\n",
    "            pooled_output = attention_output.mean(dim=1)\n",
    "\n",
    "            return pooled_output\n",
    "\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, config):  # Accept an ExpertConfig instance\n",
    "            super().__init__()\n",
    "            self.wordpiece_tokenizer = config.wordpiece_tokenizer\n",
    "            # Use attributes from config directly\n",
    "            self.embedding_layer = Expert.AdaptiveEmbeddingLayer(\n",
    "                config.wordpiece_vocab,\n",
    "                config.vocab_size,\n",
    "                config.freq_threshold,\n",
    "                config.embedding_dim,\n",
    "                config.embedding_dim // 4,  \n",
    "                max_seq_len=config.max_length  # Use max_length from config\n",
    "            )\n",
    "            self.attention_layer = Expert.MultiHeadLinformerAttention(\n",
    "                config.embedding_dim, \n",
    "                num_heads=config.heads\n",
    "            )\n",
    "            self.dropout = Expert.AdaptiveDropoutLayer(init_dropout_rate=config.dropout)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            embeddings = self.embedding_layer(input_ids)\n",
    "            attention_output = self.attention_layer(embeddings, attention_mask=attention_mask)\n",
    "            attention_output = self.dropout(attention_output)\n",
    "\n",
    "            # Mean pooling across the sequence length dimension\n",
    "            pooled_output = attention_output.mean(dim=1)\n",
    "\n",
    "            return pooled_output\n",
    "\n",
    "\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(Expert.TransformerRAG, self).__init__()\n",
    "            self.config = config\n",
    "            self.embed_size = config.embed_size  \n",
    "            self.context_encoder = Expert.DPRContextEncoder(config).to(config.device)          \n",
    "            self.language_model = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.question_encoder = Expert.DPRQuestionEncoder(config).to(config.device)\n",
    "            self.tokenizer = config.wordpiece_tokenizer\n",
    "\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= config.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "            \n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "                \n",
    "                aggregated_context_embedding = torch.zeros(config.embedding_dim, device=config.device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = torch.tensor(context['input_ids']).unsqueeze(0).to(config.device)  # Add unsqueeze(0) for batch dimension\n",
    "                    context_attention_mask = torch.tensor(context['attention_mask']).unsqueeze(0).to(config.device)  # Add unsqueeze(0) for batch dimension\n",
    "                    print(f\"context_input_ids shape: {context_input_ids.shape}\")\n",
    "                    print(f\"context_attention_mask shape: {context_attention_mask.shape}\")\n",
    "\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "                \n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "            \n",
    "            question_input_ids = question_input_ids.to(config.device).long()\n",
    "            question_attention_mask = question_attention_mask.to(config.device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "            \n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            #similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            similarities = [cos_sim(question_embeddings.unsqueeze(0), context_emb.unsqueeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=config.device))\n",
    "            \n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(config.device) for k, v in tokenized_combined_input.items()}\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            #response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            response = \" \".join(predicted_tokens).replace(\" </w>\", \"\").replace(\"</w>\", \" \").strip()\n",
    "            \n",
    "            return response\n",
    "\n",
    "        @staticmethod\n",
    "        def extract_text_from_pdf(file_path):\n",
    "            text = \"\"\n",
    "            with fitz.open(file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            return text\n",
    "\n",
    "        @staticmethod\n",
    "        def split_into_chunks(text, chunk_size):\n",
    "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        @staticmethod\n",
    "        def preprocess_text(text, tokenizer, max_length=512):\n",
    "            chunk_size = max_length - 50\n",
    "            text_chunks = Expert.TransformerRAG.split_into_chunks(text, chunk_size)\n",
    "            processed_chunks = []\n",
    "            for chunk in text_chunks:\n",
    "                # Tokenize the chunk using the WordPiece tokenizer\n",
    "                token_ids = tokenizer.tokenize(chunk)\n",
    "                \n",
    "                # Manual padding and attention mask creation\n",
    "                attention_mask = [1] * len(token_ids)\n",
    "                # Padding: Extend token_ids and attention_mask to max_length\n",
    "                while len(token_ids) < max_length:\n",
    "                    token_ids.append(tokenizer.unk_token_id)  # Use unk_token_id for padding\n",
    "                    attention_mask.append(0)  # Padding token has attention mask 0\n",
    "                \n",
    "                # Ensure token_ids and attention_mask are not longer than max_length\n",
    "                token_ids = token_ids[:max_length]\n",
    "                attention_mask = attention_mask[:max_length]\n",
    "                \n",
    "                processed_chunk = {\n",
    "                    'input_ids': token_ids,\n",
    "                    'attention_mask': attention_mask\n",
    "                }\n",
    "                processed_chunks.append(processed_chunk)\n",
    "            return processed_chunks\n",
    "\n",
    "        @staticmethod\n",
    "        def create_dataset_from_pdfs(pdf_file_paths, tokenizer):\n",
    "            dataset = []\n",
    "            for file_path in pdf_file_paths:\n",
    "                text = Expert.TransformerRAG.extract_text_from_pdf(file_path)\n",
    "                processed_text = Expert.TransformerRAG.preprocess_text(text, tokenizer)                \n",
    "                dataset.append(processed_text)\n",
    "            return dataset\n",
    "\n",
    "        def retrieve_contexts(self, dataset, query_embedding, top_k=5):\n",
    "            similarity_scores = []\n",
    "            for context in dataset:\n",
    "                context_input_ids = context['input_ids'].to(self.config.device)\n",
    "                context_attention_mask = context['attention_mask'].to(self.config.device)\n",
    "                # Use the class's context_encoder\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "                similarity_scores.append(similarity.squeeze().item())\n",
    "            top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "            top_contexts = [dataset[i] for i in top_k_indices]\n",
    "            return top_contexts\n",
    "\n",
    "        def rag_retrieve_and_generate(self, dataset, query):\n",
    "            # Tokenize the query using the class's tokenizer\n",
    "            tokenized_query = self.tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
    "            input_ids = tokenized_query['input_ids']\n",
    "            attention_mask = tokenized_query['attention_mask']\n",
    "            # Use the class's question_encoder\n",
    "            encoded_query = self.question_encoder(input_ids, attention_mask)\n",
    "            relevant_contexts = self.retrieve_contexts(dataset, encoded_query)\n",
    "            # Assuming generate_response is a method of LanguageModelTransformer that accepts tokenized contexts and generates a response\n",
    "            response = self.language_model.generate_response(relevant_contexts)\n",
    "            return response\n",
    "\n",
    "    ###############################\n",
    "    # LORA\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "            super(Expert.LORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "\n",
    "            # Original weight and bias of the linear layer\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # LORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # QLORA\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "            super(Expert.QLORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "            self.quantization_bits = quantization_bits\n",
    "\n",
    "            # Original weight and bias\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # QLORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    ###############################\n",
    "    # Language Model Transformer\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, embed_size, heads):\n",
    "            super(Expert.MultiHeadAttention, self).__init__()\n",
    "            self.embed_size = embed_size\n",
    "            self.heads = heads\n",
    "            self.head_dim = embed_size // heads\n",
    "\n",
    "            assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "            ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        def forward(self, values, keys, query, mask):\n",
    "            N = query.shape[0]\n",
    "            value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "            # Split the embedding into self.heads different pieces\n",
    "            values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "            queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            # Einsum does the matrix multiplication for query*keys for each training example\n",
    "            # with every other training example, then sum it up\n",
    "            attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "                N, query_len, self.heads * self.head_dim\n",
    "            )\n",
    "\n",
    "            out = self.fc_out(out)\n",
    "            return out\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "            super(Expert.TransformerBlock, self).__init__()\n",
    "            self.attention = Expert.MultiHeadAttention(embed_size, heads)\n",
    "            self.norm1 = nn.LayerNorm(embed_size)\n",
    "            self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                Expert.LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, value, key, query, mask):\n",
    "            attention = self.attention(value, key, query, mask)\n",
    "            x = self.dropout(self.norm1(attention + query))\n",
    "            forward = self.feed_forward(x)\n",
    "            out = self.dropout(self.norm2(forward + x))\n",
    "            return out\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "            super(Expert.LanguageModelDecoder, self).__init__()\n",
    "            self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "            # Adding BatchNorm layers\n",
    "            self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "            self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    Expert.TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # QLORA layers\n",
    "            self.qlora_feed_forward = nn.Sequential(\n",
    "                Expert.QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "            self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "            self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, trg_mask):\n",
    "            N, seq_length = x.shape\n",
    "            if seq_length > self.position_embedding.num_embeddings:\n",
    "                raise ValueError(f\"Sequence length {seq_length} exceeds maximum allowed {self.position_embedding.num_embeddings}\")\n",
    "            positions = torch.arange(0, seq_length).expand(N, seq_length).to(config.device)\n",
    "            x = x.long() # make x a long type for the embeddings\n",
    "            x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "            print(f\"Max position index: {positions.max().item()}, Position Embedding Size: {self.position_embedding.num_embeddings}\")\n",
    "\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn1(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, trg_mask)\n",
    "                if self.use_qlora:\n",
    "                    x = self.qlora_feed_forward(x)\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            out = self.fc_out(x)\n",
    "            #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "            return out\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.use_qlora = use_qlora\n",
    "\n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.vocab_size = config.vocab_size  # Use vocab_size from ExpertConfig\n",
    "            self.embed_size = config.embed_size  # Use embed_size from ExpertConfig\n",
    "            self.num_layers = config.num_layers  # Use num_layers from ExpertConfig\n",
    "            self.forward_expansion = config.forward_expansion  # Use forward_expansion from ExpertConfig\n",
    "            self.heads = config.heads  # Use heads from ExpertConfig\n",
    "            self.dropout = config.dropout  # Use dropout from ExpertConfig\n",
    "            self.max_length = config.max_length  # Use max_length from ExpertConfig\n",
    "            self.rank = config.rank  # Use rank from ExpertConfig\n",
    "            self.tokenizer_name = config.tokenizer_name  # Use tokenizer_name from ExpertConfig\n",
    "\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "            self.decoder = Expert.LanguageModelDecoder(\n",
    "                self.vocab_size,\n",
    "                self.embed_size,\n",
    "                self.num_layers,\n",
    "                self.heads,\n",
    "                self.forward_expansion,\n",
    "                self.dropout,\n",
    "                self.max_length,\n",
    "                self.rank,\n",
    "            )\n",
    "\n",
    "        def forward(self, trg, attention_mask=None):  # Remove attention_mask here since it's not used\n",
    "            print(f\"Input shape to LanguageModelTransformer: {trg.shape}\")\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            out = self.decoder(trg, trg_mask)  # Do not pass attention_mask here\n",
    "            print(f\"Language Model Transformer out shape: {out.shape}\")\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len).to(trg.device)\n",
    "            return trg_mask\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = [self.tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            return response\n",
    "\n",
    "    ###############################\n",
    "    # DPO\n",
    "    class DPO(nn.Module):\n",
    "        def __init__(self, language_model, device, embed_size):\n",
    "            super(Expert.DPO, self).__init__()\n",
    "            self.language_model = language_model\n",
    "            self.device = device\n",
    "            # Assuming embed_size is accessible and correct\n",
    "            self.projection = nn.Linear(language_model.vocab_size, embed_size)  # Project from vocab_size to embed_size\n",
    "            self.classifier = nn.Linear(embed_size, 2)  # Assuming embed_size is accessible\n",
    "            \n",
    "\n",
    "        def forward(self, input_ids, labels=None):\n",
    "            print(f\"Max input_id in DPO fwd before embedding: {input_ids.max().item()}\")\n",
    "\n",
    "            logits = self.language_model(input_ids)  # Output shape: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "            # Ensure logits are correctly projected to embed_size per token\n",
    "            projected_logits = self.projection(logits.view(-1, logits.size(-1)))\n",
    "            projected_logits = projected_logits.view(logits.size(0), logits.size(1), -1)  # Reshape to [batch_size, seq_len, embed_size]\n",
    "\n",
    "            # Apply global mean pooling across the sequence length dimension correctly\n",
    "            pooled_logits = projected_logits.mean(dim=1)  # Correctly applies mean pooling across seq_len\n",
    "\n",
    "            predictions = self.classifier(pooled_logits)\n",
    "\n",
    "            loss = None\n",
    "\n",
    "            print(f\"logits shape: {logits.shape}\")\n",
    "            print(f\"projected_logits shape: {projected_logits.shape}\")\n",
    "            print(f\"pooled_logits shape: {pooled_logits.shape}\")\n",
    "            print(f\"predictions shape: {predictions.shape}\")\n",
    "            if labels is not None:\n",
    "                print(f\"labels shape: {labels.shape}\")\n",
    "                # Ensure labels are flattened if they're not already 1D\n",
    "                if labels.dim() > 1:\n",
    "                    labels = labels.view(-1)  # Flatten labels to match predictions shape\n",
    "                loss_fct = nn.CrossEntropyLoss()  # Correctly instantiate the loss function\n",
    "                loss = loss_fct(predictions, labels)\n",
    "\n",
    "            return predictions, loss\n",
    "\n",
    "\n",
    "     \n",
    "        def forward_expert(self, input_ids, attention_mask=None, context_texts=None, question_text=None):\n",
    "            \"\"\"\n",
    "            Special forward method designed for use within the Expert Model, where the DPO component\n",
    "            is one of several being routed and managed.\n",
    "            \"\"\"\n",
    "            print(f\"Max input_id in DPO forward_expert before embedding: {input_ids.max().item()}\")\n",
    "\n",
    "            # Get the logits from the language model\n",
    "            logits = self.language_model(input_ids)  # Assuming output shape: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "            # Mean pooling over the sequence length dimension to get shape: [batch_size, vocab_size]\n",
    "            pooled_logits = logits.mean(dim=1)\n",
    "\n",
    "            # Use the projection layer to transform the logits from vocab_size to embed_size\n",
    "            # This aligns with the expected input dimensions for the classifier\n",
    "            transformed_logits = self.projection(pooled_logits)  # New shape: [batch_size, embed_size]\n",
    "\n",
    "            # Now, the transformed logits can be correctly classified\n",
    "            predictions = self.classifier(transformed_logits)\n",
    "\n",
    "            return predictions\n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self, config: ExpertConfig):\n",
    "        super().__init__()\n",
    "        # Validate configuration\n",
    "        config.validate()\n",
    "\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(\n",
    "            config.seq_len, \n",
    "            config.head_dim, \n",
    "            config.block_size, \n",
    "            config.sparsity_factor\n",
    "        )\n",
    "\n",
    "        # Inside the Expert class definition\n",
    "        self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "\n",
    "        self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "\n",
    "        # Corrected instantiation of SimplifiedLanguageModelMAMBA\n",
    "        self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "\n",
    "        # Now initialize DPO with the language model transformer\n",
    "        self.transformer_dpo = Expert.DPO(self.lmt, config.device, config.embed_size).to(config.device)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(config.seq_len)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = Expert.SwitchRouter(config)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = Expert.QLORALayer(config.input_dim, config.input_dim, config.rank)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x, x, x)  # Pass x as Q, K, and V\n",
    "        print(f\"Shape after SparseFlash2Attention: {x.shape}\") \n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "        print(f\"Shape after LayerNorm: {x.shape}\") \n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, aux_loss = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "        \"\"\"\n",
    "        Calculate a new alpha value based on the current loss.\n",
    "        \"\"\"\n",
    "        if current_loss >= initial_loss:\n",
    "            return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "        loss_ratio = current_loss / initial_loss\n",
    "        alpha_range = initial_alpha - final_alpha\n",
    "        new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "        return new_alpha  \n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Linear warmup with cosine decay\n",
    "        scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "        return optimizer, scheduler\n",
    "    ###############################\n",
    "    # TRAINING METHODS\n",
    "\n",
    "    # DPO Training\n",
    "    def train_dpo(self, train_loader, optimizer, config, save_path):\n",
    "        self.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(config.device)  # Adjusted to use 'input_ids'\n",
    "            labels = batch['labels'].to(config.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: Adjust the model's forward method to accept the single 'input_ids' input\n",
    "            logits, loss = self.transformer_dpo(input_ids, labels=labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        print(f\"Training complete. Average Loss: {average_loss}\")\n",
    "\n",
    "        # Save the model\n",
    "        torch.save(self.transformer_dpo.state_dict(), save_path)\n",
    "\n",
    "        return average_loss\n",
    "\n",
    "    # RAG Training\n",
    "    def train_language_model_rag(self, model, train_loader, device, vocab_size,num_epochs=5):\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "        \n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            model.decoder.toggle_qlora(True)\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs, targets = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                print(\"Output shape:\", outputs.shape)\n",
    "                print(\"Targets shape:\", targets.shape)\n",
    "                loss = criterion(outputs.contiguous().view(-1, 30522), targets.view(-1))\n",
    "\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        return model, average_loss\n",
    "\n",
    "\n",
    "    # DPR Training\n",
    "    def train_dpr_encoders(self, train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs, context_save_path, question_save_path):\n",
    "        loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(train_data[\"queries\"])):\n",
    "                query = train_data[\"queries\"][i]\n",
    "                context_list = train_data[\"contexts\"][i]  # This is a list of context dicts\n",
    "\n",
    "                # Tokenize query using the tokenize method and convert token IDs back to tensors for model input\n",
    "                tokenized_query = config.wordpiece_tokenizer.tokenize(query)\n",
    "                input_ids_query = torch.tensor([tokenized_query], dtype=torch.long).to(config.device)\n",
    "                attention_mask_query = torch.ones_like(input_ids_query).to(config.device)\n",
    "\n",
    "                # Since context is a list of dictionaries with 'input_ids', process each and average embeddings\n",
    "                context_embeddings_list = []\n",
    "                for context in context_list:\n",
    "                    if 'input_ids' in context:\n",
    "                        input_ids_context = torch.tensor([context['input_ids']], dtype=torch.long).to(config.device)\n",
    "                        attention_mask_context = torch.ones_like(input_ids_context, dtype=torch.bool).to(config.device)\n",
    "\n",
    "                        # Adjusted for using input_ids and attention_mask directly\n",
    "                        context_embedding = context_encoder(input_ids_context, attention_mask_context)\n",
    "                        context_embeddings_list.append(context_embedding)\n",
    "\n",
    "                # Average the context embeddings if there are multiple contexts\n",
    "                if context_embeddings_list:\n",
    "                    context_embeddings = torch.mean(torch.stack(context_embeddings_list), dim=0)\n",
    "                else:\n",
    "                    raise ValueError(\"No valid contexts found for averaging embeddings.\")\n",
    "\n",
    "                # Forward pass for the query\n",
    "                question_embeddings = question_encoder(input_ids_query, attention_mask_query)\n",
    "\n",
    "                # Compute loss with labels for positive examples\n",
    "                labels = torch.tensor([1.0] * question_embeddings.size(0), dtype=torch.float).to(config.device)\n",
    "                loss = loss_function(question_embeddings, context_embeddings, labels)\n",
    "                optimizer_context.zero_grad()\n",
    "                optimizer_question.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_context.step()\n",
    "                optimizer_question.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_data['queries'])}\")\n",
    "\n",
    "        average_loss = total_loss / len(train_data['queries'])\n",
    "        torch.save(context_encoder.state_dict(), context_save_path)\n",
    "        torch.save(question_encoder.state_dict(), question_save_path)\n",
    "        return (context_encoder, question_encoder), average_loss\n",
    "\n",
    "       \n",
    "    # LMT Training\n",
    "    def train_language_model_transformer(self, train_loader, device, vocab_size, save_path):\n",
    "        model = self.lmt\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "        num_epochs = 5\n",
    "        \n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            model.decoder.toggle_qlora(True)\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs, targets = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        return model, average_loss\n",
    "\n",
    "    # MAMBA Training\n",
    "    def train_mamba(self, train_loader, num_epochs, config):\n",
    "            # Initialize the optimizer and scheduler with MAMBA model parameters\n",
    "            optimizer, scheduler = self.setup_optimizer(self.mamba, \n",
    "                                                        config.mamba_learning_rate, \n",
    "                                                        config.weight_decay, \n",
    "                                                        config.warmup_steps, \n",
    "                                                        config.total_mamba_steps)\n",
    "\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            progress_bar = tqdm(range(num_epochs))\n",
    "\n",
    "            for epoch in progress_bar:\n",
    "                self.mamba.train()\n",
    "                total_loss = 0\n",
    "\n",
    "                for batch in train_loader:\n",
    "                    input_values, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "                    input_values = input_values.to(config.device)\n",
    "                    attention_mask = attention_mask.to(config.device)\n",
    "                    labels = labels.to(config.device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass through MAMBA model\n",
    "                    outputs = self.mamba(input_values, attention_mask)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = loss_fn(outputs.view(-1, config.vocab_size), labels.view(-1))\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Clip gradients and perform an optimization step\n",
    "                    torch.nn.utils.clip_grad_norm_(self.mamba.parameters(), config.clip_gradient)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save the trained MAMBA model\n",
    "            torch.save(self.mamba.state_dict(), config.mamba_model_path)\n",
    "            print(f\"MAMBA Training Complete. Model saved to {config.mamba_model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Full Expert Training\n",
    "    def train_expert(self, train_loader, train_data, optimizer, main_loss_function, aux_loss_weight, device,save_path, accumulation_steps=4, num_epochs=5):\n",
    "            self.train()  # Set the model to training mode\n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0\n",
    "                optimizer.zero_grad()  # Initialize gradients to zero\n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "                    inputs, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "\n",
    "                    # Calculate start and end indices for current batch in train_data\n",
    "                    start_idx = batch_idx * batch['input_ids'].size(0)\n",
    "                    end_idx = start_idx + batch['input_ids'].size(0)\n",
    "\n",
    "                    # Extract current_queries and current_contexts for the batch\n",
    "                    current_queries = train_data['queries'][start_idx:end_idx]\n",
    "                    current_contexts = train_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "                    # Call to the model forward function\n",
    "                    outputs, aux_loss = self(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "                    # Calculate loss and accumulate\n",
    "                    main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                    loss = (main_loss + aux_loss_weight * aux_loss) / accumulation_steps\n",
    "                    loss.backward()\n",
    "\n",
    "                    if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    total_loss += loss.item() * accumulation_steps  # Scale back up\n",
    "\n",
    "                average_loss = total_loss / len(train_loader)\n",
    "                print(f'End of Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            torch.save(self.state_dict(), save_path)\n",
    "            return self, average_loss\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Tokenizer\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None\n",
    "        self.frequency = 0\n",
    "        self.failure_link = None\n",
    "        self.is_end = False  # Add is_end attribute to mark the end of a word\n",
    "        self.token = None  # Add token attribute to store the token associated with the node\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self, unk_token_id=0):\n",
    "        self.root = TrieNode()\n",
    "        self.unk_token_id = unk_token_id\n",
    "\n",
    "    def insert(self, token, token_id, frequency):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "        node.frequency = frequency\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        \"\"\"Finds the most probable subwords based on frequency.\"\"\"\n",
    "        node = self.root\n",
    "        best_subwords = []\n",
    "\n",
    "        def dfs(current_node, subword='', collected_subwords=[]):\n",
    "            if current_node.token_id is not None:\n",
    "                # Update to correctly calculate total_frequency based on the structure of collected_subwords\n",
    "                total_frequency = sum(n.frequency for _, _, n in collected_subwords) + current_node.frequency\n",
    "                probability = current_node.frequency / total_frequency if total_frequency else 0\n",
    "                collected_subwords.append((subword, probability, current_node))\n",
    "\n",
    "            for char, next_node in current_node.children.items():\n",
    "                dfs(next_node, subword + char, list(collected_subwords))  # Create a copy of the list to avoid shared state\n",
    "\n",
    "        dfs(node)\n",
    "        best_subwords = sorted(best_subwords, key=lambda x: x[1], reverse=True)\n",
    "        return [subword for subword, _, _ in best_subwords][:5] or [self.unk_token_id]\n",
    "\n",
    "\n",
    "    def compute_failure_links(self):\n",
    "        root = self.root\n",
    "        root.failure_link = root  # Root's failure link points to itself\n",
    "        queue = [root]\n",
    "\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)\n",
    "\n",
    "                # Follow failure link to find the longest suffix for the child_node\n",
    "                failure_candidate = current_node.failure_link\n",
    "                while failure_candidate != root and char not in failure_candidate.children:\n",
    "                    failure_candidate = failure_candidate.failure_link\n",
    "                child_node.failure_link = failure_candidate.children.get(char, root)\n",
    "\n",
    "\n",
    "class SimpleSentencePiece:\n",
    "    def __init__(self, model_type=\"bpe\", vocab_size=30522):\n",
    "        self.vocab = {}\n",
    "        self.id_to_subword = {}\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.unk_token_id = 0\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model = None if model_type == \"bpe\" else None\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def train(self, text):\n",
    "        if self.model_type == \"bpe\":\n",
    "            self.model = BPE(num_merges=self.vocab_size, unk_token_id=self.unk_token_id)\n",
    "            self.model.train(text)\n",
    "            self.vocab = self.model.vocab\n",
    "            self.id_to_subword = {i: word for word, i in self.vocab.items()}\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported yet.\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.preprocess_text(text)  # Preprocess text before encoding\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        encoded = self.model.encode(text)\n",
    "        #print(f\"Encoded: {encoded[:10]}\")  # Print first 10 encoded tokens\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if not self.id_to_subword:\n",
    "            raise ValueError(\"Vocabulary is empty. Ensure the model is trained first.\")\n",
    "        text = \" \".join([self.id_to_subword.get(id_, self.unk_token) for id_ in ids])\n",
    "        text = text.replace(\" </w>\", \"\").replace(\"</w>\", \" \").strip()\n",
    "        return text\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Convert text to lowercase to ensure case insensitivity\n",
    "        text = text.lower()\n",
    "        # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "        text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Trim leading and trailing spaces\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        model_data = {\n",
    "            'vocab': self.vocab,\n",
    "            'id_to_subword': self.id_to_subword,\n",
    "            'model_type': self.model_type,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            # Potentially include other relevant attributes\n",
    "        }\n",
    "        # Save the high-level tokenizer settings\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(model_data, f)\n",
    "        \n",
    "        # Now save the BPE model specifically\n",
    "        if self.model_type == \"bpe\" and self.model:\n",
    "            self.model.save_model(filepath + \"_bpe\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        self.vocab = model_data['vocab']\n",
    "        self.id_to_subword = model_data['id_to_subword']\n",
    "        self.model_type = model_data['model_type']\n",
    "        self.vocab_size = model_data['vocab_size']\n",
    "        \n",
    "        # Assuming model_type is still \"bpe\", we now load the BPE model\n",
    "        if self.model_type == \"bpe\":\n",
    "            self.model = BPE(self.vocab_size, self.unk_token_id)\n",
    "            self.model.load_model(filepath + \"_bpe\")\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, num_merges=100, unk_token_id=0):  # Accept unk_token_id parameter\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        self.num_merges = num_merges\n",
    "        self.unk_token_id = unk_token_id  # Store the unknown token ID\n",
    "\n",
    "    def train(self, text):\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "        vocab = collections.Counter(words)\n",
    "        vocab = {word + '</w>': count for word, count in vocab.items()}\n",
    "        \n",
    "        for _ in range(self.num_merges):  # Use the num_merges from the instance variable\n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best, vocab)\n",
    "            self.merges.append(best)\n",
    "\n",
    "        self.vocab = {word: i for i, word in enumerate(vocab.keys())}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stats(vocab):\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[symbols[i], symbols[i+1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_vocab(pair, vocab):\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in vocab:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = vocab[word]\n",
    "        return v_out\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text into subwords using learned BPE merges.\"\"\"\n",
    "        encoded_tokens = []\n",
    "        for word in re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE):\n",
    "            word += '</w>'\n",
    "            subwords = [word]  # Start with the entire word as one subword\n",
    "            for merge in self.merges:\n",
    "                new_subwords = []\n",
    "                for subword in subwords:\n",
    "                    # If the merge is in subword, split it; otherwise, keep it as is\n",
    "                    if ' '.join(merge) in subword:\n",
    "                        new_subwords.extend(subword.replace(' '.join(merge), ''.join(merge)).split(' '))\n",
    "                    else:\n",
    "                        new_subwords.append(subword)\n",
    "                subwords = new_subwords\n",
    "            encoded_tokens.extend(subwords)\n",
    "        return [self.vocab.get(token, self.unk_token_id) for token in encoded_tokens]\n",
    "    \n",
    "        # New method to save trained model\n",
    "    def save_model(self, filepath):\n",
    "        bpe_data = {\n",
    "            'merges': self.merges,\n",
    "            'vocab': self.vocab,\n",
    "            'num_merges': self.num_merges,\n",
    "            # Include other attributes as needed\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(bpe_data, f)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            bpe_data = json.load(f)\n",
    "        \n",
    "        self.merges = bpe_data['merges']\n",
    "        self.vocab = bpe_data['vocab']\n",
    "        self.num_merges = bpe_data['num_merges']\n",
    "\n",
    "\n",
    "class WordPiece:\n",
    "    def __init__(self, vocab, unk_token_id=0, unk_token=\"[UNK]\"):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token_id = unk_token_id\n",
    "        self.unk_token = unk_token  # Define the unknown token\n",
    "        self.root = self.build_trie(vocab)\n",
    "        self.id_to_token = {id_: token for token, id_ in vocab.items()}  # Inverse mapping\n",
    "        self.compute_failure_links(self.root)\n",
    "        print(\"Trie built successfully.\")\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"\n",
    "        Convert a list of token ids back to their string token representations.\n",
    "        \"\"\"\n",
    "        return [self.id_to_token.get(id_, self.unk_token) for id_ in ids]\n",
    "\n",
    "    # Add debug prints to build_trie to confirm structure\n",
    "    def build_trie(self, vocab):\n",
    "        root = TrieNode()\n",
    "        for token in vocab:\n",
    "            node = root\n",
    "            for char in token:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.is_end = True\n",
    "            node.token = token\n",
    "        #print(\"Trie Construction Completed Successfully\")\n",
    "        return root\n",
    "\n",
    "\n",
    "    def compute_failure_links(self, root):\n",
    "        queue = [root]\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "            for char, child_node in current_node.children.items():\n",
    "                failure_node = current_node.failure_link\n",
    "                while failure_node and char not in failure_node.children:\n",
    "                    failure_node = failure_node.failure_link\n",
    "                child_node.failure_link = failure_node.children[char] if failure_node else root\n",
    "                queue.append(child_node)\n",
    "\n",
    "    # Improved debug prints in tokenize method\n",
    "                \n",
    "    def tokenize(self, text):\n",
    "        # Preprocess input text\n",
    "        text = self.preprocess_text(text)\n",
    "        node = self.root\n",
    "        token_ids = []  # Will store token IDs instead of tokens\n",
    "        i = 0\n",
    "\n",
    "        while i < len(text):\n",
    "            char = text[i]\n",
    "            if char == ' ':\n",
    "                node = self.root\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if char not in node.children:\n",
    "                if node != self.root and node.token is not None:\n",
    "                    # Convert found token to its ID\n",
    "                    token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                    token_ids.append(token_id)\n",
    "                    node = self.root  # Reset to root\n",
    "                    continue\n",
    "                else:\n",
    "                    # Append unknown token ID\n",
    "                    token_ids.append(self.unk_token_id)\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "            node = node.children[char]\n",
    "            if node.is_end:\n",
    "                if i + 1 == len(text) or text[i + 1] == ' ':\n",
    "                    # Convert found token to its ID\n",
    "                    token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                    token_ids.append(token_id)\n",
    "                    node = self.root\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        #print(f\"Token IDs: {token_ids[:10]}\")\n",
    "        return token_ids\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Convert text to lowercase to ensure case insensitivity\n",
    "        text = text.lower()\n",
    "\n",
    "        # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "        # This depends on how your vocabulary handles punctuation\n",
    "        text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Trim leading and trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = [line.strip() for line in file.readlines()]\n",
    "    return texts\n",
    "\n",
    "texts = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "#texts = load_corpus(\"/content/drive/MyDrive/EXPERT_STUFF/sample.txt\")\n",
    "# texts = load_corpus(\"C:/Users/robbi/Expert/sample.txt\")\n",
    "num_merges = 100\n",
    "\n",
    "# Initialize and train the SimpleSentencePiece model with BPE\n",
    "ssp = SimpleSentencePiece(model_type=\"bpe\", vocab_size=30522)\n",
    "# Assume `texts` is a list of text to train the tokenizer\n",
    "ssp.train('\\n'.join(texts))\n",
    "wordpiece_vocab = ExpertConfig.adapt_vocab_for_wordpiece(ssp.vocab)\n",
    "# Debugging step to ensure vocabulary completeness\n",
    "def debug_vocab(adapted_vocab):\n",
    "    print(\"Sample Vocabulary Check:\")\n",
    "    # Iterate over the first 10 key-value pairs in the adapted vocabulary\n",
    "    for i, (token, id_or_freq) in enumerate(adapted_vocab.items()):\n",
    "        print(f\"{token}: {id_or_freq}\")\n",
    "        if i >= 9:  # Stop after printing 10 entries\n",
    "            break\n",
    "    # Specifically check for subtokens if your tokenizer expects them\n",
    "    subtokens = [token for token in adapted_vocab.keys() if token.startswith(\"##\")]\n",
    "    print(f\"Found {len(subtokens)} subtokens in vocabulary.\")\n",
    "\n",
    "# Ensure wordpiece_vocab is a list of vocabulary tokens\n",
    "# debug_vocab(wordpiece_vocab)  # Call this after initializing wordpiece_vocab\n",
    "\n",
    "# Initialize WordPiece with the adapted vocabulary\n",
    "wordpiece_tokenizer = WordPiece(wordpiece_vocab, unk_token_id=0, unk_token=\"[UNK]\")\n",
    "\n",
    "def find_all_free_token_ids(vocab, max_id=30522):\n",
    "    \"\"\"\n",
    "    Find all free token IDs up to a maximum ID.\n",
    "\n",
    "    Parameters:\n",
    "    - vocab: Dictionary, mapping from token to token ID.\n",
    "    - max_id: Integer, the maximum token ID to consider.\n",
    "\n",
    "    Returns:\n",
    "    - List of free token IDs up to max_id.\n",
    "    \"\"\"\n",
    "    used_ids = set(vocab.values())\n",
    "    return [id_ for id_ in range(max_id) if id_ not in used_ids]\n",
    "\n",
    "# Assuming wordpiece_vocab is your vocabulary\n",
    "vocab = {token: id_ for token, id_ in wordpiece_vocab.items()}\n",
    "free_ids = find_all_free_token_ids(vocab, max_id=30522)  # Adjust max_id as needed\n",
    "#print(\"Free token IDs:\", free_ids[:100])  # Print the first 100 free IDs for brevity\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "# Instantiate Expert Model:\n",
    "config = ExpertConfig(wordpiece_vocab=wordpiece_vocab, wordpiece_tokenizer=wordpiece_tokenizer)\n",
    "expert_model = Expert(config)\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "# DPO TRAINING\n",
    "'''# DPO Training\n",
    "def preprocess_dpo_data(examples, tokenizer, max_length=512):\n",
    "    # Calculate the length allocated to each component\n",
    "    component_max_length = max_length // 3  # Dividing by 3 to equally distribute the max length\n",
    "\n",
    "    # Tokenize text and ensure it fits within the allocated max_length for each component\n",
    "    def tokenize_and_trim(text, tokenizer, max_length=component_max_length):\n",
    "        token_ids = tokenizer.tokenize(text)\n",
    "        # Trim to the max_length if necessary\n",
    "        token_ids = token_ids[:max_length]\n",
    "        return token_ids\n",
    "\n",
    "    # Tokenize and adjust length for each field\n",
    "    tokenized_questions = [tokenize_and_trim(question, tokenizer) for question in examples['question']]\n",
    "    tokenized_chosen = [tokenize_and_trim(chosen, tokenizer) for chosen in examples['chosen']]\n",
    "    tokenized_rejected = [tokenize_and_trim(rejected, tokenizer) for rejected in examples['rejected']]\n",
    "\n",
    "    # Generate labels (adjust logic as necessary for your task)\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(len(tokenized_questions))]\n",
    "\n",
    "    # Prepare final input IDs by concatenating the adjusted token IDs from each component\n",
    "    # Note: This step may require adjustments based on your specific model input requirements.\n",
    "    input_ids = [q + c + r for q, c, r in zip(tokenized_questions, tokenized_chosen, tokenized_rejected)]\n",
    "\n",
    "    # Ensure concatenated input_ids do not exceed the total max_length\n",
    "    input_ids = [ids[:max_length] for ids in input_ids]\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,  # Adjusted to return a single list of concatenated token IDs\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "def updated_custom_collate_fn(batch):\n",
    "    input_ids_list = [item['input_ids'] for item in batch]  # Assuming 'input_ids' is a list of token IDs\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Convert list of token IDs to tensors\n",
    "    input_ids_tensors = [torch.tensor(ids, dtype=torch.long) for ids in input_ids_list]\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # Pad the sequences so they all have the same length within this batch\n",
    "    padded_input_ids = pad_sequence(input_ids_tensors, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Return a dictionary suitable for your model's input\n",
    "    return {'input_ids': padded_input_ids, 'labels': labels_tensor}\n",
    "\n",
    "\n",
    "# Assuming the rest of your code for dataset loading and tokenizer initialization remains unchanged\n",
    "# Example usage:\n",
    "dpo_dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "max_seq_length = 512  # Adjust as needed\n",
    "# Assuming wordpiece_tokenizer is an instance of your WordPiece class\n",
    "dpo_dataset = dpo_dataset.map(lambda x: preprocess_dpo_data(x, wordpiece_tokenizer, max_seq_length), batched=True)\n",
    "\n",
    "# Convert to PyTorch tensors after processing\n",
    "dpo_dataset.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "\n",
    "# Adjust the custom collate function to accept the tokenizer and max_length as arguments\n",
    "\n",
    "train_loader = DataLoader(dpo_dataset['train'], batch_size=2, shuffle=True, collate_fn=updated_custom_collate_fn)\n",
    "\n",
    "#train_loader = DataLoader(dpo_dataset['train'], batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "# Instantiate the Expert model and optimizer\n",
    "model_vocab_size = expert_model.lmt.decoder.word_embedding.num_embeddings\n",
    "print(f\"Model vocab size: {model_vocab_size}\")\n",
    "print(f\"Model embedding size: {expert_model.lmt.decoder.word_embedding.embedding_dim}\")\n",
    "print(f\"Configured max length: {config.max_length}\")\n",
    "\n",
    "optimizer = AdamW(expert_model.parameters(), lr=1e-5)\n",
    "#save_path = 'D:/EXPERT_WEIGHTS/dpo_model.pth'\n",
    "save_path = 'C:/Users/robbi/OneDrive/Expert_stuff/dpo_model.pth'\n",
    "\n",
    "# Train the DPO model\n",
    "label_column = 'labels'\n",
    "input_columns = ['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
    "# Assuming expert_model is an instance of Expert\n",
    "avg_loss = expert_model.train_dpo(train_loader, optimizer, config, save_path)\n",
    "# Save the model\n",
    "#torch.save(expert_model.transformer_dpo.state_dict(), save_path)\n",
    "'''\n",
    "##################################################################################\n",
    "# LMT Training\n",
    "'''\n",
    "# Load the wikitext-2 dataset\n",
    "#dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\n",
    "\n",
    "# Access a portion of the dataset for inspection\n",
    "#print(dataset['train'][0])\n",
    "\n",
    "def generate_attention_mask(token_ids):\n",
    "    \"\"\"Generate an attention mask for the given token IDs.\"\"\"\n",
    "    return [1 if token_id != 0 else 0 for token_id in token_ids]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Initialize lists for token IDs, attention masks, and labels\n",
    "    token_ids, attention_masks, labels = [], [], []\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        # Tokenize text and ensure at least one token ID is generated\n",
    "        ids = wordpiece_tokenizer.tokenize(text) or [0]  # Replace [0] with your tokenizer's pad token ID\n",
    "        # Generate attention mask for the tokenized text\n",
    "        mask = [1] * len(ids)\n",
    "\n",
    "        # Check for length of ids and pad/truncate as necessary\n",
    "        if len(ids) < config.max_length:\n",
    "            # Pad\n",
    "            pad_length = config.max_length - len(ids)\n",
    "            ids += [0] * pad_length  # Assuming 0 is the padding ID\n",
    "            mask += [0] * pad_length\n",
    "        else:\n",
    "            # Truncate\n",
    "            ids = ids[:config.max_length]\n",
    "            mask = mask[:config.max_length]\n",
    "\n",
    "        token_ids.append(ids)\n",
    "        attention_masks.append(mask)\n",
    "        labels.append(ids)  # For simplicity, using the same IDs as labels; adjust as needed for your model\n",
    "\n",
    "    return {\"input_ids\": token_ids, \"attention_mask\": attention_masks, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "# Apply the custom tokenize function to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define save path for the trained model\n",
    "save_path = 'D:/EXPERT_WEIGHTS/lmt_expert_trained_custom_tokenizer.pth'\n",
    "\n",
    "\n",
    "# Train the LMT sub-model within the Expert system\n",
    "trained_model, average_loss = expert_model.train_language_model_transformer(\n",
    "    train_loader=train_loader, \n",
    "    device=config.device, \n",
    "    vocab_size=config.vocab_size, \n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "print(f\"Training complete. Model saved to {save_path}. Average Loss: {average_loss}\")\n",
    "'''\n",
    "##################################################################################\n",
    "# MAMBA Training\n",
    "'''from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Assuming the Expert class, ExpertConfig class, and your custom tokenizer have already been defined.\n",
    "\n",
    "config = ExpertConfig()\n",
    "expert_system = Expert(config)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Directly tokenize the text into input IDs using the custom tokenizer\n",
    "    tokenized_outputs = [torch.tensor(wordpiece_tokenizer.tokenize(text), dtype=torch.long) for text in examples[\"text\"]]\n",
    "    \n",
    "    # Pad sequences for uniform input size\n",
    "    padded_input_ids = pad_sequence(tokenized_outputs, batch_first=True, padding_value=wordpiece_tokenizer.unk_token_id)\n",
    "    \n",
    "    # Generate attention masks\n",
    "    attention_masks = (padded_input_ids != wordpiece_tokenizer.unk_token_id).float()\n",
    "    \n",
    "    # Shift input IDs to create labels, padding the last position\n",
    "    labels = torch.cat([padded_input_ids[:, 1:], torch.full((padded_input_ids.shape[0], 1), wordpiece_tokenizer.unk_token_id, dtype=torch.long)], dim=1)\n",
    "    \n",
    "    return {\"input_ids\": padded_input_ids, \"attention_mask\": attention_masks, \"labels\": labels}\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Extract input_ids, attention_mask, and labels from the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the maximum length in this batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=wordpiece_tokenizer.unk_token_id)\n",
    "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=wordpiece_tokenizer.unk_token_id)\n",
    "\n",
    "    # Convert lists of tensors to a single tensor for each type of data\n",
    "    batch = {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_masks_padded,\n",
    "        'labels': labels_padded\n",
    "    }\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "# Apply the custom tokenize function to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "train_loader = DataLoader(tokenized_datasets, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "# Train the MAMBA model\n",
    "expert_system.train_mamba(train_loader, 5, config)\n",
    "'''\n",
    "####################################################################################\n",
    "# RAG Transformer Training\n",
    "# Load Wikipedia dataset and preprocess\n",
    "#dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:0000001%]\")\n",
    "'''\n",
    "# Load the wikitext-2 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\n",
    "# Print the column names\n",
    "#print(dataset.column_names)\n",
    "# Access a portion of the dataset for inspection\n",
    "#print(dataset['train'][0])\n",
    "\n",
    "def generate_attention_mask(token_ids):\n",
    "    \"\"\"Generate an attention mask for the given token IDs.\"\"\"\n",
    "    return [1 if token_id != 0 else 0 for token_id in token_ids]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    token_ids, attention_masks, labels = [], [], []\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        # Tokenize each word in the text and flatten the list of token IDs\n",
    "        words = text.split()\n",
    "        ids = []\n",
    "        for word in words:\n",
    "            word_ids = config.wordpiece_tokenizer.tokenize(word)\n",
    "            ids.extend(word_ids)\n",
    "        \n",
    "        # Adjust for special tokens ([CLS] and [SEP])\n",
    "        if len(ids) > config.max_length - 2:\n",
    "            ids = ids[:config.max_length - 2]\n",
    "        \n",
    "        # Add [CLS] at the beginning and [SEP] at the end\n",
    "        ids = [config.cls_token_id] + ids + [config.sep_token_id]\n",
    "\n",
    "        attention_mask = [1] * len(ids)  # Attention mask with 1s for real tokens\n",
    "        \n",
    "        # Padding\n",
    "        padding_length = config.max_length - len(ids)\n",
    "        ids += [config.pad_token_id] * padding_length  # Pad token IDs\n",
    "        attention_mask += [0] * padding_length  # Extend attention mask for padding\n",
    "\n",
    "        token_ids.append(ids)\n",
    "        attention_masks.append(attention_mask)\n",
    "        labels.append(ids)  # Using the same IDs as labels; this might need adjustment\n",
    "\n",
    "    return {\"input_ids\": token_ids, \"attention_mask\": attention_masks, \"labels\": labels}\n",
    "\n",
    "\n",
    "# Apply the custom tokenize function to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets, batch_size=8, shuffle=True)\n",
    "\n",
    "# Train the LMT_Rag sub-model within the Expert system\n",
    "# Train th model\n",
    "model, average_loss = expert_model.train_language_model_rag(\n",
    "    expert_model.transformer_rag.language_model,\n",
    "    train_loader,\n",
    "    config.device,\n",
    "    vocab_size= len(config.wordpiece_vocab),\n",
    "    num_epochs=5\n",
    ")\n",
    "\n",
    "# Rag data\n",
    "train_rag_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        config.rag_dataset[0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        config.rag_dataset[0],\n",
    "        config.rag_dataset[0],        \n",
    "        config.rag_dataset[0],        \n",
    "        # Contexts from MAMBA.pdf\n",
    "        config.rag_dataset[1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        config.rag_dataset[1], \n",
    "        config.rag_dataset[1], \n",
    "        config.rag_dataset[1], \n",
    "        # Contexts from QLORA.pdf\n",
    "        config.rag_dataset[2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        config.rag_dataset[2],\n",
    "        config.rag_dataset[2],\n",
    "        config.rag_dataset[2],\n",
    "        # Contexts from RAG.pdf\n",
    "        config.rag_dataset[3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        config.rag_dataset[3],\n",
    "        config.rag_dataset[3],\n",
    "        config.rag_dataset[3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset[4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset[4],\n",
    "        config.rag_dataset[4],\n",
    "        config.rag_dataset[4],\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Train DPR encoders\n",
    "(context_encoder, question_encoder), average_loss = expert_model.train_dpr_encoders(\n",
    "    train_rag_data,\n",
    "    expert_model.transformer_rag.context_encoder,  \n",
    "    expert_model.transformer_rag.question_encoder,  \n",
    "    optimizer_context = AdamW(expert_model.transformer_rag.context_encoder.parameters(), lr=1e-5),  \n",
    "    optimizer_question = AdamW(expert_model.transformer_rag.question_encoder.parameters(), lr=1e-5),\n",
    "    epochs=5,\n",
    "    context_save_path=config.context_encoder_path,\n",
    "    question_save_path=config.question_encoder_path\n",
    ")\n",
    "'''\n",
    "####################################################################################\n",
    "# Expert Training\n",
    "'''\n",
    "# Load Wikipedia dataset and preprocess\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\n",
    "\n",
    "# Access a portion of the dataset for inspection\n",
    "#print(dataset['train'][0])\n",
    "\n",
    "def generate_attention_mask(token_ids):\n",
    "    \"\"\"Generate an attention mask for the given token IDs.\"\"\"\n",
    "    return [1 if token_id != 0 else 0 for token_id in token_ids]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Initialize lists for token IDs, attention masks, and labels\n",
    "    token_ids, attention_masks, labels = [], [], []\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        # Tokenize text and ensure at least one token ID is generated\n",
    "        ids = wordpiece_tokenizer.tokenize(text) or [0]  # Replace [0] with your tokenizer's pad token ID\n",
    "        # Generate attention mask for the tokenized text\n",
    "        mask = [1] * len(ids)\n",
    "\n",
    "        # Check for length of ids and pad/truncate as necessary\n",
    "        if len(ids) < config.max_length:\n",
    "            # Pad\n",
    "            pad_length = config.max_length - len(ids)\n",
    "            ids += [0] * pad_length  # Assuming 0 is the padding ID\n",
    "            mask += [0] * pad_length\n",
    "        else:\n",
    "            # Truncate\n",
    "            ids = ids[:config.max_length]\n",
    "            mask = mask[:config.max_length]\n",
    "\n",
    "        token_ids.append(ids)\n",
    "        attention_masks.append(mask)\n",
    "        labels.append(ids)  # For simplicity, using the same IDs as labels; adjust as needed for your model\n",
    "\n",
    "    return {\"input_ids\": token_ids, \"attention_mask\": attention_masks, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "# Apply the custom tokenize function to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets, batch_size=8, shuffle=True)\n",
    "\n",
    "main_loss_function = torch.nn.CrossEntropyLoss()\n",
    "aux_loss_weight = 0.1  # Adjust based on the significance of the auxiliary loss in your training\n",
    "\n",
    "optimizer = torch.optim.AdamW(expert_model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "save_path = 'D:\\\\EXPERT_WEIGHTS\\\\expert_model_weights.pth'\n",
    "# Rag data\n",
    "train_rag_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        config.rag_dataset[0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        config.rag_dataset[0],\n",
    "        config.rag_dataset[0],        \n",
    "        config.rag_dataset[0],        \n",
    "        # Contexts from MAMBA.pdf\n",
    "        config.rag_dataset[1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        config.rag_dataset[1], \n",
    "        config.rag_dataset[1], \n",
    "        config.rag_dataset[1], \n",
    "        # Contexts from QLORA.pdf\n",
    "        config.rag_dataset[2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        config.rag_dataset[2],\n",
    "        config.rag_dataset[2],\n",
    "        config.rag_dataset[2],\n",
    "        # Contexts from RAG.pdf\n",
    "        config.rag_dataset[3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        config.rag_dataset[3],\n",
    "        config.rag_dataset[3],\n",
    "        config.rag_dataset[3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset[4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset[4],\n",
    "        config.rag_dataset[4],\n",
    "        config.rag_dataset[4],\n",
    "    ]\n",
    "}\n",
    "# Train the model\n",
    "trained_expert_model, average_loss = expert_model.train_expert(\n",
    "    train_loader=train_loader,\n",
    "    train_data=train_rag_data,\n",
    "    optimizer=optimizer,\n",
    "    main_loss_function=main_loss_function,\n",
    "    aux_loss_weight=aux_loss_weight,\n",
    "    device=config.device,\n",
    "    save_path=save_path,\n",
    "    accumulation_steps=4,  # Adjust based on your preference\n",
    "    num_epochs=5  # Adjust based on your training needs\n",
    ")\n",
    "\n",
    "print(f\"Training completed. Average loss: {average_loss}\")\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwitchTransformerMoE initialized with embed_size=256, heads=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot locate reference to <class '__main__.ExpertConfig'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot pickle <class '__main__.ExpertConfig'>: __main__.ExpertConfig has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644ae79814a54d2e8058d6d8555d7fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_datasets after preprocess: torch.Size([36718, 3, 512])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 359\u001b[0m\n\u001b[0;32m    357\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Initialize gradients to zero\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m--> 359\u001b[0m     inputs, attention_mask, targets \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;66;03m# Adjust input_ids here to ensure they are within the vocabulary range\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size  \u001b[38;5;66;03m# Ensure this matches the vocab_size used in your embedding layer\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SPLASH(nn.Module):\n",
    "    def __init__(self, embed_size, heads, sequence_length, projection_dim, partition_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.sequence_length = sequence_length\n",
    "        self.projection_dim = projection_dim\n",
    "        self.partition_size = partition_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.value_projection = nn.Linear(self.head_dim, self.projection_dim//2)  # To project values to match dimensions\n",
    "        self.ponder = nn.Linear(self.partition_size, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def random_projection(self, matrix, k):\n",
    "        \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "        random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "        return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "    def cur_decomposition(self, matrix, projection_dim):\n",
    "        \"\"\"Applies CUR decomposition with C matrix dimension aligned to projection dimension.\"\"\"\n",
    "        batch_size, seq_length, heads, dim = matrix.shape\n",
    "        k = min(projection_dim // 2, dim)\n",
    "        C = torch.zeros(batch_size, seq_length, heads, k, device=matrix.device)\n",
    "        R = torch.zeros(batch_size, k, heads, dim, device=matrix.device)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for h in range(heads):\n",
    "                col_indices = np.random.choice(dim, k, replace=False)\n",
    "                row_indices = np.random.choice(seq_length, k, replace=False)\n",
    "                C[b, :, h] = matrix[b, :, h, col_indices]\n",
    "                R[b, :, h] = matrix[b, row_indices, h]\n",
    "        return C, R\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"BEGIN FORWARD\")\n",
    "        print(f\"x input shape: {x.shape}\")\n",
    "        N, seq_length = x.shape\n",
    "        x_reshaped = x.view(N, seq_length, self.heads, self.head_dim)\n",
    "        print(f\"x_reshaped shape: {x_reshaped.shape}\")\n",
    "\n",
    "        values = self.values(x_reshaped)\n",
    "        queries = self.random_projection(self.queries(x_reshaped), self.projection_dim // 2)\n",
    "        keys = self.random_projection(self.keys(x_reshaped), self.projection_dim // 2 )\n",
    "        print(\"values :\", values.shape)\n",
    "        print(\"queries :\", queries.shape)\n",
    "        print(\"keys :\", keys.shape)\n",
    "\n",
    "        attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "        print(f\"attention_scores after random projection: {attention_scores.shape}\")\n",
    "\n",
    "        for i in range(0, seq_length, self.partition_size):\n",
    "            print(f\"PARTITION START\")\n",
    "            partition_start = i\n",
    "            partition_end = min(i + self.partition_size, seq_length)\n",
    "            keys_part = keys[:, partition_start:partition_end, :, :]\n",
    "            queries_part = queries[:, partition_start:partition_end, :, :]\n",
    "\n",
    "            C_keys, R_queries = self.cur_decomposition(keys_part, self.projection_dim)\n",
    "            print(\"C_keys shape before return:\", C_keys.shape)\n",
    "\n",
    "            ponder_scores = torch.zeros(N, self.heads, partition_end - partition_start, 1, device=x.device)\n",
    "            print(f\"Partition Start {i}, Partition End {partition_end} , ponder_scores: {ponder_scores.shape}\")\n",
    "\n",
    "            for h in range(self.heads):\n",
    "                #print(f\"HEADS START\")\n",
    "                head_queries = queries_part[:, :, h, :]\n",
    "                #print(f\"head_queries: {head_queries.shape}\")\n",
    "                head_ponder_scores = self.sigmoid(self.ponder(head_queries))\n",
    "                #print(f\"head_ponder_scores: {head_ponder_scores.shape}\")\n",
    "                ponder_scores[:, h, :, 0] = head_ponder_scores.squeeze(-1)\n",
    "\n",
    "            # Correctly expand ponder_scores without adding an unnecessary dimension\n",
    "            print(\"BEFORE 1ST EINSUM:\")\n",
    "            ponder_scores_permuted = ponder_scores.permute(0, 2, 1, 3)  # Move to [2, 128, 8, 1]\n",
    "            print(\"ponder_scores_permuted shape:\", ponder_scores_permuted.shape) \n",
    "            ponder_scores_broadcastable = ponder_scores_permuted.expand(-1, -1, -1, 128)  # Expand to [2, 128, 8, 128]            \n",
    "            print(\"ponder_scores_broadcastable shape:\", ponder_scores_broadcastable.shape) \n",
    "            print(\"queries_part shape:\", queries_part.shape) \n",
    "            print(\"C_keys shape:\", C_keys.shape)\n",
    "            energy = torch.einsum('bnhd,bnhk->bnhd', queries_part, C_keys)\n",
    "            attention_weights = F.softmax(energy, dim=-1)\n",
    "            print(\"AFTER 1ST EINSUM:\")\n",
    "            print(\"energy shape:\", energy.shape) \n",
    "            print(\"attention_weights shape:\", attention_weights.shape)\n",
    "            attention = attention_weights * ponder_scores_broadcastable\n",
    "            print(\"attention shape:\", attention.shape)\n",
    "            attention_corrected = attention.permute(0, 2, 1, 3)\n",
    "            attention_scores[:, :, partition_start:partition_end, :] = attention_corrected\n",
    "\n",
    "        values = values.permute(0, 2, 1, 3)  # Swap heads and seq_length to bring heads next to head_dim\n",
    "        print(\"values shape:\", values.shape)\n",
    "        values = values.reshape(-1, self.head_dim)  # Flatten to [N*heads*seq_length, head_dim] for linear layer\n",
    "        print(\"values.reshape(-1, self.head_dim) shape:\", values.shape)\n",
    "        projected_values = self.value_projection(values)  # Now [N*heads*seq_length, projection_dim / 2]\n",
    "        print(\"self.value_projection(values) shape:\", projected_values.shape)\n",
    "        projected_values = projected_values.view(N, self.heads, seq_length, self.projection_dim // 2)\n",
    "        print(\"projected_values shape:\", projected_values.shape)\n",
    "\n",
    "        print(f\"2ND EINSUM\")\n",
    "        # Combine attention_scores and projected_values then pass through the final linear layer\n",
    "        out = torch.einsum('bnhp,bnhp->bnh', attention_scores, projected_values)\n",
    "        print(\"out shape:\", out.shape)\n",
    "       \n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the Switch Transformer / Mixture of Experts architecture\n",
    "class ExpertRouter(nn.Module):\n",
    "    def __init__(self, num_experts, project_to_dim):\n",
    "        super(ExpertRouter, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.project_to_dim = project_to_dim\n",
    "        # Initialize the projection layer as None; it will be created dynamically\n",
    "        self.projection_layer = None\n",
    "        # Placeholder for the routing layer, to be initialized later\n",
    "        self.routing_layer = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Dynamically create the projection layer based on the input dimension\n",
    "        if self.projection_layer is None:\n",
    "            input_feature_dim = x.size(-1)  # Assuming x is of shape [batch_size, ..., feature_dim]\n",
    "            self.projection_layer = nn.Linear(input_feature_dim, self.project_to_dim).to(x.device)\n",
    "            # Now that we know the projected dimension, we can initialize the routing layer\n",
    "            self.routing_layer = nn.Linear(self.project_to_dim, self.num_experts).to(x.device)\n",
    "        \n",
    "        # Project the input to the workable dimension\n",
    "        x_projected = self.projection_layer(x)\n",
    "        \n",
    "        # Generate scores for each expert\n",
    "        expert_scores = self.routing_layer(x_projected)\n",
    "        # Convert scores to probabilities (e.g., using softmax)\n",
    "        expert_probs = F.softmax(expert_scores, dim=-1)\n",
    "        # Decide which expert to route to based on the highest score/probability\n",
    "        chosen_expert_indices = torch.argmax(expert_probs, dim=1)\n",
    "        \n",
    "        return chosen_expert_indices, expert_probs\n",
    "\n",
    "    def auxiliary_loss(self, gate_scores):\n",
    "        expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "        loss_balancing = torch.std(expert_load)\n",
    "        return loss_balancing\n",
    "\n",
    "\n",
    "    def route_inputs(self,expert_indices, gate_scores, num_experts):\n",
    "        capacity_factor_tensor = torch.tensor([Expert.SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "        capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "        expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "        for idx in range(len(expert_indices)):\n",
    "            selected_expert = expert_indices[idx]\n",
    "            if expert_counts[selected_expert] < capacities[0]:\n",
    "                expert_counts[selected_expert] += 1\n",
    "            else:\n",
    "                available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                if len(available_experts) > 0:\n",
    "                    alternative_expert = available_experts[0]\n",
    "                    expert_indices[idx] = alternative_expert\n",
    "                    expert_counts[alternative_expert] += 1\n",
    "                else:\n",
    "                    print(\"No available experts to reroute. Handling overflow.\")\n",
    "        return expert_indices\n",
    "\n",
    "\n",
    "class SwitchTransformerMoE(nn.Module):\n",
    "    def __init__(self, experts, config):\n",
    "        super(SwitchTransformerMoE, self).__init__()\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.config = config\n",
    "        # Initialize SparseFlash2Attention, LayerNorm, Dropout, and QLORALayer as before\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(\n",
    "            config.seq_len, config.head_dim, config.block_size, config.sparsity_factor)\n",
    "        \n",
    "        self.splash = SPLASH(embed_size=512, heads=8, sequence_length=1024, projection_dim=256, partition_size=128)\n",
    "        self.layer_norm = nn.LayerNorm(config.seq_len)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.qlora = Expert.QLORALayer(config.input_dim, config.input_dim, config.rank)\n",
    "        # Initialize ExpertRouter with the number of experts and input feature dimension\n",
    "        self.expert_router = ExpertRouter(len(experts), project_to_dim=256)\n",
    "        print(f\"SwitchTransformerMoE initialized with embed_size={config.embed_size}, heads={config.heads}\")\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # SPLASH, LayerNorm, and Dropout operations as before\n",
    "\n",
    "        x = self.splash(x)\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # Obtain expert indices and gate scores from ExpertRouter\n",
    "        chosen_expert_indices, gate_scores = self.expert_router(x)\n",
    "\n",
    "        # Initialize an empty tensor for the final output\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Iterate over each expert, route inputs, and combine outputs\n",
    "        for idx, expert in enumerate(self.experts):\n",
    "            mask = chosen_expert_indices == idx\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                # Adapt expert forwarding based on the model's requirements\n",
    "                # This is a simplified example assuming experts have a forward method accepting selected_inputs\n",
    "                expert_output = expert.forward(selected_inputs, attention_mask[mask], context_texts, question_text)\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        # Calculate auxiliary loss for load balancing across experts\n",
    "        aux_loss = self.expert_router.auxiliary_loss(gate_scores)\n",
    "\n",
    "        x = self.qlora(final_output)\n",
    "        return x, aux_loss\n",
    "\n",
    "\n",
    "config = ExpertConfig(wordpiece_vocab=wordpiece_vocab, wordpiece_tokenizer=wordpiece_tokenizer)\n",
    "expert_model_1 = Expert(config)\n",
    "expert_model_2 = Expert(config)\n",
    "\n",
    "\n",
    "switch_transformer_moe = SwitchTransformerMoE(\n",
    "    experts=[\n",
    "        expert_model_1,\n",
    "        expert_model_2\n",
    "    ],\n",
    "    config=config\n",
    ")\n",
    "# Rag data\n",
    "train_rag_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        config.rag_dataset[0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        config.rag_dataset[0],\n",
    "        config.rag_dataset[0],        \n",
    "        config.rag_dataset[0],        \n",
    "        # Contexts from MAMBA.pdf\n",
    "        config.rag_dataset[1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        config.rag_dataset[1], \n",
    "        config.rag_dataset[1], \n",
    "        config.rag_dataset[1], \n",
    "        # Contexts from QLORA.pdf\n",
    "        config.rag_dataset[2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        config.rag_dataset[2],\n",
    "        config.rag_dataset[2],\n",
    "        config.rag_dataset[2],\n",
    "        # Contexts from RAG.pdf\n",
    "        config.rag_dataset[3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        config.rag_dataset[3],\n",
    "        config.rag_dataset[3],\n",
    "        config.rag_dataset[3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset[4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset[4],\n",
    "        config.rag_dataset[4],\n",
    "        config.rag_dataset[4],\n",
    "    ]\n",
    "}\n",
    "# Example training loop (simplified)\n",
    "num_epochs = 5\n",
    "# Load Wikipedia dataset and preprocess\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\n",
    "\n",
    "# Access a portion of the dataset for inspection\n",
    "#print(dataset['train'][0])\n",
    "\n",
    "def generate_attention_mask(token_ids):\n",
    "    \"\"\"Generate an attention mask for the given token IDs.\"\"\"\n",
    "    return [1 if token_id != 0 else 0 for token_id in token_ids]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Initialize lists for token IDs, attention masks, and labels\n",
    "    token_ids, attention_masks, labels = [], [], []\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        # Tokenize text and ensure at least one token ID is generated\n",
    "        ids = wordpiece_tokenizer.tokenize(text) or [0]  # Replace [0] with your tokenizer's pad token ID\n",
    "        # Generate attention mask for the tokenized text\n",
    "        mask = [1] * len(ids)\n",
    "\n",
    "        # Check for length of ids and pad/truncate as necessary\n",
    "        if len(ids) < config.max_length:\n",
    "            # Pad\n",
    "            pad_length = config.max_length - len(ids)\n",
    "            ids += [0] * pad_length  # Assuming 0 is the padding ID\n",
    "            mask += [0] * pad_length\n",
    "        else:\n",
    "            # Truncate\n",
    "            ids = ids[:config.max_length]\n",
    "            mask = mask[:config.max_length]\n",
    "\n",
    "        token_ids.append(ids)\n",
    "        attention_masks.append(mask)\n",
    "        labels.append(ids)  # For simplicity, using the same IDs as labels; adjust as needed for your model\n",
    "\n",
    "    return {\"input_ids\": token_ids, \"attention_mask\": attention_masks, \"labels\": labels}\n",
    "\n",
    "\n",
    "# Apply the custom tokenize function to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "#print(f\"tokenized_datasets before preprocess: {tokenized_datasets}\")\n",
    "\n",
    "def preprocess_data(tokenized_texts, embed_size=512, sequence_length=1024):\n",
    "\n",
    "    embedded_texts = [torch.randn(len(text), embed_size) for text in tokenized_texts]\n",
    "    \n",
    "    # Pad or truncate each sequence to `sequence_length`\n",
    "    padded_texts = pad_sequence(embedded_texts, batch_first=True, padding_value=0)\n",
    "    padded_texts = padded_texts[:, :sequence_length, :]  # Truncate if longer than `sequence_length`\n",
    "    \n",
    "    return padded_texts\n",
    "\n",
    "tokenized_datasets = preprocess_data(tokenized_datasets)\n",
    "print(f\"tokenized_datasets after preprocess: {tokenized_datasets.shape}\")\n",
    "train_loader = DataLoader(tokenized_datasets, batch_size=1, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(switch_transformer_moe.parameters(), lr=1e-4)\n",
    "main_loss_function = torch.nn.CrossEntropyLoss()\n",
    "aux_loss_weight = 0.2\n",
    "accumulation_steps = 4\n",
    "\n",
    "def adjust_input_ids(input_ids, vocab_size):\n",
    "    # Ensure the operation is done on the same device as input_ids\n",
    "    adjusted_ids = torch.where(input_ids < vocab_size, input_ids, torch.tensor(vocab_size - 1, device=input_ids.device))\n",
    "    return adjusted_ids\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()  # Initialize gradients to zero\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs, attention_mask, targets = batch['input_ids'].to(config.device), batch['attention_mask'].to(config.device), batch['labels'].to(config.device)\n",
    "        # Adjust input_ids here to ensure they are within the vocabulary range\n",
    "        vocab_size = config.vocab_size  # Ensure this matches the vocab_size used in your embedding layer\n",
    "        inputs = adjust_input_ids(inputs, vocab_size)\n",
    "        # Calculate start and end indices for current batch in train_data\n",
    "        start_idx = batch_idx * batch['input_ids'].size(0)\n",
    "        end_idx = start_idx + batch['input_ids'].size(0)\n",
    "\n",
    "        # Extract current_queries and current_contexts for the batch\n",
    "        current_queries = train_rag_data['queries'][start_idx:end_idx]\n",
    "        current_contexts = train_rag_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "        # Call to the model forward function\n",
    "        outputs, aux_loss = switch_transformer_moe(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "        # Calculate loss and accumulate\n",
    "        main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        loss = (main_loss + aux_loss_weight * aux_loss) / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps  # Scale back up\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'End of Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "average_loss = total_loss / len(train_loader)\n",
    "#torch.save(self.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New ATTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN FORWARD\n",
      "x input shape: torch.Size([32, 1024, 512])\n",
      "x_reshaped shape: torch.Size([32, 1024, 8, 64])\n",
      "values : torch.Size([32, 1024, 8, 64])\n",
      "queries : torch.Size([32, 1024, 8, 128])\n",
      "keys : torch.Size([32, 1024, 8, 128])\n",
      "attention_scores after random projection: torch.Size([32, 8, 1024, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 0, Partition End 128 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 128, Partition End 256 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 256, Partition End 384 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 384, Partition End 512 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 512, Partition End 640 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 640, Partition End 768 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 768, Partition End 896 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 896, Partition End 1024 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "values shape: torch.Size([32, 8, 1024, 64])\n",
      "values.reshape(-1, self.head_dim) shape: torch.Size([262144, 64])\n",
      "self.value_projection(values) shape: torch.Size([262144, 128])\n",
      "projected_values shape: torch.Size([32, 8, 1024, 128])\n",
      "2ND EINSUM\n",
      "out shape: torch.Size([32, 8, 1024])\n",
      "BEGIN FORWARD\n",
      "x input shape: torch.Size([32, 1024, 512])\n",
      "x_reshaped shape: torch.Size([32, 1024, 8, 64])\n",
      "values : torch.Size([32, 1024, 8, 64])\n",
      "queries : torch.Size([32, 1024, 8, 128])\n",
      "keys : torch.Size([32, 1024, 8, 128])\n",
      "attention_scores after random projection: torch.Size([32, 8, 1024, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 0, Partition End 128 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 128, Partition End 256 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 256, Partition End 384 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 384, Partition End 512 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 512, Partition End 640 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 640, Partition End 768 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 768, Partition End 896 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 896, Partition End 1024 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "values shape: torch.Size([32, 8, 1024, 64])\n",
      "values.reshape(-1, self.head_dim) shape: torch.Size([262144, 64])\n",
      "self.value_projection(values) shape: torch.Size([262144, 128])\n",
      "projected_values shape: torch.Size([32, 8, 1024, 128])\n",
      "2ND EINSUM\n",
      "out shape: torch.Size([32, 8, 1024])\n",
      "BEGIN FORWARD\n",
      "x input shape: torch.Size([32, 1024, 512])\n",
      "x_reshaped shape: torch.Size([32, 1024, 8, 64])\n",
      "values : torch.Size([32, 1024, 8, 64])\n",
      "queries : torch.Size([32, 1024, 8, 128])\n",
      "keys : torch.Size([32, 1024, 8, 128])\n",
      "attention_scores after random projection: torch.Size([32, 8, 1024, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 0, Partition End 128 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 128, Partition End 256 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 256, Partition End 384 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 384, Partition End 512 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 512, Partition End 640 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 640, Partition End 768 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 768, Partition End 896 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 896, Partition End 1024 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "values shape: torch.Size([32, 8, 1024, 64])\n",
      "values.reshape(-1, self.head_dim) shape: torch.Size([262144, 64])\n",
      "self.value_projection(values) shape: torch.Size([262144, 128])\n",
      "projected_values shape: torch.Size([32, 8, 1024, 128])\n",
      "2ND EINSUM\n",
      "out shape: torch.Size([32, 8, 1024])\n",
      "BEGIN FORWARD\n",
      "x input shape: torch.Size([4, 1024, 512])\n",
      "x_reshaped shape: torch.Size([4, 1024, 8, 64])\n",
      "values : torch.Size([4, 1024, 8, 64])\n",
      "queries : torch.Size([4, 1024, 8, 128])\n",
      "keys : torch.Size([4, 1024, 8, 128])\n",
      "attention_scores after random projection: torch.Size([4, 8, 1024, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([4, 128, 8, 128])\n",
      "Partition Start 0, Partition End 128 , ponder_scores: torch.Size([4, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([4, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([4, 128, 8, 128])\n",
      "queries_part shape: torch.Size([4, 128, 8, 128])\n",
      "C_keys shape: torch.Size([4, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([4, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([4, 128, 8, 128])\n",
      "attention shape: torch.Size([4, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([4, 128, 8, 128])\n",
      "Partition Start 128, Partition End 256 , ponder_scores: torch.Size([4, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([4, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([4, 128, 8, 128])\n",
      "queries_part shape: torch.Size([4, 128, 8, 128])\n",
      "C_keys shape: torch.Size([4, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([4, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([4, 128, 8, 128])\n",
      "attention shape: torch.Size([4, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([4, 128, 8, 128])\n",
      "Partition Start 256, Partition End 384 , ponder_scores: torch.Size([4, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([4, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([4, 128, 8, 128])\n",
      "queries_part shape: torch.Size([4, 128, 8, 128])\n",
      "C_keys shape: torch.Size([4, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([4, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([4, 128, 8, 128])\n",
      "attention shape: torch.Size([4, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([4, 128, 8, 128])\n",
      "Partition Start 384, Partition End 512 , ponder_scores: torch.Size([4, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([4, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([4, 128, 8, 128])\n",
      "queries_part shape: torch.Size([4, 128, 8, 128])\n",
      "C_keys shape: torch.Size([4, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([4, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([4, 128, 8, 128])\n",
      "attention shape: torch.Size([4, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([4, 128, 8, 128])\n",
      "Partition Start 512, Partition End 640 , ponder_scores: torch.Size([4, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([4, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([4, 128, 8, 128])\n",
      "queries_part shape: torch.Size([4, 128, 8, 128])\n",
      "C_keys shape: torch.Size([4, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([4, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([4, 128, 8, 128])\n",
      "attention shape: torch.Size([4, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([4, 128, 8, 128])\n",
      "Partition Start 640, Partition End 768 , ponder_scores: torch.Size([4, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([4, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([4, 128, 8, 128])\n",
      "queries_part shape: torch.Size([4, 128, 8, 128])\n",
      "C_keys shape: torch.Size([4, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([4, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([4, 128, 8, 128])\n",
      "attention shape: torch.Size([4, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([4, 128, 8, 128])\n",
      "Partition Start 768, Partition End 896 , ponder_scores: torch.Size([4, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([4, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([4, 128, 8, 128])\n",
      "queries_part shape: torch.Size([4, 128, 8, 128])\n",
      "C_keys shape: torch.Size([4, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([4, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([4, 128, 8, 128])\n",
      "attention shape: torch.Size([4, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([4, 128, 8, 128])\n",
      "Partition Start 896, Partition End 1024 , ponder_scores: torch.Size([4, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([4, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([4, 128, 8, 128])\n",
      "queries_part shape: torch.Size([4, 128, 8, 128])\n",
      "C_keys shape: torch.Size([4, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([4, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([4, 128, 8, 128])\n",
      "attention shape: torch.Size([4, 128, 8, 128])\n",
      "values shape: torch.Size([4, 8, 1024, 64])\n",
      "values.reshape(-1, self.head_dim) shape: torch.Size([32768, 64])\n",
      "self.value_projection(values) shape: torch.Size([32768, 128])\n",
      "projected_values shape: torch.Size([4, 8, 1024, 128])\n",
      "2ND EINSUM\n",
      "out shape: torch.Size([4, 8, 1024])\n",
      "Epoch: 0, Loss: 2.28125\n",
      "BEGIN FORWARD\n",
      "x input shape: torch.Size([32, 1024, 512])\n",
      "x_reshaped shape: torch.Size([32, 1024, 8, 64])\n",
      "values : torch.Size([32, 1024, 8, 64])\n",
      "queries : torch.Size([32, 1024, 8, 128])\n",
      "keys : torch.Size([32, 1024, 8, 128])\n",
      "attention_scores after random projection: torch.Size([32, 8, 1024, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 0, Partition End 128 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 128, Partition End 256 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 256, Partition End 384 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 384, Partition End 512 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 512, Partition End 640 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 640, Partition End 768 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 768, Partition End 896 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 896, Partition End 1024 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "values shape: torch.Size([32, 8, 1024, 64])\n",
      "values.reshape(-1, self.head_dim) shape: torch.Size([262144, 64])\n",
      "self.value_projection(values) shape: torch.Size([262144, 128])\n",
      "projected_values shape: torch.Size([32, 8, 1024, 128])\n",
      "2ND EINSUM\n",
      "out shape: torch.Size([32, 8, 1024])\n",
      "BEGIN FORWARD\n",
      "x input shape: torch.Size([32, 1024, 512])\n",
      "x_reshaped shape: torch.Size([32, 1024, 8, 64])\n",
      "values : torch.Size([32, 1024, 8, 64])\n",
      "queries : torch.Size([32, 1024, 8, 128])\n",
      "keys : torch.Size([32, 1024, 8, 128])\n",
      "attention_scores after random projection: torch.Size([32, 8, 1024, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 0, Partition End 128 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 128, Partition End 256 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 256, Partition End 384 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 384, Partition End 512 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 512, Partition End 640 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 640, Partition End 768 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 768, Partition End 896 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 896, Partition End 1024 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "values shape: torch.Size([32, 8, 1024, 64])\n",
      "values.reshape(-1, self.head_dim) shape: torch.Size([262144, 64])\n",
      "self.value_projection(values) shape: torch.Size([262144, 128])\n",
      "projected_values shape: torch.Size([32, 8, 1024, 128])\n",
      "2ND EINSUM\n",
      "out shape: torch.Size([32, 8, 1024])\n",
      "BEGIN FORWARD\n",
      "x input shape: torch.Size([32, 1024, 512])\n",
      "x_reshaped shape: torch.Size([32, 1024, 8, 64])\n",
      "values : torch.Size([32, 1024, 8, 64])\n",
      "queries : torch.Size([32, 1024, 8, 128])\n",
      "keys : torch.Size([32, 1024, 8, 128])\n",
      "attention_scores after random projection: torch.Size([32, 8, 1024, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 0, Partition End 128 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 128, Partition End 256 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 256, Partition End 384 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 384, Partition End 512 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 512, Partition End 640 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 640, Partition End 768 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([32, 128, 8, 128])\n",
      "Partition Start 768, Partition End 896 , ponder_scores: torch.Size([32, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([32, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([32, 128, 8, 128])\n",
      "queries_part shape: torch.Size([32, 128, 8, 128])\n",
      "C_keys shape: torch.Size([32, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([32, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([32, 128, 8, 128])\n",
      "attention shape: torch.Size([32, 128, 8, 128])\n",
      "PARTITION START\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 197\u001b[0m\n\u001b[0;32m    193\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m# Calculate loss (assuming classification task and targets are class indices)\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(predictions, targets)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 154\u001b[0m, in \u001b[0;36mAttentionClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    153\u001b[0m     N, seq_length, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 154\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Ensure out has dimensions [batch_size, seq_length, num_heads * (projection_dim // 2)]\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# You might need to adjust this depending on your attention model's exact output\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Change shape to [batch_size, sequence_length, num_heads]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 74\u001b[0m, in \u001b[0;36mSPLASH.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     71\u001b[0m keys_part \u001b[38;5;241m=\u001b[39m keys[:, partition_start:partition_end, :, :]\n\u001b[0;32m     72\u001b[0m queries_part \u001b[38;5;241m=\u001b[39m queries[:, partition_start:partition_end, :, :]\n\u001b[1;32m---> 74\u001b[0m C_keys, R_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcur_decomposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys_part\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC_keys shape before return:\u001b[39m\u001b[38;5;124m\"\u001b[39m, C_keys\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     77\u001b[0m ponder_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(N, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, partition_end \u001b[38;5;241m-\u001b[39m partition_start, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mSPLASH.cur_decomposition\u001b[1;34m(self, matrix, projection_dim)\u001b[0m\n\u001b[0;32m     45\u001b[0m         row_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(seq_length, k, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     46\u001b[0m         C[b, :, h] \u001b[38;5;241m=\u001b[39m matrix[b, :, h, col_indices]\n\u001b[1;32m---> 47\u001b[0m         R[b, :, h] \u001b[38;5;241m=\u001b[39m matrix[b, row_indices, h]\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m C, R\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SPLASH(nn.Module):\n",
    "    def __init__(self, embed_size, heads, sequence_length, projection_dim, partition_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.sequence_length = sequence_length\n",
    "        self.projection_dim = projection_dim\n",
    "        self.partition_size = partition_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.value_projection = nn.Linear(self.head_dim, self.projection_dim//2)  # To project values to match dimensions\n",
    "        self.ponder = nn.Linear(self.partition_size, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def random_projection(self, matrix, k):\n",
    "        \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "        random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "        return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "    def cur_decomposition(self, matrix, projection_dim):\n",
    "        \"\"\"Applies CUR decomposition with C matrix dimension aligned to projection dimension.\"\"\"\n",
    "        batch_size, seq_length, heads, dim = matrix.shape\n",
    "        k = min(projection_dim // 2, dim)\n",
    "        C = torch.zeros(batch_size, seq_length, heads, k, device=matrix.device)\n",
    "        R = torch.zeros(batch_size, k, heads, dim, device=matrix.device)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for h in range(heads):\n",
    "                col_indices = np.random.choice(dim, k, replace=False)\n",
    "                row_indices = np.random.choice(seq_length, k, replace=False)\n",
    "                C[b, :, h] = matrix[b, :, h, col_indices]\n",
    "                R[b, :, h] = matrix[b, row_indices, h]\n",
    "        return C, R\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"BEGIN FORWARD\")\n",
    "        print(f\"x input shape: {x.shape}\")\n",
    "        N, seq_length, _ = x.shape\n",
    "        x_reshaped = x.view(N, seq_length, self.heads, self.head_dim)\n",
    "        print(f\"x_reshaped shape: {x_reshaped.shape}\")\n",
    "\n",
    "        values = self.values(x_reshaped)\n",
    "        queries = self.random_projection(self.queries(x_reshaped), self.projection_dim // 2)\n",
    "        keys = self.random_projection(self.keys(x_reshaped), self.projection_dim // 2 )\n",
    "        print(\"values :\", values.shape)\n",
    "        print(\"queries :\", queries.shape)\n",
    "        print(\"keys :\", keys.shape)\n",
    "\n",
    "        attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "        print(f\"attention_scores after random projection: {attention_scores.shape}\")\n",
    "\n",
    "        for i in range(0, seq_length, self.partition_size):\n",
    "            print(f\"PARTITION START\")\n",
    "            partition_start = i\n",
    "            partition_end = min(i + self.partition_size, seq_length)\n",
    "            keys_part = keys[:, partition_start:partition_end, :, :]\n",
    "            queries_part = queries[:, partition_start:partition_end, :, :]\n",
    "\n",
    "            C_keys, R_queries = self.cur_decomposition(keys_part, self.projection_dim)\n",
    "            print(\"C_keys shape before return:\", C_keys.shape)\n",
    "\n",
    "            ponder_scores = torch.zeros(N, self.heads, partition_end - partition_start, 1, device=x.device)\n",
    "            print(f\"Partition Start {i}, Partition End {partition_end} , ponder_scores: {ponder_scores.shape}\")\n",
    "\n",
    "            for h in range(self.heads):\n",
    "                #print(f\"HEADS START\")\n",
    "                head_queries = queries_part[:, :, h, :]\n",
    "                #print(f\"head_queries: {head_queries.shape}\")\n",
    "                head_ponder_scores = self.sigmoid(self.ponder(head_queries))\n",
    "                #print(f\"head_ponder_scores: {head_ponder_scores.shape}\")\n",
    "                ponder_scores[:, h, :, 0] = head_ponder_scores.squeeze(-1)\n",
    "\n",
    "            # Correctly expand ponder_scores without adding an unnecessary dimension\n",
    "            print(\"BEFORE 1ST EINSUM:\")\n",
    "            ponder_scores_permuted = ponder_scores.permute(0, 2, 1, 3)  # Move to [2, 128, 8, 1]\n",
    "            print(\"ponder_scores_permuted shape:\", ponder_scores_permuted.shape) \n",
    "            ponder_scores_broadcastable = ponder_scores_permuted.expand(-1, -1, -1, 128)  # Expand to [2, 128, 8, 128]            \n",
    "            print(\"ponder_scores_broadcastable shape:\", ponder_scores_broadcastable.shape) \n",
    "            print(\"queries_part shape:\", queries_part.shape) \n",
    "            print(\"C_keys shape:\", C_keys.shape)\n",
    "            energy = torch.einsum('bnhd,bnhk->bnhd', queries_part, C_keys)\n",
    "            attention_weights = F.softmax(energy, dim=-1)\n",
    "            print(\"AFTER 1ST EINSUM:\")\n",
    "            print(\"energy shape:\", energy.shape) \n",
    "            print(\"attention_weights shape:\", attention_weights.shape)\n",
    "            attention = attention_weights * ponder_scores_broadcastable\n",
    "            print(\"attention shape:\", attention.shape)\n",
    "            attention_corrected = attention.permute(0, 2, 1, 3)\n",
    "            attention_scores[:, :, partition_start:partition_end, :] = attention_corrected\n",
    "\n",
    "        values = values.permute(0, 2, 1, 3)  # Swap heads and seq_length to bring heads next to head_dim\n",
    "        print(\"values shape:\", values.shape)\n",
    "        values = values.reshape(-1, self.head_dim)  # Flatten to [N*heads*seq_length, head_dim] for linear layer\n",
    "        print(\"values.reshape(-1, self.head_dim) shape:\", values.shape)\n",
    "        projected_values = self.value_projection(values)  # Now [N*heads*seq_length, projection_dim / 2]\n",
    "        print(\"self.value_projection(values) shape:\", projected_values.shape)\n",
    "        projected_values = projected_values.view(N, self.heads, seq_length, self.projection_dim // 2)\n",
    "        print(\"projected_values shape:\", projected_values.shape)\n",
    "\n",
    "        print(f\"2ND EINSUM\")\n",
    "        # Combine attention_scores and projected_values then pass through the final linear layer\n",
    "        out = torch.einsum('bnhp,bnhp->bnh', attention_scores, projected_values)\n",
    "        print(\"out shape:\", out.shape)\n",
    "       \n",
    "        return out\n",
    "\n",
    "'''\n",
    "# Example usage\n",
    "model = PartitionedLinformerAttentionACT(embed_size=512, heads=8, sequence_length=1024, projection_dim=256, partition_size=128)\n",
    "input_tensor = torch.rand(2, model.sequence_length, model.embed_size)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  \n",
    "'''\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming the input tensor size is [batch_size, sequence_length, embed_size]\n",
    "embed_size = 512  # Size of each word embedding\n",
    "heads = 8  # Number of attention heads\n",
    "sequence_length = 1024  # Length of the input sequence\n",
    "projection_dim = 256  # Dimension to project the input embeddings\n",
    "partition_size = 128  # Size of partitions for processing\n",
    "num_classes = 10\n",
    "# Instantiate the model\n",
    "attention_model  = SPLASH(embed_size, heads, sequence_length, projection_dim, partition_size)\n",
    "\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, attention_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.attention_model = attention_model\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global pooling\n",
    "        # Assuming the attention model's output dimensions here; adjust as necessary\n",
    "        projection_dim = attention_model.projection_dim\n",
    "        heads = attention_model.heads\n",
    "        self.classifier = nn.Linear(heads * (projection_dim // 2), num_classes)  # Classifier layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, _ = x.shape\n",
    "        out = self.attention_model(x)\n",
    "        # Ensure out has dimensions [batch_size, seq_length, num_heads * (projection_dim // 2)]\n",
    "        # You might need to adjust this depending on your attention model's exact output\n",
    "        out = out.permute(0, 2, 1)  # Change shape to [batch_size, sequence_length, num_heads]\n",
    "        out = self.global_pool(out).squeeze(-1)  # Global pooling, resulting in [batch_size, num_heads]\n",
    "        out = out.view(N, -1)  # Flatten\n",
    "        out = self.classifier(out)  # Pass through classifier\n",
    "        return out\n",
    "# Move the model to the appropriate device\n",
    "model = AttentionClassifier(attention_model, num_classes).to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Create a DataLoader with pinned memory\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Initialize your data here\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # Return the size of your dataset\n",
    "        return 100  # Example size\n",
    "    def __getitem__(self, idx):\n",
    "        # Example: Pad the sequence to length 1024 and ensure embed_size is 512\n",
    "        # This is a simplified example; adjust according to your actual data\n",
    "        input_tensor = torch.randn(1024, 512)  # Padded/modeled input to match expected dimensions\n",
    "        target = torch.randint(0, 10, (1,)).item()  # Generate a random class index (0 to 9 for 10 classes)\n",
    "        return input_tensor, target\n",
    "\n",
    "\n",
    "dataset = MyDataset()\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(10):  # Example: 10 epochs\n",
    "    for inputs, targets in loader:\n",
    "        # Move inputs and targets to the correct device\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            # Forward pass through the model\n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            # Calculate loss (assuming classification task and targets are class indices)\n",
    "            loss = nn.functional.cross_entropy(predictions, targets)\n",
    "        \n",
    "        # Backward pass and optimizer step using gradient scaling for mixed precision\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
