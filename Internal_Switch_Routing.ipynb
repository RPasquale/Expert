{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerDPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_split\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLORALayer\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, output_dim, rank, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\__init__.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfingerprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_caching, enable_caching, is_caching_enabled, set_caching_enabled\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo, MetricInfo\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     get_dataset_config_info,\n\u001b[0;32m     33\u001b[0m     get_dataset_config_names,\n\u001b[0;32m     34\u001b[0m     get_dataset_infos,\n\u001b[0;32m     35\u001b[0m     get_dataset_split_names,\n\u001b[0;32m     36\u001b[0m     inspect_dataset,\n\u001b[0;32m     37\u001b[0m     inspect_metric,\n\u001b[0;32m     38\u001b[0m     list_datasets,\n\u001b[0;32m     39\u001b[0m     list_metrics,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterable_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IterableDataset\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, load_dataset_builder, load_from_disk, load_metric\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\inspect.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingDownloadManager\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     dataset_module_factory,\n\u001b[0;32m     33\u001b[0m     get_dataset_builder_class,\n\u001b[0;32m     34\u001b[0m     import_main_class,\n\u001b[0;32m     35\u001b[0m     load_dataset_builder,\n\u001b[0;32m     36\u001b[0m     metric_module_factory,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m relative_to_absolute_path\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\load.py:58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m camelcase_to_snakecase, snakecase_to_camelcase\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackaged_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     59\u001b[0m     _EXTENSION_TO_MODULE,\n\u001b[0;32m     60\u001b[0m     _MODULE_SUPPORTS_METADATA,\n\u001b[0;32m     61\u001b[0m     _MODULE_TO_EXTENSIONS,\n\u001b[0;32m     62\u001b[0m     _PACKAGED_DATASETS_MODULES,\n\u001b[0;32m     63\u001b[0m     _hash_python_lines,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Split\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\packaged_modules\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sha256\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arrow\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudiofolder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audiofolder\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csv\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\packaged_modules\\arrow\\arrow.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table_cast\n\u001b[1;32m---> 11\u001b[0m logger \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mArrowConfig\u001b[39;00m(datasets\u001b[38;5;241m.\u001b[39mBuilderConfig):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"BuilderConfig for Arrow.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class LORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "        super(LORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weight and bias of the linear layer\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # LORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"LORALayer Input Shape:\", x.shape)\n",
    "        \n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        \n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "class QLORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "        super(QLORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "\n",
    "        # Original weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # QLORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def quantize(self, x, num_bits):\n",
    "        # Implement a simple quantization method\n",
    "        scale = x.abs().max()\n",
    "        x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "        return x_quantized, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "        B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "        lora_adjustment = self.dropout(lora_adjustment)\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def update_alpha(self, new_alpha):\n",
    "        \"\"\"\n",
    "        Update the alpha scaling factor.\n",
    "        \"\"\"\n",
    "        self.alpha = new_alpha\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does the matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, then sum it up\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class LanguageModelDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "        super(LanguageModelDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Adding BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # QLORA layers\n",
    "        self.qlora_feed_forward = nn.Sequential(\n",
    "            QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "        self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length) #.to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, trg_mask)\n",
    "            if self.use_qlora:\n",
    "                x = self.qlora_feed_forward(x)\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.use_qlora = use_qlora\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "        self.decoder = LanguageModelDecoder(\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            rank,\n",
    "        )\n",
    "\n",
    "    def forward(self, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        out = self.decoder(trg, trg_mask)\n",
    "        return out\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ).to(trg.device)\n",
    "\n",
    "        return trg_mask\n",
    "    \n",
    "    def process_dpo_inputs(self, chosen_ids, chosen_mask, rejected_ids, rejected_mask):\n",
    "        \"\"\"\n",
    "        Process inputs for Direct Preference Optimization, generating logits for both 'chosen' and 'rejected' responses.\n",
    "\n",
    "        Parameters:\n",
    "        chosen_ids (torch.Tensor): Token IDs for chosen responses.\n",
    "        chosen_mask (torch.Tensor): Attention mask for chosen responses.\n",
    "        rejected_ids (torch.Tensor): Token IDs for rejected responses.\n",
    "        rejected_mask (torch.Tensor): Attention mask for rejected responses.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary containing logits for 'chosen' and 'rejected' responses.\n",
    "        \"\"\"\n",
    "\n",
    "        # Process 'chosen' responses\n",
    "        chosen_outputs = self.decoder(chosen_ids, chosen_mask)\n",
    "        chosen_logits = chosen_outputs  # Assuming your decoder returns logits directly\n",
    "\n",
    "        # Process 'rejected' responses\n",
    "        rejected_outputs = self.decoder(rejected_ids, rejected_mask)\n",
    "        rejected_logits = rejected_outputs  # Assuming your decoder returns logits directly\n",
    "\n",
    "        return {\n",
    "            \"chosen_logits\": chosen_logits,\n",
    "            \"rejected_logits\": rejected_logits\n",
    "        }\n",
    "\n",
    "# Define vocabulary size and dummy data parameters\n",
    "NUM_WORDS = 1000  # Example vocabulary size\n",
    "sequence_length = 30  # Sequence length for the LanguageDataset\n",
    "dummy_data_size = 1000  # Total number of tokens in the dummy dataset\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=sequence_length)\n",
    "    \n",
    "    # Shift input_ids to create labels and truncate the last token\n",
    "    labels = [seq[1:] + [tokenizer.pad_token_id] for seq in tokenized_output['input_ids']]\n",
    "    tokenized_output['labels'] = labels\n",
    "    \n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=64, shuffle=True)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the model instance\n",
    "model = LanguageModelTransformer(\n",
    "    vocab_size=vocab_size,  # Use the vocab size from the tokenizer\n",
    "    embed_size=256,\n",
    "    num_layers=6,\n",
    "    forward_expansion=4,\n",
    "    heads=8,\n",
    "    dropout=0,\n",
    "    max_length=100,\n",
    "    rank=16\n",
    ").to(device)\n",
    "\n",
    "\n",
    "def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "    \"\"\"\n",
    "    Calculate a new alpha value based on the current loss.\n",
    "    \"\"\"\n",
    "    if current_loss >= initial_loss:\n",
    "        return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "    loss_ratio = current_loss / initial_loss\n",
    "    alpha_range = initial_alpha - final_alpha\n",
    "    new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "    return new_alpha\n",
    "\n",
    "# Enable QLORA during training\n",
    "model.decoder.toggle_qlora(True)\n",
    "\n",
    "initial_loss = None\n",
    "# Training loop\n",
    "# Assuming model is an instance of LanguageModelTransformer\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    model.decoder.toggle_qlora(True)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        # Check for NaN in loss\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Encountered NaN loss, stopping training\")\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Set the initial_loss after the first batch of the first epoch\n",
    "        if initial_loss is None and batch_idx == 0:\n",
    "            initial_loss = loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Check for NaN in total_loss\n",
    "    if math.isnan(total_loss):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Update alpha at the end of each epoch based on the average loss\n",
    "    new_alpha = calculate_new_alpha(average_loss, initial_loss)\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, QLORALayer):\n",
    "            layer.update_alpha(new_alpha)\n",
    "\n",
    "    #model.decoder.toggle_qlora(False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7378959834575654\n",
      "Epoch 2/5, Loss: 0.3635985404253006\n",
      "Epoch 3/5, Loss: 0.162116739153862\n",
      "Epoch 4/5, Loss: 0.10286747217178345\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 671\u001b[0m\n\u001b[0;32m    669\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;66;03m# Train the models\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m \u001b[43mtrain_dpr_encoders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    672\u001b[0m train_language_model(model, train_loader, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 529\u001b[0m, in \u001b[0;36mtrain_dpr_encoders\u001b[1;34m(train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs)\u001b[0m\n\u001b[0;32m    527\u001b[0m optimizer_context\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    528\u001b[0m optimizer_question\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 529\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    530\u001b[0m optimizer_context\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    531\u001b[0m optimizer_question\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import BertModel\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    # Split the text into chunks of chunk_size\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def preprocess_text(text, max_length=512):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Split the text into smaller chunks to maintain context\n",
    "    # The chunk size is slightly less than max_length to account for special tokens\n",
    "    chunk_size = max_length - 50  # Adjust this value based on the model's requirements\n",
    "    text_chunks = split_into_chunks(text, chunk_size)\n",
    "\n",
    "    # Process each chunk\n",
    "    processed_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        processed_chunk = {\n",
    "            'input_ids': tokenized_output['input_ids'],\n",
    "            'attention_mask': tokenized_output['attention_mask']\n",
    "        }\n",
    "        processed_chunks.append(processed_chunk)\n",
    "\n",
    "    return processed_chunks\n",
    "\n",
    "\n",
    "def create_dataset_from_pdfs(pdf_file_paths):\n",
    "    dataset = []\n",
    "    for file_path in pdf_file_paths:\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        processed_text = preprocess_text(text)\n",
    "        dataset.append(processed_text)\n",
    "    return dataset\n",
    "\n",
    "def retrieve_contexts(dataset, query_embedding, top_k=5):\n",
    "    # Assume dataset is a list of dictionaries with each dictionary containing 'input_ids' and 'attention_mask'\n",
    "    # for a particular context and that each context has been processed through a DPRContextEncoder to get embeddings\n",
    "\n",
    "    # Placeholder for storing similarity scores\n",
    "    similarity_scores = []\n",
    "\n",
    "    # Iterate over each context in the dataset\n",
    "    for context in dataset:\n",
    "        context_input_ids = context['input_ids'].to(device)\n",
    "        context_attention_mask = context['attention_mask'].to(device)\n",
    "\n",
    "        # Assuming context_encoder is an instance of CustomDPRContextEncoder that's already trained\n",
    "        # and available in your scope\n",
    "        context_embedding = context_encoder(context_input_ids, context_attention_mask)\n",
    "\n",
    "        # Compute similarity (e.g., using dot product)\n",
    "        similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "\n",
    "        similarity_scores.append(similarity.squeeze().item())\n",
    "\n",
    "    # Sort contexts based on similarity scores and retrieve top_k indices\n",
    "    top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "\n",
    "    # Retrieve top_k contexts\n",
    "    top_contexts = [dataset[i] for i in top_k_indices]\n",
    "\n",
    "    return top_contexts\n",
    "\n",
    "def rag_retrieve_and_generate(dataset, query):\n",
    "    # Instantiate the question encoder\n",
    "    question_encoder = DPRQuestionEncoder()\n",
    "\n",
    "    # Encode the query\n",
    "    encoded_query = question_encoder(query)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    # This involves finding the most similar documents in the dataset\n",
    "    # For simplicity, this is represented as a function 'retrieve_contexts'\n",
    "    relevant_contexts = retrieve_contexts(dataset, encoded_query)\n",
    "\n",
    "    # Language model for generation\n",
    "    language_model = LanguageModelTransformer()\n",
    "\n",
    "    # Generate a response based on the retrieved contexts\n",
    "    # This step may involve further formatting or preprocessing\n",
    "    response = language_model.generate_response(relevant_contexts)\n",
    "\n",
    "    return response\n",
    "\n",
    "# pdfs\n",
    "pdf_file_paths = [r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "                  r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf']\n",
    "\n",
    "dataset = create_dataset_from_pdfs(pdf_file_paths)\n",
    "\n",
    "\n",
    "class CustomDPRContextEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(CustomDPRContextEncoder, self).__init__()\n",
    "        # Transformer-based model, e.g., BERT\n",
    "        self.bert_model = BertModel.from_pretrained(model_name)\n",
    "        # Additional layer to produce fixed-size embeddings\n",
    "        self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Generate outputs from the BERT model\n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the pooled output for creating embeddings\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # Pass through the embedding layer\n",
    "        context_embeddings = self.embedding_layer(pooled_output)\n",
    "        return context_embeddings\n",
    "\n",
    "class DPRQuestionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(DPRQuestionEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.embedding_layer = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        embeddings = self.embedding_layer(pooled_output)\n",
    "        return embeddings\n",
    "\n",
    "class LORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "        super(LORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weight and bias of the linear layer\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # LORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"LORALayer Input Shape:\", x.shape)\n",
    "        \n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        \n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "class QLORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "        super(QLORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "\n",
    "        # Original weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # QLORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def quantize(self, x, num_bits):\n",
    "        # Implement a simple quantization method\n",
    "        scale = x.abs().max()\n",
    "        x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "        return x_quantized, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "        B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "        lora_adjustment = self.dropout(lora_adjustment)\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def update_alpha(self, new_alpha):\n",
    "        \"\"\"\n",
    "        Update the alpha scaling factor.\n",
    "        \"\"\"\n",
    "        self.alpha = new_alpha\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does the matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, then sum it up\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class LanguageModelDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "        super(LanguageModelDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Adding BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # QLORA layers\n",
    "        self.qlora_feed_forward = nn.Sequential(\n",
    "            QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "        self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length) #.to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, trg_mask)\n",
    "            if self.use_qlora:\n",
    "                x = self.qlora_feed_forward(x)\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "        return out\n",
    "\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.use_qlora = use_qlora\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "        self.decoder = LanguageModelDecoder(\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            rank,\n",
    "        )\n",
    "\n",
    "    def forward(self, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        out = self.decoder(trg, trg_mask)\n",
    "        return out\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ).to(trg.device)\n",
    "\n",
    "        return trg_mask\n",
    "    \n",
    "    # Function to enable or disable QLORA layers (for fine-tuning purposes)\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "    def generate_response(self, input_ids, attention_mask):\n",
    "        # Assuming you have a forward method that returns logits\n",
    "        logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # For simplicity, using greedy decoding here. You might want to use beam search or sampling.\n",
    "        predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "        \n",
    "        # Convert predicted token ids to tokens\n",
    "        predicted_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "        \n",
    "        # Join tokens to form the response string. This is a very basic way to generate text and might not produce the best results.\n",
    "        response = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Define vocabulary size and dummy data parameters\n",
    "NUM_WORDS = 1000  # Example vocabulary size\n",
    "sequence_length = 30  # Sequence length for the LanguageDataset\n",
    "dummy_data_size = 1000  # Total number of tokens in the dummy dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=sequence_length)\n",
    "    \n",
    "    # Shift input_ids to create labels and truncate the last token\n",
    "    labels = [seq[1:] + [tokenizer.pad_token_id] for seq in tokenized_output['input_ids']]\n",
    "    tokenized_output['labels'] = labels\n",
    "    \n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=64, shuffle=True)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the model instance\n",
    "model = LanguageModelTransformer(\n",
    "    vocab_size=vocab_size,  # Use the vocab size from the tokenizer\n",
    "    embed_size=256,\n",
    "    num_layers=6,\n",
    "    forward_expansion=4,\n",
    "    heads=8,\n",
    "    dropout=0,\n",
    "    max_length=100,\n",
    "    rank=16\n",
    ").to(device)\n",
    "\n",
    "\n",
    "def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "    \"\"\"\n",
    "    Calculate a new alpha value based on the current loss.\n",
    "    \"\"\"\n",
    "    if current_loss >= initial_loss:\n",
    "        return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "    loss_ratio = current_loss / initial_loss\n",
    "    alpha_range = initial_alpha - final_alpha\n",
    "    new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "    return new_alpha\n",
    "\n",
    "# Assuming the context and question encoders and language model are already defined as per your provided code\n",
    "\n",
    "def train_dpr_encoders(train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs):\n",
    "    loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(len(train_data[\"queries\"])):\n",
    "            query = train_data[\"queries\"][i]\n",
    "            context = train_data[\"contexts\"][i]\n",
    "\n",
    "            # Ensure query is a string\n",
    "            if not isinstance(query, str):\n",
    "                raise ValueError(\"Query must be a string.\")\n",
    "            tokenized_query = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "            # Ensure context is a string\n",
    "            if isinstance(context, dict):\n",
    "                # If context is a dictionary, extract the text content. This is a placeholder and might need adjustment.\n",
    "                context_text = context.get(\"text\", \"\")\n",
    "            elif isinstance(context, str):\n",
    "                context_text = context\n",
    "            else:\n",
    "                raise ValueError(\"Context must be a string or a dictionary containing a text field.\")\n",
    "            tokenized_context = tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "            question_embeddings = question_encoder(input_ids=tokenized_query['input_ids'], attention_mask=tokenized_query['attention_mask'])\n",
    "            context_embeddings = context_encoder(input_ids=tokenized_context['input_ids'], attention_mask=tokenized_context['attention_mask'])\n",
    "\n",
    "            # The labels tensor should have the same first dimension size as the input tensors\n",
    "            labels = torch.tensor([1.0] * question_embeddings.size(0), dtype=torch.float).to(question_embeddings.device)\n",
    "\n",
    "            loss = loss_function(question_embeddings, context_embeddings, labels)\n",
    "\n",
    "            optimizer_context.zero_grad()\n",
    "            optimizer_question.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_context.step()\n",
    "            optimizer_question.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_data['queries'])}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "def train_language_model(model, train_loader, device, num_epochs=5, lr=1e-8, weight_decay=1e-4):\n",
    "    # Enable QLORA during training\n",
    "    model.decoder.toggle_qlora(True)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "\n",
    "    initial_loss = None\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            targets = batch['labels'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "            # Check for NaN in loss\n",
    "            if math.isnan(loss.item()):\n",
    "                print(\"Encountered NaN loss, stopping training\")\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Set the initial_loss after the first batch of the first epoch\n",
    "            if initial_loss is None and batch_idx == 0:\n",
    "                initial_loss = loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check for NaN in total_loss\n",
    "        if math.isnan(total_loss):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Update alpha at the end of each epoch based on the average loss\n",
    "        new_alpha = calculate_new_alpha(average_loss, initial_loss)\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, QLORALayer):\n",
    "                layer.update_alpha(new_alpha)\n",
    "\n",
    "    # Toggle QLORA off after training\n",
    "    model.decoder.toggle_qlora(False)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Sample training data structure (You need to construct this with real data)\n",
    "train_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        dataset['train'][0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        dataset['train'][0],\n",
    "        dataset['train'][0],\n",
    "        dataset['train'][0],\n",
    "        # Contexts from MAMBA.pdf\n",
    "        dataset['train'][1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        dataset['train'][1],\n",
    "        dataset['train'][1],\n",
    "        dataset['train'][1],\n",
    "        # Contexts from QLORA.pdf\n",
    "        dataset['train'][2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        dataset['train'][2],\n",
    "        dataset['train'][2],\n",
    "        dataset['train'][2],\n",
    "        # Contexts from RAG.pdf\n",
    "        dataset['train'][3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        dataset['train'][3],\n",
    "        dataset['train'][3],\n",
    "        dataset['train'][3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        dataset['train'][4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        dataset['train'][4],\n",
    "        dataset['train'][4],\n",
    "        dataset['train'][4]\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate models and optimizers\n",
    "context_encoder = CustomDPRContextEncoder()\n",
    "question_encoder = DPRQuestionEncoder()\n",
    "rag_language_model = LanguageModelTransformer(vocab_size=vocab_size)\n",
    "\n",
    "# Define optimizers for each model component\n",
    "optimizer_context = AdamW(context_encoder.parameters(), lr=1e-5)\n",
    "optimizer_question = AdamW(question_encoder.parameters(), lr=1e-5)\n",
    "optimizer_language_model = AdamW(rag_language_model.parameters(), lr=1e-5)\n",
    "epochs = 5\n",
    "# Train the models\n",
    "train_dpr_encoders(train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs=epochs)\n",
    "train_language_model(model, train_loader, device, num_epochs=5, lr=1e-8, weight_decay=1e-4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017504d9cd74456a837254411bd8a30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n",
      "Input shape: torch.Size([64, 30, 512])\n",
      "projected_inputs pre-reshape shape: torch.Size([64, 30, 512])\n",
      "projected_inputs post-reshape shape: torch.Size([64, 512, 30])\n",
      "projected_inputs post convolution reshape: torch.Size([64, 30, 512])\n",
      "projected_inputs post swiglu shape: torch.Size([64, 30, 512])\n",
      "output shape: torch.Size([64, 30, 1024])\n",
      "shape of output before loss fn: torch.Size([64, 30, 30522])\n",
      "shape of labels before loss fn: torch.Size([1920])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 267\u001b[0m\n\u001b[0;32m    259\u001b[0m             scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;66;03m# avg_loss = evaluate(model, val_loader, loss_fn)  # Uncomment and define evaluate function\u001b[39;00m\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;66;03m# print(f'\\nEpoch {epoch}: Loss={avg_loss}\\n')\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 243\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, loader, optimizer, scheduler)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    242\u001b[0m     input_values, attention_mask, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 243\u001b[0m     input_values \u001b[38;5;241m=\u001b[39m \u001b[43minput_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    245\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "'''\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "    \n",
    "    # Convert lists to PyTorch tensors\n",
    "    input_ids = torch.tensor(tokenized_output['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(tokenized_output['attention_mask']).to(device)\n",
    "\n",
    "    # Creating labels by shifting the input_ids\n",
    "    labels = input_ids[:, :-1].clone().to(device)\n",
    "    labels = torch.nn.functional.pad(labels, (0, 1), value=tokenizer.pad_token_id)  # Pad labels to match sequence length\n",
    "    \n",
    "    tokenized_output['input_ids'] = input_ids\n",
    "    tokenized_output['attention_mask'] = attention_mask\n",
    "    tokenized_output['labels'] = labels\n",
    "\n",
    "    return tokenized_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "# DataLoader - same as your working code\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "#val_loader = DataLoader(tokenized_datasets['val'], batch_size=8, sh\n",
    "\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "# DataLoader - same as your working code\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "#val_loader = DataLoader(tokenized_datasets['val'], batch_size=8, shuffle=True)\n",
    "'''\n",
    "# RoPE\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "        self.register_buffer('sin', freqs.sin())\n",
    "        self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "        sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "        # Apply RoPE to even and odd indices separately\n",
    "        x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SWIGLU\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(SwiGLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "        self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = torch.sigmoid(self.fc2(x))\n",
    "        return self.fc1(x) * gate\n",
    "\n",
    "class SimplifiedMAMBA(nn.Module):\n",
    "    # Adjusted to include SwiGLU blocks\n",
    "    def __init__(self, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_state),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_state, d_model)\n",
    "        )\n",
    "        self.input_embedding = nn.Linear(d_model, d_model)\n",
    "        self.convs = nn.Sequential(*[nn.Conv1d(d_model, d_model, kernel_size=d_conv, padding=(d_conv // 2)) for _ in range(num_layers)])\n",
    "        self.swiglu = SwiGLU(d_model, d_model)\n",
    "        self.output_projection = nn.Linear(d_model, d_model * expansion_factor)  # Adjusted to match the output of SwiGLU\n",
    "\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "        nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "        nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "    def forward(self, inputs, attention_mask=None):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "        # Apply the attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "        projected_inputs = self.input_embedding(inputs)\n",
    "        print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            projected_inputs = conv(projected_inputs)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = self.swiglu(projected_inputs)\n",
    "        print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "        output = self.output_projection(projected_inputs)\n",
    "        print(\"output shape:\", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "    # Including rotary positional encodings if required\n",
    "    def __init__(self, vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = RotaryPositionalEncoding(d_model)\n",
    "        self.simplified_mamba = SimplifiedMAMBA(num_layers, d_model, d_state, d_conv, expansion_factor)\n",
    "        self.output_projection = nn.Linear(d_model*2, vocab_size)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = 1.0\n",
    "\n",
    "        nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "        logits = self.output_projection(simplified_mamba_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY =  0.1\n",
    "WARMUP_STEPS = 100\n",
    "TOTAL_STEPS = 1000 # we want this to be : epochs * (size of dataset / batch_size )\n",
    "EPOCHS = 100\n",
    "VOCAB_SIZE = 30522\n",
    "NUM_LAYERS = 4\n",
    "BATCH_SIZE = 8\n",
    "EXPANSION_FACTOR = 2\n",
    "CLIP_GRADIENT = 1.0\n",
    "D_MODEL = 512  # Dimensionality of the model's embeddings\n",
    "D_STATE = 2048  # Dimensionality of the intermediate state in feedforward\n",
    "D_CONV = 3  # Kernel size for convolutional layers\n",
    "\n",
    "\n",
    "# Instantiate the model with the required parameters\n",
    "model = SimplifiedLanguageModelMAMBA(vocab_size=VOCAB_SIZE, \n",
    "                                     num_layers=NUM_LAYERS, \n",
    "                                     d_model=D_MODEL, \n",
    "                                     d_state=D_STATE, \n",
    "                                     d_conv=D_CONV, \n",
    "                                     expansion_factor=EXPANSION_FACTOR)\n",
    "\n",
    "model.to(device)\n",
    "print(\"Using\", device)\n",
    "\n",
    "def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Linear warmup with cosine decay\n",
    "    scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "# Initialize the optimizer and scheduler with appropriate parameters\n",
    "optimizer, scheduler = setup_optimizer(model, LEARNING_RATE, WEIGHT_DECAY, WARMUP_STEPS, TOTAL_STEPS)\n",
    "# Adjust the train loop\n",
    "def train_loop(model, loader, optimizer, scheduler):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    progress_bar = tqdm(range(EPOCHS))\n",
    "\n",
    "    for epoch in progress_bar:\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            input_values, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "            input_values = input_values.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_values, attention_mask)\n",
    "            print(f\"shape of output before loss fn: {outputs.shape}\")\n",
    "            print(f\"shape of labels before loss fn: {labels.view(-1).shape}\")\n",
    "\n",
    "            loss = loss_fn(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRADIENT)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "        # avg_loss = evaluate(model, val_loader, loss_fn)  # Uncomment and define evaluate function\n",
    "        # print(f'\\nEpoch {epoch}: Loss={avg_loss}\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loop(model, train_loader, optimizer, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3 Switch Internal Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Switch Routing Dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabb72a818014248abc4d3186fec503d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764a9dbe2a1c4443a248d857b316f7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0d60249b0747b28ea4b97855deb170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset, here using Python examples\n",
    "dataset = load_dataset(\"code_search_net\", \"python\")\n",
    "# Preprocess and tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['func_code_string'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"] #.select(range(2001, 4001))  \n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before embedding: torch.Size([8, 512])\n",
      "Input shape after embedding: torch.Size([8, 512])\n",
      "switch gate input x : torch.Size([8, 512])\n",
      "x.float : torch.Size([8, 512])\n",
      "F.relu(self.fc1(x)) : torch.Size([8, 256])\n",
      "F.softmax(self.fc2(x), dim=-1) : torch.Size([8, 3])\n",
      "No available experts to reroute. Handling overflow.\n",
      "No available experts to reroute. Handling overflow.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "process_for_transformer_rag() missing 2 required positional arguments: 'attention_mask' and 'context_texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 363\u001b[0m\n\u001b[0;32m    360\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m outputs, aux_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Compute the main task loss\u001b[39;00m\n\u001b[0;32m    366\u001b[0m main_loss \u001b[38;5;241m=\u001b[39m main_loss_function(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 297\u001b[0m, in \u001b[0;36mSwitchTransformerRouting.forward\u001b[1;34m(self, x, attention_mask)\u001b[0m\n\u001b[0;32m    293\u001b[0m selected_attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[mask]\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expert, TransformerRAG):\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;66;03m# Convert tensor inputs to text, tokenize, and process (or modify TransformerRAG to handle tensors)\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m     expert_output \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_for_transformer_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# Process as usual for other experts\u001b[39;00m\n\u001b[0;32m    300\u001b[0m     expert_output \u001b[38;5;241m=\u001b[39m expert(selected_inputs, selected_attention_mask)\n",
      "\u001b[1;31mTypeError\u001b[0m: process_for_transformer_rag() missing 2 required positional arguments: 'attention_mask' and 'context_texts'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset, here using Python examples\n",
    "dataset = load_dataset(\"code_search_net\", \"python\")\n",
    "# Preprocess and tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the code strings\n",
    "    tokenized_output = tokenizer(examples['func_code_string'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Create labels for language modeling by shifting the input_ids\n",
    "    labels = [row[:-1] + [tokenizer.pad_token_id] for row in tokenized_output[\"input_ids\"]]\n",
    "    tokenized_output[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_output\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Set the dataset format to include 'input_ids', 'attention_mask', and 'labels'\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Prepare the DataLoader\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "def load_model_weights(model, model_path):\n",
    "    # Load the saved file\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    \n",
    "    # Extract the model state dictionary and load it into the model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "def auxiliary_loss(gate_scores, expert_capacity):\n",
    "    # Load per expert\n",
    "    expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "\n",
    "    # Balancing loss: Encourage each expert to be utilized equally\n",
    "    loss_balancing = torch.std(expert_load)\n",
    "    return loss_balancing\n",
    "\n",
    "CAPACITY_FACTOR = 1  # Adjustable factor\n",
    "\n",
    "def route_inputs(expert_indices, gate_scores, num_experts, capacity_factor=CAPACITY_FACTOR):\n",
    "    # Convert capacity_factor to a tensor for operations\n",
    "    capacity_factor_tensor = torch.tensor([capacity_factor], dtype=torch.float32)\n",
    "\n",
    "    # Calculate the capacity for each expert\n",
    "    capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "\n",
    "    expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "    for idx in range(len(expert_indices)):\n",
    "        selected_expert = expert_indices[idx]\n",
    "        if expert_counts[selected_expert] < capacities[0]:  # Access the first element of capacities tensor\n",
    "            expert_counts[selected_expert] += 1\n",
    "        else:\n",
    "            # Find alternative expert with available capacity\n",
    "            available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "            if len(available_experts) > 0:\n",
    "                alternative_expert = available_experts[0]\n",
    "                expert_indices[idx] = alternative_expert\n",
    "                expert_counts[alternative_expert] += 1\n",
    "            else:\n",
    "                # Handle the scenario when no experts are available\n",
    "                print(\"No available experts to reroute. Handling overflow.\")\n",
    "                # Implement logic as needed for handling this case\n",
    "    return expert_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Transformer with DPO:\n",
    "tran_dpo = r'C:\\Users\\robbi\\IEEMM\\language_model_weights.pth'\n",
    "# 2. MAMBA:\n",
    "mamba_model_path = r'C:\\Users\\robbi\\IEEMM\\mamba_model_weights.pth'\n",
    "# 3. Transformer and RAG:\n",
    "context_encoder = r'C:\\Users\\robbi\\IEEMM\\context_encoder.pth'\n",
    "language_model = r'C:\\Users\\robbi\\IEEMM\\language_model.pth'\n",
    "question_encoder = r'C:\\Users\\robbi\\IEEMM\\question_encoder.pth'\n",
    "\n",
    "class TransformerDPO(nn.Module):\n",
    "    def __init__(self, model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank):\n",
    "        super(TransformerDPO, self).__init__()\n",
    "        self.model = LanguageModelTransformer(\n",
    "            vocab_size=vocab_size, \n",
    "            embed_size=embed_size, \n",
    "            num_layers=num_layers, \n",
    "            forward_expansion=forward_expansion, \n",
    "            heads=heads, \n",
    "            dropout=dropout, \n",
    "            max_length=100,  # Update this to match the saved model's configuration\n",
    "            rank=rank\n",
    "        )\n",
    "        # Load pre-trained weights\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
    "\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        # Assuming input_values and attention_mask are appropriate for your model\n",
    "        return self.model(input_values, attention_mask)\n",
    "\n",
    "\n",
    "class MAMBA(nn.Module):\n",
    "    def __init__(self, model_path, vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super(MAMBA, self).__init__()\n",
    "        # Initialize MAMBA architecture with the configuration used during training\n",
    "        self.model = SimplifiedLanguageModelMAMBA(\n",
    "            vocab_size=vocab_size, \n",
    "            num_layers=num_layers, \n",
    "            d_model=d_model, \n",
    "            d_state=d_state, \n",
    "            d_conv=d_conv, \n",
    "            expansion_factor=expansion_factor\n",
    "        )\n",
    "        # Load pre-trained weights\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        return self.model(input_values, attention_mask)\n",
    "\n",
    "def process_for_transformer_rag(tokenizer, input_ids, attention_mask, context_texts):\n",
    "    # Convert input_ids back to text\n",
    "    # Note: This is a simplified representation. In practice, this might not be straightforward\n",
    "    input_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "\n",
    "    # Process each input text with the corresponding context\n",
    "    outputs = []\n",
    "    for input_text, context in zip(input_texts, context_texts):\n",
    "        combined_input = input_text + \" \" + context\n",
    "        tokenized_input = tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        output = some_transformer_rag_model_processing_function(tokenized_input)\n",
    "        outputs.append(output)\n",
    "\n",
    "    return torch.stack(outputs)\n",
    "\n",
    "\n",
    "class TransformerRAG(nn.Module):\n",
    "    def __init__(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n",
    "        super(TransformerRAG, self).__init__()\n",
    "\n",
    "        # Initialize and load the pre-trained Context Encoder\n",
    "        self.context_encoder = CustomDPRContextEncoder()  # Ensure this matches the actual class name and import\n",
    "        self.context_encoder = load_model_weights(self.context_encoder, context_encoder_path)\n",
    "\n",
    "        # Initialize and load the pre-trained Language Model\n",
    "        self.language_model = LanguageModelTransformer(vocab_size=vocab_size)  # Ensure `vocab_size` is correctly set\n",
    "        self.language_model = load_model_weights(self.language_model, language_model_path)\n",
    "\n",
    "        # Initialize and load the pre-trained Question Encoder\n",
    "        self.question_encoder = DPRQuestionEncoder()  # Ensure this matches the actual class name and import\n",
    "        self.question_encoder = load_model_weights(self.question_encoder, question_encoder_path)\n",
    "\n",
    "        # Tokenizer (ensure it's the same one used during training)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, context_texts, question_inputs):\n",
    "        # Generate question embeddings\n",
    "        question_embeddings = self.question_encoder(input_ids=question_inputs['input_ids'],\n",
    "                                                    attention_mask=question_inputs['attention_mask'])\n",
    "\n",
    "        # Process each context text\n",
    "        context_embeddings = []\n",
    "        for context_text in context_texts:\n",
    "            tokenized_context = self.tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512, add_special_tokens=True)\n",
    "            context_embedding = self.context_encoder(**tokenized_context)\n",
    "            context_embeddings.append(context_embedding)\n",
    "\n",
    "        # Simple retrieval mechanism based on cosine similarity\n",
    "        cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "        similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in context_embeddings]\n",
    "        most_relevant_context_idx = torch.argmax(torch.tensor(similarities))\n",
    "\n",
    "        # Use the most relevant context to generate a response\n",
    "        combined_input = question_inputs['input_text'] + \" \" + context_texts[most_relevant_context_idx]\n",
    "        tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        response_logits = self.language_model(**tokenized_combined_input)\n",
    "\n",
    "        # Convert logits to probabilities and then to tokens\n",
    "        probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "        predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "        predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "\n",
    "        # Assemble the predicted tokens into a coherent response\n",
    "        response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "class SwitchGate(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(SwitchGate, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "        self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"switch gate input x : {x.shape}\")\n",
    "        x = x.float()\n",
    "        print(f\"x.float : {x.shape}\")\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(f\"F.relu(self.fc1(x)) : {x.shape}\")\n",
    "        gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "        print(f\"F.softmax(self.fc2(x), dim=-1) : {gate_scores.shape}\")\n",
    "\n",
    "        return gate_scores\n",
    "\n",
    "class SwitchTransformerRouting(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 num_experts, \n",
    "                 context_encoder_path, \n",
    "                 language_model_path, \n",
    "                 question_encoder_path, \n",
    "                 dpo_model_path, \n",
    "                 vocab_size, \n",
    "                 embed_size, \n",
    "                 num_layers, \n",
    "                 forward_expansion, \n",
    "                 heads, \n",
    "                 dropout, \n",
    "                 max_length, \n",
    "                 rank,\n",
    "                 mamba_model_path):\n",
    "        super(SwitchTransformerRouting, self).__init__()\n",
    "        self.router = SwitchGate(input_dim, num_experts)\n",
    "        # Instantiate TransformerRAG with the required model paths\n",
    "        transformer_rag = TransformerRAG(\n",
    "            context_encoder_path=context_encoder_path,\n",
    "            language_model_path=language_model_path,\n",
    "            question_encoder_path=question_encoder_path,\n",
    "            vocab_size=vocab_size \n",
    "        )\n",
    "\n",
    "        # Instantiate TransformerDPO with the required configurations\n",
    "        transformer_dpo = TransformerDPO(\n",
    "            model_path=dpo_model_path,\n",
    "            vocab_size=vocab_size,  # Ensure `vocab_size` is correctly set\n",
    "            embed_size=256,        # Assuming these values match your saved model's configuration\n",
    "            num_layers=6,\n",
    "            forward_expansion=4,\n",
    "            heads=8,\n",
    "            dropout=0.1,\n",
    "            max_length=512,        # This should match the saved model's configuration\n",
    "            rank=16\n",
    "        )\n",
    "\n",
    "        # Instantiate MAMBA with the required configurations\n",
    "        mamba = MAMBA(\n",
    "            model_path=mamba_model_path,\n",
    "            vocab_size=30522,  # Example values, adjust according to your training setup\n",
    "            num_layers=4,\n",
    "            d_model=512,\n",
    "            d_state=2048,\n",
    "            d_conv=3,\n",
    "            expansion_factor=2\n",
    "        )\n",
    "        #self.input_embedding = nn.Linear(actual_input_feature_size, 512)\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            transformer_rag,\n",
    "            transformer_dpo,\n",
    "            mamba\n",
    "        ])\n",
    "\n",
    "        self.input_embedding = nn.Linear(512, input_dim)  # Assuming the actual feature size is 512\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        print(f\"Input shape before embedding: {x.shape}\")\n",
    "        x = x.float()  # Convert x to float\n",
    "\n",
    "        x = self.input_embedding(x)  # Embedding the input to the required dimension\n",
    "        print(f\"Input shape after embedding: {x.shape}\")        \n",
    "        gate_scores = self.router(x)\n",
    "        expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "        expert_indices = route_inputs(expert_indices, gate_scores, num_experts=len(self.experts))\n",
    "\n",
    "        final_output = torch.zeros_like(x)\n",
    "        aux_loss = 0\n",
    "\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_indices == i\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                selected_attention_mask = attention_mask[mask]\n",
    "\n",
    "                if isinstance(expert, TransformerRAG):\n",
    "                    # Assuming 'context_texts' is a list of context strings for each input\n",
    "                    context_texts = ['context for each input'] # Replace with actual context texts\n",
    "                    expert_output = process_for_transformer_rag(\n",
    "                        tokenizer=self.tokenizer,\n",
    "                        input_ids=selected_inputs,\n",
    "                        attention_mask=selected_attention_mask,\n",
    "                        context_texts=context_texts\n",
    "                    )\n",
    "                else:\n",
    "                    # Process as usual for other experts\n",
    "                    expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        # Compute auxiliary loss for load balancing\n",
    "        aux_loss += auxiliary_loss(gate_scores, expert_capacity=torch.tensor([CAPACITY_FACTOR] * len(self.experts)))\n",
    "\n",
    "        return final_output, aux_loss\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# Define necessary parameters for initializing the SwitchTransformerRouting model\n",
    "input_dim = 512  # Example value, adjust as per your model's input dimension\n",
    "num_experts = 3  # Since you have three experts (TransformerDPO, MAMBA, TransformerRAG)\n",
    "\n",
    "# Instantiate the SwitchTransformerRouting model\n",
    "model = SwitchTransformerRouting(\n",
    "    input_dim=input_dim,\n",
    "    num_experts=num_experts,\n",
    "    context_encoder_path=context_encoder,\n",
    "    language_model_path=language_model,\n",
    "    question_encoder_path=question_encoder,\n",
    "    dpo_model_path=tran_dpo,\n",
    "    vocab_size=30522,  # Example value, adjust as per your setup\n",
    "    embed_size=256,    # Example value\n",
    "    num_layers=6,      # Example value\n",
    "    forward_expansion=4,  # Example value\n",
    "    heads=8,          # Example value\n",
    "    dropout=0.1,      # Example value\n",
    "    max_length=512,   # Example value\n",
    "    rank=16,          # Example value\n",
    "    mamba_model_path=mamba_model_path\n",
    ").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\n",
    "\n",
    "# Define main task loss function and optimizer\n",
    "main_loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameter for the weight of the auxiliary loss\n",
    "aux_loss_weight = 0.1\n",
    "# Training loop\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "# Determine the device from the model's parameters\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Extract inputs, attention_mask, and targets from the batch\n",
    "        inputs = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        targets = batch['labels']\n",
    "\n",
    "        # Move data to the same device as the model\n",
    "        inputs, attention_mask, targets = inputs.to(device), attention_mask.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, aux_loss = model(inputs, attention_mask)\n",
    "\n",
    "        # Compute the main task loss\n",
    "        main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "        # Combine the main task loss with the auxiliary loss\n",
    "        total_loss = main_loss + aux_loss_weight * aux_loss\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        if batch_idx % 100 == 0:  # Adjust print frequency according to your needs\n",
    "            print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {total_loss.item()}')\n",
    "\n",
    "    print(f'End of Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape from DataLoader: torch.Size([8, 512])\n",
      "Input shape at the beginning of forward: torch.Size([8, 512])\n",
      "Output shape after linear layer: torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"code_search_net\", \"python\")\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocess and tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['func_code_string'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Define training dataset and DataLoader\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define a dummy model for testing\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DummyModel, self).__init__()\n",
    "        self.linear = nn.Linear(512, 10)  # An arbitrary linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape at the beginning of forward: {x.shape}\")\n",
    "        x = x.to(torch.float32)  # Convert input to float\n",
    "        x = self.linear(x)\n",
    "        print(f\"Output shape after linear layer: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the dummy model\n",
    "model = DummyModel()\n",
    "\n",
    "# Iterate over the DataLoader and pass the batches through the model\n",
    "for batch in train_loader:\n",
    "    inputs = batch['input_ids']\n",
    "    print(f\"Batch shape from DataLoader: {inputs.shape}\")\n",
    "    outputs = model(inputs)  # Forward pass through the model\n",
    "\n",
    "    # Break after first batch to avoid long outputs\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: datasets in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (2.14.7)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\robbi\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\robbi\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\robbi\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\robbi\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall datasets\n",
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v4 Switch Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['decoder.word_embedding.weight', 'decoder.position_embedding.weight', 'decoder.bn1.weight', 'decoder.bn1.bias', 'decoder.bn1.running_mean', 'decoder.bn1.running_var', 'decoder.bn1.num_batches_tracked', 'decoder.bn2.weight', 'decoder.bn2.bias', 'decoder.bn2.running_mean', 'decoder.bn2.running_var', 'decoder.bn2.num_batches_tracked', 'decoder.layers.0.attention.values.weight', 'decoder.layers.0.attention.keys.weight', 'decoder.layers.0.attention.queries.weight', 'decoder.layers.0.attention.fc_out.weight', 'decoder.layers.0.attention.fc_out.bias', 'decoder.layers.0.norm1.weight', 'decoder.layers.0.norm1.bias', 'decoder.layers.0.norm2.weight', 'decoder.layers.0.norm2.bias', 'decoder.layers.0.feed_forward.0.weight', 'decoder.layers.0.feed_forward.0.bias', 'decoder.layers.0.feed_forward.0.A', 'decoder.layers.0.feed_forward.0.B', 'decoder.layers.0.feed_forward.2.weight', 'decoder.layers.0.feed_forward.2.bias', 'decoder.layers.0.feed_forward.2.A', 'decoder.layers.0.feed_forward.2.B', 'decoder.layers.1.attention.values.weight', 'decoder.layers.1.attention.keys.weight', 'decoder.layers.1.attention.queries.weight', 'decoder.layers.1.attention.fc_out.weight', 'decoder.layers.1.attention.fc_out.bias', 'decoder.layers.1.norm1.weight', 'decoder.layers.1.norm1.bias', 'decoder.layers.1.norm2.weight', 'decoder.layers.1.norm2.bias', 'decoder.layers.1.feed_forward.0.weight', 'decoder.layers.1.feed_forward.0.bias', 'decoder.layers.1.feed_forward.0.A', 'decoder.layers.1.feed_forward.0.B', 'decoder.layers.1.feed_forward.2.weight', 'decoder.layers.1.feed_forward.2.bias', 'decoder.layers.1.feed_forward.2.A', 'decoder.layers.1.feed_forward.2.B', 'decoder.layers.2.attention.values.weight', 'decoder.layers.2.attention.keys.weight', 'decoder.layers.2.attention.queries.weight', 'decoder.layers.2.attention.fc_out.weight', 'decoder.layers.2.attention.fc_out.bias', 'decoder.layers.2.norm1.weight', 'decoder.layers.2.norm1.bias', 'decoder.layers.2.norm2.weight', 'decoder.layers.2.norm2.bias', 'decoder.layers.2.feed_forward.0.weight', 'decoder.layers.2.feed_forward.0.bias', 'decoder.layers.2.feed_forward.0.A', 'decoder.layers.2.feed_forward.0.B', 'decoder.layers.2.feed_forward.2.weight', 'decoder.layers.2.feed_forward.2.bias', 'decoder.layers.2.feed_forward.2.A', 'decoder.layers.2.feed_forward.2.B', 'decoder.layers.3.attention.values.weight', 'decoder.layers.3.attention.keys.weight', 'decoder.layers.3.attention.queries.weight', 'decoder.layers.3.attention.fc_out.weight', 'decoder.layers.3.attention.fc_out.bias', 'decoder.layers.3.norm1.weight', 'decoder.layers.3.norm1.bias', 'decoder.layers.3.norm2.weight', 'decoder.layers.3.norm2.bias', 'decoder.layers.3.feed_forward.0.weight', 'decoder.layers.3.feed_forward.0.bias', 'decoder.layers.3.feed_forward.0.A', 'decoder.layers.3.feed_forward.0.B', 'decoder.layers.3.feed_forward.2.weight', 'decoder.layers.3.feed_forward.2.bias', 'decoder.layers.3.feed_forward.2.A', 'decoder.layers.3.feed_forward.2.B', 'decoder.layers.4.attention.values.weight', 'decoder.layers.4.attention.keys.weight', 'decoder.layers.4.attention.queries.weight', 'decoder.layers.4.attention.fc_out.weight', 'decoder.layers.4.attention.fc_out.bias', 'decoder.layers.4.norm1.weight', 'decoder.layers.4.norm1.bias', 'decoder.layers.4.norm2.weight', 'decoder.layers.4.norm2.bias', 'decoder.layers.4.feed_forward.0.weight', 'decoder.layers.4.feed_forward.0.bias', 'decoder.layers.4.feed_forward.0.A', 'decoder.layers.4.feed_forward.0.B', 'decoder.layers.4.feed_forward.2.weight', 'decoder.layers.4.feed_forward.2.bias', 'decoder.layers.4.feed_forward.2.A', 'decoder.layers.4.feed_forward.2.B', 'decoder.layers.5.attention.values.weight', 'decoder.layers.5.attention.keys.weight', 'decoder.layers.5.attention.queries.weight', 'decoder.layers.5.attention.fc_out.weight', 'decoder.layers.5.attention.fc_out.bias', 'decoder.layers.5.norm1.weight', 'decoder.layers.5.norm1.bias', 'decoder.layers.5.norm2.weight', 'decoder.layers.5.norm2.bias', 'decoder.layers.5.feed_forward.0.weight', 'decoder.layers.5.feed_forward.0.bias', 'decoder.layers.5.feed_forward.0.A', 'decoder.layers.5.feed_forward.0.B', 'decoder.layers.5.feed_forward.2.weight', 'decoder.layers.5.feed_forward.2.bias', 'decoder.layers.5.feed_forward.2.A', 'decoder.layers.5.feed_forward.2.B', 'decoder.qlora_feed_forward.0.weight', 'decoder.qlora_feed_forward.0.bias', 'decoder.qlora_feed_forward.0.A', 'decoder.qlora_feed_forward.0.B', 'decoder.qlora_feed_forward.0.layer_norm.weight', 'decoder.qlora_feed_forward.0.layer_norm.bias', 'decoder.qlora_feed_forward.2.weight', 'decoder.qlora_feed_forward.2.bias', 'decoder.qlora_feed_forward.2.A', 'decoder.qlora_feed_forward.2.B', 'decoder.qlora_feed_forward.2.layer_norm.weight', 'decoder.qlora_feed_forward.2.layer_norm.bias', 'decoder.fc_out.weight', 'decoder.fc_out.bias'])\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(tran_dpo, map_location='cpu')\n",
    "print(checkpoint.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m     tokenized_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_output\n\u001b[1;32m---> 36\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcode_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m tokenized_dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# DataLoader\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\dataset_dict.py:853\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 853\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\dataset_dict.py:854\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    853\u001b[0m     {\n\u001b[1;32m--> 854\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    874\u001b[0m     }\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:3086\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3084\u001b[0m transformed_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3085\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3086\u001b[0m     transformed_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_processed_shard_from_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3087\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading cached processed dataset at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache_file_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3088\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NonExistentDatasetError:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map.<locals>.load_processed_shard_from_cache\u001b[1;34m(shard_kwargs)\u001b[0m\n\u001b[0;32m   3071\u001b[0m         info\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m features\n\u001b[0;32m   3072\u001b[0m         info\u001b[38;5;241m.\u001b[39mtask_templates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_file_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3074\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NonExistentDatasetError\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:762\u001b[0m, in \u001b[0;36mDataset.from_file\u001b[1;34m(cls, filename, info, split, indices_filename, in_memory)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    743\u001b[0m     in_memory: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    744\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    745\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Instantiate a Dataset backed by an Arrow table at filename.\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;124;03m        [`Dataset`]\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 762\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43mArrowReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m indices_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    765\u001b[0m         indices_pa_table \u001b[38;5;241m=\u001b[39m ArrowReader\u001b[38;5;241m.\u001b[39mread_table(indices_filename, in_memory\u001b[38;5;241m=\u001b[39min_memory)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_reader.py:357\u001b[0m, in \u001b[0;36mArrowReader.read_table\u001b[1;34m(filename, in_memory)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;124;03mRead table from file.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    pyarrow.Table\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m table_cls \u001b[38;5;241m=\u001b[39m InMemoryTable \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m MemoryMappedTable\n\u001b[1;32m--> 357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\table.py:1059\u001b[0m, in \u001b[0;36mMemoryMappedTable.from_file\u001b[1;34m(cls, filename, replays)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m, replays\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1059\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43m_memory_mapped_arrow_table_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_replays(table, replays)\n\u001b[0;32m   1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(table, filename, replays)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\table.py:66\u001b[0m, in \u001b[0;36m_memory_mapped_arrow_table_from_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_memory_mapped_arrow_table_from_file\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable:\n\u001b[0;32m     65\u001b[0m     opened_stream \u001b[38;5;241m=\u001b[39m _memory_mapped_record_batch_reader_from_file(filename)\n\u001b[1;32m---> 66\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mopened_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa_table\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#############################################################################\n",
    "# 1. Preprocessing Data\n",
    "\n",
    "# Load the dataset\n",
    "code_dataset = load_dataset(\"code_search_net\", \"python\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocess and tokenize dataset\n",
    "def preprocess_function(examples):\n",
    "    tokenized_output = tokenizer(examples['func_code_string'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = [row[:-1] + [tokenizer.pad_token_id] for row in tokenized_output[\"input_ids\"]]\n",
    "    tokenized_output[\"labels\"] = labels\n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_dataset = code_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#############################################################################\n",
    "# 2. Sub-Model Weights\n",
    "# 1. Transformer with DPO:\n",
    "tran_dpo = r'C:\\Users\\robbi\\IEEMM\\language_model_weights.pth'\n",
    "# 2. MAMBA:\n",
    "mamba_model_path = r'C:\\Users\\robbi\\IEEMM\\mamba_model_weights.pth'\n",
    "# 3. Transformer and RAG:\n",
    "context_encoder = r'C:\\Users\\robbi\\IEEMM\\context_encoder.pth'\n",
    "language_model = r'C:\\Users\\robbi\\IEEMM\\language_model.pth'\n",
    "question_encoder = r'C:\\Users\\robbi\\IEEMM\\question_encoder.pth'\n",
    "\n",
    "# Load model weights function\n",
    "def load_model_weights(model, model_path):\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    if isinstance(checkpoint, dict):\n",
    "        # Check for 'state_dict' or 'model_state_dict' keys\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        elif 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            # If no known key is found, try loading it as a raw state dictionary\n",
    "            try:\n",
    "                model.load_state_dict(checkpoint)\n",
    "            except RuntimeError as e:\n",
    "                raise ValueError(f\"Error loading state dict: {e}\")\n",
    "    elif isinstance(checkpoint, nn.Module):\n",
    "        # If the checkpoint is a model object, assign it directly\n",
    "        model = checkpoint\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "#############################################################################\n",
    "# 3. MAMBA\n",
    "# RoPE\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "        self.register_buffer('sin', freqs.sin())\n",
    "        self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "        sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "        # Apply RoPE to even and odd indices separately\n",
    "        x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "# SWIGLU\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(SwiGLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "        self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = torch.sigmoid(self.fc2(x))\n",
    "        return self.fc1(x) * gate\n",
    "\n",
    "class SimplifiedMAMBA(nn.Module):\n",
    "    # Adjusted to include SwiGLU blocks\n",
    "    def __init__(self, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_state),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_state, d_model)\n",
    "        )\n",
    "        self.input_embedding = nn.Linear(d_model, d_model)\n",
    "        self.convs = nn.Sequential(*[nn.Conv1d(d_model, d_model, kernel_size=d_conv, padding=(d_conv // 2)) for _ in range(num_layers)])\n",
    "        self.swiglu = SwiGLU(d_model, d_model)\n",
    "        self.output_projection = nn.Linear(d_model, d_model * expansion_factor)  # Adjusted to match the output of SwiGLU\n",
    "\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "        nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "        nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "    def forward(self, inputs, attention_mask=None):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "        # Apply the attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "        projected_inputs = self.input_embedding(inputs)\n",
    "        print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            projected_inputs = conv(projected_inputs)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = self.swiglu(projected_inputs)\n",
    "        print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "        output = self.output_projection(projected_inputs)\n",
    "        print(\"output shape:\", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "    # Including rotary positional encodings if required\n",
    "    def __init__(self, vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = RotaryPositionalEncoding(d_model)\n",
    "        self.simplified_mamba = SimplifiedMAMBA(num_layers, d_model, d_state, d_conv, expansion_factor)\n",
    "        self.output_projection = nn.Linear(d_model*2, vocab_size)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = 1.0\n",
    "\n",
    "        nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "        logits = self.output_projection(simplified_mamba_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY =  0.1\n",
    "WARMUP_STEPS = 100\n",
    "TOTAL_STEPS = 1000 # we want this to be : epochs * (size of dataset / batch_size )\n",
    "EPOCHS = 100\n",
    "VOCAB_SIZE = 30522\n",
    "NUM_LAYERS = 4\n",
    "BATCH_SIZE = 8\n",
    "EXPANSION_FACTOR = 2\n",
    "CLIP_GRADIENT = 1.0\n",
    "D_MODEL = 512  # Dimensionality of the model's embeddings\n",
    "D_STATE = 2048  # Dimensionality of the intermediate state in feedforward\n",
    "D_CONV = 3  # Kernel size for convolutional layers\n",
    "\n",
    "\n",
    "def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Linear warmup with cosine decay\n",
    "    scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "# Initialize the optimizer and scheduler with appropriate parameters\n",
    "#mamba_optimizer, mamba_scheduler = setup_optimizer(mamba_model, LEARNING_RATE, WEIGHT_DECAY, WARMUP_STEPS, TOTAL_STEPS)\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "# 4. RAG\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    # Split the text into chunks of chunk_size\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def preprocess_text(text, max_length=512):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Split the text into smaller chunks to maintain context\n",
    "    # The chunk size is slightly less than max_length to account for special tokens\n",
    "    chunk_size = max_length - 50  # Adjust this value based on the model's requirements\n",
    "    text_chunks = split_into_chunks(text, chunk_size)\n",
    "\n",
    "    # Process each chunk\n",
    "    processed_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        processed_chunk = {\n",
    "            'input_ids': tokenized_output['input_ids'],\n",
    "            'attention_mask': tokenized_output['attention_mask']\n",
    "        }\n",
    "        processed_chunks.append(processed_chunk)\n",
    "\n",
    "    return processed_chunks\n",
    "\n",
    "\n",
    "def create_dataset_from_pdfs(pdf_file_paths):\n",
    "    dataset = []\n",
    "    for file_path in pdf_file_paths:\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        processed_text = preprocess_text(text)\n",
    "        dataset.append(processed_text)\n",
    "    return dataset\n",
    "\n",
    "def retrieve_contexts(dataset, query_embedding, top_k=5):\n",
    "    # Assume dataset is a list of dictionaries with each dictionary containing 'input_ids' and 'attention_mask'\n",
    "    # for a particular context and that each context has been processed through a DPRContextEncoder to get embeddings\n",
    "\n",
    "    # Placeholder for storing similarity scores\n",
    "    similarity_scores = []\n",
    "\n",
    "    # Iterate over each context in the dataset\n",
    "    for context in dataset:\n",
    "        context_input_ids = context['input_ids']\n",
    "        context_attention_mask = context['attention_mask']\n",
    "\n",
    "        # Assuming context_encoder is an instance of CustomDPRContextEncoder that's already trained\n",
    "        # and available in your scope\n",
    "        context_embedding = context_encoder(context_input_ids, context_attention_mask)\n",
    "\n",
    "        # Compute similarity (e.g., using dot product)\n",
    "        similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "\n",
    "        similarity_scores.append(similarity.squeeze().item())\n",
    "\n",
    "    # Sort contexts based on similarity scores and retrieve top_k indices\n",
    "    top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "\n",
    "    # Retrieve top_k contexts\n",
    "    top_contexts = [dataset[i] for i in top_k_indices]\n",
    "\n",
    "    return top_contexts\n",
    "\n",
    "def rag_retrieve_and_generate(dataset, query):\n",
    "    # Instantiate the question encoder\n",
    "    question_encoder = DPRQuestionEncoder()\n",
    "\n",
    "    # Encode the query\n",
    "    encoded_query = question_encoder(query)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    # This involves finding the most similar documents in the dataset\n",
    "    # For simplicity, this is represented as a function 'retrieve_contexts'\n",
    "    relevant_contexts = retrieve_contexts(dataset, encoded_query)\n",
    "\n",
    "    # Language model for generation\n",
    "    language_model = LanguageModelTransformer()\n",
    "\n",
    "    # Generate a response based on the retrieved contexts\n",
    "    # This step may involve further formatting or preprocessing\n",
    "    response = language_model.generate_response(relevant_contexts)\n",
    "\n",
    "    return response\n",
    "\n",
    "# pdfs\n",
    "pdf_file_paths = [r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "                  r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf']\n",
    "\n",
    "rag_dataset = create_dataset_from_pdfs(pdf_file_paths)\n",
    "\n",
    "class CustomDPRContextEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(CustomDPRContextEncoder, self).__init__()\n",
    "        # Transformer-based model, e.g., BERT\n",
    "        self.bert_model = BertModel.from_pretrained(model_name)\n",
    "        # Additional layer to produce fixed-size embeddings\n",
    "        self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Generate outputs from the BERT model\n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the pooled output for creating embeddings\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # Pass through the embedding layer\n",
    "        context_embeddings = self.embedding_layer(pooled_output)\n",
    "        return context_embeddings\n",
    "\n",
    "class DPRQuestionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(DPRQuestionEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.embedding_layer = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        embeddings = self.embedding_layer(pooled_output)\n",
    "        return embeddings\n",
    "\n",
    "# TransformerRAG class\n",
    "class TransformerRAG(nn.Module):\n",
    "    def __init__(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n",
    "        super(TransformerRAG, self).__init__()\n",
    "        self.context_encoder = CustomDPRContextEncoder()\n",
    "        #self.context_encoder = load_model_weights(self.context_encoder, context_encoder_path)\n",
    "        self.language_model = LanguageModelTransformer(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_size=256, \n",
    "            num_layers=6, \n",
    "            forward_expansion=4, \n",
    "            heads=8, \n",
    "            dropout=0, \n",
    "            max_length=100,  # Set to 512 to match the tokenization max_length\n",
    "            rank=16\n",
    "        )        \n",
    "        self.language_model = load_model_weights(self.language_model, language_model_path)\n",
    "        self.question_encoder = DPRQuestionEncoder()\n",
    "        #self.question_encoder = load_model_weights(self.question_encoder, question_encoder_path)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "        print(f\"question_input_ids: {question_input_ids.shape}\")\n",
    "        print(f\"question_attention_mask: {question_attention_mask.shape}\")\n",
    "        print(f\"context_texts: {len(context_texts)}\")  # Updated this line\n",
    "        print(f\"question_text: {question_text}\")\n",
    "        # Convert question_input_ids and question_attention_mask to LongTensor if they are not already\n",
    "        question_input_ids = question_input_ids.long()\n",
    "        question_attention_mask = question_attention_mask.long()\n",
    "        question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "        context_embeddings = []\n",
    "        for context_text in context_texts:\n",
    "            tokenized_context = self.tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512, add_special_tokens=True)\n",
    "            context_embedding = self.context_encoder(**tokenized_context)\n",
    "            context_embeddings.append(context_embedding)\n",
    "\n",
    "        cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "        similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in context_embeddings]\n",
    "        most_relevant_context_idx = torch.argmax(torch.tensor(similarities))\n",
    "\n",
    "        # Use the provided question_text\n",
    "        combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "        tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        response_logits = self.language_model(**tokenized_combined_input)\n",
    "        probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "        predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "        predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "        response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "\n",
    "        return response\n",
    "\n",
    "###########################################################################\n",
    "# 5. LanguageModelTransformer\n",
    "\n",
    "class LORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "        super(LORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weight and bias of the linear layer\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # LORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"LORALayer Input Shape:\", x.shape)\n",
    "        \n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        \n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "class QLORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "        super(QLORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "\n",
    "        # Original weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # QLORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def quantize(self, x, num_bits):\n",
    "        # Implement a simple quantization method\n",
    "        scale = x.abs().max()\n",
    "        x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "        return x_quantized, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "        B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "        lora_adjustment = self.dropout(lora_adjustment)\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def update_alpha(self, new_alpha):\n",
    "        \"\"\"\n",
    "        Update the alpha scaling factor.\n",
    "        \"\"\"\n",
    "        self.alpha = new_alpha\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does the matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, then sum it up\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class LanguageModelDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "        super(LanguageModelDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Adding BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # QLORA layers\n",
    "        self.qlora_feed_forward = nn.Sequential(\n",
    "            QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "        self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length) #.to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, trg_mask)\n",
    "            if self.use_qlora:\n",
    "                x = self.qlora_feed_forward(x)\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "        return out\n",
    "\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.use_qlora = use_qlora\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "        self.decoder = LanguageModelDecoder(\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            rank,\n",
    "        )\n",
    "\n",
    "    def forward(self, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        out = self.decoder(trg, trg_mask)\n",
    "        return out\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ).to(trg.device)\n",
    "\n",
    "        return trg_mask\n",
    "    \n",
    "    # Function to enable or disable QLORA layers (for fine-tuning purposes)\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "    def generate_response(self, input_ids, attention_mask):\n",
    "        # Assuming you have a forward method that returns logits\n",
    "        logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # For simplicity, using greedy decoding here. You might want to use beam search or sampling.\n",
    "        predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "        \n",
    "        # Convert predicted token ids to tokens\n",
    "        predicted_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "        \n",
    "        # Join tokens to form the response string. This is a very basic way to generate text and might not produce the best results.\n",
    "        response = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "        \n",
    "        return response\n",
    "\n",
    "###########################################################################\n",
    "# 7. Internal Switch Routing\n",
    "\n",
    "# Auxiliary loss function\n",
    "def auxiliary_loss(gate_scores, expert_capacity):\n",
    "    expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "    loss_balancing = torch.std(expert_load)\n",
    "    return loss_balancing\n",
    "\n",
    "# Routing function\n",
    "CAPACITY_FACTOR = 1\n",
    "\n",
    "def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "    capacity_factor_tensor = torch.tensor([CAPACITY_FACTOR], dtype=torch.float32)\n",
    "    capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "    expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "    for idx in range(len(expert_indices)):\n",
    "        selected_expert = expert_indices[idx]\n",
    "        if expert_counts[selected_expert] < capacities[0]:\n",
    "            expert_counts[selected_expert] += 1\n",
    "        else:\n",
    "            available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "            if len(available_experts) > 0:\n",
    "                alternative_expert = available_experts[0]\n",
    "                expert_indices[idx] = alternative_expert\n",
    "                expert_counts[alternative_expert] += 1\n",
    "            else:\n",
    "                print(\"No available experts to reroute. Handling overflow.\")\n",
    "    return expert_indices\n",
    "\n",
    "# SwitchGate \n",
    "class SwitchGate(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(SwitchGate, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "        self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "        return gate_scores\n",
    "\n",
    "# SwitchRouter \n",
    "class SwitchRouter(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank):\n",
    "        super(SwitchRouter, self).__init__()\n",
    "        self.router = SwitchGate(input_dim, num_experts)\n",
    "        self.transformer_rag = TransformerRAG(context_encoder_path, \n",
    "                                              language_model_path, \n",
    "                                              question_encoder_path, \n",
    "                                              vocab_size)\n",
    "        self.transformer_dpo = LanguageModelTransformer(vocab_size, \n",
    "                                                        embed_size, \n",
    "                                                        num_layers, \n",
    "                                                        forward_expansion, \n",
    "                                                        heads, dropout, \n",
    "                                                        max_length, \n",
    "                                                        rank)\n",
    "        self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "        self.mamba = SimplifiedLanguageModelMAMBA(vocab_size=VOCAB_SIZE, \n",
    "                                     num_layers=NUM_LAYERS, \n",
    "                                     d_model=D_MODEL, \n",
    "                                     d_state=D_STATE, \n",
    "                                     d_conv=D_CONV, \n",
    "                                     expansion_factor=EXPANSION_FACTOR)\n",
    "        self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "        self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "        self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        x = self.input_embedding(x.float())\n",
    "        gate_scores = self.router(x)\n",
    "        expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "        expert_indices = route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "        final_output = torch.zeros_like(x)\n",
    "        aux_loss = 0\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_indices == i\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                selected_attention_mask = attention_mask[mask]\n",
    "\n",
    "                if isinstance(expert, TransformerRAG):\n",
    "                    # Now passing the required arguments to TransformerRAG\n",
    "                    expert_output = self.transformer_rag(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                else:\n",
    "                    # Process as usual for other experts\n",
    "                    expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        # Compute auxiliary loss for load balancing\n",
    "        aux_loss += auxiliary_loss(gate_scores, expert_capacity=torch.tensor([CAPACITY_FACTOR] * len(self.experts)))\n",
    "\n",
    "        return final_output, aux_loss\n",
    "\n",
    "###########################################################################\n",
    "# 8.Training loop\n",
    "input_dim = 512\n",
    "num_experts = 3\n",
    "\n",
    "\n",
    "model = SwitchRouter(input_dim, num_experts, mamba_model_path, context_encoder, language_model, question_encoder, tran_dpo, 30522, 256, 6, 4, 8, 0.1, 100, 16).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "main_loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "aux_loss_weight = 0.1\n",
    "\n",
    "# Ensure train_data is accessible here, with 'queries' and 'contexts' keys\n",
    "train_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        rag_dataset[0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        rag_dataset[0],\n",
    "        rag_dataset[0],\n",
    "        rag_dataset[0],\n",
    "        # Contexts from MAMBA.pdf\n",
    "        rag_dataset[1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        rag_dataset[1],\n",
    "        rag_dataset[1],\n",
    "        rag_dataset[1],\n",
    "        # Contexts from QLORA.pdf\n",
    "        rag_dataset[2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        rag_dataset[2],\n",
    "        rag_dataset[2],\n",
    "        rag_dataset[2],\n",
    "        # Contexts from RAG.pdf\n",
    "        rag_dataset[3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        rag_dataset[3],\n",
    "        rag_dataset[3],\n",
    "        rag_dataset[3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        rag_dataset[4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        rag_dataset[4],\n",
    "        rag_dataset[4],\n",
    "        rag_dataset[4]\n",
    "    ]\n",
    "}\n",
    "model.train()\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        current_queries = train_data['queries'][start_idx:end_idx]\n",
    "        current_contexts = train_data['contexts'][start_idx:end_idx]  # Use the context chunks directly\n",
    "\n",
    "        # Now passing the actual queries and contexts\n",
    "        outputs, aux_loss = model(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "        main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        total_loss = main_loss + aux_loss_weight * aux_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {total_loss.item()}')\n",
    "\n",
    "    print(f'End of Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v5 cpu version of code to check it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True vocab size: 30522\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 345\u001b[0m\n\u001b[0;32m    338\u001b[0m pdf_file_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrobbi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIEEMM\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDPO.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m    339\u001b[0m                   \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrobbi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIEEMM\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMAMBA.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    340\u001b[0m                   \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrobbi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIEEMM\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mQLORA.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    341\u001b[0m                   \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrobbi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIEEMM\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRAG.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    342\u001b[0m                   \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrobbi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIEEMM\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSWITCH_TRANSFORMER.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m#rag_dataset = create_dataset_from_pdfs(pdf_file_paths, device)\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m rag_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset_from_pdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomDPRContextEncoder\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m):\n",
      "Cell \u001b[1;32mIn[6], line 283\u001b[0m, in \u001b[0;36mcreate_dataset_from_pdfs\u001b[1;34m(pdf_file_paths, device)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m pdf_file_paths:\n\u001b[0;32m    282\u001b[0m     text, device \u001b[38;5;241m=\u001b[39m extract_text_from_pdf(file_path, device)\n\u001b[1;32m--> 283\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mappend(processed_text)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "Cell \u001b[1;32mIn[6], line 269\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text, max_length)\u001b[0m\n\u001b[0;32m    267\u001b[0m processed_chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[1;32m--> 269\u001b[0m     tokenized_output \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     processed_chunk \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: tokenized_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: tokenized_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    273\u001b[0m     }\n\u001b[0;32m    274\u001b[0m     processed_chunks\u001b[38;5;241m.\u001b[39mappend(processed_chunk)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2802\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2860\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 2860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2861\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2862\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2863\u001b[0m     )\n\u001b[0;32m   2865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   2866\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2867\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2868\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2869\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#############################################################################\n",
    "# 1. Preprocessing Data\n",
    "\n",
    "# Load the dataset\n",
    "code_dataset = load_dataset(\"code_search_net\", \"python\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocess and tokenize dataset\n",
    "def preprocess_function(examples):\n",
    "    tokenized_output = tokenizer(examples['func_code_string'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = [row[:-1] + [tokenizer.pad_token_id] for row in tokenized_output[\"input_ids\"]]\n",
    "    tokenized_output[\"labels\"] = labels\n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_dataset = code_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"True vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "#############################################################################\n",
    "# 2. Sub-Model Weights\n",
    "# 1. Transformer with DPO:\n",
    "tran_dpo = r'C:\\Users\\robbi\\IEEMM\\language_model_weights.pth'\n",
    "# 2. MAMBA:\n",
    "mamba_model_path = r'C:\\Users\\robbi\\IEEMM\\mamba_model_weights.pth'\n",
    "# 3. Transformer and RAG:\n",
    "context_encoder = r'C:\\Users\\robbi\\IEEMM\\context_encoder.pth'\n",
    "language_model = r'C:\\Users\\robbi\\IEEMM\\language_model.pth'\n",
    "question_encoder = r'C:\\Users\\robbi\\IEEMM\\question_encoder.pth'\n",
    "\n",
    "# Load model weights function\n",
    "def load_model_weights(model, model_path):\n",
    "    #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    checkpoint = torch.load(model_path, map_location= 'cpu')\n",
    "\n",
    "    if isinstance(checkpoint, dict):\n",
    "        # Check for 'state_dict' or 'model_state_dict' keys\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        elif 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            # If no known key is found, try loading it as a raw state dictionary\n",
    "            try:\n",
    "                model.load_state_dict(checkpoint)\n",
    "            except RuntimeError as e:\n",
    "                raise ValueError(f\"Error loading state dict: {e}\")\n",
    "    elif isinstance(checkpoint, nn.Module):\n",
    "        # If the checkpoint is a model object, assign it directly\n",
    "        model = checkpoint\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "#############################################################################\n",
    "# 3. MAMBA\n",
    "# RoPE\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "        self.register_buffer('sin', freqs.sin())\n",
    "        self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "        sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "        # Apply RoPE to even and odd indices separately\n",
    "        x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "# SWIGLU\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(SwiGLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "        self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = torch.sigmoid(self.fc2(x))\n",
    "        return self.fc1(x) * gate\n",
    "\n",
    "class SimplifiedMAMBA(nn.Module):\n",
    "    # Adjusted to include SwiGLU blocks\n",
    "    def __init__(self, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_state),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_state, d_model)\n",
    "        )\n",
    "        self.input_embedding = nn.Linear(d_model, d_model)\n",
    "        self.convs = nn.Sequential(*[nn.Conv1d(d_model, d_model, kernel_size=d_conv, padding=(d_conv // 2)) for _ in range(num_layers)])\n",
    "        self.swiglu = SwiGLU(d_model, d_model)\n",
    "        self.output_projection = nn.Linear(d_model, d_model * expansion_factor)  # Adjusted to match the output of SwiGLU\n",
    "\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "        nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "        nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "    def forward(self, inputs, attention_mask=None):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "        # Apply the attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "        projected_inputs = self.input_embedding(inputs)\n",
    "        print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            projected_inputs = conv(projected_inputs)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = self.swiglu(projected_inputs)\n",
    "        print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "        output = self.output_projection(projected_inputs)\n",
    "        print(\"output shape:\", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "    # Including rotary positional encodings if required\n",
    "    def __init__(self, vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = RotaryPositionalEncoding(d_model)\n",
    "        self.simplified_mamba = SimplifiedMAMBA(num_layers, d_model, d_state, d_conv, expansion_factor)\n",
    "        self.output_projection = nn.Linear(d_model*2, vocab_size)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = 1.0\n",
    "\n",
    "        nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "        logits = self.output_projection(simplified_mamba_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY =  0.1\n",
    "WARMUP_STEPS = 100\n",
    "TOTAL_STEPS = 1000 # we want this to be : epochs * (size of dataset / batch_size )\n",
    "EPOCHS = 100\n",
    "VOCAB_SIZE = 30522\n",
    "NUM_LAYERS = 4\n",
    "BATCH_SIZE = 8\n",
    "EXPANSION_FACTOR = 2\n",
    "CLIP_GRADIENT = 1.0\n",
    "D_MODEL = 512  # Dimensionality of the model's embeddings\n",
    "D_STATE = 2048  # Dimensionality of the intermediate state in feedforward\n",
    "D_CONV = 3  # Kernel size for convolutional layers\n",
    "\n",
    "\n",
    "def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Linear warmup with cosine decay\n",
    "    scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "# Initialize the optimizer and scheduler with appropriate parameters\n",
    "#mamba_optimizer, mamba_scheduler = setup_optimizer(mamba_model, LEARNING_RATE, WEIGHT_DECAY, WARMUP_STEPS, TOTAL_STEPS)\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "# 4. RAG\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    # Split the text into chunks of chunk_size\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def preprocess_text(text, max_length=512):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Split the text into smaller chunks to maintain context\n",
    "    # The chunk size is slightly less than max_length to account for special tokens\n",
    "    chunk_size = max_length - 50  # Adjust this value based on the model's requirements\n",
    "    text_chunks = split_into_chunks(text, chunk_size)\n",
    "\n",
    "    # Process each chunk\n",
    "    processed_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        processed_chunk = {\n",
    "            'input_ids': tokenized_output['input_ids'],\n",
    "            'attention_mask': tokenized_output['attention_mask']\n",
    "        }\n",
    "        processed_chunks.append(processed_chunk)\n",
    "\n",
    "    return processed_chunks\n",
    "\n",
    "\n",
    "def create_dataset_from_pdfs(pdf_file_paths):\n",
    "    dataset = []\n",
    "    for file_path in pdf_file_paths:\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        processed_text = preprocess_text(text)\n",
    "        dataset.append(processed_text)\n",
    "    return dataset\n",
    "\n",
    "def retrieve_contexts(dataset, query_embedding, top_k=5):\n",
    "    # Assume dataset is a list of dictionaries with each dictionary containing 'input_ids' and 'attention_mask'\n",
    "    # for a particular context and that each context has been processed through a DPRContextEncoder to get embeddings\n",
    "\n",
    "    # Placeholder for storing similarity scores\n",
    "    similarity_scores = []\n",
    "\n",
    "    # Iterate over each context in the dataset\n",
    "    for context in dataset:\n",
    "        context_input_ids = context['input_ids']\n",
    "        context_attention_mask = context['attention_mask']\n",
    "\n",
    "        # Assuming context_encoder is an instance of CustomDPRContextEncoder that's already trained\n",
    "        # and available in your scope\n",
    "        context_embedding = context_encoder(context_input_ids, context_attention_mask)\n",
    "\n",
    "        # Compute similarity (e.g., using dot product)\n",
    "        similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "\n",
    "        similarity_scores.append(similarity.squeeze().item())\n",
    "\n",
    "    # Sort contexts based on similarity scores and retrieve top_k indices\n",
    "    top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "\n",
    "    # Retrieve top_k contexts\n",
    "    top_contexts = [dataset[i] for i in top_k_indices]\n",
    "\n",
    "    return top_contexts\n",
    "\n",
    "def rag_retrieve_and_generate(dataset, query):\n",
    "    # Instantiate the question encoder\n",
    "    question_encoder = DPRQuestionEncoder()\n",
    "\n",
    "    # Encode the query\n",
    "    encoded_query = question_encoder(query)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    # This involves finding the most similar documents in the dataset\n",
    "    # For simplicity, this is represented as a function 'retrieve_contexts'\n",
    "    relevant_contexts = retrieve_contexts(dataset, encoded_query)\n",
    "\n",
    "    # Language model for generation\n",
    "    language_model = LanguageModelTransformer()\n",
    "\n",
    "    # Generate a response based on the retrieved contexts\n",
    "    # This step may involve further formatting or preprocessing\n",
    "    response = language_model.generate_response(relevant_contexts)\n",
    "\n",
    "    return response\n",
    "\n",
    "# pdfs\n",
    "pdf_file_paths = [r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "                  r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf']\n",
    "\n",
    "rag_dataset = create_dataset_from_pdfs(pdf_file_paths)\n",
    "\n",
    "class CustomDPRContextEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(CustomDPRContextEncoder, self).__init__()\n",
    "        # Transformer-based model, e.g., BERT\n",
    "        self.bert_model = BertModel.from_pretrained(model_name)\n",
    "        # Additional layer to produce fixed-size embeddings\n",
    "        self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Generate outputs from the BERT model\n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the pooled output for creating embeddings\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # Pass through the embedding layer\n",
    "        context_embeddings = self.embedding_layer(pooled_output)\n",
    "        return context_embeddings\n",
    "\n",
    "class DPRQuestionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(DPRQuestionEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.embedding_layer = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        embeddings = self.embedding_layer(pooled_output)\n",
    "        return embeddings\n",
    "\n",
    "# TransformerRAG class\n",
    "class TransformerRAG(nn.Module):\n",
    "    def __init__(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n",
    "        super(TransformerRAG, self).__init__()\n",
    "        self.context_encoder = CustomDPRContextEncoder()\n",
    "        self.language_model = LanguageModelTransformer(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_size=256, \n",
    "            num_layers=6, \n",
    "            forward_expansion=4, \n",
    "            heads=8, \n",
    "            dropout=0, \n",
    "            max_length=100,  # Set to 512 to match the tokenization max_length\n",
    "            rank=16\n",
    "        )\n",
    "        self.language_model = load_model_weights(self.language_model, language_model_path)\n",
    "        self.question_encoder = DPRQuestionEncoder()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "        if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "            raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "\n",
    "        # Process each context_text\n",
    "        aggregated_context_embeddings = []\n",
    "        for context_list in context_texts:\n",
    "            if not all(isinstance(context, dict) for context in context_list):\n",
    "                raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "\n",
    "            # Create a tensor of zeros with the correct shape\n",
    "            aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size)\n",
    "            for context in context_list:\n",
    "                context_input_ids = context['input_ids']\n",
    "                context_attention_mask = context['attention_mask']\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "\n",
    "            aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "\n",
    "\n",
    "        print(f\"question_input_ids: {question_input_ids.shape}\")\n",
    "        print(f\"question_attention_mask: {question_attention_mask.shape}\")\n",
    "        print(f\"context_texts: {len(context_texts)}\")\n",
    "        print(f\"question_text: {question_text}\")\n",
    "\n",
    "        question_input_ids = question_input_ids.long()\n",
    "        question_attention_mask = question_attention_mask.long()\n",
    "        question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "\n",
    "        cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "        similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "        most_relevant_context_idx = torch.argmax(torch.tensor(similarities))\n",
    "\n",
    "        combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "        tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        response_logits = self.language_model(**tokenized_combined_input)\n",
    "        probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "        predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "        predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "        response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "\n",
    "        return response\n",
    "###########################################################################\n",
    "# 5. LanguageModelTransformer\n",
    "\n",
    "class LORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "        super(LORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weight and bias of the linear layer\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # LORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"LORALayer Input Shape:\", x.shape)\n",
    "        \n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        \n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "class QLORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "        super(QLORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "\n",
    "        # Original weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # QLORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def quantize(self, x, num_bits):\n",
    "        # Implement a simple quantization method\n",
    "        scale = x.abs().max()\n",
    "        x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "        return x_quantized, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "        B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "        lora_adjustment = self.dropout(lora_adjustment)\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def update_alpha(self, new_alpha):\n",
    "        \"\"\"\n",
    "        Update the alpha scaling factor.\n",
    "        \"\"\"\n",
    "        self.alpha = new_alpha\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does the matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, then sum it up\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class LanguageModelDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "        super(LanguageModelDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Adding BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # QLORA layers\n",
    "        self.qlora_feed_forward = nn.Sequential(\n",
    "            QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "        self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length) #.to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, trg_mask)\n",
    "            if self.use_qlora:\n",
    "                x = self.qlora_feed_forward(x)\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "        return out\n",
    "\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.use_qlora = use_qlora\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "        self.decoder = LanguageModelDecoder(\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            rank,\n",
    "        )\n",
    "\n",
    "    def forward(self, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        out = self.decoder(trg, trg_mask)\n",
    "        return out\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ).to(trg.device)\n",
    "\n",
    "        return trg_mask\n",
    "    \n",
    "    # Function to enable or disable QLORA layers (for fine-tuning purposes)\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "    def generate_response(self, input_ids, attention_mask):\n",
    "        # Assuming you have a forward method that returns logits\n",
    "        logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # For simplicity, using greedy decoding here. You might want to use beam search or sampling.\n",
    "        predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "        \n",
    "        # Convert predicted token ids to tokens\n",
    "        predicted_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "        \n",
    "        # Join tokens to form the response string. This is a very basic way to generate text and might not produce the best results.\n",
    "        response = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "        \n",
    "        return response\n",
    "\n",
    "###########################################################################\n",
    "# 7. Internal Switch Routing\n",
    "\n",
    "# Auxiliary loss function\n",
    "def auxiliary_loss(gate_scores, expert_capacity):\n",
    "    expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "    loss_balancing = torch.std(expert_load)\n",
    "    return loss_balancing\n",
    "\n",
    "# Routing function\n",
    "CAPACITY_FACTOR = 1\n",
    "\n",
    "def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "    capacity_factor_tensor = torch.tensor([CAPACITY_FACTOR], dtype=torch.float32)\n",
    "    capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "    expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "    for idx in range(len(expert_indices)):\n",
    "        selected_expert = expert_indices[idx]\n",
    "        if expert_counts[selected_expert] < capacities[0]:\n",
    "            expert_counts[selected_expert] += 1\n",
    "        else:\n",
    "            available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "            if len(available_experts) > 0:\n",
    "                alternative_expert = available_experts[0]\n",
    "                expert_indices[idx] = alternative_expert\n",
    "                expert_counts[alternative_expert] += 1\n",
    "            else:\n",
    "                print(\"No available experts to reroute. Handling overflow.\")\n",
    "    return expert_indices\n",
    "\n",
    "# SwitchGate \n",
    "class SwitchGate(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(SwitchGate, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "        self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "        return gate_scores\n",
    "\n",
    "# SwitchRouter \n",
    "class SwitchRouter(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank):\n",
    "        super(SwitchRouter, self).__init__()\n",
    "        self.router = SwitchGate(input_dim, num_experts)\n",
    "        self.transformer_rag = TransformerRAG(context_encoder_path, \n",
    "                                              language_model_path, \n",
    "                                              question_encoder_path, \n",
    "                                              vocab_size)\n",
    "        self.transformer_dpo = LanguageModelTransformer(vocab_size, \n",
    "                                                        embed_size, \n",
    "                                                        num_layers, \n",
    "                                                        forward_expansion, \n",
    "                                                        heads, dropout, \n",
    "                                                        max_length, \n",
    "                                                        rank)\n",
    "        self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "        self.mamba = SimplifiedLanguageModelMAMBA(vocab_size=VOCAB_SIZE, \n",
    "                                     num_layers=NUM_LAYERS, \n",
    "                                     d_model=D_MODEL, \n",
    "                                     d_state=D_STATE, \n",
    "                                     d_conv=D_CONV, \n",
    "                                     expansion_factor=EXPANSION_FACTOR)\n",
    "        self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "        self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "        self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        x = self.input_embedding(x.float())\n",
    "        gate_scores = self.router(x)\n",
    "        expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "        expert_indices = route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "        final_output = torch.zeros_like(x)\n",
    "        aux_loss = 0\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_indices == i\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                selected_attention_mask = attention_mask[mask]\n",
    "\n",
    "                if isinstance(expert, TransformerRAG):\n",
    "                    # Now passing the required arguments to TransformerRAG\n",
    "                    expert_output = self.transformer_rag(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                else:\n",
    "                    # Process as usual for other experts\n",
    "                    expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        # Compute auxiliary loss for load balancing\n",
    "        aux_loss += auxiliary_loss(gate_scores, expert_capacity=torch.tensor([CAPACITY_FACTOR] * len(self.experts)))\n",
    "\n",
    "        return final_output, aux_loss\n",
    "\n",
    "###########################################################################\n",
    "# 8.Training loop\n",
    "input_dim = 512\n",
    "num_experts = 3\n",
    "\n",
    "\n",
    "model = SwitchRouter(input_dim, num_experts, mamba_model_path, context_encoder, language_model, question_encoder, tran_dpo, 30522, 256, 6, 4, 8, 0.1, 100, 16) #.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "main_loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "aux_loss_weight = 0.1\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model_embedding_size = model.transformer_dpo.decoder.word_embedding.num_embeddings\n",
    "assert tokenizer.vocab_size == VOCAB_SIZE, f\"Tokenizer vocab size ({tokenizer.vocab_size}) doesn't match expected vocab size ({VOCAB_SIZE})\"\n",
    "\n",
    "# Ensure train_data is accessible here, with 'queries' and 'contexts' keys\n",
    "train_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        rag_dataset[0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        rag_dataset[0],\n",
    "        rag_dataset[0],\n",
    "        rag_dataset[0],\n",
    "        # Contexts from MAMBA.pdf\n",
    "        rag_dataset[1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        rag_dataset[1],\n",
    "        rag_dataset[1],\n",
    "        rag_dataset[1],\n",
    "        # Contexts from QLORA.pdf\n",
    "        rag_dataset[2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        rag_dataset[2],\n",
    "        rag_dataset[2],\n",
    "        rag_dataset[2],\n",
    "        # Contexts from RAG.pdf\n",
    "        rag_dataset[3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        rag_dataset[3],\n",
    "        rag_dataset[3],\n",
    "        rag_dataset[3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        rag_dataset[4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        rag_dataset[4],\n",
    "        rag_dataset[4],\n",
    "        rag_dataset[4]\n",
    "    ]\n",
    "}\n",
    "# Before training, ensure the embedding size of each component matches the tokenizer vocab size\n",
    "assert model.transformer_dpo.decoder.word_embedding.num_embeddings == tokenizer.vocab_size, \\\n",
    "    \"Transformer DPO's embedding size does not match tokenizer vocab size\"\n",
    "\n",
    "assert model.transformer_rag.context_encoder.bert_model.config.vocab_size == tokenizer.vocab_size, \\\n",
    "    \"RAG context encoder vocab size does not match tokenizer vocab size\"\n",
    "\n",
    "assert model.transformer_rag.question_encoder.bert.config.vocab_size == tokenizer.vocab_size, \\\n",
    "    \"RAG question encoder vocab size does not match tokenizer vocab size\"\n",
    "\n",
    "\n",
    "def count_total_parameters(models):\n",
    "    total_params = 0\n",
    "    for model in models:\n",
    "        model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params += model_params\n",
    "        print(f\"Parameters in {model.__class__.__name__}: {model_params}\")\n",
    "    return total_params\n",
    "\n",
    "# Usage\n",
    "models = [model.transformer_rag, model.transformer_dpo, model.mamba, model]\n",
    "total_params = count_total_parameters(models)\n",
    "print(f\"Total trainable parameters across all models: {total_params}\")\n",
    "\n",
    "# Start training\n",
    "model.train()\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "\n",
    "        # Ensure inputs do not exceed the tokenizer's vocabulary size\n",
    "        if inputs.max() >= tokenizer.vocab_size:\n",
    "            raise ValueError(\"Input IDs exceed tokenizer's vocabulary size\")\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        # Ensure batch indices are within the range of train_data\n",
    "        assert end_idx <= len(train_data['queries']), \"Batch index exceeds size of 'queries' data\"\n",
    "        assert end_idx <= len(train_data['contexts']), \"Batch index exceeds size of 'contexts' data\"\n",
    "\n",
    "        # Debug prints for shapes and value ranges\n",
    "        print(f\"Batch index: {batch_idx}\")\n",
    "        print(f\"Input IDs shape: {inputs.shape}, Max ID: {inputs.max()}\")\n",
    "        print(f\"Attention Mask shape: {attention_mask.shape}\")\n",
    "        print(f\"Targets shape: {targets.shape}, Max ID: {targets.max()}\")\n",
    "\n",
    "        current_queries = train_data['queries'][start_idx:end_idx]\n",
    "        current_contexts = train_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "        # Debug prints for current queries and contexts\n",
    "        for i, (q, context_list) in enumerate(zip(current_queries, current_contexts)):\n",
    "            print(f\"Processing query-context pair {i}:\")\n",
    "            print(f\"Query: {q}\")\n",
    "            #for c in context_list:  # Assuming context_list is a list of dictionaries\n",
    "                #print(f\"Context input IDs shape: {torch.tensor(c['input_ids']).shape}\")\n",
    "                #print(f\"Context attention mask shape: {torch.tensor(c['attention_mask']).shape}\")\n",
    "\n",
    "        # Call to the model forward function\n",
    "        outputs, aux_loss = model(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "        # Calculate loss\n",
    "        main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        total_loss = main_loss + aux_loss_weight * aux_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()  \n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {total_loss.item()}')\n",
    "\n",
    "    print(f'End of Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77cdf0a53b3422e9ad1fe4ed7c03ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d144794cb48a4bf9ae50c0886d713209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad627f9a15c042858e573cb7f7e86cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True vocab size: 30522\n",
      "Parameters in TransformerRAG: 240217658\n",
      "Parameters in LanguageModelTransformer: 20071994\n",
      "Parameters in SimplifiedLanguageModelMAMBA: 53473082\n",
      "Parameters in SwitchRouter: 314157489\n",
      "Total trainable parameters across all models: 627920223\n",
      "Batch index: 0\n",
      "Input IDs shape: torch.Size([4, 512]), Max ID: 27507\n",
      "Attention Mask shape: torch.Size([4, 512])\n",
      "Targets shape: torch.Size([4, 512]), Max ID: 27507\n",
      "Processing query-context pair 0:\n",
      "Query: What is Direct Preference Optimization (DPO)?\n",
      "Processing query-context pair 1:\n",
      "Query: How does Direct Preference Optimization work?\n",
      "Processing query-context pair 2:\n",
      "Query: How can I implement Direct Preference Optimization in my organization?\n",
      "Processing query-context pair 3:\n",
      "Query: Why does Direct Preference Optimization improve the efficiency of language modelling?\n",
      "No available experts to reroute. Handling overflow.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.79 GiB is allocated by PyTorch, and 13.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 955\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;66;03m#for c in context_list:  # Assuming context_list is a list of dictionaries\u001b[39;00m\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;66;03m#print(f\"Context input IDs shape: {torch.tensor(c['input_ids']).shape}\")\u001b[39;00m\n\u001b[0;32m    952\u001b[0m         \u001b[38;5;66;03m#print(f\"Context attention mask shape: {torch.tensor(c['attention_mask']).shape}\")\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# Call to the model forward function\u001b[39;00m\n\u001b[1;32m--> 955\u001b[0m outputs, aux_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_queries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m    958\u001b[0m main_loss \u001b[38;5;241m=\u001b[39m main_loss_function(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 809\u001b[0m, in \u001b[0;36mSwitchRouter.forward\u001b[1;34m(self, x, attention_mask, context_texts, question_text)\u001b[0m\n\u001b[0;32m    805\u001b[0m selected_attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[mask]\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expert, TransformerRAG):\n\u001b[0;32m    808\u001b[0m     \u001b[38;5;66;03m# Now passing the required arguments to TransformerRAG\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m     expert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;66;03m# Process as usual for other experts\u001b[39;00m\n\u001b[0;32m    812\u001b[0m     expert_output \u001b[38;5;241m=\u001b[39m expert(selected_inputs, selected_attention_mask)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 410\u001b[0m, in \u001b[0;36mTransformerRAG.forward\u001b[1;34m(self, context_texts, question_input_ids, question_attention_mask, question_text)\u001b[0m\n\u001b[0;32m    408\u001b[0m     context_input_ids \u001b[38;5;241m=\u001b[39m context[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    409\u001b[0m     context_attention_mask \u001b[38;5;241m=\u001b[39m context[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 410\u001b[0m     context_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    411\u001b[0m     aggregated_context_embedding \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m context_embedding\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    413\u001b[0m aggregated_context_embeddings\u001b[38;5;241m.\u001b[39mappend(aggregated_context_embedding \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(context_list))\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 357\u001b[0m, in \u001b[0;36mCustomDPRContextEncoder.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;66;03m# Generate outputs from the BERT model\u001b[39;00m\n\u001b[1;32m--> 357\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;66;03m# Use the pooled output for creating embeddings\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:349\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    346\u001b[0m         relative_position_scores_key \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhrd,lrd->bhlr\u001b[39m\u001b[38;5;124m\"\u001b[39m, key_layer, positional_embedding)\n\u001b[0;32m    347\u001b[0m         attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m relative_position_scores_query \u001b[38;5;241m+\u001b[39m relative_position_scores_key\n\u001b[1;32m--> 349\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_head_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.79 GiB is allocated by PyTorch, and 13.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#############################################################################\n",
    "# 1. Preprocessing Data\n",
    "\n",
    "# Load the dataset\n",
    "code_dataset = load_dataset(\"code_search_net\", \"python\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocess and tokenize dataset\n",
    "def preprocess_function(examples):\n",
    "    tokenized_output = tokenizer(examples['func_code_string'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = [row[:-1] + [tokenizer.pad_token_id] for row in tokenized_output[\"input_ids\"]]\n",
    "    tokenized_output[\"labels\"] = labels\n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_dataset = code_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"True vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "#############################################################################\n",
    "# 2. Sub-Model Weights\n",
    "# 1. Transformer with DPO:\n",
    "tran_dpo = r'C:\\Users\\robbi\\IEEMM\\language_model_weights.pth'\n",
    "# 2. MAMBA:\n",
    "mamba_model_path = r'C:\\Users\\robbi\\IEEMM\\mamba_model_weights.pth'\n",
    "# 3. Transformer and RAG:\n",
    "context_encoder = r'C:\\Users\\robbi\\IEEMM\\context_encoder.pth'\n",
    "language_model = r'C:\\Users\\robbi\\IEEMM\\language_model.pth'\n",
    "question_encoder = r'C:\\Users\\robbi\\IEEMM\\question_encoder.pth'\n",
    "\n",
    "# Load model weights function\n",
    "def load_model_weights(model, model_path):\n",
    "    #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "    if isinstance(checkpoint, dict):\n",
    "        # Check for 'state_dict' or 'model_state_dict' keys\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        elif 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            # If no known key is found, try loading it as a raw state dictionary\n",
    "            try:\n",
    "                model.load_state_dict(checkpoint)\n",
    "            except RuntimeError as e:\n",
    "                raise ValueError(f\"Error loading state dict: {e}\")\n",
    "    elif isinstance(checkpoint, nn.Module):\n",
    "        # If the checkpoint is a model object, assign it directly\n",
    "        model = checkpoint\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "#############################################################################\n",
    "# 3. MAMBA\n",
    "# RoPE\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "        self.register_buffer('sin', freqs.sin())\n",
    "        self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "        sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "        # Apply RoPE to even and odd indices separately\n",
    "        x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "# SWIGLU\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(SwiGLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "        self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = torch.sigmoid(self.fc2(x))\n",
    "        return self.fc1(x) * gate\n",
    "\n",
    "class SimplifiedMAMBA(nn.Module):\n",
    "    # Adjusted to include SwiGLU blocks\n",
    "    def __init__(self, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_state),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_state, d_model)\n",
    "        )\n",
    "        self.input_embedding = nn.Linear(d_model, d_model)\n",
    "        self.convs = nn.Sequential(*[nn.Conv1d(d_model, d_model, kernel_size=d_conv, padding=(d_conv // 2)) for _ in range(num_layers)])\n",
    "        self.swiglu = SwiGLU(d_model, d_model)\n",
    "        self.output_projection = nn.Linear(d_model, d_model * expansion_factor)  # Adjusted to match the output of SwiGLU\n",
    "\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "        nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "        nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "    def forward(self, inputs, attention_mask=None):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "        # Apply the attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "        projected_inputs = self.input_embedding(inputs)\n",
    "        print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            projected_inputs = conv(projected_inputs)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = self.swiglu(projected_inputs)\n",
    "        print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "        output = self.output_projection(projected_inputs)\n",
    "        print(\"output shape:\", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "    # Including rotary positional encodings if required\n",
    "    def __init__(self, vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = RotaryPositionalEncoding(d_model)\n",
    "        self.simplified_mamba = SimplifiedMAMBA(num_layers, d_model, d_state, d_conv, expansion_factor)\n",
    "        self.output_projection = nn.Linear(d_model*2, vocab_size)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = 1.0\n",
    "\n",
    "        nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "        logits = self.output_projection(simplified_mamba_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY =  0.1\n",
    "WARMUP_STEPS = 100\n",
    "TOTAL_STEPS = 1000 # we want this to be : epochs * (size of dataset / batch_size )\n",
    "EPOCHS = 100\n",
    "VOCAB_SIZE = 30522\n",
    "NUM_LAYERS = 4\n",
    "BATCH_SIZE = 8\n",
    "EXPANSION_FACTOR = 2\n",
    "CLIP_GRADIENT = 1.0\n",
    "D_MODEL = 512  # Dimensionality of the model's embeddings\n",
    "D_STATE = 2048  # Dimensionality of the intermediate state in feedforward\n",
    "D_CONV = 3  # Kernel size for convolutional layers\n",
    "\n",
    "\n",
    "def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Linear warmup with cosine decay\n",
    "    scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "# Initialize the optimizer and scheduler with appropriate parameters\n",
    "#mamba_optimizer, mamba_scheduler = setup_optimizer(mamba_model, LEARNING_RATE, WEIGHT_DECAY, WARMUP_STEPS, TOTAL_STEPS)\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "# 4. RAG\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    # Split the text into chunks of chunk_size\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def preprocess_text(text, max_length=512):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Split the text into smaller chunks to maintain context\n",
    "    # The chunk size is slightly less than max_length to account for special tokens\n",
    "    chunk_size = max_length - 50  # Adjust this value based on the model's requirements\n",
    "    text_chunks = split_into_chunks(text, chunk_size)\n",
    "\n",
    "    # Process each chunk\n",
    "    processed_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        processed_chunk = {\n",
    "            'input_ids': tokenized_output['input_ids'],\n",
    "            'attention_mask': tokenized_output['attention_mask']\n",
    "        }\n",
    "        processed_chunks.append(processed_chunk)\n",
    "\n",
    "    return processed_chunks\n",
    "\n",
    "\n",
    "def create_dataset_from_pdfs(pdf_file_paths):\n",
    "    dataset = []\n",
    "    for file_path in pdf_file_paths:\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        processed_text = preprocess_text(text)\n",
    "        dataset.append(processed_text)\n",
    "    return dataset\n",
    "\n",
    "def retrieve_contexts(dataset, query_embedding, top_k=5):\n",
    "    # Assume dataset is a list of dictionaries with each dictionary containing 'input_ids' and 'attention_mask'\n",
    "    # for a particular context and that each context has been processed through a DPRContextEncoder to get embeddings\n",
    "\n",
    "    # Placeholder for storing similarity scores\n",
    "    similarity_scores = []\n",
    "\n",
    "    # Iterate over each context in the dataset\n",
    "    for context in dataset:\n",
    "        context_input_ids = context['input_ids']\n",
    "        context_attention_mask = context['attention_mask']\n",
    "\n",
    "        # Assuming context_encoder is an instance of CustomDPRContextEncoder that's already trained\n",
    "        # and available in your scope\n",
    "        context_embedding = context_encoder(context_input_ids, context_attention_mask)\n",
    "\n",
    "        # Compute similarity (e.g., using dot product)\n",
    "        similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "\n",
    "        similarity_scores.append(similarity.squeeze().item())\n",
    "\n",
    "    # Sort contexts based on similarity scores and retrieve top_k indices\n",
    "    top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "\n",
    "    # Retrieve top_k contexts\n",
    "    top_contexts = [dataset[i] for i in top_k_indices]\n",
    "\n",
    "    return top_contexts\n",
    "\n",
    "def rag_retrieve_and_generate(dataset, query):\n",
    "    # Instantiate the question encoder\n",
    "    question_encoder = DPRQuestionEncoder()\n",
    "\n",
    "    # Encode the query\n",
    "    encoded_query = question_encoder(query)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    # This involves finding the most similar documents in the dataset\n",
    "    # For simplicity, this is represented as a function 'retrieve_contexts'\n",
    "    relevant_contexts = retrieve_contexts(dataset, encoded_query)\n",
    "\n",
    "    # Language model for generation\n",
    "    language_model = LanguageModelTransformer()\n",
    "\n",
    "    # Generate a response based on the retrieved contexts\n",
    "    # This step may involve further formatting or preprocessing\n",
    "    response = language_model.generate_response(relevant_contexts)\n",
    "\n",
    "    return response\n",
    "\n",
    "# pdfs\n",
    "pdf_file_paths = [r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "                  r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf']\n",
    "\n",
    "rag_dataset = create_dataset_from_pdfs(pdf_file_paths)\n",
    "\n",
    "class CustomDPRContextEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(CustomDPRContextEncoder, self).__init__()\n",
    "        # Transformer-based model, e.g., BERT\n",
    "        self.bert_model = BertModel.from_pretrained(model_name)\n",
    "        # Additional layer to produce fixed-size embeddings\n",
    "        self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Generate outputs from the BERT model\n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the pooled output for creating embeddings\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # Pass through the embedding layer\n",
    "        context_embeddings = self.embedding_layer(pooled_output)\n",
    "        return context_embeddings\n",
    "\n",
    "class DPRQuestionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(DPRQuestionEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.embedding_layer = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        embeddings = self.embedding_layer(pooled_output)\n",
    "        return embeddings\n",
    "\n",
    "# TransformerRAG class\n",
    "class TransformerRAG(nn.Module):\n",
    "    def __init__(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n",
    "        super(TransformerRAG, self).__init__()\n",
    "        self.context_encoder = CustomDPRContextEncoder().to(device)\n",
    "        self.language_model = LanguageModelTransformer(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_size=256, \n",
    "            num_layers=6, \n",
    "            forward_expansion=4, \n",
    "            heads=8, \n",
    "            dropout=0, \n",
    "            max_length=100,  # Set to 512 to match the tokenization max_length\n",
    "            rank=16\n",
    "        ).to(device)\n",
    "        self.language_model = load_model_weights(self.language_model, language_model_path)\n",
    "        self.question_encoder = DPRQuestionEncoder().to(device)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "        if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "            raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "\n",
    "        # Process each context_text\n",
    "        aggregated_context_embeddings = []\n",
    "        for context_list in context_texts:\n",
    "            if not all(isinstance(context, dict) for context in context_list):\n",
    "                raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "\n",
    "            # Create a tensor of zeros with the correct shape on the GPU\n",
    "            aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "            for context in context_list:\n",
    "                context_input_ids = context['input_ids'].to(device)\n",
    "                context_attention_mask = context['attention_mask'].to(device)\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "\n",
    "            aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "\n",
    "        question_input_ids = question_input_ids.to(device).long()\n",
    "        question_attention_mask = question_attention_mask.to(device).long()\n",
    "        question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "\n",
    "        cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "        similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "        most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=device))\n",
    "\n",
    "        combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "        tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        tokenized_combined_input = {k: v.to(device) for k, v in tokenized_combined_input.items()}  # Move to GPU\n",
    "        response_logits = self.language_model(**tokenized_combined_input)\n",
    "        probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "        predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "        predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "        response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# 5. LanguageModelTransformer\n",
    "\n",
    "class LORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "        super(LORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weight and bias of the linear layer\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # LORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"LORALayer Input Shape:\", x.shape)\n",
    "        \n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        \n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "class QLORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "        super(QLORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "\n",
    "        # Original weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # QLORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def quantize(self, x, num_bits):\n",
    "        # Implement a simple quantization method\n",
    "        scale = x.abs().max()\n",
    "        x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "        return x_quantized, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "        B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "        lora_adjustment = self.dropout(lora_adjustment)\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def update_alpha(self, new_alpha):\n",
    "        \"\"\"\n",
    "        Update the alpha scaling factor.\n",
    "        \"\"\"\n",
    "        self.alpha = new_alpha\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does the matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, then sum it up\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class LanguageModelDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "        super(LanguageModelDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Adding BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # QLORA layers\n",
    "        self.qlora_feed_forward = nn.Sequential(\n",
    "            QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "        self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length) #.to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, trg_mask)\n",
    "            if self.use_qlora:\n",
    "                x = self.qlora_feed_forward(x)\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "        return out\n",
    "\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.use_qlora = use_qlora\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "        self.decoder = LanguageModelDecoder(\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            rank,\n",
    "        )\n",
    "\n",
    "    def forward(self, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        out = self.decoder(trg, trg_mask)\n",
    "        return out\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ).to(trg.device)\n",
    "\n",
    "        return trg_mask\n",
    "    \n",
    "    # Function to enable or disable QLORA layers (for fine-tuning purposes)\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "    def generate_response(self, input_ids, attention_mask):\n",
    "        # Assuming you have a forward method that returns logits\n",
    "        logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # For simplicity, using greedy decoding here. You might want to use beam search or sampling.\n",
    "        predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "        \n",
    "        # Convert predicted token ids to tokens\n",
    "        predicted_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "        \n",
    "        # Join tokens to form the response string. This is a very basic way to generate text and might not produce the best results.\n",
    "        response = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "        \n",
    "        return response\n",
    "\n",
    "###########################################################################\n",
    "# 7. Internal Switch Routing\n",
    "\n",
    "# Auxiliary loss function\n",
    "def auxiliary_loss(gate_scores, expert_capacity):\n",
    "    expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "    loss_balancing = torch.std(expert_load)\n",
    "    return loss_balancing\n",
    "\n",
    "# Routing function\n",
    "CAPACITY_FACTOR = 1\n",
    "\n",
    "def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "    capacity_factor_tensor = torch.tensor([CAPACITY_FACTOR], dtype=torch.float32)\n",
    "    capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "    expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "    for idx in range(len(expert_indices)):\n",
    "        selected_expert = expert_indices[idx]\n",
    "        if expert_counts[selected_expert] < capacities[0]:\n",
    "            expert_counts[selected_expert] += 1\n",
    "        else:\n",
    "            available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "            if len(available_experts) > 0:\n",
    "                alternative_expert = available_experts[0]\n",
    "                expert_indices[idx] = alternative_expert\n",
    "                expert_counts[alternative_expert] += 1\n",
    "            else:\n",
    "                print(\"No available experts to reroute. Handling overflow.\")\n",
    "    return expert_indices\n",
    "\n",
    "# SwitchGate \n",
    "class SwitchGate(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(SwitchGate, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "        self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "        return gate_scores\n",
    "\n",
    "# SwitchRouter \n",
    "class SwitchRouter(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank):\n",
    "        super(SwitchRouter, self).__init__()\n",
    "        self.router = SwitchGate(input_dim, num_experts)\n",
    "        self.transformer_rag = TransformerRAG(context_encoder_path, \n",
    "                                              language_model_path, \n",
    "                                              question_encoder_path, \n",
    "                                              vocab_size).to(device)\n",
    "        self.transformer_dpo = LanguageModelTransformer(vocab_size, \n",
    "                                                        embed_size, \n",
    "                                                        num_layers, \n",
    "                                                        forward_expansion, \n",
    "                                                        heads, dropout, \n",
    "                                                        max_length, \n",
    "                                                        rank).to(device)\n",
    "        self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "        self.mamba = SimplifiedLanguageModelMAMBA(vocab_size=VOCAB_SIZE, \n",
    "                                     num_layers=NUM_LAYERS, \n",
    "                                     d_model=D_MODEL, \n",
    "                                     d_state=D_STATE, \n",
    "                                     d_conv=D_CONV, \n",
    "                                     expansion_factor=EXPANSION_FACTOR).to(device)\n",
    "        self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "        self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "        self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        x = self.input_embedding(x.float())\n",
    "        gate_scores = self.router(x)\n",
    "        expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "        expert_indices = route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "        final_output = torch.zeros_like(x)\n",
    "        aux_loss = 0\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_indices == i\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                selected_attention_mask = attention_mask[mask]\n",
    "\n",
    "                if isinstance(expert, TransformerRAG):\n",
    "                    # Now passing the required arguments to TransformerRAG\n",
    "                    expert_output = self.transformer_rag(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                else:\n",
    "                    # Process as usual for other experts\n",
    "                    expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        # Compute auxiliary loss for load balancing\n",
    "        aux_loss += auxiliary_loss(gate_scores, expert_capacity=torch.tensor([CAPACITY_FACTOR] * len(self.experts)))\n",
    "\n",
    "        return final_output, aux_loss\n",
    "\n",
    "###########################################################################\n",
    "# 8.Training loop\n",
    "input_dim = 512\n",
    "num_experts = 3\n",
    "\n",
    "\n",
    "model = SwitchRouter(input_dim, num_experts, mamba_model_path, context_encoder, language_model, question_encoder, tran_dpo, 30522, 256, 6, 4, 8, 0.1, 100, 16) #.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "model.to(device)\n",
    "main_loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "aux_loss_weight = 0.1\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model_embedding_size = model.transformer_dpo.decoder.word_embedding.num_embeddings\n",
    "assert tokenizer.vocab_size == VOCAB_SIZE, f\"Tokenizer vocab size ({tokenizer.vocab_size}) doesn't match expected vocab size ({VOCAB_SIZE})\"\n",
    "\n",
    "# Ensure train_data is accessible here, with 'queries' and 'contexts' keys\n",
    "train_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        rag_dataset[0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        rag_dataset[0],\n",
    "        rag_dataset[0],\n",
    "        rag_dataset[0],\n",
    "        # Contexts from MAMBA.pdf\n",
    "        rag_dataset[1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        rag_dataset[1],\n",
    "        rag_dataset[1],\n",
    "        rag_dataset[1],\n",
    "        # Contexts from QLORA.pdf\n",
    "        rag_dataset[2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        rag_dataset[2],\n",
    "        rag_dataset[2],\n",
    "        rag_dataset[2],\n",
    "        # Contexts from RAG.pdf\n",
    "        rag_dataset[3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        rag_dataset[3],\n",
    "        rag_dataset[3],\n",
    "        rag_dataset[3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        rag_dataset[4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        rag_dataset[4],\n",
    "        rag_dataset[4],\n",
    "        rag_dataset[4]\n",
    "    ]\n",
    "}\n",
    "# Before training, ensure the embedding size of each component matches the tokenizer vocab size\n",
    "assert model.transformer_dpo.decoder.word_embedding.num_embeddings == tokenizer.vocab_size, \\\n",
    "    \"Transformer DPO's embedding size does not match tokenizer vocab size\"\n",
    "\n",
    "assert model.transformer_rag.context_encoder.bert_model.config.vocab_size == tokenizer.vocab_size, \\\n",
    "    \"RAG context encoder vocab size does not match tokenizer vocab size\"\n",
    "\n",
    "assert model.transformer_rag.question_encoder.bert.config.vocab_size == tokenizer.vocab_size, \\\n",
    "    \"RAG question encoder vocab size does not match tokenizer vocab size\"\n",
    "\n",
    "\n",
    "def count_total_parameters(models):\n",
    "    total_params = 0\n",
    "    for model in models:\n",
    "        model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params += model_params\n",
    "        print(f\"Parameters in {model.__class__.__name__}: {model_params}\")\n",
    "    return total_params\n",
    "\n",
    "# Usage\n",
    "models = [model.transformer_rag, model.transformer_dpo, model.mamba, model]\n",
    "total_params = count_total_parameters(models)\n",
    "print(f\"Total trainable parameters across all models: {total_params}\")\n",
    "\n",
    "# Start training\n",
    "model.train()\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "\n",
    "        # Ensure inputs do not exceed the tokenizer's vocabulary size\n",
    "        if inputs.max() >= tokenizer.vocab_size:\n",
    "            raise ValueError(\"Input IDs exceed tokenizer's vocabulary size\")\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        # Ensure batch indices are within the range of train_data\n",
    "        assert end_idx <= len(train_data['queries']), \"Batch index exceeds size of 'queries' data\"\n",
    "        assert end_idx <= len(train_data['contexts']), \"Batch index exceeds size of 'contexts' data\"\n",
    "\n",
    "        # Debug prints for shapes and value ranges\n",
    "        print(f\"Batch index: {batch_idx}\")\n",
    "        print(f\"Input IDs shape: {inputs.shape}, Max ID: {inputs.max()}\")\n",
    "        print(f\"Attention Mask shape: {attention_mask.shape}\")\n",
    "        print(f\"Targets shape: {targets.shape}, Max ID: {targets.max()}\")\n",
    "\n",
    "        current_queries = train_data['queries'][start_idx:end_idx]\n",
    "        current_contexts = train_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "        # Debug prints for current queries and contexts\n",
    "        for i, (q, context_list) in enumerate(zip(current_queries, current_contexts)):\n",
    "            print(f\"Processing query-context pair {i}:\")\n",
    "            print(f\"Query: {q}\")\n",
    "            #for c in context_list:  # Assuming context_list is a list of dictionaries\n",
    "                #print(f\"Context input IDs shape: {torch.tensor(c['input_ids']).shape}\")\n",
    "                #print(f\"Context attention mask shape: {torch.tensor(c['attention_mask']).shape}\")\n",
    "\n",
    "        # Call to the model forward function\n",
    "        outputs, aux_loss = model(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "        # Calculate loss\n",
    "        main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        total_loss = main_loss + aux_loss_weight * aux_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()  \n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {total_loss.item()}')\n",
    "\n",
    "    print(f'End of Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v7- grad accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True vocab size: 30522\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.78 GiB is allocated by PyTorch, and 13.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 827\u001b[0m\n\u001b[0;32m    823\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[0;32m    824\u001b[0m num_experts \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m--> 827\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSwitchRouter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_experts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmamba_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtran_dpo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30522\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\u001b[39;00m\n\u001b[0;32m    828\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    829\u001b[0m main_loss_function \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[1;32mIn[3], line 771\u001b[0m, in \u001b[0;36mSwitchRouter.__init__\u001b[1;34m(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28msuper\u001b[39m(SwitchRouter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrouter \u001b[38;5;241m=\u001b[39m SwitchGate(input_dim, num_experts)\n\u001b[1;32m--> 771\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_rag \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_encoder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mlanguage_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mquestion_encoder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_dpo \u001b[38;5;241m=\u001b[39m LanguageModelTransformer(vocab_size, \n\u001b[0;32m    776\u001b[0m                                                 embed_size, \n\u001b[0;32m    777\u001b[0m                                                 num_layers, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    780\u001b[0m                                                 max_length, \n\u001b[0;32m    781\u001b[0m                                                 rank)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_dpo \u001b[38;5;241m=\u001b[39m load_model_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_dpo, dpo_model_path)\n",
      "Cell \u001b[1;32mIn[3], line 380\u001b[0m, in \u001b[0;36mTransformerRAG.__init__\u001b[1;34m(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28msuper\u001b[39m(TransformerRAG, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDPRContextEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model \u001b[38;5;241m=\u001b[39m LanguageModelTransformer(\n\u001b[0;32m    382\u001b[0m         vocab_size\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[0;32m    383\u001b[0m         embed_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    389\u001b[0m         rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m\n\u001b[0;32m    390\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model \u001b[38;5;241m=\u001b[39m load_model_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model, language_model_path)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.78 GiB is allocated by PyTorch, and 13.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#############################################################################\n",
    "# 1. Preprocessing Data\n",
    "\n",
    "# Load the dataset\n",
    "code_dataset = load_dataset(\"code_search_net\", \"python\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocess and tokenize dataset\n",
    "def preprocess_function(examples):\n",
    "    tokenized_output = tokenizer(examples['func_code_string'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = [row[:-1] + [tokenizer.pad_token_id] for row in tokenized_output[\"input_ids\"]]\n",
    "    tokenized_output[\"labels\"] = labels\n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_dataset = code_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"True vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "#############################################################################\n",
    "# 2. Sub-Model Weights\n",
    "# 1. Transformer with DPO:\n",
    "tran_dpo = r'C:\\Users\\robbi\\IEEMM\\language_model_weights.pth'\n",
    "# 2. MAMBA:\n",
    "mamba_model_path = r'C:\\Users\\robbi\\IEEMM\\mamba_model_weights.pth'\n",
    "# 3. Transformer and RAG:\n",
    "context_encoder = r'C:\\Users\\robbi\\IEEMM\\context_encoder.pth'\n",
    "language_model = r'C:\\Users\\robbi\\IEEMM\\language_model.pth'\n",
    "question_encoder = r'C:\\Users\\robbi\\IEEMM\\question_encoder.pth'\n",
    "\n",
    "# Load model weights function\n",
    "def load_model_weights(model, model_path):\n",
    "    #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "    if isinstance(checkpoint, dict):\n",
    "        # Check for 'state_dict' or 'model_state_dict' keys\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        elif 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            # If no known key is found, try loading it as a raw state dictionary\n",
    "            try:\n",
    "                model.load_state_dict(checkpoint)\n",
    "            except RuntimeError as e:\n",
    "                raise ValueError(f\"Error loading state dict: {e}\")\n",
    "    elif isinstance(checkpoint, nn.Module):\n",
    "        # If the checkpoint is a model object, assign it directly\n",
    "        model = checkpoint\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "#############################################################################\n",
    "# 3. MAMBA\n",
    "# RoPE\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "        self.register_buffer('sin', freqs.sin())\n",
    "        self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "        sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "        # Apply RoPE to even and odd indices separately\n",
    "        x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "        return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "# SWIGLU\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(SwiGLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "        self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = torch.sigmoid(self.fc2(x))\n",
    "        return self.fc1(x) * gate\n",
    "\n",
    "class SimplifiedMAMBA(nn.Module):\n",
    "    # Adjusted to include SwiGLU blocks\n",
    "    def __init__(self, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_state),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_state, d_model)\n",
    "        )\n",
    "        self.input_embedding = nn.Linear(d_model, d_model)\n",
    "        self.convs = nn.Sequential(*[nn.Conv1d(d_model, d_model, kernel_size=d_conv, padding=(d_conv // 2)) for _ in range(num_layers)])\n",
    "        self.swiglu = SwiGLU(d_model, d_model)\n",
    "        self.output_projection = nn.Linear(d_model, d_model * expansion_factor)  # Adjusted to match the output of SwiGLU\n",
    "\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "        nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "        nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "    def forward(self, inputs, attention_mask=None):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "        # Apply the attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "        projected_inputs = self.input_embedding(inputs)\n",
    "        print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            projected_inputs = conv(projected_inputs)\n",
    "\n",
    "        projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "        print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "        projected_inputs = self.swiglu(projected_inputs)\n",
    "        print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "        output = self.output_projection(projected_inputs)\n",
    "        print(\"output shape:\", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "    # Including rotary positional encodings if required\n",
    "    def __init__(self, vocab_size, num_layers, d_model, d_state, d_conv, expansion_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = RotaryPositionalEncoding(d_model)\n",
    "        self.simplified_mamba = SimplifiedMAMBA(num_layers, d_model, d_state, d_conv, expansion_factor)\n",
    "        self.output_projection = nn.Linear(d_model*2, vocab_size)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        gain = 1.0\n",
    "\n",
    "        nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "        logits = self.output_projection(simplified_mamba_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY =  0.1\n",
    "WARMUP_STEPS = 100\n",
    "TOTAL_STEPS = 1000 # we want this to be : epochs * (size of dataset / batch_size )\n",
    "EPOCHS = 100\n",
    "VOCAB_SIZE = 30522\n",
    "NUM_LAYERS = 4\n",
    "BATCH_SIZE = 8\n",
    "EXPANSION_FACTOR = 2\n",
    "CLIP_GRADIENT = 1.0\n",
    "D_MODEL = 512  # Dimensionality of the model's embeddings\n",
    "D_STATE = 2048  # Dimensionality of the intermediate state in feedforward\n",
    "D_CONV = 3  # Kernel size for convolutional layers\n",
    "\n",
    "\n",
    "def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Linear warmup with cosine decay\n",
    "    scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "# Initialize the optimizer and scheduler with appropriate parameters\n",
    "#mamba_optimizer, mamba_scheduler = setup_optimizer(mamba_model, LEARNING_RATE, WEIGHT_DECAY, WARMUP_STEPS, TOTAL_STEPS)\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "# 4. RAG\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    # Split the text into chunks of chunk_size\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def preprocess_text(text, max_length=512):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Split the text into smaller chunks to maintain context\n",
    "    # The chunk size is slightly less than max_length to account for special tokens\n",
    "    chunk_size = max_length - 50  # Adjust this value based on the model's requirements\n",
    "    text_chunks = split_into_chunks(text, chunk_size)\n",
    "\n",
    "    # Process each chunk\n",
    "    processed_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        processed_chunk = {\n",
    "            'input_ids': tokenized_output['input_ids'],\n",
    "            'attention_mask': tokenized_output['attention_mask']\n",
    "        }\n",
    "        processed_chunks.append(processed_chunk)\n",
    "\n",
    "    return processed_chunks\n",
    "\n",
    "def create_dataset_from_pdfs(pdf_file_paths):\n",
    "    dataset = []\n",
    "    for file_path in pdf_file_paths:\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        processed_text = preprocess_text(text)\n",
    "        dataset.append(processed_text)\n",
    "    return dataset\n",
    "\n",
    "def retrieve_contexts(dataset, query_embedding, top_k=5):\n",
    "    # Assume dataset is a list of dictionaries with each dictionary containing 'input_ids' and 'attention_mask'\n",
    "    # for a particular context and that each context has been processed through a DPRContextEncoder to get embeddings\n",
    "\n",
    "    # Placeholder for storing similarity scores\n",
    "    similarity_scores = []\n",
    "\n",
    "    # Iterate over each context in the dataset\n",
    "    for context in dataset:\n",
    "        context_input_ids = context['input_ids']\n",
    "        context_attention_mask = context['attention_mask']\n",
    "\n",
    "        # Assuming context_encoder is an instance of CustomDPRContextEncoder that's already trained\n",
    "        # and available in your scope\n",
    "        context_embedding = context_encoder(context_input_ids, context_attention_mask)\n",
    "\n",
    "        # Compute similarity (e.g., using dot product)\n",
    "        similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "\n",
    "        similarity_scores.append(similarity.squeeze().item())\n",
    "\n",
    "    # Sort contexts based on similarity scores and retrieve top_k indices\n",
    "    top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "\n",
    "    # Retrieve top_k contexts\n",
    "    top_contexts = [dataset[i] for i in top_k_indices]\n",
    "\n",
    "    return top_contexts\n",
    "\n",
    "def rag_retrieve_and_generate(dataset, query):\n",
    "    # Instantiate the question encoder\n",
    "    question_encoder = DPRQuestionEncoder()\n",
    "\n",
    "    # Encode the query\n",
    "    encoded_query = question_encoder(query)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    # This involves finding the most similar documents in the dataset\n",
    "    # For simplicity, this is represented as a function 'retrieve_contexts'\n",
    "    relevant_contexts = retrieve_contexts(dataset, encoded_query)\n",
    "\n",
    "    # Language model for generation\n",
    "    language_model = LanguageModelTransformer()\n",
    "\n",
    "    # Generate a response based on the retrieved contexts\n",
    "    # This step may involve further formatting or preprocessing\n",
    "    response = language_model.generate_response(relevant_contexts)\n",
    "\n",
    "    return response\n",
    "\n",
    "# pdfs\n",
    "pdf_file_paths = [r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "                  r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "                  r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf']\n",
    "\n",
    "rag_dataset = create_dataset_from_pdfs(pdf_file_paths)\n",
    "\n",
    "class CustomDPRContextEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(CustomDPRContextEncoder, self).__init__()\n",
    "        # Transformer-based model, e.g., BERT\n",
    "        self.bert_model = BertModel.from_pretrained(model_name)\n",
    "        # Additional layer to produce fixed-size embeddings\n",
    "        self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Generate outputs from the BERT model\n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the pooled output for creating embeddings\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # Pass through the embedding layer\n",
    "        context_embeddings = self.embedding_layer(pooled_output)\n",
    "        return context_embeddings\n",
    "\n",
    "class DPRQuestionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', embedding_dim=768):\n",
    "        super(DPRQuestionEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.embedding_layer = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        embeddings = self.embedding_layer(pooled_output)\n",
    "        return embeddings\n",
    "\n",
    "# TransformerRAG class\n",
    "class TransformerRAG(nn.Module):\n",
    "    def __init__(self, context_encoder_path, language_model_path, question_encoder_path, vocab_size):\n",
    "        super(TransformerRAG, self).__init__()\n",
    "        self.context_encoder = CustomDPRContextEncoder().to(device)\n",
    "        self.language_model = LanguageModelTransformer(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_size=256, \n",
    "            num_layers=6, \n",
    "            forward_expansion=4, \n",
    "            heads=8, \n",
    "            dropout=0, \n",
    "            max_length=100,  # Set to 512 to match the tokenization max_length\n",
    "            rank=16\n",
    "        ).to(device)\n",
    "        self.language_model = load_model_weights(self.language_model, language_model_path)\n",
    "        self.question_encoder = DPRQuestionEncoder().to(device)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "        if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "            raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "\n",
    "        # Process each context_text\n",
    "        aggregated_context_embeddings = []\n",
    "        for context_list in context_texts:\n",
    "            if not all(isinstance(context, dict) for context in context_list):\n",
    "                raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "\n",
    "            # Create a tensor of zeros with the correct shape on the GPU\n",
    "            aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "            for context in context_list:\n",
    "                context_input_ids = context['input_ids'].to(device)\n",
    "                context_attention_mask = context['attention_mask'].to(device)\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "\n",
    "            aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "\n",
    "        question_input_ids = question_input_ids.to(device).long()\n",
    "        question_attention_mask = question_attention_mask.to(device).long()\n",
    "        question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "\n",
    "        cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "        similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "        most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=device))\n",
    "\n",
    "        combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "        tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        tokenized_combined_input = {k: v.to(device) for k, v in tokenized_combined_input.items()}  # Move to GPU\n",
    "        response_logits = self.language_model(**tokenized_combined_input)\n",
    "        probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "        predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "        predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "        response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# 5. LanguageModelTransformer\n",
    "\n",
    "class LORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "        super(LORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weight and bias of the linear layer\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # LORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"LORALayer Input Shape:\", x.shape)\n",
    "        \n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "        \n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "class QLORALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "        super(QLORALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "\n",
    "        # Original weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "        # QLORA specific parameters\n",
    "        self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.A, 0, 0.02)\n",
    "        nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "    def quantize(self, x, num_bits):\n",
    "        # Implement a simple quantization method\n",
    "        scale = x.abs().max()\n",
    "        x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "        return x_quantized, scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "        original_size = x.size()\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "        A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "        B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "        # Compute lora_adjustment for each input in the batch\n",
    "        lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "        lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "        lora_adjustment = self.dropout(lora_adjustment)\n",
    "        #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "        #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "        # Apply linear transformation to x_flattened\n",
    "        x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "        x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # Add lora_adjustment to the transformed x\n",
    "        x = x_transformed + lora_adjustment\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def update_alpha(self, new_alpha):\n",
    "        \"\"\"\n",
    "        Update the alpha scaling factor.\n",
    "        \"\"\"\n",
    "        self.alpha = new_alpha\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does the matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, then sum it up\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class LanguageModelDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "        super(LanguageModelDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Adding BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # QLORA layers\n",
    "        self.qlora_feed_forward = nn.Sequential(\n",
    "            QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "            nn.ReLU(),\n",
    "            QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "        )\n",
    "        self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length) #.to(x.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, trg_mask)\n",
    "            if self.use_qlora:\n",
    "                x = self.qlora_feed_forward(x)\n",
    "\n",
    "        # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "        return out\n",
    "\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.use_qlora = use_qlora\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=100, rank=16):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "\n",
    "        self.decoder = LanguageModelDecoder(\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            rank,\n",
    "        )\n",
    "\n",
    "    def forward(self, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        out = self.decoder(trg, trg_mask)\n",
    "        return out\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ).to(trg.device)\n",
    "\n",
    "        return trg_mask\n",
    "    \n",
    "    # Function to enable or disable QLORA layers (for fine-tuning purposes)\n",
    "    def toggle_qlora(self, use_qlora: bool):\n",
    "        self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "    def generate_response(self, input_ids, attention_mask):\n",
    "        # Assuming you have a forward method that returns logits\n",
    "        logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # For simplicity, using greedy decoding here. You might want to use beam search or sampling.\n",
    "        predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "        \n",
    "        # Convert predicted token ids to tokens\n",
    "        predicted_tokens = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "        \n",
    "        # Join tokens to form the response string. This is a very basic way to generate text and might not produce the best results.\n",
    "        response = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "        \n",
    "        return response\n",
    "\n",
    "###########################################################################\n",
    "# 7. Internal Switch Routing\n",
    "\n",
    "# Auxiliary loss function\n",
    "def auxiliary_loss(gate_scores, expert_capacity):\n",
    "    expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "    loss_balancing = torch.std(expert_load)\n",
    "    return loss_balancing\n",
    "\n",
    "# Routing function\n",
    "CAPACITY_FACTOR = 1\n",
    "\n",
    "def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "    capacity_factor_tensor = torch.tensor([CAPACITY_FACTOR], dtype=torch.float32)\n",
    "    capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "    expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "    for idx in range(len(expert_indices)):\n",
    "        selected_expert = expert_indices[idx]\n",
    "        if expert_counts[selected_expert] < capacities[0]:\n",
    "            expert_counts[selected_expert] += 1\n",
    "        else:\n",
    "            available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "            if len(available_experts) > 0:\n",
    "                alternative_expert = available_experts[0]\n",
    "                expert_indices[idx] = alternative_expert\n",
    "                expert_counts[alternative_expert] += 1\n",
    "            else:\n",
    "                print(\"No available experts to reroute. Handling overflow.\")\n",
    "    return expert_indices\n",
    "\n",
    "# SwitchGate \n",
    "class SwitchGate(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(SwitchGate, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "        self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "        return gate_scores\n",
    "\n",
    "# SwitchRouter \n",
    "class SwitchRouter(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, mamba_model_path, context_encoder_path, language_model_path, question_encoder_path, dpo_model_path, vocab_size, embed_size, num_layers, forward_expansion, heads, dropout, max_length, rank):\n",
    "        super(SwitchRouter, self).__init__()\n",
    "        self.router = SwitchGate(input_dim, num_experts)\n",
    "        self.transformer_rag = TransformerRAG(context_encoder_path, \n",
    "                                              language_model_path, \n",
    "                                              question_encoder_path, \n",
    "                                              vocab_size).to(device)\n",
    "        self.transformer_dpo = LanguageModelTransformer(vocab_size, \n",
    "                                                        embed_size, \n",
    "                                                        num_layers, \n",
    "                                                        forward_expansion, \n",
    "                                                        heads, dropout, \n",
    "                                                        max_length, \n",
    "                                                        rank).to(device)\n",
    "        self.transformer_dpo = load_model_weights(self.transformer_dpo, dpo_model_path)\n",
    "        self.mamba = SimplifiedLanguageModelMAMBA(vocab_size=VOCAB_SIZE, \n",
    "                                     num_layers=NUM_LAYERS, \n",
    "                                     d_model=D_MODEL, \n",
    "                                     d_state=D_STATE, \n",
    "                                     d_conv=D_CONV, \n",
    "                                     expansion_factor=EXPANSION_FACTOR).to(device)\n",
    "        self.mamba = load_model_weights(self.mamba, mamba_model_path)\n",
    "        self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "        self.input_embedding = nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        x = self.input_embedding(x.float())\n",
    "        gate_scores = self.router(x)\n",
    "        expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "        expert_indices = route_inputs(expert_indices, gate_scores, len(self.experts))\n",
    "        final_output = torch.zeros_like(x)\n",
    "        aux_loss = 0\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_indices == i\n",
    "            if mask.any():\n",
    "                selected_inputs = x[mask]\n",
    "                selected_attention_mask = attention_mask[mask]\n",
    "\n",
    "                if isinstance(expert, TransformerRAG):\n",
    "                    # Now passing the required arguments to TransformerRAG\n",
    "                    expert_output = self.transformer_rag(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                else:\n",
    "                    # Process as usual for other experts\n",
    "                    expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "\n",
    "                final_output[mask] = expert_output\n",
    "\n",
    "        # Compute auxiliary loss for load balancing\n",
    "        aux_loss += auxiliary_loss(gate_scores, expert_capacity=torch.tensor([CAPACITY_FACTOR] * len(self.experts)))\n",
    "\n",
    "        return final_output, aux_loss\n",
    "\n",
    "###########################################################################\n",
    "# 8.Training loop\n",
    "input_dim = 512\n",
    "num_experts = 3\n",
    "\n",
    "\n",
    "model = SwitchRouter(input_dim, num_experts, mamba_model_path, context_encoder, language_model, question_encoder, tran_dpo, 30522, 256, 6, 4, 8, 0.1, 100, 16) #.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "model.to(device)\n",
    "main_loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "aux_loss_weight = 0.1\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model_embedding_size = model.transformer_dpo.decoder.word_embedding.num_embeddings\n",
    "assert tokenizer.vocab_size == VOCAB_SIZE, f\"Tokenizer vocab size ({tokenizer.vocab_size}) doesn't match expected vocab size ({VOCAB_SIZE})\"\n",
    "\n",
    "# Ensure train_data is accessible here, with 'queries' and 'contexts' keys\n",
    "train_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        rag_dataset[0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        rag_dataset[0],\n",
    "        rag_dataset[0],\n",
    "        rag_dataset[0],\n",
    "        # Contexts from MAMBA.pdf\n",
    "        rag_dataset[1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        rag_dataset[1],\n",
    "        rag_dataset[1],\n",
    "        rag_dataset[1],\n",
    "        # Contexts from QLORA.pdf\n",
    "        rag_dataset[2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        rag_dataset[2],\n",
    "        rag_dataset[2],\n",
    "        rag_dataset[2],\n",
    "        # Contexts from RAG.pdf\n",
    "        rag_dataset[3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        rag_dataset[3],\n",
    "        rag_dataset[3],\n",
    "        rag_dataset[3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        rag_dataset[4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        rag_dataset[4],\n",
    "        rag_dataset[4],\n",
    "        rag_dataset[4]\n",
    "    ]\n",
    "}\n",
    "# Before training, ensure the embedding size of each component matches the tokenizer vocab size\n",
    "assert model.transformer_dpo.decoder.word_embedding.num_embeddings == tokenizer.vocab_size, \\\n",
    "    \"Transformer DPO's embedding size does not match tokenizer vocab size\"\n",
    "\n",
    "assert model.transformer_rag.context_encoder.bert_model.config.vocab_size == tokenizer.vocab_size, \\\n",
    "    \"RAG context encoder vocab size does not match tokenizer vocab size\"\n",
    "\n",
    "assert model.transformer_rag.question_encoder.bert.config.vocab_size == tokenizer.vocab_size, \\\n",
    "    \"RAG question encoder vocab size does not match tokenizer vocab size\"\n",
    "\n",
    "\n",
    "def count_total_parameters(models):\n",
    "    total_params = 0\n",
    "    for model in models:\n",
    "        model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params += model_params\n",
    "        print(f\"Parameters in {model.__class__.__name__}: {model_params}\")\n",
    "    return total_params\n",
    "\n",
    "# Usage\n",
    "models = [model.transformer_rag, model.transformer_dpo, model.mamba, model]\n",
    "total_params = count_total_parameters(models)\n",
    "print(f\"Total trainable parameters across all models: {total_params}\")\n",
    "\n",
    "# Start training\n",
    "accumulation_steps = 4\n",
    "model.train()\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()  # Initialize gradients to zero\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "\n",
    "        # Calculate start and end indices for current batch in train_data\n",
    "        start_idx = batch_idx * batch['input_ids'].size(0)\n",
    "        end_idx = start_idx + batch['input_ids'].size(0)\n",
    "\n",
    "        # Extract current_queries and current_contexts for the batch\n",
    "        current_queries = train_data['queries'][start_idx:end_idx]\n",
    "        current_contexts = train_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "        # Call to the model forward function\n",
    "        outputs, aux_loss = model(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "        # Calculate loss and accumulate\n",
    "        main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        loss = (main_loss + aux_loss_weight * aux_loss) / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps  # Scale back up\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'End of Epoch {epoch+1}, Average Loss: {average_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
