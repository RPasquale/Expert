{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision (Image and Video) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIT\n",
    "class FlexiblePatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_size=768, temporal_patch_size=1, is_3d=False):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_size = embed_size\n",
    "        self.is_3d = is_3d\n",
    "\n",
    "        if is_3d:\n",
    "            self.num_patches = int(((img_size // patch_size) ** 2) * temporal_patch_size)\n",
    "            self.projection = nn.Conv3d(in_channels, embed_size, kernel_size=(temporal_patch_size, patch_size, patch_size), \n",
    "\n",
    "                                        stride=(temporal_patch_size, patch_size, patch_size))\n",
    "\n",
    "        else:\n",
    "            self.num_patches = (img_size // patch_size) ** 2\n",
    "            self.projection = nn.Conv2d(in_channels, embed_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)  # [B, E, T/P, H/P, W/P] or [B, E, H/P, W/P]\n",
    "        x = x.flatten(2)  # Flatten spatial and temporal dimensions\n",
    "        x = x.transpose(1, 2)  # [B, N, E]\n",
    "        return x\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_patches, embed_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, embed_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        cls_token = torch.zeros(batch_size, 1, x.shape[-1], device=x.device)\n",
    "        x = torch.cat([cls_token, x], dim=1)  # [B, 1+N, E]\n",
    "        x += self.positional_embedding\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        # Feedforward network\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, src):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "class FlexibleVisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_size=768, num_heads=12, num_layers=12, num_classes=1000, temporal_patch_size=1, is_3d=False):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = FlexiblePatchEmbedding(img_size, patch_size, in_channels, embed_size, temporal_patch_size, is_3d)\n",
    "        self.positional_embedding = PositionalEmbedding(self.patch_embedding.num_patches, embed_size)\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_size),\n",
    "            nn.Linear(embed_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = self.positional_embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        if return_embeddings:\n",
    "            return x  # Return the sequence of embeddings directly\n",
    "        cls_token = x[:, 0]\n",
    "        x = self.mlp_head(cls_token)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trie Construction Completed Successfully\n",
      "Trie built successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot locate reference to <class '__main__.LanguageExpert.WordPiece'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot pickle <class '__main__.LanguageExpert.WordPiece'>: __main__.LanguageExpert.WordPiece has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ef0826f86e47f098163e56f509e985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All token IDs are within the expected range.\n",
      "Input device: cpu, Input shape: torch.Size([8, 128]) , Input Type: <built-in method type of Tensor object at 0x0000027A15D06510>\n",
      "Model device: cpu\n",
      "Before embedding: input_ids.shape=torch.Size([8, 128]), max input_id=2287, vocab_size=2749\n",
      "After embedding: torch.Size([8, 128, 512])\n",
      "x_reshaped shape: torch.Size([8, 128, 8, 64])\n",
      "values : torch.Size([8, 128, 8, 64])\n",
      "queries : torch.Size([8, 128, 8, 128])\n",
      "keys : torch.Size([8, 128, 8, 128])\n",
      "attention_scores after random projection: torch.Size([8, 8, 128, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([8, 128, 8, 128])\n",
      "Partition Start 0, Partition End 128 , ponder_scores: torch.Size([8, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([8, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([8, 128, 8, 128])\n",
      "queries_part shape: torch.Size([8, 128, 8, 128])\n",
      "C_keys shape: torch.Size([8, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([8, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([8, 128, 8, 128])\n",
      "attention shape: torch.Size([8, 128, 8, 128])\n",
      "values shape: torch.Size([8, 8, 128, 64])\n",
      "values.reshape(-1, self.head_dim) shape: torch.Size([8192, 64])\n",
      "self.value_projection(values) shape: torch.Size([8192, 128])\n",
      "projected_values shape: torch.Size([8, 8, 128, 128])\n",
      "2ND EINSUM\n",
      "out shape after einsum: torch.Size([8, 8, 128, 128])\n",
      "out reshaped for projection: torch.Size([1024, 1024])\n",
      "out after final_projection: torch.Size([1024, 512])\n",
      "Before embedding: input_ids.shape=torch.Size([1024, 512]), max input_id=0, vocab_size=2749\n",
      "After embedding: torch.Size([1024, 512, 512])\n",
      "x_reshaped shape: torch.Size([1024, 512, 8, 64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1315\u001b[0m\n\u001b[0;32m   1312\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/EXPERT_WEIGHTS/lmt_expert_trained_custom_tokenizer.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;66;03m# Train the LMT sub-model within the Expert system\u001b[39;00m\n\u001b[1;32m-> 1315\u001b[0m trained_model, average_loss \u001b[38;5;241m=\u001b[39m \u001b[43mlanguage_expert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_language_model_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete. Model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[46], line 1174\u001b[0m, in \u001b[0;36mLanguageExpert.train_language_model_transformer\u001b[1;34m(self, train_loader, device, vocab_size, save_path)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model_transformer\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1173\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m-> 1174\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1175\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;66;03m# Check for NaN in loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[46], line 376\u001b[0m, in \u001b[0;36mLanguageExpert.LanguageModelTransformer.forward\u001b[1;34m(self, input_ids, embeddings, attention_mask)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither input_ids or embeddings must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# Continue processing with embeddings\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# Now, embeddings are passed to the decoder or subsequent layers\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust your decoder to accept embeddings\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[46], line 326\u001b[0m, in \u001b[0;36mLanguageExpert.LanguageModelDecoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# Note: No need to apply dropout here as x are token IDs\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 326\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# SPLASH is called within TransformerBlock\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# Apply dropout after all transformer blocks and before the final projection layer\u001b[39;00m\n\u001b[0;32m    329\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[46], line 351\u001b[0m, in \u001b[0;36mLanguageExpert.TransformerBlock.forward\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids):\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;66;03m# SPLASH already expects token IDs and handles embedding\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplash\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(attention_output))\n\u001b[0;32m    353\u001b[0m     forward_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(attention_output)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[46], line 124\u001b[0m, in \u001b[0;36mLanguageExpert.SPLASH.forward\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m    122\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues(x_reshaped)\n\u001b[0;32m    123\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_projection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries(x_reshaped), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 124\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_projection(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m(x_reshaped), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m )\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues :\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries :\u001b[39m\u001b[38;5;124m\"\u001b[39m, queries\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1682\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1673\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1675\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[1;32m-> 1682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1684\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import re\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "# Test on larger text\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "\n",
    "# Set the HF_HOME environment variable to a new cache directory on the D drive\n",
    "os.environ['HF_HOME'] = 'D:/hf_datasets_cache'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = wordpiece_tokenizer\n",
    "        self.vocab_size = len(wordpiece_tokenizer.vocab)\n",
    "        self.embed_size = 512  # Size of each embedding vector\n",
    "        self.heads = 8  # Number of attention heads\n",
    "        self.num_layers = 6  # Number of transformer blocks\n",
    "        self.forward_expansion = 4  # Expansion size for the feedforward layer\n",
    "        self.dropout = 0.1\n",
    "        self.max_length = 1024  # Maximum length of the input sequences\n",
    "        self.rank = 64  # Rank for LORA adjustments\n",
    "        self.sequence_length = 1024  # Input sequence length for SPLASH\n",
    "        self.projection_dim = 256  # Projection dimension for SPLASH\n",
    "        self.partition_size = 128  # Partition size for SPLASH processing\n",
    "        self.device = 'cpu'\n",
    "        #self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.alpha = 2  # Alpha parameter for LORA layers\n",
    "        self.quantization_bits = 8  # Bit size for quantization in QLORA\n",
    "        self.freq_threshold = 1000  # Frequency threshold for adaptive embeddings\n",
    "        self.large_embed_dim = 512  # Embedding dimension for frequent tokens\n",
    "        self.small_embed_dim = 128  # Embedding dimension for infrequent tokens\n",
    "        self.head_dim = self.embed_size // self.heads\n",
    "\n",
    "  # Language Model Transformer\n",
    "class LanguageExpert:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.language_model_transformer = self.LanguageModelTransformer(config, tokenizer=config.tokenizer)\n",
    "        self.splash = self.SPLASH(config)\n",
    "        self.dpo = self.DPO(config, self.language_model_transformer)\n",
    "\n",
    "    ###############################\n",
    "\n",
    "    # SPLASH\n",
    "    class SPLASH(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.vocab_size = config.vocab_size\n",
    "            self.embed_size = config.embed_size\n",
    "            self.heads = config.heads\n",
    "            self.sequence_length = config.sequence_length\n",
    "            self.projection_dim = config.projection_dim\n",
    "            self.partition_size = config.partition_size\n",
    "            self.head_dim = self.embed_size // self.heads\n",
    "\n",
    "            self.embedding = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "            self.values = nn.Linear(config.head_dim, config.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(config.head_dim, config.projection_dim, bias=False)\n",
    "            self.queries = nn.Linear(config.head_dim, config.projection_dim, bias=False)\n",
    "            self.value_projection = nn.Linear(config.head_dim, config.projection_dim//2)  # To project values to match dimensions\n",
    "            self.ponder = nn.Linear(config.partition_size, 1, bias=True)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            #self.final_projection = nn.Linear(config.heads * (config.projection_dim // 2), config.vocab_size)\n",
    "            self.final_projection = nn.Linear(config.heads * (config.projection_dim // 2), config.embed_size)\n",
    "\n",
    "        def random_projection(self, matrix, k):\n",
    "            \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "            random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "            return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "        def cur_decomposition(self, matrix, projection_dim):\n",
    "            \"\"\"Applies CUR decomposition with C matrix dimension aligned to projection dimension.\"\"\"\n",
    "            batch_size, seq_length, heads, dim = matrix.shape\n",
    "            k = min(projection_dim // 2, dim, seq_length)\n",
    "            C = torch.zeros(batch_size, seq_length, heads, k, device=matrix.device)\n",
    "            R = torch.zeros(batch_size, k, heads, dim, device=matrix.device)\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                for h in range(heads):\n",
    "                    # Using torch.randperm for random indices in PyTorch\n",
    "                    col_indices = torch.randperm(dim, device=matrix.device)[:k]\n",
    "                    row_indices = torch.randperm(seq_length, device=matrix.device)[:k]\n",
    "                    C[b, :, h] = matrix[b, :, h, col_indices]\n",
    "                    R[b, :, h] = matrix[b, row_indices, h]\n",
    "            return C, R\n",
    "\n",
    "        def forward(self, input_ids):\n",
    "            if input_ids.dim() != 2 or input_ids.dtype != torch.long:\n",
    "                raise ValueError(f\"input_ids must be a 2D tensor of long integers, got shape {input_ids.shape} and dtype {input_ids.dtype}\")\n",
    "            # Check for out-of-range token IDs right before embedding call\n",
    "            if input_ids.max() >= self.vocab_size:\n",
    "                print(f\"Error: Max token ID {input_ids.max().item()} exceeds vocab size {self.vocab_size}\")\n",
    "\n",
    "            print(f\"Before embedding: input_ids.shape={input_ids.shape}, max input_id={input_ids.max().item()}, vocab_size={self.vocab_size}\")\n",
    "\n",
    "            x = self.embedding(input_ids.long())            \n",
    "            print(f\"After embedding: {x.shape}\")\n",
    "            N, seq_length, _ = x.shape\n",
    "            x_reshaped = x.view(N, seq_length, self.heads, self.head_dim)\n",
    "            print(f\"x_reshaped shape: {x_reshaped.shape}\")\n",
    "\n",
    "            values = self.values(x_reshaped)\n",
    "            queries = self.random_projection(self.queries(x_reshaped), self.projection_dim // 2)\n",
    "            keys = self.random_projection(self.keys(x_reshaped), self.projection_dim // 2 )\n",
    "            print(\"values :\", values.shape)\n",
    "            print(\"queries :\", queries.shape)\n",
    "            print(\"keys :\", keys.shape)\n",
    "\n",
    "            attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "            print(f\"attention_scores after random projection: {attention_scores.shape}\")\n",
    "\n",
    "            for i in range(0, seq_length, self.partition_size):\n",
    "                print(f\"PARTITION START\")\n",
    "                partition_start = i\n",
    "                partition_end = min(i + self.partition_size, seq_length)\n",
    "                keys_part = keys[:, partition_start:partition_end, :, :]\n",
    "                queries_part = queries[:, partition_start:partition_end, :, :]\n",
    "\n",
    "                C_keys, R_queries = self.cur_decomposition(keys_part, self.projection_dim)\n",
    "                print(\"C_keys shape before return:\", C_keys.shape)\n",
    "\n",
    "                ponder_scores = torch.zeros(N, self.heads, partition_end - partition_start, 1, device=x.device)\n",
    "                print(f\"Partition Start {i}, Partition End {partition_end} , ponder_scores: {ponder_scores.shape}\")\n",
    "\n",
    "                for h in range(self.heads):\n",
    "                    #print(f\"HEADS START\")\n",
    "                    head_queries = queries_part[:, :, h, :]\n",
    "                    #print(f\"head_queries: {head_queries.shape}\")\n",
    "                    head_ponder_scores = self.sigmoid(self.ponder(head_queries))\n",
    "                    #print(f\"head_ponder_scores: {head_ponder_scores.shape}\")\n",
    "                    ponder_scores[:, h, :, 0] = head_ponder_scores.squeeze(-1)\n",
    "\n",
    "                # Correctly expand ponder_scores without adding an unnecessary dimension\n",
    "                print(\"BEFORE 1ST EINSUM:\")\n",
    "                ponder_scores_permuted = ponder_scores.permute(0, 2, 1, 3)  # Move to [2, 128, 8, 1]\n",
    "                print(\"ponder_scores_permuted shape:\", ponder_scores_permuted.shape) \n",
    "                ponder_scores_broadcastable = ponder_scores_permuted.expand(-1, -1, -1, 128)  # Expand to [2, 128, 8, 128]            \n",
    "                print(\"ponder_scores_broadcastable shape:\", ponder_scores_broadcastable.shape) \n",
    "                print(\"queries_part shape:\", queries_part.shape) \n",
    "                print(\"C_keys shape:\", C_keys.shape)\n",
    "                energy = torch.einsum('bnhd,bnhk->bnhd', queries_part, C_keys)\n",
    "                attention_weights = F.softmax(energy, dim=-1)\n",
    "                print(\"AFTER 1ST EINSUM:\")\n",
    "                print(\"energy shape:\", energy.shape) \n",
    "                print(\"attention_weights shape:\", attention_weights.shape)\n",
    "                attention = attention_weights * ponder_scores_broadcastable\n",
    "                print(\"attention shape:\", attention.shape)\n",
    "                attention_corrected = attention.permute(0, 2, 1, 3)\n",
    "                attention_scores[:, :, partition_start:partition_end, :] = attention_corrected\n",
    "\n",
    "            values = values.permute(0, 2, 1, 3)  # Swap heads and seq_length to bring heads next to head_dim\n",
    "            print(\"values shape:\", values.shape)\n",
    "            values = values.reshape(-1, self.head_dim)  # Flatten to [N*heads*seq_length, head_dim] for linear layer\n",
    "            print(\"values.reshape(-1, self.head_dim) shape:\", values.shape)\n",
    "            projected_values = self.value_projection(values)  # Now [N*heads*seq_length, projection_dim / 2]\n",
    "            print(\"self.value_projection(values) shape:\", projected_values.shape)\n",
    "            projected_values = projected_values.view(N, self.heads, seq_length, self.projection_dim // 2)\n",
    "            print(\"projected_values shape:\", projected_values.shape)\n",
    "\n",
    "            print(f\"2ND EINSUM\")\n",
    "            # Combine attention_scores and projected_values then pass through the final linear layer\n",
    "            out = torch.einsum('bnhp,bnhp->bnhp', attention_scores, projected_values)\n",
    "            print(\"out shape after einsum:\", out.shape)\n",
    "\n",
    "            # Correct reshaping: Flatten batch and sequence length dimensions, keep the last two dimensions for projection\n",
    "            out = out.reshape(-1, self.heads * (self.projection_dim // 2))\n",
    "            print(\"out reshaped for projection:\", out.shape)\n",
    "\n",
    "            # Ensure the final_projection layer matches the flattened shape expected after reshaping\n",
    "            # Assuming final_projection is defined as nn.Linear(self.heads * (self.projection_dim // 2), vocab_size)\n",
    "            out = self.final_projection(out)\n",
    "            print(f\"out after final_projection:\", out.shape)\n",
    "\n",
    "            # At this point, out should have a shape of [batch_size * sequence_length, vocab_size], ready for loss calculation\n",
    "            return out\n",
    "\n",
    "\n",
    "    # LORA\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, config, input_dim, output_dim):\n",
    "            super(LanguageExpert.LORALayer, self).__init__()\n",
    "            self.rank = config.rank\n",
    "            self.alpha = config.alpha\n",
    "\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, self.rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(self.rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # QLORA\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, config, input_dim, output_dim):\n",
    "            super(LanguageExpert.QLORALayer, self).__init__()\n",
    "            self.rank = config.rank\n",
    "            self.alpha = config.alpha\n",
    "            self.quantization_bits = config.quantization_bits\n",
    "\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, self.rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(self.rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(config.dropout)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "    \n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config  # Already stored\n",
    "            self.layers = nn.ModuleList([\n",
    "                LanguageExpert.TransformerBlock(config) for _ in range(config.num_layers)\n",
    "            ])\n",
    "            self.fc_out = nn.Linear(config.embed_size, config.vocab_size)\n",
    "            self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Ensure x is token IDs for SPLASH\n",
    "            x = x.to(dtype=torch.long)\n",
    "            # Note: No need to apply dropout here as x are token IDs\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)  # SPLASH is called within TransformerBlock\n",
    "\n",
    "            # Apply dropout after all transformer blocks and before the final projection layer\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            out = self.fc_out(x)  # Final projection from transformer block output to vocab size\n",
    "\n",
    "            return out\n",
    "\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.splash = LanguageExpert.SPLASH(config=config)\n",
    "            self.norm1 = nn.LayerNorm(config.embed_size)\n",
    "            self.norm2 = nn.LayerNorm(config.embed_size)\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                nn.Linear(config.embed_size, config.forward_expansion * config.embed_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(config.forward_expansion * config.embed_size, config.embed_size),\n",
    "            )\n",
    "            self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        def forward(self, input_ids):\n",
    "            # SPLASH already expects token IDs and handles embedding\n",
    "            attention_output = self.splash(input_ids)\n",
    "            attention_output = self.dropout(self.norm1(attention_output))\n",
    "            forward_output = self.feed_forward(attention_output)\n",
    "            output = self.dropout(self.norm2(forward_output + attention_output))\n",
    "            return output\n",
    "        \n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, config, tokenizer):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.tokenizer = tokenizer\n",
    "            self.splash = LanguageExpert.SPLASH(config)\n",
    "            self.decoder = LanguageExpert.LanguageModelDecoder(config)\n",
    "\n",
    "\n",
    "        def forward(self, input_ids=None, embeddings=None, attention_mask=None):\n",
    "            if input_ids is not None:\n",
    "                # Explicitly cast input_ids to long integers\n",
    "                input_ids = input_ids.long()\n",
    "                embeddings = self.splash(input_ids)\n",
    "            elif embeddings is None:\n",
    "                raise ValueError(\"Either input_ids or embeddings must be provided.\")\n",
    "            \n",
    "            # Continue processing with embeddings\n",
    "            # Now, embeddings are passed to the decoder or subsequent layers\n",
    "            out = self.decoder(embeddings)  # Adjust your decoder to accept embeddings\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len).to(trg.device)\n",
    "            return trg_mask\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = [self.tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            return response\n",
    "\n",
    "    \n",
    "    ##################################################\n",
    "    # Tokenizer\n",
    "\n",
    "    class TrieNode:\n",
    "        def __init__(self):\n",
    "            self.children = {}\n",
    "            self.token_id = None\n",
    "            self.frequency = 0\n",
    "            self.failure_link = None\n",
    "            self.is_end = False  # Add is_end attribute to mark the end of a word\n",
    "            self.token = None  # Add token attribute to store the token associated with the node\n",
    "\n",
    "\n",
    "    class Trie:\n",
    "        def __init__(self, unk_token_id=0):\n",
    "            self.root = LanguageExpert.TrieNode()\n",
    "            self.unk_token_id = unk_token_id\n",
    "\n",
    "        def insert(self, token, token_id, frequency):\n",
    "            node = self.root\n",
    "            for char in token:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = LanguageExpert.TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.token_id = token_id\n",
    "            node.frequency = frequency\n",
    "\n",
    "        def find_subwords(self, token):\n",
    "            \"\"\"Finds the most probable subwords based on frequency.\"\"\"\n",
    "            node = self.root\n",
    "            best_subwords = []\n",
    "\n",
    "            def dfs(current_node, subword='', collected_subwords=[]):\n",
    "                if current_node.token_id is not None:\n",
    "                    # Update to correctly calculate total_frequency based on the structure of collected_subwords\n",
    "                    total_frequency = sum(n.frequency for _, _, n in collected_subwords) + current_node.frequency\n",
    "                    probability = current_node.frequency / total_frequency if total_frequency else 0\n",
    "                    collected_subwords.append((subword, probability, current_node))\n",
    "\n",
    "                for char, next_node in current_node.children.items():\n",
    "                    dfs(next_node, subword + char, list(collected_subwords))  # Create a copy of the list to avoid shared state\n",
    "\n",
    "            dfs(node)\n",
    "            best_subwords = sorted(best_subwords, key=lambda x: x[1], reverse=True)\n",
    "            return [subword for subword, _, _ in best_subwords][:5] or [self.unk_token_id]\n",
    "\n",
    "\n",
    "        def compute_failure_links(self):\n",
    "            root = self.root\n",
    "            root.failure_link = root  # Root's failure link points to itself\n",
    "            queue = [root]\n",
    "\n",
    "            while queue:\n",
    "                current_node = queue.pop(0)\n",
    "\n",
    "                for char, child_node in current_node.children.items():\n",
    "                    queue.append(child_node)\n",
    "\n",
    "                    # Follow failure link to find the longest suffix for the child_node\n",
    "                    failure_candidate = current_node.failure_link\n",
    "                    while failure_candidate != root and char not in failure_candidate.children:\n",
    "                        failure_candidate = failure_candidate.failure_link\n",
    "                    child_node.failure_link = failure_candidate.children.get(char, root)\n",
    "\n",
    "\n",
    "    class SimpleSentencePiece:\n",
    "        def __init__(self, model_type=\"bpe\", vocab_size=30522):\n",
    "            self.vocab = {}\n",
    "            self.id_to_subword = {}\n",
    "            self.unk_token = \"[UNK]\"\n",
    "            self.unk_token_id = 0\n",
    "            self.vocab_size = vocab_size\n",
    "            self.model = None if model_type == \"bpe\" else None\n",
    "            self.model_type = model_type\n",
    "\n",
    "        def train(self, text):\n",
    "            if self.model_type == \"bpe\":\n",
    "                self.model = LanguageExpert.BPE(num_merges=self.vocab_size, unk_token_id=self.unk_token_id)\n",
    "                self.model.train(text)\n",
    "                self.vocab = self.model.vocab\n",
    "                self.id_to_subword = {i: word for word, i in self.vocab.items()}\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Model type {self.model_type} not supported yet.\")\n",
    "\n",
    "        def encode(self, text):\n",
    "            text = self.preprocess_text(text)  # Preprocess text before encoding\n",
    "            if not self.model:\n",
    "                raise ValueError(\"Model has not been trained yet.\")\n",
    "            encoded = self.model.encode(text)\n",
    "            #print(f\"Encoded: {encoded[:10]}\")  # Print first 10 encoded tokens\n",
    "            return encoded\n",
    "\n",
    "        def decode(self, ids):\n",
    "            if not self.id_to_subword:\n",
    "                raise ValueError(\"Vocabulary is empty. Ensure the model is trained first.\")\n",
    "            text = \" \".join([self.id_to_subword.get(id_, self.unk_token) for id_ in ids])\n",
    "            text = text.replace(\" </w>\", \"\").replace(\"</w>\", \" \").strip()\n",
    "            return text\n",
    "\n",
    "        def preprocess_text(self, text):\n",
    "            # Convert text to lowercase to ensure case insensitivity\n",
    "            text = text.lower()\n",
    "            # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "            text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "            # Replace multiple spaces with a single space\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            # Trim leading and trailing spaces\n",
    "            text = text.strip()\n",
    "            return text\n",
    "        \n",
    "        def save_model(self, filepath):\n",
    "            model_data = {\n",
    "                'vocab': self.vocab,\n",
    "                'id_to_subword': self.id_to_subword,\n",
    "                'model_type': self.model_type,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                # Potentially include other relevant attributes\n",
    "            }\n",
    "            # Save the high-level tokenizer settings\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(model_data, f)\n",
    "            \n",
    "            # Now save the BPE model specifically\n",
    "            if self.model_type == \"bpe\" and self.model:\n",
    "                self.model.save_model(filepath + \"_bpe\")\n",
    "\n",
    "        def load_model(self, filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                model_data = json.load(f)\n",
    "            \n",
    "            self.vocab = model_data['vocab']\n",
    "            self.id_to_subword = model_data['id_to_subword']\n",
    "            self.model_type = model_data['model_type']\n",
    "            self.vocab_size = model_data['vocab_size']\n",
    "            \n",
    "            # Assuming model_type is still \"bpe\", we now load the BPE model\n",
    "            if self.model_type == \"bpe\":\n",
    "                self.model = LanguageExpert.BPE(self.vocab_size, self.unk_token_id)\n",
    "                self.model.load_model(filepath + \"_bpe\")\n",
    "\n",
    "    class BPE:\n",
    "        def __init__(self, num_merges=100, unk_token_id=0):  # Accept unk_token_id parameter\n",
    "            self.vocab = {}\n",
    "            self.merges = []\n",
    "            self.num_merges = num_merges\n",
    "            self.unk_token_id = unk_token_id  # Store the unknown token ID\n",
    "\n",
    "        def train(self, text):\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "            vocab = collections.Counter(words)\n",
    "            vocab = {word + '</w>': count for word, count in vocab.items()}\n",
    "            \n",
    "            for _ in range(self.num_merges):  # Use the num_merges from the instance variable\n",
    "                pairs = self.get_stats(vocab)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                best = max(pairs, key=pairs.get)\n",
    "                vocab = self.merge_vocab(best, vocab)\n",
    "                self.merges.append(best)\n",
    "\n",
    "            self.vocab = {word: i for i, word in enumerate(vocab.keys())}\n",
    "\n",
    "        @staticmethod\n",
    "        def get_stats(vocab):\n",
    "            pairs = collections.defaultdict(int)\n",
    "            for word, freq in vocab.items():\n",
    "                symbols = word.split()\n",
    "                for i in range(len(symbols)-1):\n",
    "                    pairs[symbols[i], symbols[i+1]] += freq\n",
    "            return pairs\n",
    "\n",
    "        @staticmethod\n",
    "        def merge_vocab(pair, vocab):\n",
    "            v_out = {}\n",
    "            bigram = re.escape(' '.join(pair))\n",
    "            p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "            for word in vocab:\n",
    "                w_out = p.sub(''.join(pair), word)\n",
    "                v_out[w_out] = vocab[word]\n",
    "            return v_out\n",
    "\n",
    "        def encode(self, text):\n",
    "            \"\"\"Encode text into subwords using learned BPE merges.\"\"\"\n",
    "            encoded_tokens = []\n",
    "            for word in re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE):\n",
    "                word += '</w>'\n",
    "                subwords = [word]  # Start with the entire word as one subword\n",
    "                for merge in self.merges:\n",
    "                    new_subwords = []\n",
    "                    for subword in subwords:\n",
    "                        # If the merge is in subword, split it; otherwise, keep it as is\n",
    "                        if ' '.join(merge) in subword:\n",
    "                            new_subwords.extend(subword.replace(' '.join(merge), ''.join(merge)).split(' '))\n",
    "                        else:\n",
    "                            new_subwords.append(subword)\n",
    "                    subwords = new_subwords\n",
    "                encoded_tokens.extend(subwords)\n",
    "            return [self.vocab.get(token, self.unk_token_id) for token in encoded_tokens]\n",
    "        \n",
    "            # New method to save trained model\n",
    "        def save_model(self, filepath):\n",
    "            bpe_data = {\n",
    "                'merges': self.merges,\n",
    "                'vocab': self.vocab,\n",
    "                'num_merges': self.num_merges,\n",
    "                # Include other attributes as needed\n",
    "            }\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(bpe_data, f)\n",
    "\n",
    "        def load_model(self, filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                bpe_data = json.load(f)\n",
    "            \n",
    "            self.merges = bpe_data['merges']\n",
    "            self.vocab = bpe_data['vocab']\n",
    "            self.num_merges = bpe_data['num_merges']\n",
    "\n",
    "\n",
    "    class WordPiece:\n",
    "        def __init__(self, vocab, unk_token_id=0, unk_token=\"[UNK]\"):\n",
    "            self.vocab = vocab\n",
    "            self.unk_token_id = unk_token_id\n",
    "            self.unk_token = unk_token  # Define the unknown token\n",
    "            self.root = self.build_trie(vocab)\n",
    "            self.id_to_token = {id_: token for token, id_ in vocab.items()}  # Inverse mapping\n",
    "            self.compute_failure_links(self.root)\n",
    "            print(\"Trie built successfully.\")\n",
    "\n",
    "        def convert_ids_to_tokens(self, ids):\n",
    "            \"\"\"\n",
    "            Convert a list of token ids back to their string token representations.\n",
    "            \"\"\"\n",
    "            return [self.id_to_token.get(id_, self.unk_token) for id_ in ids]\n",
    "\n",
    "        # Add debug prints to build_trie to confirm structure\n",
    "        def build_trie(self, vocab):\n",
    "            root = LanguageExpert.TrieNode()\n",
    "            for token in vocab:\n",
    "                node = root\n",
    "                for char in token:\n",
    "                    if char not in node.children:\n",
    "                        node.children[char] = LanguageExpert.TrieNode()\n",
    "                    node = node.children[char]\n",
    "                node.is_end = True\n",
    "                node.token = token\n",
    "            print(\"Trie Construction Completed Successfully\")\n",
    "            return root\n",
    "\n",
    "\n",
    "        def compute_failure_links(self, root):\n",
    "            queue = [root]\n",
    "            while queue:\n",
    "                current_node = queue.pop(0)\n",
    "                for char, child_node in current_node.children.items():\n",
    "                    failure_node = current_node.failure_link\n",
    "                    while failure_node and char not in failure_node.children:\n",
    "                        failure_node = failure_node.failure_link\n",
    "                    child_node.failure_link = failure_node.children[char] if failure_node else root\n",
    "                    queue.append(child_node)\n",
    "\n",
    "        # Improved debug prints in tokenize method\n",
    "                    \n",
    "        def tokenize(self, text):\n",
    "            # Preprocess input text\n",
    "            text = self.preprocess_text(text)\n",
    "            node = self.root\n",
    "            token_ids = []  # Will store token IDs instead of tokens\n",
    "            i = 0\n",
    "\n",
    "            while i < len(text):\n",
    "                char = text[i]\n",
    "                if char == ' ':\n",
    "                    node = self.root\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                if char not in node.children:\n",
    "                    if node != self.root and node.token is not None:\n",
    "                        # Convert found token to its ID\n",
    "                        token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                        token_ids.append(token_id)\n",
    "                        node = self.root  # Reset to root\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Append unknown token ID\n",
    "                        token_ids.append(self.unk_token_id)\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                node = node.children[char]\n",
    "                if node.is_end:\n",
    "                    if i + 1 == len(text) or text[i + 1] == ' ':\n",
    "                        # Convert found token to its ID\n",
    "                        token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                        token_ids.append(token_id)\n",
    "                        node = self.root\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            #print(f\"Token IDs: {token_ids[:10]}\")\n",
    "            return token_ids\n",
    "\n",
    "        def preprocess_text(self, text):\n",
    "            # Convert text to lowercase to ensure case insensitivity\n",
    "            text = text.lower()\n",
    "\n",
    "            # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "            # This depends on how your vocabulary handles punctuation\n",
    "            text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "\n",
    "            # Replace multiple spaces with a single space\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "            # Trim leading and trailing spaces\n",
    "            text = text.strip()\n",
    "\n",
    "            return text\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "    # DPO\n",
    "    class DPO(nn.Module):\n",
    "        def __init__(self, config, language_model):\n",
    "            super(LanguageExpert.DPO, self).__init__()\n",
    "            self.language_model = language_model\n",
    "            self.device = config.device\n",
    "            self.projection = nn.Linear(config.vocab_size, config.embed_size)\n",
    "            self.classifier = nn.Linear(config.embed_size, 2)\n",
    "\n",
    "        def forward(self, input_ids_question, input_ids_chosen=None, input_ids_rejected=None, labels=None):\n",
    "            combined_input_ids = torch.cat((input_ids_question, input_ids_chosen, input_ids_rejected), dim=1)\n",
    "\n",
    "            # Assuming combined_input_ids has shape [batch_size, sequence_length]\n",
    "            logits = self.language_model(combined_input_ids)  # Output shape: [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "            # Project logits to embedding space before pooling\n",
    "            projected_logits = self.projection(logits)  # New shape: [batch_size, sequence_length, embed_size]\n",
    "            \n",
    "            # Apply global mean pooling across the sequence length dimension\n",
    "            pooled_logits = projected_logits.mean(dim=1)  # New shape: [batch_size, embed_size]\n",
    "\n",
    "            # Pass the pooled representation through the classifier\n",
    "            predictions = self.classifier(pooled_logits)  # New shape: [batch_size, 2]\n",
    "\n",
    "            # Calculate loss if labels are provided\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(predictions, labels)\n",
    "\n",
    "            return predictions, loss\n",
    "\n",
    "\n",
    "    ###############################\n",
    "    # RAG\n",
    "    \n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.PositionalEncoding, self).__init__()\n",
    "            self.d_model = config.embed_size\n",
    "            self.max_len = config.max_length\n",
    "            pe = torch.zeros(self.max_len, self.d_model)\n",
    "            position = torch.arange(0, self.max_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * -(math.log(10000.0) / self.d_model))\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "            pe = pe.unsqueeze(0)\n",
    "            self.register_buffer('pe', pe)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # x: Tensor of shape [Batch Size, Sequence Length, Embedding Dimension]\n",
    "            # Adjust positional encoding to match the input size and device\n",
    "            pe = self.pe[:, :x.size(1)]\n",
    "            # Assuming x is on the correct device, pe will be automatically aligned to the same device\n",
    "            return pe\n",
    "\n",
    "\n",
    "    class AdaptiveDropoutLayer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.AdaptiveDropoutLayer, self).__init__()\n",
    "            self.log_alpha = nn.Parameter(torch.tensor(math.log(config.dropout / (1 - config.dropout))).float())\n",
    "\n",
    "        def forward(self, x):\n",
    "            p = torch.sigmoid(self.log_alpha)\n",
    "            # Convert p from a tensor to a float\n",
    "            p_value = p.item()  # This extracts the scalar value as a Python float\n",
    "            return nn.functional.dropout(x, p=p_value, training=self.training)\n",
    "\n",
    "\n",
    "    class AdaptiveEmbeddingLayer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.AdaptiveEmbeddingLayer, self).__init__()\n",
    "            self.vocab = config.wordpiece_vocab  # Ensure this is part of your config\n",
    "            self.vocab_size = config.vocab_size\n",
    "            self.freq_threshold = config.freq_threshold\n",
    "            self.large_embed_dim = config.large_embed_dim\n",
    "            self.small_embed_dim = config.small_embed_dim\n",
    "            self.max_seq_len = config.max_length\n",
    "            self.split_vocab(self.vocab, self.freq_threshold)\n",
    "            self.frequent_embeddings = nn.Embedding(len(self.frequent_vocab), self.large_embed_dim)\n",
    "            self.infrequent_embeddings = nn.Embedding(len(self.infrequent_vocab), self.small_embed_dim)\n",
    "            self.infrequent_projection = nn.Linear(self.small_embed_dim, self.large_embed_dim)\n",
    "            self.positional_embeddings = LanguageExpert.PositionalEncoding(config)\n",
    "\n",
    "\n",
    "\n",
    "        def split_vocab(self, vocab, freq_threshold):\n",
    "            token_counts = [(token, count) for token, count in vocab.items()]\n",
    "            token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "            split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "            \n",
    "            self.frequent_vocab = dict(token_counts[:split_point])\n",
    "            self.infrequent_vocab = dict(token_counts[split_point:])\n",
    "\n",
    "        def forward(self, token_ids):\n",
    "            device = token_ids.device\n",
    "            seq_len = token_ids.size(1)\n",
    "            batch_size = token_ids.size(0)  # Obtain batch size from token_ids tensor\n",
    "\n",
    "            # Initialize embeddings tensor\n",
    "            embeddings = torch.zeros(token_ids.shape[0], seq_len, self.large_embed_dim, device=device)\n",
    "\n",
    "            # Map token_ids to indices for frequent and infrequent vocab\n",
    "            frequent_indices = torch.zeros_like(token_ids)\n",
    "            infrequent_indices = torch.zeros_like(token_ids)\n",
    "            \n",
    "            for token_id, index in self.vocab.items():\n",
    "                mask = token_ids == token_id\n",
    "                if token_id in self.frequent_vocab:\n",
    "                    # Map to index in frequent_vocab\n",
    "                    frequent_indices[mask] = self.frequent_vocab[token_id]\n",
    "                elif token_id in self.infrequent_vocab:\n",
    "                    # Map to index in infrequent_vocab\n",
    "                    infrequent_indices[mask] = self.infrequent_vocab[token_id]\n",
    "\n",
    "            # Create masks for frequent and infrequent tokens\n",
    "            frequent_mask = frequent_indices > 0\n",
    "            infrequent_mask = infrequent_indices > 0\n",
    "\n",
    "            # Embed frequent tokens\n",
    "            if frequent_mask.any():\n",
    "                frequent_embeddings = self.frequent_embeddings(frequent_indices[frequent_mask])\n",
    "                embeddings[frequent_mask] = frequent_embeddings\n",
    "\n",
    "            # Embed and project infrequent tokens\n",
    "            if infrequent_mask.any():\n",
    "                infrequent_embeddings = self.infrequent_embeddings(infrequent_indices[infrequent_mask])\n",
    "                infrequent_embeddings_projected = self.infrequent_projection(infrequent_embeddings)\n",
    "                embeddings[infrequent_mask] = infrequent_embeddings_projected\n",
    "\n",
    "            # Apply positional embeddings\n",
    "            position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            position_embeddings = self.positional_embeddings(position_ids)  # Generate for seq_len\n",
    "\n",
    "            # Ensure positional embeddings are broadcastable to the embeddings tensor\n",
    "            if position_embeddings.size(0) != batch_size:\n",
    "                position_embeddings = position_embeddings.expand(batch_size, -1, -1)\n",
    "\n",
    "            print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "            print(f\"Positional embeddings shape: {position_embeddings.shape}\")\n",
    "            embeddings += position_embeddings\n",
    "\n",
    "            return embeddings\n",
    "\n",
    "\n",
    "    class DPRContextEncoder(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.DPRContextEncoder, self).__init__()\n",
    "            self.wordpiece_tokenizer = config.wordpiece_tokenizer\n",
    "            self.embedding_layer = LanguageExpert.AdaptiveEmbeddingLayer(config)\n",
    "            self.attention_layer = LanguageExpert.SPLASH(config=config).to(config.device)\n",
    "            self.dropout = LanguageExpert.AdaptiveDropoutLayer(config)\n",
    " \n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            embeddings = self.embedding_layer(input_ids)\n",
    "            attention_output = self.attention_layer(embeddings, attention_mask=attention_mask)\n",
    "            attention_output = self.dropout(attention_output)\n",
    "\n",
    "            # Mean pooling across the sequence length dimension\n",
    "            pooled_output = attention_output.mean(dim=1)\n",
    "\n",
    "            return pooled_output\n",
    "\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.DPRQuestionEncoder, self).__init__()\n",
    "            self.wordpiece_tokenizer = config.wordpiece_tokenizer\n",
    "            self.embedding_layer = LanguageExpert.AdaptiveEmbeddingLayer(config)\n",
    "            self.attention_layer = LanguageExpert.SPLASH(config=config).to(config.device)\n",
    "            self.dropout = LanguageExpert.AdaptiveDropoutLayer(config)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            embeddings = self.embedding_layer(input_ids)\n",
    "            attention_output = self.attention_layer(embeddings, attention_mask=attention_mask)\n",
    "            attention_output = self.dropout(attention_output)\n",
    "\n",
    "            # Mean pooling across the sequence length dimension\n",
    "            pooled_output = attention_output.mean(dim=1)\n",
    "\n",
    "            return pooled_output\n",
    "\n",
    "\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.TransformerRAG, self).__init__()\n",
    "            self.config = config\n",
    "            self.context_encoder = LanguageExpert.DPRContextEncoder(config).to(config.device)\n",
    "            self.language_model = LanguageExpert.LanguageModelTransformer(config, config.wordpiece_tokenizer).to(config.device)\n",
    "            self.question_encoder = LanguageExpert.DPRQuestionEncoder(config).to(config.device)\n",
    "            self.tokenizer = config.wordpiece_tokenizer\n",
    "\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= config.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "            \n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "                \n",
    "                aggregated_context_embedding = torch.zeros(config.embedding_dim, device=config.device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = torch.tensor(context['input_ids']).unsqueeze(0).to(config.device)  # Add unsqueeze(0) for batch dimension\n",
    "                    context_attention_mask = torch.tensor(context['attention_mask']).unsqueeze(0).to(config.device)  # Add unsqueeze(0) for batch dimension\n",
    "                    print(f\"context_input_ids shape: {context_input_ids.shape}\")\n",
    "                    print(f\"context_attention_mask shape: {context_attention_mask.shape}\")\n",
    "\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "                \n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "            \n",
    "            question_input_ids = question_input_ids.to(config.device).long()\n",
    "            question_attention_mask = question_attention_mask.to(config.device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "            \n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            #similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            similarities = [cos_sim(question_embeddings.unsqueeze(0), context_emb.unsqueeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=config.device))\n",
    "            \n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(config.device) for k, v in tokenized_combined_input.items()}\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            #response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            response = \" \".join(predicted_tokens).replace(\" </w>\", \"\").replace(\"</w>\", \" \").strip()\n",
    "            \n",
    "            return response\n",
    "\n",
    "        @staticmethod\n",
    "        def extract_text_from_pdf(file_path):\n",
    "            text = \"\"\n",
    "            with fitz.open(file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            return text\n",
    "\n",
    "        @staticmethod\n",
    "        def split_into_chunks(text, chunk_size):\n",
    "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        @staticmethod\n",
    "        def preprocess_text(text, tokenizer, max_length=512):\n",
    "            chunk_size = max_length - 50\n",
    "            text_chunks = LanguageExpert.TransformerRAG.split_into_chunks(text, chunk_size)\n",
    "            processed_chunks = []\n",
    "            for chunk in text_chunks:\n",
    "                # Tokenize the chunk using the WordPiece tokenizer\n",
    "                token_ids = tokenizer.tokenize(chunk)\n",
    "                \n",
    "                # Manual padding and attention mask creation\n",
    "                attention_mask = [1] * len(token_ids)\n",
    "                # Padding: Extend token_ids and attention_mask to max_length\n",
    "                while len(token_ids) < max_length:\n",
    "                    token_ids.append(tokenizer.unk_token_id)  # Use unk_token_id for padding\n",
    "                    attention_mask.append(0)  # Padding token has attention mask 0\n",
    "                \n",
    "                # Ensure token_ids and attention_mask are not longer than max_length\n",
    "                token_ids = token_ids[:max_length]\n",
    "                attention_mask = attention_mask[:max_length]\n",
    "                \n",
    "                processed_chunk = {\n",
    "                    'input_ids': token_ids,\n",
    "                    'attention_mask': attention_mask\n",
    "                }\n",
    "                processed_chunks.append(processed_chunk)\n",
    "            return processed_chunks\n",
    "\n",
    "        @staticmethod\n",
    "        def create_dataset_from_pdfs(pdf_file_paths, tokenizer):\n",
    "            dataset = []\n",
    "            for file_path in pdf_file_paths:\n",
    "                text = LanguageExpert.TransformerRAG.extract_text_from_pdf(file_path)\n",
    "                processed_text = LanguageExpert.TransformerRAG.preprocess_text(text, tokenizer)                \n",
    "                dataset.append(processed_text)\n",
    "            return dataset\n",
    "\n",
    "        def retrieve_contexts(self, dataset, query_embedding, top_k=5):\n",
    "            similarity_scores = []\n",
    "            for context in dataset:\n",
    "                context_input_ids = context['input_ids'].to(self.config.device)\n",
    "                context_attention_mask = context['attention_mask'].to(self.config.device)\n",
    "                # Use the class's context_encoder\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "                similarity_scores.append(similarity.squeeze().item())\n",
    "            top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "            top_contexts = [dataset[i] for i in top_k_indices]\n",
    "            return top_contexts\n",
    "\n",
    "        def rag_retrieve_and_generate(self, dataset, query):\n",
    "            # Tokenize the query using the class's tokenizer\n",
    "            tokenized_query = self.tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
    "            input_ids = tokenized_query['input_ids']\n",
    "            attention_mask = tokenized_query['attention_mask']\n",
    "            # Use the class's question_encoder\n",
    "            encoded_query = self.question_encoder(input_ids, attention_mask)\n",
    "            relevant_contexts = self.retrieve_contexts(dataset, encoded_query)\n",
    "            # Assuming generate_response is a method of LanguageModelTransformer that accepts tokenized contexts and generates a response\n",
    "            response = self.language_model.generate_response(relevant_contexts)\n",
    "            return response\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "        \"\"\"\n",
    "        Calculate a new alpha value based on the current loss.\n",
    "        \"\"\"\n",
    "        if current_loss >= initial_loss:\n",
    "            return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "        loss_ratio = current_loss / initial_loss\n",
    "        alpha_range = initial_alpha - final_alpha\n",
    "        new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "        return new_alpha  \n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Linear warmup with cosine decay\n",
    "        scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "        return optimizer, scheduler\n",
    "        \n",
    "    # DPO Training\n",
    "    def train_dpo(self, train_loader, optimizer, config, save_path):\n",
    "            self.train()  # Set the model to training mode\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                input_ids_question = batch['input_ids_question'].to(config.device)\n",
    "                input_ids_chosen = batch['input_ids_chosen'].to(config.device)\n",
    "                input_ids_rejected = batch['input_ids_rejected'].to(config.device)\n",
    "                labels = batch['labels'].to(config.device)\n",
    "                print(f\"train_dpo input_ids_question: {input_ids_question.shape}\")\n",
    "                print(f\"train_dpo input_ids_chosen: {input_ids_chosen.shape}\")\n",
    "                print(f\"train_dpo input_ids_rejected: {input_ids_rejected.shape}\")\n",
    "                print(f\"train_dpo labels: {labels.shape}\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                logit, loss = self.transformer_dpo(input_ids_question, input_ids_chosen, input_ids_rejected, labels)\n",
    "                print(f\"Logits shape: {logit.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            print(f\"Training complete. Average Loss: {average_loss}\")\n",
    "            \n",
    "            # Save the model\n",
    "            torch.save(self.transformer_dpo.state_dict(), save_path)\n",
    "\n",
    "            return average_loss\n",
    "\n",
    "    # DPR Training\n",
    "    def train_dpr_encoders(self, train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs , context_save_path, question_save_path):\n",
    "        loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(train_data[\"queries\"])):\n",
    "                query = train_data[\"queries\"][i]\n",
    "                context = train_data[\"contexts\"][i]\n",
    "\n",
    "                # Ensure query is a string\n",
    "                if not isinstance(query, str):\n",
    "                    raise ValueError(\"Query must be a string.\")\n",
    "                tokenized_query = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                # Ensure context is a string\n",
    "                if isinstance(context, dict):\n",
    "                    # If context is a dictionary, extract the text content. This is a placeholder and might need adjustment.\n",
    "                    context_text = context.get(\"text\", \"\")\n",
    "                elif isinstance(context, str):\n",
    "                    context_text = context\n",
    "                else:\n",
    "                    raise ValueError(\"Context must be a string or a dictionary containing a text field.\")\n",
    "                tokenized_context = tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                question_embeddings = question_encoder(input_ids=tokenized_query['input_ids'], attention_mask=tokenized_query['attention_mask'])\n",
    "                context_embeddings = context_encoder(input_ids=tokenized_context['input_ids'], attention_mask=tokenized_context['attention_mask'])\n",
    "\n",
    "                # The labels tensor should have the same first dimension size as the input tensors\n",
    "                labels = torch.tensor([1.0] * question_embeddings.size(0), dtype=torch.float).to(question_embeddings.device)\n",
    "\n",
    "                loss = loss_function(question_embeddings, context_embeddings, labels)\n",
    "\n",
    "                optimizer_context.zero_grad()\n",
    "                optimizer_question.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_context.step()\n",
    "                optimizer_question.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_data['queries'])}\")\n",
    "        average_loss = total_loss / len(train_data['queries'])\n",
    "        torch.save(context_encoder.state_dict(), context_save_path)\n",
    "        torch.save(question_encoder.state_dict(), question_save_path)\n",
    "        return (context_encoder, question_encoder), average_loss\n",
    "       \n",
    "    # LMT Training\n",
    "    # Define the function to check token IDs\n",
    "    @staticmethod\n",
    "    def check_token_ids_for_embedding(token_ids, vocab_size):\n",
    "        if token_ids.max() >= vocab_size:\n",
    "            print(f\"Out-of-range token ID found: {token_ids.max()}. Max allowed: {vocab_size - 1}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"All token IDs are within the expected range.\")\n",
    "            return True\n",
    "\n",
    "    def train_language_model_transformer(self, train_loader, device, vocab_size, save_path):\n",
    "        # Reference the correct attribute for the model\n",
    "        model = self.language_model_transformer.to(device)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "        num_epochs = 5\n",
    "        \n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            # Assuming you have a method or way to toggle QLORA within your model, if applicable\n",
    "            # model.toggle_qlora(True)\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs, targets = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "\n",
    "                # Check if any token IDs are out-of-range before proceeding\n",
    "                if not LanguageExpert.check_token_ids_for_embedding(inputs, vocab_size):\n",
    "                    print(\"Halting training due to out-of-range token IDs\")\n",
    "                    return None, None  # You might want to handle this situation more gracefully\n",
    "                print(f\"Input device: {inputs.device}, Input shape: {inputs.shape} , Input Type: {inputs.type}\")\n",
    "                print(\"Model device:\", next(self.language_model_transformer.parameters()).device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss, if your model uses QLORA or a similar mechanism\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        return model, average_loss\n",
    "\n",
    "\n",
    "##################################################################\n",
    "    \n",
    "# Test Language Expert\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = [line.strip() for line in file.readlines()]\n",
    "    return texts\n",
    "\n",
    "texts = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "\n",
    "num_merges = 100\n",
    "def adapt_vocab_for_wordpiece(ssp_vocab):\n",
    "    adapted_vocab = {}\n",
    "    for token, id_or_freq in ssp_vocab.items():\n",
    "        if not token.startswith(\" \") and not token.endswith(\"</w>\"):\n",
    "            adapted_token = \"##\" + token.replace(\"</w>\", \"\")\n",
    "        else:\n",
    "            adapted_token = token.replace(\"</w>\", \"\")\n",
    "        adapted_vocab[adapted_token] = id_or_freq\n",
    "    return adapted_vocab\n",
    "\n",
    "# Initialize and train the SimpleSentencePiece model with BPE\n",
    "ssp = LanguageExpert.SimpleSentencePiece(model_type=\"bpe\", vocab_size=30522)\n",
    "# Assume `texts` is a list of text to train the tokenizer\n",
    "ssp.train('\\n'.join(texts))\n",
    "wordpiece_vocab = adapt_vocab_for_wordpiece(ssp.vocab)\n",
    "# Debugging step to ensure vocabulary completeness\n",
    "def debug_vocab(adapted_vocab):\n",
    "    print(\"Sample Vocabulary Check:\")\n",
    "    # Iterate over the first 10 key-value pairs in the adapted vocabulary\n",
    "    for i, (token, id_or_freq) in enumerate(adapted_vocab.items()):\n",
    "        print(f\"{token}: {id_or_freq}\")\n",
    "        if i >= 9:  # Stop after printing 10 entries\n",
    "            break\n",
    "    # Specifically check for subtokens if your tokenizer expects them\n",
    "    subtokens = [token for token in adapted_vocab.keys() if token.startswith(\"##\")]\n",
    "    print(f\"Found {len(subtokens)} subtokens in vocabulary.\")\n",
    "\n",
    "# Ensure wordpiece_vocab is a list of vocabulary tokens\n",
    "# debug_vocab(wordpiece_vocab)  # Call this after initializing wordpiece_vocab\n",
    "\n",
    "# Initialize WordPiece with the adapted vocabulary\n",
    "wordpiece_tokenizer = LanguageExpert.WordPiece(wordpiece_vocab, unk_token_id=0, unk_token=\"[UNK]\")\n",
    "\n",
    "\n",
    "class WikiTextDatasetForLM(Dataset):\n",
    "    def __init__(self, texts, tokenizer, sequence_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sequence_length = sequence_length\n",
    "        self.inputs, self.labels = self.process_texts(texts)\n",
    "        \n",
    "    def process_texts(self, texts):\n",
    "        inputs, labels = [], []\n",
    "        step_size = 256  # Example step size for overlapping sequences\n",
    "        for text in texts:\n",
    "            token_ids = self.tokenizer.tokenize(text)\n",
    "            for i in range(0, len(token_ids) - self.sequence_length, step_size):  # Adjust step_size for overlap\n",
    "                inputs.append(token_ids[i:i+self.sequence_length])\n",
    "                labels.append(token_ids[i+1:i+self.sequence_length+1])\n",
    "        return torch.tensor(inputs, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "# Initialize configuration\n",
    "config = ModelConfig()\n",
    "language_expert = LanguageExpert(config)\n",
    "####################################################################################\n",
    "# LMT Training\n",
    "# Load the wikitext-2 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\n",
    "\n",
    "def generate_attention_mask(token_ids):\n",
    "    \"\"\"Generate an attention mask for the given token IDs.\"\"\"\n",
    "    return [1 if token_id != 0 else 0 for token_id in token_ids]\n",
    "\n",
    "def tokenize_and_prepare_labels(examples):\n",
    "    token_ids = [wordpiece_tokenizer.tokenize(text) for text in examples[\"text\"]]\n",
    "    labels = [[wordpiece_tokenizer.unk_token_id] + ids[:-1] for ids in token_ids]  # Shift labels\n",
    "    return {\"input_ids\": token_ids, \"labels\": labels}\n",
    "\n",
    "# Load dataset and apply tokenization\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\n",
    "tokenized_datasets = dataset.map(tokenize_and_prepare_labels, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch_input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    batch_labels = [torch.tensor(item['labels']) for item in batch]\n",
    "    input_ids_padded = pad_sequence(batch_input_ids, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(batch_labels, batch_first=True, padding_value=-100)\n",
    "    attention_masks_padded = (input_ids_padded != 0).long()\n",
    "    return {'input_ids': input_ids_padded, 'attention_mask': attention_masks_padded, 'labels': labels_padded}\n",
    "\n",
    "# Initialize DataLoader with the custom collate function\n",
    "train_loader = DataLoader(tokenized_datasets, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "# Define save path for the trained model\n",
    "save_path = 'D:/EXPERT_WEIGHTS/lmt_expert_trained_custom_tokenizer.pth'\n",
    "\n",
    "# Train the LMT sub-model within the Expert system\n",
    "trained_model, average_loss = language_expert.train_language_model_transformer(\n",
    "    train_loader=train_loader, \n",
    "    device=config.device, \n",
    "    vocab_size=config.vocab_size, \n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "print(f\"Training complete. Model saved to {save_path}. Average Loss: {average_loss}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary: {'import</w>': 8, 'numpy</w>': 1, 'as</w>': 107, 'np</w>': 3, 'torch</w>': 19, '.</w>': 1079, 'nn</w>': 14, 'functional</w>': 2, 'F</w>': 2, 'from</w>': 89, 'utils</w>': 1, 'data</w>': 33, 'DataLoader</w>': 3, ',</w>': 910, 'Dataset</w>': 2, 'cuda</w>': 3, 'amp</w>': 1, 'GradScaler</w>': 2, 'autocast</w>': 2, 'optim</w>': 1, 'Adam</w>': 4, 'def</w>': 9, 'random_projection</w>': 3, '(</w>': 414, 'matrix</w>': 12, 'k</w>': 20, ')</w>': 416, ':</w>': 170, '\"</w>': 97, 'Random</w>': 4, 'projection</w>': 4, 'to</w>': 333, 'reduce</w>': 2, 'dimensionality</w>': 1, 'of</w>': 345, 'dimensions</w>': 5, 'random_matrix</w>': 2, '=</w>': 149, 'randn</w>': 2, 'size</w>': 15, '-</w>': 434, '1</w>': 124, 'device</w>': 18, 'return</w>': 7, 'matmul</w>': 1, 'cur_decomposition</w>': 2, 'projection_dim</w>': 21, '#</w>': 59, 'Change</w>': 2, \"'</w>\": 16, 'argument</w>': 1, 'Applies</w>': 1, 'CUR</w>': 1, 'decomposition</w>': 1, 'with</w>': 87, 'C</w>': 8, 'dimension</w>': 5, 'aligned</w>': 1, 'batch_size</w>': 9, 'seq_length</w>': 14, 'heads</w>': 24, 'dim</w>': 5, 'shape</w>': 45, 'min</w>': 2, '/</w>': 69, '2</w>': 89, 'Use</w>': 1, 'determine</w>': 3, 'zeros</w>': 4, 'R</w>': 5, 'for</w>': 135, 'b</w>': 7, 'in</w>': 173, 'range</w>': 5, 'h</w>': 10, 'col_indices</w>': 2, 'random</w>': 35, 'choice</w>': 6, 'replace</w>': 4, 'False</w>': 5, 'row_indices</w>': 2, '[</w>': 112, ']</w>': 111, 'class</w>': 62, 'PartitionedLinformerAttentionACT</w>': 3, 'Module</w>': 2, '__init__</w>': 5, 'self</w>': 58, 'embed_size</w>': 10, 'sequence_length</w>': 9, 'partition_size</w>': 9, 'super</w>': 2, 'head_dim</w>': 11, 'values</w>': 16, 'Linear</w>': 8, 'bias</w>': 7, 'keys</w>': 6, 'queries</w>': 7, 'value_projection</w>': 3, 'To</w>': 8, 'project</w>': 2, 'match</w>': 11, 'ponder</w>': 2, 'True</w>': 5, 'sigmoid</w>': 2, 'Sigmoid</w>': 1, 'forward</w>': 3, 'x</w>': 26, 'print</w>': 34, 'f</w>': 13, 'BEGIN</w>': 1, 'FORWARD</w>': 1, 'input</w>': 43, '{</w>': 24, '}</w>': 24, 'N</w>': 26, '_</w>': 4, 'x_reshaped</w>': 6, 'view</w>': 3, 'attention_scores</w>': 6, 'after</w>': 8, 'i</w>': 16, '0</w>': 67, 'PARTITION</w>': 1, 'START</w>': 2, 'partition_start</w>': 5, 'partition_end</w>': 6, '+</w>': 43, 'keys_part</w>': 2, 'queries_part</w>': 5, 'C_keys</w>': 6, 'R_queries</w>': 1, 'before</w>': 2, 'ponder_scores</w>': 6, 'Partition</w>': 2, 'Start</w>': 1, 'End</w>': 1, 'HEADS</w>': 1, 'head_queries</w>': 4, 'head_ponder_scores</w>': 4, 'squeeze</w>': 2, 'Correctly</w>': 1, 'expand</w>': 2, 'without</w>': 12, 'adding</w>': 1, 'an</w>': 54, 'unnecessary</w>': 1, 'BEFORE</w>': 1, '1ST</w>': 2, 'EINSUM</w>': 3, 'ponder_scores_permuted</w>': 4, 'permute</w>': 4, '3</w>': 70, 'Move</w>': 3, '128</w>': 6, '8</w>': 18, 'ponder_scores_broadcastable</w>': 6, 'Expand</w>': 1, 'energy</w>': 4, 'einsum</w>': 2, 'bnhd</w>': 2, 'bnhk</w>': 1, '></w>': 20, 'attention_weights</w>': 4, 'softmax</w>': 8, 'AFTER</w>': 1, 'attention</w>': 8, '*</w>': 8, 'attention_corrected</w>': 2, 'Swap</w>': 1, 'and</w>': 329, 'bring</w>': 1, 'next</w>': 3, 'reshape</w>': 2, 'Flatten</w>': 2, 'linear</w>': 21, 'layer</w>': 63, 'projected_values</w>': 8, 'Now</w>': 1, '2ND</w>': 1, 'Combine</w>': 1, 'then</w>': 20, 'pass</w>': 6, 'through</w>': 6, 'the</w>': 716, 'final</w>': 5, 'out</w>': 20, 'bnhp</w>': 2, 'bnh</w>': 1, 'Example</w>': 5, 'usage</w>': 4, 'model</w>': 55, '512</w>': 4, '1024</w>': 5, '256</w>': 2, 'input_tensor</w>': 4, 'rand</w>': 1, 'output</w>': 13, 'if</w>': 10, 'is_available</w>': 1, 'else</w>': 2, 'cpu</w>': 1, 'Assuming</w>': 2, 'tensor</w>': 1, 'is</w>': 222, 'Size</w>': 2, 'each</w>': 57, 'word</w>': 20, 'embedding</w>': 32, 'Number</w>': 1, 'Length</w>': 1, 'sequence</w>': 19, 'Dimension</w>': 1, 'embeddings</w>': 3, 'partitions</w>': 1, 'processing</w>': 12, 'num_classes</w>': 4, '10</w>': 12, 'Instantiate</w>': 1, 'attention_model</w>': 8, 'AttentionClassifier</w>': 2, 'global_pool</w>': 2, 'AdaptiveAvgPool1d</w>': 1, 'Global</w>': 2, 'pooling</w>': 5, 's</w>': 13, 'here</w>': 10, ';</w>': 35, 'adjust</w>': 3, 'necessary</w>': 1, 'classifier</w>': 14, 'Classifier</w>': 1, 'Ensure</w>': 1, 'has</w>': 30, 'num_heads</w>': 3, 'You</w>': 1, 'might</w>': 3, 'need</w>': 6, 'this</w>': 57, 'depending</w>': 5, 'on</w>': 126, 'your</w>': 4, 'exact</w>': 4, 'resulting</w>': 2, 'Pass</w>': 1, 'appropriate</w>': 1, 'Create</w>': 1, 'a</w>': 278, 'pinned</w>': 1, 'memory</w>': 1, 'MyDataset</w>': 2, 'Initialize</w>': 2, '__len__</w>': 1, 'Return</w>': 1, 'dataset</w>': 64, '100</w>': 4, '__getitem__</w>': 1, 'idx</w>': 1, 'Pad</w>': 1, 'length</w>': 13, 'ensure</w>': 1, 'This</w>': 27, 'simplified</w>': 1, 'example</w>': 25, 'according</w>': 2, 'actual</w>': 3, 'Padded</w>': 1, 'modeled</w>': 1, 'expected</w>': 7, 'target</w>': 34, 'randint</w>': 1, 'item</w>': 1, 'Generate</w>': 1, 'index</w>': 2, '9</w>': 8, 'classes</w>': 50, 'loader</w>': 2, '32</w>': 1, 'shuffle</w>': 1, 'pin_memory</w>': 1, 'optimizer</w>': 4, 'parameters</w>': 10, 'lr</w>': 1, '1e</w>': 1, 'scaler</w>': 4, 'epoch</w>': 2, 'epochs</w>': 2, 'inputs</w>': 5, 'targets</w>': 6, 'correct</w>': 1, 'non_blocking</w>': 2, 'zero_grad</w>': 1, 'Forward</w>': 1, 'predictions</w>': 4, 'Calculate</w>': 1, 'loss</w>': 15, 'assuming</w>': 1, 'classification</w>': 25, 'task</w>': 33, 'are</w>': 110, 'indices</w>': 2, 'cross_entropy</w>': 1, 'Backward</w>': 1, 'step</w>': 7, 'using</w>': 28, 'gradient</w>': 4, 'scaling</w>': 1, 'mixed</w>': 1, 'precision</w>': 1, 'scale</w>': 1, 'backward</w>': 2, 'update</w>': 1, 'Epoch</w>': 1, 'Loss</w>': 1, 'Fast</w>': 1, 'WordPiece</w>': 16, 'Tokenization</w>': 6, 'Xinying</w>': 1, 'Song</w>': 2, '</w>': 6, 'Alex</w>': 1, 'Salcianu</w>': 1, 'Yang</w>': 3, '</w>': 3, '</w>': 8, 'Dave</w>': 1, 'Dopson</w>': 1, 'Denny</w>': 1, 'Zhou</w>': 1, 'Google</w>': 10, 'Research</w>': 3, 'Mountain</w>': 1, 'View</w>': 1, 'CA</w>': 1, 'xysong</w>': 1, 'salcianu</w>': 1, 'ddopson</w>': 1, 'dennyzhou</w>': 1, '@</w>': 9, 'google</w>': 5, 'com</w>': 8, 'Kuaishou</w>': 1, 'Technology</w>': 1, 'Beijing</w>': 1, 'China</w>': 1, 'yangsong</w>': 1, 'kuaishou</w>': 1, 'Abstract</w>': 4, 'fundamental</w>': 2, 'preprocessing</w>': 12, 'almost</w>': 8, 'all</w>': 17, 'NLP</w>': 11, 'tasks</w>': 20, 'In</w>': 36, 'paper</w>': 15, 'we</w>': 94, 'propose</w>': 9, 'efficient</w>': 8, 'algorithms</w>': 12, 'tokenization</w>': 32, 'used</w>': 23, 'BERT</w>': 6, 'singleword</w>': 1, 'general</w>': 23, 'text</w>': 37, 'e</w>': 22, 'g</w>': 16, 'sentence</w>': 4, 'When</w>': 6, 'tokenizing</w>': 1, 'single</w>': 9, 'uses</w>': 7, 'longest</w>': 11, 'matchfirst</w>': 1, 'strategy</w>': 2, 'known</w>': 5, 'maximum</w>': 6, 'matching</w>': 23, 'The</w>': 85, 'best</w>': 3, 'so</w>': 15, 'far</w>': 4, '</w>': 17, '</w>': 12, 'where</w>': 26, 'or</w>': 55, '</w>': 6, '</w>': 5, 'vocabulary</w>': 33, 'token</w>': 12, 'We</w>': 78, 'novel</w>': 4, 'algorithm</w>': 17, 'whose</w>': 4, 'complexity</w>': 11, 'strictly</w>': 4, 'Our</w>': 7, 'method</w>': 13, 'inspired</w>': 2, 'by</w>': 67, 'Aho</w>': 8, 'Corasick</w>': 8, 'introduce</w>': 2, 'additional</w>': 2, 'linkages</w>': 1, 'top</w>': 14, 'trie</w>': 15, 'built</w>': 3, 'allowing</w>': 3, 'smart</w>': 2, 'transitions</w>': 2, 'when</w>': 26, 'cannot</w>': 4, 'continue</w>': 3, 'For</w>': 21, 'further</w>': 7, 'that</w>': 133, 'combines</w>': 2, 'pre</w>': 14, 'splitting</w>': 6, 'into</w>': 36, 'words</w>': 6, 'our</w>': 35, 'time</w>': 17, 'Experimental</w>': 3, 'results</w>': 30, 'show</w>': 15, '2x</w>': 2, 'faster</w>': 6, 'than</w>': 34, 'HuggingFace</w>': 3, 'Tokenizers</w>': 2, '5</w>': 59, '1x</w>': 2, 'TensorFlow</w>': 3, 'Text</w>': 4, 'average</w>': 6, 'Introduction</w>': 4, 'process</w>': 4, 'smaller</w>': 7, 'units</w>': 5, 'called</w>': 5, 'tokens</w>': 12, 'It</w>': 12, 'applications</w>': 4, 'sentiment</w>': 1, 'analysis</w>': 5, 'question</w>': 2, 'answering</w>': 1, 'machine</w>': 5, 'translation</w>': 5, 'information</w>': 5, 'retrieval</w>': 1, 'etc</w>': 6, 'Modern</w>': 2, 'models</w>': 16, 'like</w>': 6, 'Devlin</w>': 3, 'et</w>': 53, 'al</w>': 53, '2019</w>': 5, 'GPT</w>': 2, 'Brown</w>': 2, '2020</w>': 5, 'XLNet</w>': 2, 'tokenize</w>': 3, 'subword</w>': 31, 'Schuster</w>': 3, 'Nakajima</w>': 3, '2012</w>': 9, 'Sennrich</w>': 5, '2016</w>': 8, 'Kudo</w>': 7, '2018</w>': 10, 'As</w>': 7, 'midpoint</w>': 1, 'between</w>': 16, 'characters</w>': 12, 'retain</w>': 1, 'linguistic</w>': 1, 'meaning</w>': 4, 'morphemes</w>': 1, 'while</w>': 8, 'alleviating</w>': 1, 'situations</w>': 1, 'even</w>': 16, 'relatively</w>': 4, 'small</w>': 9, 'conducted</w>': 3, 'working</w>': 1, 'at</w>': 33, 'Given</w>': 3, 'Unicode</w>': 9, 'already</w>': 1, 'been</w>': 22, 'cleaned</w>': 1, 'up</w>': 4, 'normalized</w>': 7, 'two</w>': 32, 'steps</w>': 2, 'punctuation</w>': 1, 'whitespaces</w>': 5, 'wordpieces</w>': 4, 'greedy</w>': 1, 'first</w>': 37, 'iteratively</w>': 2, 'pick</w>': 2, 'prefix</w>': 5, 'remaining</w>': 4, 'matches</w>': 5, 'well</w>': 19, 'Maximum</w>': 2, 'Matching</w>': 8, 'MaxMatch</w>': 13, 'Palmer</w>': 2, '2000</w>': 3, 'which</w>': 50, 'also</w>': 26, 'Chinese</w>': 4, 'segmentation</w>': 16, 'since</w>': 7, '1980s</w>': 2, 'Liu</w>': 2, 'Liang</w>': 2, '1986</w>': 2, 'Despite</w>': 1, 'its</w>': 12, 'wide</w>': 2, 'use</w>': 26, 'decades</w>': 2, 'knowledge</w>': 4, 'most</w>': 5, 'see</w>': 2, 'Section</w>': 19, '</w>': 28, 'worth</w>': 3, 'noting</w>': 1, 'latter</w>': 3, 'vocabularyspecific</w>': 1, 'multiplicative</w>': 3, 'factor</w>': 2, 'can</w>': 52, 'be</w>': 69, 'large</w>': 13, 'contains</w>': 6, 'long</w>': 2, 'LinMaxMatch</w>': 3, 'any</w>': 12, 'specific</w>': 19, 'factors</w>': 1, 'Inspired</w>': 1, '1975</w>': 2, 'organize</w>': 1, 'Fredkin</w>': 1, '1960</w>': 1, 'precomputed</w>': 2, 'failure</w>': 9, 'links</w>': 4, 'pops</w>': 4, 'During</w>': 1, 'character</w>': 11, 'does</w>': 9, 'not</w>': 44, 'edge</w>': 6, 'perform</w>': 20, 'avoid</w>': 2, 'backtracking</w>': 1, 'earlier</w>': 1, 'involves</w>': 3, 'collecting</w>': 1, 'recognized</w>': 2, 'moving</w>': 2, 'node</w>': 14, 'via</w>': 5, 'link</w>': 2, 'same</w>': 23, 'referred</w>': 1, 'end</w>': 17, 'E2E</w>': 1, 'arXiv</w>': 4, '15524v3</w>': 1, 'cs</w>': 4, 'CL</w>': 2, 'Oct</w>': 1, '2021</w>': 1, '4</w>': 52, 'Although</w>': 3, 'other</w>': 21, 'it</w>': 37, 'still</w>': 4, 'improving</w>': 4, 'performance</w>': 45, 'prerequisite</w>': 1, 'improvement</w>': 3, 'efficiency</w>': 1, 'helps</w>': 2, 'latency</w>': 2, 'entire</w>': 6, 'inference</w>': 2, 'One</w>': 6, 'potential</w>': 1, 'impact</w>': 4, 'work</w>': 15, 'mobile</w>': 1, 'Ondevice</w>': 1, 'generally</w>': 2, 'highly</w>': 1, 'optimized</w>': 2, 'reducing</w>': 1, 'distilling</w>': 1, 'compressing</w>': 1, 'larger</w>': 4, 'Thus</w>': 5, 'significant</w>': 3, 'Another</w>': 3, 'aggregate</w>': 3, 'computational</w>': 3, 'savings</w>': 2, 'Web</w>': 3, 'services</w>': 1, 'Facebook</w>': 1, 'Twitter</w>': 3, 'power</w>': 2, 'search</w>': 5, 'nowadays</w>': 1, 'serves</w>': 1, 'billions</w>': 1, 'per</w>': 23, 'day</w>': 1, 'processes</w>': 1, 'hundreds</w>': 1, 'trillions</w>': 1, 'pages</w>': 1, 'building</w>': 3, 'By</w>': 4, 'employing</w>': 1, 'system</w>': 3, 'would</w>': 8, 'material</w>': 4, 'benefits</w>': 2, 'environment</w>': 2, 'less</w>': 8, 'consumption</w>': 1, 'makes</w>': 8, 'theoretical</w>': 1, 'contribution</w>': 1, 'proposed</w>': 9, 'solves</w>': 1, 'old</w>': 2, 'problem</w>': 7, 'optimal</w>': 6, 'idea</w>': 5, 'applicable</w>': 3, 'string</w>': 15, 'rewriting</w>': 1, 'problems</w>': 4, '6</w>': 33, 'code</w>': 5, 'will</w>': 13, 'available</w>': 4, 'https</w>': 3, 'www</w>': 2, 'tensorflow</w>': 1, 'org</w>': 2, 'Related</w>': 2, 'Work</w>': 2, 'CWS</w>': 2, 'Recent</w>': 2, 'focuses</w>': 2, 'learning</w>': 41, 'based</w>': 13, 'approaches</w>': 7, 'but</w>': 18, 'remains</w>': 2, 'commonly</w>': 2, 'referenced</w>': 1, 'baseline</w>': 1, 'Chang</w>': 1, '2008</w>': 1, 'More</w>': 2, 'recently</w>': 1, 'techniques</w>': 4, 'have</w>': 26, 'become</w>': 3, 'near</w>': 4, 'universal</w>': 1, 'feature</w>': 6, 'modern</w>': 2, 'including</w>': 8, 'Common</w>': 1, 'include</w>': 6, 'Byte</w>': 2, 'Pair</w>': 2, 'Encoding</w>': 2, 'BPE</w>': 6, 'SentencePiece</w>': 32, 'unigram</w>': 6, 'lan1https</w>': 1, 'blog</w>': 1, 'products</w>': 1, 'language</w>': 18, 'understanding</w>': 1, 'bert</w>': 1, 'guage</w>': 1, 'modeling</w>': 4, 'widely</w>': 3, 'adopted</w>': 1, 'original</w>': 9, 'starts</w>': 3, 'possible</w>': 10, 'decrements</w>': 1, 'Jie</w>': 1, '1989</w>': 1, 'A</w>': 41, 'variant</w>': 2, 'shortest</w>': 2, 'substring</w>': 4, 'increases</w>': 4, 'Webster</w>': 1, 'Kit</w>': 1, '1992</w>': 1, 'Reps</w>': 4, '1998</w>': 4, 'Sassano</w>': 2, '2014</w>': 6, 'worst</w>': 3, 'case</w>': 14, 'previous</w>': 3, 'higher</w>': 15, '23</w>': 7, 'notations</w>': 2, 'Lookup</w>': 1, 't</w>': 2, 'c</w>': 2, 'Figure</w>': 29, 'their</w>': 21, 'may</w>': 13, 'take</w>': 4, 'similar</w>': 17, 'recognizes</w>': 1, 'regular</w>': 5, 'expressions</w>': 1, 'context</w>': 2, 'compilers</w>': 1, '|</w>': 18, '</w>': 2, 'number</w>': 29, 'states</w>': 1, 'automaton</w>': 3, 'grammar</w>': 2, 'If</w>': 8, 'applied</w>': 6, 'finite</w>': 4, 'strings</w>': 1, 'refined</w>': 1, 'designed</w>': 6, 'address</w>': 1, 'different</w>': 22, 'scenario</w>': 5, 'every</w>': 3, 'finds</w>': 1, 'quadratic</w>': 2, 'overall</w>': 1, 'comparison</w>': 4, 'achieves</w>': 3, 'due</w>': 8, 'definition</w>': 2, 'newly</w>': 1, 'introduced</w>': 3, 'way</w>': 34, 'emitting</w>': 1, 'clarifying</w>': 1, 'difference</w>': 1, 'tabular</w>': 1, 'solution</w>': 3, 'table</w>': 3, 'failed_previously</w>': 1, 'store</w>': 2, 'whether</w>': 8, 'state</w>': 17, '<</w>': 18, 'q</w>': 3, 'seen</w>': 8, 'failed</w>': 2, 'attempt</w>': 2, 'position</w>': 3, 'wasteful</w>': 1, 'revisits</w>': 1, 'entries</w>': 1, 'depend</w>': 4, 'both</w>': 17, 'contrast</w>': 2, 'capture</w>': 1, '2The</w>': 1, 'depends</w>': 4, 'implementation</w>': 4, 'details</w>': 5, 'hashes</w>': 1, 'computed</w>': 8, 'scratch</w>': 3, 'incrementally</w>': 1, 'how</w>': 11, 'substrings</w>': 1, 'searched</w>': 1, '3Previous</w>': 1, 'studies</w>': 3, 'usually</w>': 6, 'do</w>': 10, 'explicitly</w>': 1, 'related</w>': 5, 'just</w>': 10, 'treat</w>': 2, 'hidden</w>': 1, 'constant</w>': 2, 'transit</w>': 1, 'Definition</w>': 1, 'they</w>': 18, 'only</w>': 25, 'independent</w>': 7, 'Finally</w>': 3, 'discuss</w>': 1, 'Note</w>': 3, 'topic</w>': 1, 'found</w>': 9, 'Viterbi</w>': 2, '1967</w>': 1, 'implemented</w>': 4, 'ways</w>': 2, 'enumerate</w>': 1, 'symbol</w>': 7, 'pairs</w>': 4, 'order</w>': 8, 'were</w>': 7, 'added</w>': 1, 'phase</w>': 2, 'pair</w>': 3, 'scan</w>': 2, 'current</w>': 2, 'occurrences</w>': 1, 'merged</w>': 2, '</w>': 6, 'approach</w>': 30, 'repeatedly</w>': 1, 'select</w>': 1, 'symbols</w>': 5, 'highest</w>': 1, 'priority</w>': 2, 'frequency</w>': 1, 'Using</w>': 1, 'heap</w>': 2, 'done</w>': 2, '</w>': 1, 'Time</w>': 1, 'Single</w>': 1, 'Word</w>': 1, 'section</w>': 2, 'present</w>': 1, 'Background</w>': 1, 'Notations</w>': 2, 'tokenizes</w>': 3, 'until</w>': 2, 'segmented</w>': 5, 'tokenized</w>': 10, 'mapped</w>': 1, 'special</w>': 4, 'unk</w>': 3, 'distinguishes</w>': 1, 'start</w>': 2, 'starting</w>': 1, 'middle</w>': 8, 'suffix</w>': 5, 'indicator</w>': 5, 'denoted</w>': 1, '</w>': 9, 'works</w>': 1, 'arbitrary</w>': 3, 'empty</w>': 3, 'no</w>': 6, 'distinction</w>': 2, 'kinds</w>': 1, 'johanson</w>': 1, 'johan</w>': 1, 'son</w>': 1, 'running</w>': 1, 'Table</w>': 10, 'summarizes</w>': 2, 'construct</w>': 2, '</w>': 3, '</w>': 8, '</w>': 7, '</w>': 10, 'denote</w>': 1, 'label</w>': 7, 'there</w>': 13, 'outgoing</w>': 1, '4The</w>': 1, 'construction</w>': 1, 'outside</w>': 1, 'scope</w>': 1, 'refer</w>': 2, 'interested</w>': 2, 'reader</w>': 1, '</w>': 2, 'Let</w>': 2, '</w>': 3, 'represented</w>': 3, 'obtained</w>': 3, 'concatenating</w>': 1, 'labels</w>': 4, 'along</w>': 1, 'path</w>': 1, 'root</w>': 3, '</w>': 6, 'Obviously</w>': 1, '</w>': 2, '</w>': 3, 'denotes</w>': 4, 'depth</w>': 5, 'defined</w>': 5, 'excluding</w>': 1, 'Hence</w>': 1, '1a</w>': 2, 'nodes</w>': 3, 'Symbol</w>': 1, 'Meaning</w>': 1, 'unkown</w>': 1, '</w>': 1, '</w>': 1, 'whitespace</w>': 4, 'Trie</w>': 2, 'often</w>': 2, 'parent</w>': 2, 'Null</w>': 1, '</w>': 1, '</w>': 1, 'Failure</w>': 1, '</w>': 1, 'sum</w>': 1, 'lengths</w>': 1, 'Intuition</w>': 1, 'motivate</w>': 1, 'let</w>': 1, 'consider</w>': 1, 'alternative</w>': 2, 'simple</w>': 17, 'searching</w>': 1, 'iterates</w>': 1, 'over</w>': 23, 'left</w>': 8, 'right</w>': 6, 'following</w>': 8, 'find</w>': 10, 'prefixes</w>': 1, 'Consider</w>': 1, 'abcdz</w>': 1, 'dz</w>': 1, 'Starting</w>': 1, 'follow</w>': 5, 'edges</w>': 1, 'd</w>': 7, 'arriving</w>': 1, 'No</w>': 2, 'exits</w>': 1, 'z</w>': 4, '</w>': 1, 'challenge</w>': 1, 'tokenizer</w>': 3, 'detokenizer</w>': 4, 'Neural</w>': 7, 'Processing</w>': 2, 'Taku</w>': 1, 'John</w>': 1, 'Richardson</w>': 1, 'Inc</w>': 1, 'taku</w>': 1, 'johnri</w>': 1, 'describes</w>': 3, 'Machine</w>': 1, 'Translation</w>': 1, 'provides</w>': 2, 'open</w>': 1, 'source</w>': 2, 'Python</w>': 6, 'implementations</w>': 1, 'While</w>': 4, 'existing</w>': 3, 'tools</w>': 4, 'assume</w>': 2, 'sequences</w>': 3, 'train</w>': 19, 'directly</w>': 8, 'raw</w>': 9, 'sentences</w>': 7, 'allows</w>': 3, 'us</w>': 3, 'make</w>': 8, 'purely</w>': 3, 'validation</w>': 10, 'experiment</w>': 7, 'NMT</w>': 18, 'English</w>': 1, 'Japanese</w>': 4, 'achieve</w>': 7, 'comparable</w>': 3, 'accuracy</w>': 19, 'direct</w>': 2, 'training</w>': 59, 'compare</w>': 11, 'various</w>': 4, 'configurations</w>': 3, 'under</w>': 2, 'Apache</w>': 1, 'license</w>': 1, 'github</w>': 3, 'sentencepiece</w>': 2, 'Deep</w>': 1, 'neural</w>': 24, 'networks</w>': 86, 'demonstrating</w>': 1, 'Natural</w>': 3, 'Language</w>': 1, 'Bahdanau</w>': 1, 'Luong</w>': 1, '2015</w>': 3, 'Wu</w>': 1, 'Vaswani</w>': 1, '2017</w>': 6, 'especially</w>': 2, 'gained</w>': 1, 'increasing</w>': 1, 'popularity</w>': 1, 'leverage</w>': 1, 'translations</w>': 1, 'toend</w>': 2, 'architecture</w>': 6, 'shown</w>': 6, 'remarkable</w>': 1, 'several</w>': 8, 'shared</w>': 2, 'Denkowski</w>': 1, 'Neubig</w>': 1, 'Nakazawa</w>': 1, 'effective</w>': 3, 'had</w>': 1, 'strong</w>': 2, 'influence</w>': 1, 'such</w>': 12, 'dialog</w>': 1, 'generation</w>': 2, 'Vinyals</w>': 9, 'Le</w>': 2, 'automatic</w>': 1, 'summarization</w>': 1, 'Rush</w>': 1, 'potentially</w>': 1, 'many</w>': 6, 'systems</w>': 7, 'relying</w>': 1, 'dependent</w>': 4, 'postprocessors</w>': 2, 'traditional</w>': 1, 'statistical</w>': 1, 'SMT</w>': 2, 'Moses1</w>': 1, 'de</w>': 1, 'facto</w>': 1, 'standard</w>': 5, 'toolkit</w>': 2, 'implements</w>': 4, 'reasonably</w>': 1, 'useful</w>': 5, 'postprocessor</w>': 2, 'However</w>': 10, 'upon</w>': 2, 'hand</w>': 5, 'crafted</w>': 3, 'rules</w>': 10, 'effectiveness</w>': 3, 'proven</w>': 1, 'addition</w>': 5, 'these</w>': 24, 'mainly</w>': 2, 'European</w>': 2, 'languages</w>': 4, 'non</w>': 14, 'Korean</w>': 1, 'run</w>': 2, 'segmenters</w>': 1, 'independently</w>': 2, 'Such</w>': 6, 'languagedependent</w>': 1, 'hard</w>': 5, 'multilingual</w>': 2, 'Johnson</w>': 2, 'carefully</w>': 3, 'manage</w>': 1, 'internal</w>': 2, 'deep</w>': 7, 'architectures</w>': 2, 'languageindependent</w>': 1, 'standardized</w>': 1, 'more</w>': 19, 'agnostic</w>': 1, 'becoming</w>': 1, 'important</w>': 3, 'community</w>': 1, 'develop</w>': 1, 'reproducible</w>': 1, 'easily</w>': 2, 'integrated</w>': 3, 'Network</w>': 1, 'demo</w>': 1, 'describe</w>': 1, 'Networkbased</w>': 1, 'predetermined</w>': 1, 'prior</w>': 1, 'byte</w>': 1, 'pairencoding</w>': 1, 'extension</w>': 3, 'enables</w>': 1, 'languagespecific</w>': 1, '1http</w>': 1, 'statmt</w>': 1, 'moses</w>': 1, '1808</w>': 1, '06226v1</w>': 1, '19</w>': 5, 'Aug</w>': 1, '%</w>': 66, 'spm_train</w>': 5, '</w>': 34, 'txt</w>': 3, 'model_prefix</w>': 3, 'spm</w>': 12, 'vocab_size</w>': 4, '1000</w>': 8, 'echo</w>': 4, 'Hello</w>': 13, 'world</w>': 13, 'spm_encode</w>': 4, '_He</w>': 2, 'll</w>': 2, 'o</w>': 2, '_world</w>': 2, 'output_format</w>': 1, 'id</w>': 10, '151</w>': 4, '88</w>': 4, '21</w>': 5, '887</w>': 4, 'spm_decode</w>': 4, 'input_format</w>': 1, 'Commandline</w>': 1, 'System</w>': 1, 'Overview</w>': 2, 'comprises</w>': 2, 'four</w>': 5, 'main</w>': 4, 'components</w>': 1, 'Normalizer</w>': 3, 'Trainer</w>': 4, 'Encoder</w>': 4, 'Decoder</w>': 4, 'module</w>': 1, 'normalize</w>': 3, 'semanticallyequivalent</w>': 1, 'canonical</w>': 1, 'forms</w>': 2, 'trains</w>': 2, 'corpus</w>': 1, 'specify</w>': 1, 'type</w>': 2, 'parameter</w>': 3, 'internally</w>': 1, 'executes</w>': 1, 'trained</w>': 40, 'converts</w>': 1, 'roles</w>': 1, 'correspond</w>': 1, 'postprocessing</w>': 1, 'detokenization</w>': 1, 'respectively</w>': 1, 'call</w>': 6, 'them</w>': 8, 'encoding</w>': 4, 'decoding</w>': 3, 'manages</w>': 2, 'mapping</w>': 5, 'convert</w>': 2, 'vice</w>': 3, 'versa</w>': 3, 'Direct</w>': 1, '?</w>': 6, 'presents</w>': 3, 'reversibly</w>': 2, 'converted</w>': 3, 'Library</w>': 2, 'Design</w>': 2, 'design</w>': 5, 'command</w>': 4, 'line</w>': 11, 'snippets</w>': 1, 'Lossless</w>': 1, '</w>': 14, 'Raw</w>': 3, 'Tokenized</w>': 4, 'observation</w>': 2, 'convertible</w>': 1, 'space</w>': 17, 'exists</w>': 2, '</w>': 12, '</w>': 12, 'kept</w>': 1, 'Detokenization</w>': 1, 'restore</w>': 1, 'irreversible</w>': 1, 'operations</w>': 3, 'puts</w>': 1, 'primitive</w>': 1, 'spaces</w>': 2, 'required</w>': 2, '</w>': 1, '</w>': 2, '</w>': 1, '</w>': 1, 'manually</w>': 1, 'expensive</w>': 2, 'write</w>': 1, 'maintain</w>': 1, 'inverse</w>': 1, 'operation</w>': 1, 'Decode</w>': 2, 'Encode</w>': 3, 'Normalize</w>': 2, 'lossless</w>': 5, 'reproduce</w>': 4, 'preserved</w>': 2, 'encoder</w>': 2, 'basic</w>': 1, 'Even</w>': 2, 'handled</w>': 1, 'normal</w>': 1, 'sake</w>': 1, 'clarity</w>': 2, 'escapes</w>': 1, 'meta</w>': 20, 'U</w>': 13, '2581</w>': 1, 'Hello_world</w>': 1, '_wor</w>': 1, 'ld</w>': 2, 'detokenize</w>': 1, 'ambiguities</w>': 2, 'detok</w>': 1, 'join</w>': 2, 'should</w>': 3, 'noted</w>': 2, 'nmt2</w>': 1, 'adopts</w>': 2, 'representation</w>': 11, 'subwords</w>': 1, 'intra</w>': 1, 'boundary</w>': 1, 'marker</w>': 2, '2https</w>': 1, 'rsennrich</w>': 1, 'nmt</w>': 2, 'wor</w>': 1, 'always</w>': 3, 'ambiguity</w>': 1, 'treatment</w>': 8, 'specifically</w>': 2, 'encode</w>': 3, 'consecutive</w>': 1, 'Efficient</w>': 1, 'Existing</w>': 1, 'was</w>': 13, 'pretokenization</w>': 1, 'difficult</w>': 2, 'employs</w>': 1, 'speed</w>': 2, 'amount</w>': 2, 'given</w>': 15, 'requires</w>': 3, 'O</w>': 2, 'N2</w>': 1, 'cost</w>': 2, 'naively</w>': 1, 'iteration</w>': 1, 'log</w>': 3, 'managed</w>': 1, 'binary</w>': 2, 'queue</w>': 1, 'complexities</w>': 1, 'Vocabulary</w>': 1, 'management</w>': 1, 'specified</w>': 4, 'flag</w>': 3, 'specifies</w>': 3, 'merge</w>': 2, 'reserves</w>': 1, 'ids</w>': 4, 'unknown</w>': 1, 'BOS</w>': 1, 'EOS</w>': 1, 'padding</w>': 1, 'pad</w>': 1, 'Their</w>': 3, 'configured</w>': 1, 'flags</w>': 2, 'define</w>': 5, 'custom</w>': 4, 'contextual</w>': 1, 'virtual</w>': 1, 'Examples</w>': 2, 'languageindicators</w>': 1, '2ja</w>': 1, '2de</w>': 1, '41</w>': 3, '302</w>': 3, '300</w>': 2, 'tab</w>': 2, '1EA6</w>': 1, '301</w>': 1, '1EA4</w>': 1, 'Custom</w>': 1, 'normalization</w>': 19, 'rule</w>': 2, 'TSV</w>': 5, 'Customizable</w>': 1, 'Character</w>': 2, 'handling</w>': 1, 'real</w>': 1, 'consists</w>': 1, 'semantically</w>': 2, 'equivalent</w>': 6, 'fullwidth</w>': 1, 'Latin</w>': 2, 'ASCII</w>': 1, 'Lowercasing</w>': 1, 'application</w>': 1, 'Recently</w>': 2, 'Normalization</w>': 1, 'Forms</w>': 1, 'NFC</w>': 1, 'NFKC</w>': 6, 'because</w>': 18, 'better</w>': 19, 'reproducibility</w>': 2, 'support</w>': 25, 'default</w>': 3, 'normalizes</w>': 1, 'normalization_rule_name</w>': 1, 'nfkc</w>': 1, 'Sentencepiece</w>': 1, 'leftmost</w>': 1, 'compiled</w>': 2, 'transducer</w>': 3, 'normalization3</w>': 1, 'supports</w>': 2, 'file</w>': 9, 'shows</w>': 6, '1EA64</w>': 1, 'conversion</w>': 1, 'User</w>': 1, 'files</w>': 2, 'normalization_rule_tsv</w>': 1, 'Task</w>': 1, 'extending</w>': 1, 'provided</w>': 6, 'package</w>': 1, 'Self</w>': 1, 'contained</w>': 4, 'researchers</w>': 2, 'pretrained</w>': 1, 'reproduciblity</w>': 1, '3The</w>': 1, 'CCC</w>': 2, 'Canonical</w>': 1, 'Combining</w>': 1, 'Class</w>': 1, 'reordering</w>': 2, 'handle</w>': 3, 'full</w>': 1, 'subset</w>': 6, '4Note</w>': 1, 'tabs</w>': 1, 'delimiter</w>': 2, 'individual</w>': 4, 'experimental</w>': 4, 'stated</w>': 1, 'preprocessed</w>': 2, 'Post</w>': 1, 'reported</w>': 7, 'subtle</w>': 1, 'differences</w>': 2, 'schemes</w>': 1, 'change</w>': 2, 'BLEU</w>': 1, 'scores</w>': 1, 'Moses</w>': 2, 'guaranteed</w>': 1, 'settings</w>': 2, 'unless</w>': 1, 'version</w>': 4, 'clearly</w>': 1, 'Strictly</w>': 1, 'speaking</w>': 1, 'yield</w>': 3, 'Ideally</w>': 1, 'must</w>': 6, 'embedded</w>': 9, 'manner</w>': 2, 'setting</w>': 4, 'includes</w>': 1, 'behavior</w>': 2, 'determined</w>': 4, 'external</w>': 1, 'dependencies</w>': 1, 'guarantees</w>': 1, 'perfect</w>': 1, 'distribute</w>': 1, 'part</w>': 2, 'developers</w>': 2, 'refine</w>': 1, 'having</w>': 4, 'worry</w>': 1, 'about</w>': 3, 'breaking</w>': 1, 'behaviors</w>': 1, 'stored</w>': 1, 'wire</w>': 1, 'format</w>': 1, 'Protocol</w>': 2, 'buffer5</w>': 1, 'platform</w>': 1, 'neutral</w>': 1, 'extensible</w>': 2, 'mechanism</w>': 2, 'serializing</w>': 1, 'structured</w>': 2, 'buffers</w>': 2, 'help</w>': 3, 'safely</w>': 1, 'serialize</w>': 1, 'keeping</w>': 2, 'compatibility</w>': 1, 'extensibility</w>': 1, 'API</w>': 4, 'fly</w>': 3, 'considered</w>': 2, 'offline</w>': 2, 'Prior</w>': 1, 'standalone</w>': 3, 'preprocessor</w>': 1, 'off</w>': 4, 'First</w>': 3, 'user</w>': 2, 'facing</w>': 1, 'preprocess</w>': 1, 'Second</w>': 2, 'employ</w>': 1, 'subsentence</w>': 1, 'level</w>': 10, 'augmentation</w>': 1, 'noise</w>': 3, 'injection</w>': 2, 'aim</w>': 2, 'robustness</w>': 1, 'There</w>': 3, 'inject</w>': 1, 'ran5https</w>': 1, 'protocol</w>': 1, 'sentencepiece_processor</w>': 1, 'sentencepiece_trainer</w>': 1, 'SentencePieceTrainer</w>': 2, 'Train</w>': 2, 'SentencePieceProcessor</w>': 2, 'sp</w>': 10, 'Load</w>': 2, 'std</w>': 4, 'vector</w>': 11, 'pieces</w>': 2, '&</w>': 7, 'int</w>': 1, 'params</w>': 2, 'EncodeAsPieces</w>': 1, 'EncodeAsIds</w>': 1, 'DecodeIds</w>': 1, 'domly</w>': 1, 'changing</w>': 1, 'proposes</w>': 1, 'regularization</w>': 3, 'randomly</w>': 13, 'changes</w>': 1, 'during</w>': 5, 'Lample</w>': 1, 'Artetxe</w>': 1, 'denoising</w>': 1, 'autoencoder</w>': 2, 'alter</w>': 1, 'reconstruct</w>': 1, 'emulate</w>': 1, 'dynamic</w>': 1, 'sampling</w>': 1, 'tool</w>': 2, 'Tensorflow</w>': 1, 'library</w>': 1, 'frameworks</w>': 1, 'Figures</w>': 1, 'usages</w>': 1, 'API6</w>': 1, 'one</w>': 25, 'sampled</w>': 2, 'New</w>': 1, 'York</w>': 1, 'differently</w>': 1, 'How</w>': 1, 'transferable</w>': 1, 'features</w>': 63, 'Jason</w>': 1, 'Yosinski</w>': 1, 'Jeff</w>': 1, 'Clune</w>': 1, 'Yoshua</w>': 1, 'Bengio</w>': 3, 'Hod</w>': 1, 'Lipson4</w>': 1, 'Dept</w>': 4, 'Computer</w>': 3, 'Science</w>': 3, 'Cornell</w>': 2, 'University</w>': 6, 'Wyoming</w>': 1, 'Operations</w>': 1, 'Montreal</w>': 1, 'Mechanical</w>': 1, 'Aerospace</w>': 1, 'Engineering</w>': 1, 'Many</w>': 1, 'natural</w>': 15, 'images</w>': 15, 'exhibit</w>': 4, 'curious</w>': 4, 'phenomenon</w>': 3, 'common</w>': 3, 'learn</w>': 16, 'Gabor</w>': 5, 'filters</w>': 11, 'color</w>': 8, 'blobs</w>': 4, 'appear</w>': 1, 'particular</w>': 7, 'datasets</w>': 11, 'Features</w>': 3, 'eventually</w>': 1, 'transition</w>': 6, 'last</w>': 5, 'network</w>': 69, 'studied</w>': 1, 'extensively</w>': 1, 'experimentally</w>': 2, 'quantify</w>': 4, 'generality</w>': 4, 'versus</w>': 1, 'specificity</w>': 5, 'neurons</w>': 5, 'convolutional</w>': 7, 'report</w>': 2, 'few</w>': 27, 'surprising</w>': 5, 'Transferability</w>': 1, 'negatively</w>': 2, 'affected</w>': 2, 'distinct</w>': 2, 'issues</w>': 5, 'specialization</w>': 2, 'expense</w>': 2, 'optimization</w>': 4, 'difficulties</w>': 4, 'co</w>': 10, 'adapted</w>': 6, 'ImageNet</w>': 14, 'demonstrate</w>': 3, 'either</w>': 7, 'dominate</w>': 3, 'transferred</w>': 13, 'bottom</w>': 4, 'document</w>': 1, 'transferability</w>': 6, 'decreases</w>': 2, 'distance</w>': 32, 'base</w>': 26, 'transferring</w>': 8, 'distant</w>': 3, 'result</w>': 9, 'initializing</w>': 3, 'layers</w>': 60, 'produce</w>': 9, 'boost</w>': 8, 'generalization</w>': 9, 'lingers</w>': 2, 'fine</w>': 24, 'tuning</w>': 12, 'tend</w>': 2, 'resemble</w>': 1, 'appearance</w>': 1, 'obtaining</w>': 1, 'anything</w>': 1, 'image</w>': 6, 'causes</w>': 1, 'suspicion</w>': 1, 'poorly</w>': 2, 'chosen</w>': 6, 'hyperparameters</w>': 3, 'software</w>': 1, 'bug</w>': 1, 'occurs</w>': 1, 'very</w>': 3, 'objectives</w>': 1, 'supervised</w>': 3, 'Krizhevsky</w>': 2, 'unsupervised</w>': 2, 'density</w>': 5, 'Lee</w>': 1, '2009</w>': 7, 'sparse</w>': 1, 'representations</w>': 2, '2011</w>': 6, 'Because</w>': 1, 'finding</w>': 3, 'seems</w>': 3, 'occur</w>': 2, 'regardless</w>': 1, 'function</w>': 7, 'On</w>': 6, 'know</w>': 1, 'greatly</w>': 4, 'dimensional</w>': 10, 'successfully</w>': 1, 'toward</w>': 8, 'objective</w>': 1, 'unit</w>': 3, 'thus</w>': 3, 'These</w>': 10, 'intuitive</w>': 1, 'notions</w>': 1, 'provide</w>': 4, 'rigorous</w>': 1, 'definitions</w>': 1, 'below</w>': 1, '1411</w>': 1, '1792v1</w>': 1, 'LG</w>': 2, 'Nov</w>': 1, 'somewhere</w>': 1, 'raises</w>': 1, 'questions</w>': 2, 'Can</w>': 1, 'degree</w>': 5, 'Does</w>': 1, 'suddenly</w>': 1, 'spread</w>': 1, 'Where</w>': 1, 'place</w>': 2, 'answers</w>': 1, 'extent</w>': 6, 'within</w>': 7, 'able</w>': 2, 'transfer</w>': 24, 'Caruana</w>': 1, '1995</w>': 1, 'repurpose</w>': 1, 'learned</w>': 16, 'second</w>': 5, 'suitable</w>': 1, 'instead</w>': 6, 'significantly</w>': 1, 'powerful</w>': 1, 'enable</w>': 3, 'overfitting</w>': 7, 'taken</w>': 1, 'advantage</w>': 1, 'fact</w>': 2, 'obtain</w>': 3, 'art</w>': 8, 'Donahue</w>': 2, '2013a</w>': 1, 'Zeiler</w>': 1, 'Fergus</w>': 1, '2013</w>': 2, 'Sermanet</w>': 1, 'collectively</w>': 2, 'suggesting</w>': 2, 'indeed</w>': 1, 'compute</w>': 3, 'fairly</w>': 1, 'emphasize</w>': 1, 'importance</w>': 1, 'studying</w>': 1, 'nature</w>': 2, 'usual</w>': 1, 'copy</w>': 2, 'n</w>': 23, 'initialized</w>': 4, 'choose</w>': 3, 'backpropagate</w>': 1, 'errors</w>': 1, 'new</w>': 18, 'copied</w>': 6, 'tune</w>': 3, 'frozen</w>': 10, 'tuned</w>': 10, 'improve</w>': 3, 'Of</w>': 2, 'course</w>': 1, 'little</w>': 1, 'lower</w>': 9, 'could</w>': 8, '</w>': 6, 'sections</w>': 2, 'contributions</w>': 1, 'namely</w>': 1, 'another</w>': 5, 'characterize</w>': 1, 'yields</w>': 2, 'separate</w>': 7, 'cause</w>': 1, 'degradation</w>': 2, 'themselves</w>': 1, 'ii</w>': 1, 'neighboring</w>': 2, 'effects</w>': 4, 'dissimilar</w>': 6, 'previously</w>': 3, 'Jarrett</w>': 5, 'weights</w>': 14, 'vs</w>': 7, 'particularly</w>': 6, 'effect</w>': 11, 'persists</w>': 1, 'extensive</w>': 1, 'Generality</w>': 1, 'Specificity</w>': 1, 'Measured</w>': 1, 'Transfer</w>': 2, 'Performance</w>': 7, 'tendency</w>': 1, 'study</w>': 3, 'set</w>': 31, 'B</w>': 29, 'note</w>': 2, 'similarity</w>': 2, 'create</w>': 5, 'constructing</w>': 1, 'overlapping</w>': 1, 'subsets</w>': 2, 'split</w>': 10, 'groups</w>': 1, 'containing</w>': 6, '500</w>': 5, 'approximately</w>': 2, 'half</w>': 10, '645</w>': 1, '000</w>': 3, 'examples</w>': 21, 'eight</w>': 3, 'baseA</w>': 4, 'baseB</w>': 13, 'rows</w>': 2, '7</w>': 28, 'explanation</w>': 2, 'selffer</w>': 9, 'B3B</w>': 7, 'five</w>': 3, '</w>': 2, 'control</w>': 2, 'row</w>': 6, 'A3B</w>': 6, 'Intuitively</w>': 1, 'classify</w>': 3, 'performs</w>': 1, 'evidence</w>': 4, 'third</w>': 2, 'least</w>': 3, 'respect</w>': 3, 'suffers</w>': 1, 'repeated</w>': 1, 'directions</w>': 1, 'AnB</w>': 16, 'BnA</w>': 3, 'above</w>': 5, 'versions</w>': 4, 'assign</w>': 1, 'clusters</w>': 1, 'dogs</w>': 1, 'cats</w>': 1, '13</w>': 2, 'biological</w>': 1, 'family</w>': 4, 'Felidae</w>': 1, 'tabby</w>': 1, 'cat</w>': 5, 'tiger</w>': 2, 'Persian</w>': 1, 'Siamese</w>': 1, 'Egyptian</w>': 1, 'mountain</w>': 1, 'lion</w>': 2, 'lynx</w>': 1, 'leopard</w>': 2, 'snow</w>': 1, 'jaguar</w>': 1, 'cheetah</w>': 1, 'contain</w>': 2, 'felid</w>': 3, 'levels</w>': 2, 'some</w>': 3, 'types</w>': 1, 'felids</w>': 1, 'generalizing</w>': 2, 'expect</w>': 3, 'high</w>': 3, 'detectors</w>': 2, 'low</w>': 1, 'created</w>': 2, 'assigning</w>': 2, 'Fortunately</w>': 1, 'hierarchy</w>': 1, 'allowed</w>': 2, 'halves</w>': 1, 'man</w>': 8, 'made</w>': 11, 'entities</w>': 2, 'quite</w>': 3, '551</w>': 2, 'group</w>': 2, '449</w>': 2, 'Further</w>': 2, 'supplementary</w>': 3, '1The</w>': 1, 'released</w>': 1, 'Large</w>': 2, 'Scale</w>': 1, 'Visual</w>': 1, 'Recognition</w>': 1, 'Challenge</w>': 1, 'ILSVRC2012</w>': 1, 'Deng</w>': 1, '281</w>': 1, '167</w>': 1, 'labeled</w>': 7, '50</w>': 11, 'test</w>': 21, '2Note</w>': 1, 'doesn</w>': 1, 'sense</w>': 3, 'B8B</w>': 1, 'A8B</w>': 1, 'never</w>': 1, 'WA1</w>': 3, 'WA2</w>': 2, 'WA3</w>': 2, 'WA4</w>': 1, 'WA5</w>': 1, 'WA6</w>': 1, 'WA7</w>': 1, 'WA8</w>': 1, 'WB1</w>': 2, 'WB2</w>': 2, 'WB3</w>': 2, 'WB4</w>': 1, 'WB5</w>': 1, 'WB6</w>': 1, 'WB7</w>': 1, 'WB8</w>': 1, 'treatments</w>': 1, 'controls</w>': 2, 'Top</w>': 7, 'backprop</w>': 1, 'rectangles</w>': 1, 'represent</w>': 6, 'weight</w>': 5, 'indicating</w>': 2, 'originally</w>': 2, 'vertical</w>': 1, 'ellipsoidal</w>': 1, 'bars</w>': 2, 'vectors</w>': 5, 'activations</w>': 1, 'Third</w>': 1, 'upper</w>': 9, 'locked</w>': 1, 'reveals</w>': 2, 'occurrence</w>': 1, 'fragile</w>': 4, 'coadaptation</w>': 1, 'adapt</w>': 1, 'rediscovered</w>': 1, 'Fourth</w>': 1, 'except</w>': 1, 'tests</w>': 1, 'Setup</w>': 1, 'Since</w>': 3, 'won</w>': 1, 'competition</w>': 1, 'much</w>': 7, 'interest</w>': 1, 'tweaking</w>': 1, 'maximize</w>': 2, 'absolute</w>': 1, 'rather</w>': 6, 'reference</w>': 2, 'Caffe</w>': 1, 'Jia</w>': 1, 'setup</w>': 1, 'rates</w>': 1, 'experiments</w>': 11, 'http</w>': 1, 'yosinski</w>': 1, 'Results</w>': 2, 'Discussion</w>': 1, 'performed</w>': 5, 'three</w>': 4, 'sets</w>': 1, 'splits</w>': 10, 'discussed</w>': 3, '52</w>': 1, '54</w>': 4, '56</w>': 3, '58</w>': 2, '60</w>': 8, '62</w>': 2, '64</w>': 7, '66</w>': 2, 'BnB</w>': 13, 'Layer</w>': 3, 'chopped</w>': 2, 'retrained</w>': 2, 'improves</w>': 4, 'Fine</w>': 3, 'recovers</w>': 1, 'interactions</w>': 1, 'drops</w>': 3, 'adaptation</w>': 4, 'Each</w>': 4, 'figure</w>': 1, 'represents</w>': 2, 'white</w>': 4, 'circles</w>': 4, 'points</w>': 33, 'tested</w>': 1, 'dark</w>': 3, 'blue</w>': 4, 'dot</w>': 1, 'Light</w>': 1, 'Dark</w>': 1, 'red</w>': 5, 'diamonds</w>': 6, 'light</w>': 3, 'Points</w>': 1, 'shifted</w>': 1, 'slightly</w>': 2, 'visual</w>': 1, 'Bottom</w>': 2, 'Lines</w>': 2, 'connecting</w>': 1, 'means</w>': 3, 'Numbered</w>': 1, 'descriptions</w>': 1, 'interpretation</w>': 1, 'applies</w>': 1, 'Similar</w>': 1, 'Datasets</w>': 3, 'shown3</w>': 1, 'conclusions</w>': 1, 'interpretations</w>': 1, 'dotted</w>': 1, '3AnA</w>': 1, 'statistically</w>': 2, 'cases</w>': 2, 'simplify</w>': 1, 'notation</w>': 1, 'Similarly</w>': 1, 'aggregated</w>': 2, 'identical</w>': 3, 'attains</w>': 1, '625</w>': 1, '37</w>': 1, 'error</w>': 5, '42</w>': 2, 'attained</w>': 1, 'lead</w>': 2, 'net</w>': 1, 'mistakes</w>': 1, 'That</w>': 2, 'save</w>': 1, 'reinitialize</w>': 1, 'whole</w>': 1, 'retrain</w>': 1, 'holds</w>': 1, 'true</w>': 2, 'worse</w>': 2, 'drop</w>': 9, 'successive</w>': 1, 'interact</w>': 1, 'complex</w>': 1, 'relearned</w>': 1, 'alone</w>': 1, 'Gradient</w>': 1, 'descent</w>': 4, 'good</w>': 2, 'jointly</w>': 1, 'nearly</w>': 2, 'back</w>': 1, 'get</w>': 1, 'closer</w>': 2, 'relearn</w>': 1, 'apparently</w>': 1, 'relearning</w>': 1, 'enough</w>': 3, 'Alternately</w>': 1, 'say</w>': 1, 'observed</w>': 3, 'literature</w>': 4, 'prevents</w>': 1, 'measure</w>': 1, 'Layers</w>': 1, 'perfectly</w>': 1, 'giving</w>': 2, 'blob</w>': 1, 'slight</w>': 1, 'Thanks</w>': 1, 'tell</w>': 1, 'combination</w>': 2, 'lost</w>': 1, 'dominates</w>': 3, 'whereas</w>': 1, 'diminishes</w>': 1, 'successful</w>': 2, 'elsewhere</w>': 1, 'Girshick</w>': 1, '2013b</w>': 1, 'limited</w>': 3, 'noticing</w>': 2, 'believe</w>': 1, 'quantified</w>': 2, 'decoupled</w>': 2, 'showing</w>': 3, 'regime</w>': 2, 'generalize</w>': 5, 'those</w>': 4, 'Previously</w>': 1, 'reason</w>': 2, 'want</w>': 1, 'suggests</w>': 3, 'attributed</w>': 1, 'longer</w>': 2, 'total</w>': 2, '450k</w>': 4, 'iterations</w>': 3, 'finetuned</w>': 1, 'plausible</w>': 1, 'beginning</w>': 1, 'completely</w>': 1, 'linger</w>': 1, 'boosting</w>': 1, 'retraining</w>': 3, 'keep</w>': 3, 'initialize</w>': 1, 'anywhere</w>': 1, 'seven</w>': 1, 'produces</w>': 2, 'improved</w>': 2, 'across</w>': 3, '4We</w>': 1, 'point</w>': 10, 'computationally</w>': 1, 'days</w>': 1, 'GPU</w>': 1, 'publication</w>': 1, 'aggregation</w>': 1, 'averaged</w>': 3, 'ranges</w>': 1, 'mean</w>': 13, 'Dissimilar</w>': 1, 'Splitting</w>': 1, 'Man</w>': 2, 'Classes</w>': 1, 'Into</w>': 1, 'Separate</w>': 1, 'mentioned</w>': 1, 'decline</w>': 1, 'hypothesis</w>': 1, 'comparing</w>': 1, 'object</w>': 2, 'creates</w>': 1, 'subplot</w>': 3, 'orange</w>': 1, 'hexagons</w>': 2, 'lines</w>': 1, 'categories</w>': 2, 'simply</w>': 3, 'being</w>': 3, 'easier</w>': 1, 'Weights</w>': 1, 'untrained</w>': 3, 'showed</w>': 2, 'strikingly</w>': 1, 'rectification</w>': 1, 'local</w>': 3, 'They</w>': 2, 'Caltech</w>': 4, '101</w>': 2, 'Fei</w>': 2, '2004</w>': 1, 'ask</w>': 1, 'carries</w>': 1, 'deeper</w>': 1, 'choices</w>': 4, 'falls</w>': 1, 'quickly</w>': 2, 'chance</w>': 2, 'getting</w>': 1, 'straightforward</w>': 4, 'Whereas</w>': 1, 'max</w>': 2, 'did</w>': 2, 'nonlinearity</w>': 2, 'relu</w>': 1, 'abs</w>': 1, 'tanh</w>': 1, 'sizes</w>': 2, 'Additionally</w>': 1, 'hyperparameter</w>': 1, 'architectural</w>': 2, 'datapoint</w>': 1, 'tweak</w>': 1, 'initialization</w>': 2, 'subtracting</w>': 2, 'performances</w>': 1, 'plotted</w>': 1, 'things</w>': 1, 'apparent</w>': 1, 'gap</w>': 2, 'grows</w>': 2, '25</w>': 3, 'differ</w>': 2, 'fully</w>': 2, 'ours</w>': 3, 'informative</w>': 1, 'however</w>': 7, 'draws</w>': 2, 'multiple</w>': 4, 'runs</w>': 1, 'converge</w>': 1, 'producing</w>': 2, 'Much</w>': 1, 'convergence</w>': 1, '30</w>': 5, '20</w>': 16, '15</w>': 4, '05</w>': 1, '00</w>': 1, 'Relative</w>': 1, 'm</w>': 1, 'Degradation</w>': 1, 'connects</w>': 2, 'consist</w>': 2, 'plots</w>': 1, 'compared</w>': 2, 'making</w>': 3, 'extra</w>': 1, 'overfit</w>': 2, 'Conclusions</w>': 1, 'demonstrated</w>': 2, 'quantifying</w>': 1, 'fragilely</w>': 1, 'substantial</w>': 2, 'technique</w>': 1, 'Prototypical</w>': 10, 'Networks</w>': 4, 'Few</w>': 8, 'shot</w>': 80, 'Learning</w>': 3, 'Jake</w>': 1, 'Snell</w>': 1, 'Toronto</w>': 2, 'Kevin</w>': 1, 'Swersky</w>': 1, 'Richard</w>': 1, 'S</w>': 4, 'Zemel</w>': 1, 'Vector</w>': 1, 'Institute</w>': 1, 'prototypical</w>': 29, 'metric</w>': 6, 'computing</w>': 2, 'distances</w>': 7, 'prototype</w>': 14, 'Compared</w>': 1, 'recent</w>': 6, 'reflect</w>': 1, 'simpler</w>': 3, 'inductive</w>': 3, 'beneficial</w>': 2, 'excellent</w>': 1, 'decisions</w>': 2, 'improvements</w>': 1, 'involving</w>': 1, 'complicated</w>': 1, 'extend</w>': 3, 'zero</w>': 12, 'theart</w>': 1, 'CU</w>': 1, 'Birds</w>': 2, '16</w>': 6, 'accommodate</w>': 1, 'naive</w>': 1, 're</w>': 1, 'severely</w>': 2, 'humans</w>': 1, 'ability</w>': 1, 'Two</w>': 1, 'progress</w>': 1, '29</w>': 14, 'predict</w>': 2, 'unlabeled</w>': 1, 'query</w>': 18, 'interpreted</w>': 1, 'weighted</w>': 3, 'nearest</w>': 8, 'neighbor</w>': 5, 'Notably</w>': 1, 'utilizes</w>': 3, 'mini</w>': 1, 'batches</w>': 1, 'episodes</w>': 17, 'episode</w>': 16, 'mimic</w>': 1, 'subsampling</w>': 1, 'faithful</w>': 1, 'thereby</w>': 1, 'Ravi</w>': 7, 'Larochelle</w>': 7, '22</w>': 9, 'episodic</w>': 5, 'LSTM</w>': 7, 'updates</w>': 2, 'Here</w>': 1, 'learner</w>': 1, 'learns</w>': 3, 'attack</w>': 1, 'addressing</w>': 1, 'key</w>': 3, 'issue</w>': 1, 'assumption</w>': 1, 'cluster</w>': 9, 'around</w>': 1, 'Classification</w>': 5, 'tackle</w>': 1, 'comes</w>': 1, 'description</w>': 1, 'therefore</w>': 2, 'serve</w>': 2, 'Initial</w>': 1, 'author</w>': 1, '1703</w>': 1, '05175v2</w>': 1, 'Jun</w>': 1, 'c1</w>': 2, 'c2</w>': 2, 'c3</w>': 2, 'v1</w>': 1, 'v2</w>': 1, 'v3</w>': 1, 'Zero</w>': 6, 'scenarios</w>': 3, 'Left</w>': 1, 'prototypes</w>': 10, 'ck</w>': 12, 'Right</w>': 1, 'produced</w>': 1, 'vk</w>': 3, 'classified</w>': 1, 'p</w>': 3, 'y</w>': 5, '</w>': 1, 'exp</w>': 4, 'f</w>': 9, 'formulate</w>': 1, 'draw</w>': 1, 'connections</w>': 1, 'analyze</w>': 1, 'underlying</w>': 1, 'relate</w>': 1, 'clustering</w>': 3, 'justify</w>': 1, 'Bregman</w>': 11, 'divergence</w>': 7, 'squared</w>': 6, 'Euclidean</w>': 14, 'empirically</w>': 2, 'vital</w>': 1, 'outperforms</w>': 1, 'cosine</w>': 6, 'benchmark</w>': 1, 'appealing</w>': 1, 'Notation</w>': 1, 'x1</w>': 2, 'y1</w>': 2, 'xN</w>': 2, 'yN</w>': 2, 'xi</w>': 6, '</w>': 6, 'D</w>': 5, 'yi</w>': 6, 'K</w>': 6, 'corresponding</w>': 5, 'Sk</w>': 6, 'Model</w>': 5, 'M</w>': 1, 'RM</w>': 4, '</w>': 2, 'learnable</w>': 2, '</w>': 3, 'belonging</w>': 1, 'X</w>': 2, '</w>': 6, '</w>': 1, 'distribution</w>': 4, 'P</w>': 1, 'k0</w>': 2, 'ck0</w>': 1, 'proceeds</w>': 1, 'minimizing</w>': 1, 'negative</w>': 1, 'probability</w>': 1, 'J</w>': 6, 'SGD</w>': 3, 'Training</w>': 4, 'formed</w>': 1, 'selecting</w>': 1, 'choosing</w>': 1, 'act</w>': 1, 'remainder</w>': 1, 'Pseudocode</w>': 1, 'Algorithm</w>': 2, 'computation</w>': 3, 'NC</w>': 6, '</w>': 1, 'NS</w>': 5, 'NQ</w>': 3, 'RANDOMSAMPLE</w>': 4, 'elements</w>': 2, 'uniformly</w>': 1, 'replacement</w>': 1, 'Input</w>': 1, 'Dk</w>': 1, 'Output</w>': 1, 'generated</w>': 2, 'V</w>': 1, '</w>': 6, 'Select</w>': 3, 'DVk</w>': 2, 'Qk</w>': 2, '\\\\</w>': 1, 'Compute</w>': 1, 'logX</w>': 1, 'Update</w>': 1, 'Mixture</w>': 1, 'Density</w>': 1, 'Estimation</w>': 1, 'functions</w>': 4, 'divergences</w>': 5, 'performing</w>': 2, 'mixture</w>': 4, 'estimation</w>': 3, 'exponential</w>': 3, 'd</w>': 2, '</w>': 1, 'differentiable</w>': 1, 'convex</w>': 1, 'Legendre</w>': 1, 'kz</w>': 1, '0k</w>': 1, 'Mahalanobis</w>': 2, 'Prototype</w>': 1, 'viewed</w>': 1, 'terms</w>': 2, 'assigned</w>': 2, 'representative</w>': 1, 'achieving</w>': 1, 'minimal</w>': 2, 'Equation</w>': 3, 'representatives</w>': 1, 'Moreover</w>': 1, 'p</w>': 1, '</w>': 2, 'cumulant</w>': 1, '</w>': 1, 'written</w>': 2, 'uniquely</w>': 1, 'equally</w>': 1, 'assignment</w>': 1, 'prediction</w>': 1, '</w>': 1, 'k</w>': 1, 'effectively</w>': 2, 'assumptions</w>': 1, 'classconditional</w>': 1, 'Reinterpretation</w>': 1, 'gaining</w>': 1, 'insight</w>': 2, 'parameterization</w>': 1, 'term</w>': 1, 'affect</w>': 1, 'probabilities</w>': 1, 'focus</w>': 1, 'primarily</w>': 2, 'spherical</w>': 2, 'Gaussian</w>': 1, 'densities</w>': 1, 'indicate</w>': 2, 'despite</w>': 1, 'equivalence</w>': 3, 'hypothesize</w>': 1, 'linearity</w>': 1, 'Indeed</w>': 1, 'currently</w>': 1, '14</w>': 2, '28</w>': 7, 'Comparison</w>': 2, 'xk</w>': 1, 'fixed</w>': 3, 'greater</w>': 1, 'require</w>': 2, 'partitioning</w>': 2, 'scheme</w>': 1, 'Mensink</w>': 2, 'Rippel</w>': 1, 'methods</w>': 4, 'ordinary</w>': 2, 'extensions</w>': 2, 'decoupling</w>': 1, 'conditional</w>': 2, 'FCE</w>': 5, 'takes</w>': 1, 'account</w>': 1, 'likewise</w>': 1, 'incorporated</w>': 1, 'increase</w>': 1, 'imposes</w>': 1, 'ordering</w>': 1, 'bi</w>': 1, 'directional</w>': 1, 'Instead</w>': 1, 'outline</w>': 1, 'Choices</w>': 1, 'Distance</w>': 1, 'apply</w>': 1, 'permissible</w>': 1, 'conjecture</w>': 2, 'hold</w>': 1, 'Episode</w>': 1, 'composition</w>': 1, 'Nc</w>': 4, 'situation</w>': 1, 'comprised</w>': 1, 'extremely</w>': 1, 'held</w>': 1, 'consideration</w>': 1, 'Shot</w>': 1, 'differs</w>': 1, 'accuracies</w>': 3, 'Omniglot</w>': 8, 'Acc</w>': 4, 'Dist</w>': 2, 'Tune</w>': 2, 'MATCHING</w>': 4, 'NETWORKS</w>': 6, 'Cosine</w>': 9, '98</w>': 9, '93</w>': 3, 'Y</w>': 1, '97</w>': 1, 'NEURAL</w>': 1, 'STATISTICIAN</w>': 1, '99</w>': 2, 'PROTOTYPICAL</w>': 2, 'OURS</w>': 3, 'Euclid</w>': 6, '96</w>': 1, 'advance</w>': 1, 'Modifying</w>': 1, 'deal</w>': 1, 'g</w>': 1, 'An</w>': 2, 'illustration</w>': 1, 'procedure</w>': 6, 'relates</w>': 1, 'come</w>': 3, 'domains</w>': 1, 'helpful</w>': 2, 'fix</w>': 1, 'constrain</w>': 1, 'Experiments</w>': 1, 'miniImageNet</w>': 5, 'ILSVRC</w>': 2, '26</w>': 3, 'UCSD</w>': 2, 'bird</w>': 3, 'CUB</w>': 7, '200</w>': 5, '31</w>': 3, '1623</w>': 1, 'handwritten</w>': 1, 'collected</w>': 1, 'alphabets</w>': 1, 'associated</w>': 1, 'drawn</w>': 1, 'human</w>': 1, 'subject</w>': 1, 'resizing</w>': 1, 'grayscale</w>': 1, 'augmenting</w>': 1, 'rotations</w>': 3, 'multiples</w>': 1, '90</w>': 1, 'degrees</w>': 1, '1200</w>': 1, 'plus</w>': 2, '800</w>': 1, 'mirrors</w>': 1, 'composed</w>': 1, 'blocks</w>': 1, 'block</w>': 2, 'filter</w>': 1, 'convolution</w>': 1, 'batch</w>': 2, 'ReLU</w>': 1, 'All</w>': 2, '11</w>': 2, 'initial</w>': 1, 'rate</w>': 4, 'cut</w>': 1, 'advantageous</w>': 1, 'fewer</w>': 1, 'against</w>': 1, 'baselines</w>': 3, 'statistician</w>': 4, 'derived</w>': 1, '12</w>': 2, '84</w>': 3, 'divided</w>': 2, '600</w>': 3, 'monitoring</w>': 1, 'though</w>': 1, '1600</w>': 1, 'increased</w>': 2, '95</w>': 2, 'confidence</w>': 2, 'intervals</w>': 2, 'BASELINE</w>': 1, 'NEAREST</w>': 1, 'NEIGHBORS</w>': 1, '86</w>': 1, '</w>': 10, '49</w>': 2, '79</w>': 2, '43</w>': 3, '40</w>': 4, '78</w>': 2, '51</w>': 1, '09</w>': 1, '71</w>': 2, '55</w>': 1, '73</w>': 1, 'META</w>': 1, 'LEARNER</w>': 1, '44</w>': 2, '77</w>': 1, '68</w>': 1, '70</w>': 2, '80</w>': 2, 'Accuracy</w>': 2, 'Proto</w>': 2, 'Nets</w>': 3, 'axis</w>': 2, 'indicates</w>': 2, 'configuration</w>': 1, 'Error</w>': 1, 'schedule</w>': 1, 'stops</w>': 1, 'variants</w>': 1, 'Meta</w>': 1, 'Learner</w>': 1, 'margin</w>': 5, 'own</w>': 2, 'difficulty</w>': 1, 'forces</w>': 1, 'grained</w>': 1, 'Also</w>': 1, 'substantially</w>': 1, 'pronounced</w>': 1, 'naturally</w>': 2, 'suited</w>': 1, 'assess</w>': 1, 'suitability</w>': 1, '788</w>': 1, 'species</w>': 2, 'closely</w>': 1, 'Reed</w>': 1, 'preparing</w>': 1, 'Image</w>': 1, 'ALE</w>': 2, 'Fisher</w>': 1, 'SJE</w>': 9, 'AlexNet</w>': 3, 'SAMPLE</w>': 1, 'CLUSTERING</w>': 1, '17</w>': 2, 'GoogLeNet</w>': 5, 'DS</w>': 3, 'DA</w>': 3, 'PROTO</w>': 1, 'NETS</w>': 1, 'divide</w>': 1, '024</w>': 2, 'extracted</w>': 1, 'applying</w>': 2, 'crops</w>': 1, 'horizontally</w>': 1, 'flipped</w>': 1, 'image2</w>': 1, 'At</w>': 1, 'crop</w>': 1, '312</w>': 2, 'continuous</w>': 1, 'attribute</w>': 4, 'attributes</w>': 3, 'characteristics</w>': 1, 'feather</w>': 1, 'patterns</w>': 1, 'domain</w>': 2, 'constructed</w>': 1, 'decay</w>': 1, 'Early</w>': 1, 'stopping</w>': 1, 'utilizing</w>': 2, 'SVM</w>': 1, 'relative</w>': 1, 'vast</w>': 1, 'summarize</w>': 1, 'relevant</w>': 2, 'Neighborhood</w>': 1, 'Components</w>': 1, 'Analysis</w>': 1, 'NCA</w>': 4, 'KNN</w>': 3, 'leave</w>': 1, 'transformed</w>': 2, 'Salakhutdinov</w>': 1, 'Hinton</w>': 1, '27</w>': 2, 'transformation</w>': 2, 'LMNN</w>': 2, 'attempts</w>': 1, 'optimize</w>': 2, 'hinge</w>': 1, 'encourages</w>': 1, 'neighborhood</w>': 1, 'DNet</w>': 1, 'opposed</w>': 1, 'form</w>': 1, 'concise</w>': 1, 'obviates</w>': 1, 'developed</w>': 2, 'rapidly</w>': 1, 'incorporate</w>': 1, 'relies</w>': 1, 'downloaded</w>': 1, 'reedscot</w>': 1, 'cvpr2016</w>': 1, 'linearly</w>': 1, 'embed</w>': 1, 'couple</w>': 1, 'multi</w>': 1, 'modal</w>': 1, 'generalizes</w>': 1, 'dynamics</w>': 1, 'itself</w>': 1, 'goal</w>': 1, 'classifiers</w>': 1, 'dynamically</w>': 1, 'core</w>': 1, 'rely</w>': 1, 'nets</w>': 2, 'secondary</w>': 1, 'generative</w>': 2, 'extends</w>': 1, 'variational</w>': 1, '24</w>': 1, 'component</w>': 1, 'statistic</w>': 5, 'taking</w>': 1, 'sample</w>': 1, 'post</w>': 1, 'approximate</w>': 2, 'posterior</w>': 3, 'Edwards</w>': 1, 'Storkey</w>': 1, 'considering</w>': 2, 'KL</w>': 1, 'inferred</w>': 1, 'Like</w>': 1, 'summary</w>': 1, 'discriminative</w>': 2, 'befits</w>': 1, 'With</w>': 1, 'resembles</w>': 1, 'multimodal</w>': 1, 'Unlike</w>': 1, 'empirical</w>': 2, 'risk</w>': 1, 'Neither</w>': 1, 'nor</w>': 1, 'regularize</w>': 1, 'Conclusion</w>': 1, 'sophisticated</w>': 1, 'although</w>': 1, 'modifying</w>': 1, 'direction</w>': 1, 'future</w>': 1, 'utilize</w>': 1, 'distributions</w>': 1, 'beyond</w>': 1, 'Gaussians</w>': 1, 'preliminary</w>': 1, 'explorations</w>': 1, 'variance</w>': 1, 'gains</w>': 1, 'flexibility</w>': 1, 'requiring</w>': 1, 'fitted</w>': 1, 'Overall</w>': 1, 'simplicity</w>': 1, 'promising</w>': 1}\n",
      "Subwords: ['import</w>']\n",
      "BPE Vocabulary: {'import</w>': 0, 'numpy</w>': 1, 'as</w>': 2, 'np</w>': 3, 'torch</w>': 4, '.</w>': 5, 'nn</w>': 6, 'functional</w>': 7, 'F</w>': 8, 'from</w>': 9, 'utils</w>': 10, 'data</w>': 11, 'DataLoader</w>': 12, ',</w>': 13, 'Dataset</w>': 14, 'cuda</w>': 15, 'amp</w>': 16, 'GradScaler</w>': 17, 'autocast</w>': 18, 'optim</w>': 19, 'Adam</w>': 20, 'def</w>': 21, 'random_projection</w>': 22, '(</w>': 23, 'matrix</w>': 24, 'k</w>': 25, ')</w>': 26, ':</w>': 27, '\"</w>': 28, 'Random</w>': 29, 'projection</w>': 30, 'to</w>': 31, 'reduce</w>': 32, 'dimensionality</w>': 33, 'of</w>': 34, 'dimensions</w>': 35, 'random_matrix</w>': 36, '=</w>': 37, 'randn</w>': 38, 'size</w>': 39, '-</w>': 40, '1</w>': 41, 'device</w>': 42, 'return</w>': 43, 'matmul</w>': 44, 'cur_decomposition</w>': 45, 'projection_dim</w>': 46, '#</w>': 47, 'Change</w>': 48, \"'</w>\": 49, 'argument</w>': 50, 'Applies</w>': 51, 'CUR</w>': 52, 'decomposition</w>': 53, 'with</w>': 54, 'C</w>': 55, 'dimension</w>': 56, 'aligned</w>': 57, 'batch_size</w>': 58, 'seq_length</w>': 59, 'heads</w>': 60, 'dim</w>': 61, 'shape</w>': 62, 'min</w>': 63, '/</w>': 64, '2</w>': 65, 'Use</w>': 66, 'determine</w>': 67, 'zeros</w>': 68, 'R</w>': 69, 'for</w>': 70, 'b</w>': 71, 'in</w>': 72, 'range</w>': 73, 'h</w>': 74, 'col_indices</w>': 75, 'random</w>': 76, 'choice</w>': 77, 'replace</w>': 78, 'False</w>': 79, 'row_indices</w>': 80, '[</w>': 81, ']</w>': 82, 'class</w>': 83, 'PartitionedLinformerAttentionACT</w>': 84, 'Module</w>': 85, '__init__</w>': 86, 'self</w>': 87, 'embed_size</w>': 88, 'sequence_length</w>': 89, 'partition_size</w>': 90, 'super</w>': 91, 'head_dim</w>': 92, 'values</w>': 93, 'Linear</w>': 94, 'bias</w>': 95, 'keys</w>': 96, 'queries</w>': 97, 'value_projection</w>': 98, 'To</w>': 99, 'project</w>': 100, 'match</w>': 101, 'ponder</w>': 102, 'True</w>': 103, 'sigmoid</w>': 104, 'Sigmoid</w>': 105, 'forward</w>': 106, 'x</w>': 107, 'print</w>': 108, 'f</w>': 109, 'BEGIN</w>': 110, 'FORWARD</w>': 111, 'input</w>': 112, '{</w>': 113, '}</w>': 114, 'N</w>': 115, '_</w>': 116, 'x_reshaped</w>': 117, 'view</w>': 118, 'attention_scores</w>': 119, 'after</w>': 120, 'i</w>': 121, '0</w>': 122, 'PARTITION</w>': 123, 'START</w>': 124, 'partition_start</w>': 125, 'partition_end</w>': 126, '+</w>': 127, 'keys_part</w>': 128, 'queries_part</w>': 129, 'C_keys</w>': 130, 'R_queries</w>': 131, 'before</w>': 132, 'ponder_scores</w>': 133, 'Partition</w>': 134, 'Start</w>': 135, 'End</w>': 136, 'HEADS</w>': 137, 'head_queries</w>': 138, 'head_ponder_scores</w>': 139, 'squeeze</w>': 140, 'Correctly</w>': 141, 'expand</w>': 142, 'without</w>': 143, 'adding</w>': 144, 'an</w>': 145, 'unnecessary</w>': 146, 'BEFORE</w>': 147, '1ST</w>': 148, 'EINSUM</w>': 149, 'ponder_scores_permuted</w>': 150, 'permute</w>': 151, '3</w>': 152, 'Move</w>': 153, '128</w>': 154, '8</w>': 155, 'ponder_scores_broadcastable</w>': 156, 'Expand</w>': 157, 'energy</w>': 158, 'einsum</w>': 159, 'bnhd</w>': 160, 'bnhk</w>': 161, '></w>': 162, 'attention_weights</w>': 163, 'softmax</w>': 164, 'AFTER</w>': 165, 'attention</w>': 166, '*</w>': 167, 'attention_corrected</w>': 168, 'Swap</w>': 169, 'and</w>': 170, 'bring</w>': 171, 'next</w>': 172, 'reshape</w>': 173, 'Flatten</w>': 174, 'linear</w>': 175, 'layer</w>': 176, 'projected_values</w>': 177, 'Now</w>': 178, '2ND</w>': 179, 'Combine</w>': 180, 'then</w>': 181, 'pass</w>': 182, 'through</w>': 183, 'the</w>': 184, 'final</w>': 185, 'out</w>': 186, 'bnhp</w>': 187, 'bnh</w>': 188, 'Example</w>': 189, 'usage</w>': 190, 'model</w>': 191, '512</w>': 192, '1024</w>': 193, '256</w>': 194, 'input_tensor</w>': 195, 'rand</w>': 196, 'output</w>': 197, 'if</w>': 198, 'is_available</w>': 199, 'else</w>': 200, 'cpu</w>': 201, 'Assuming</w>': 202, 'tensor</w>': 203, 'is</w>': 204, 'Size</w>': 205, 'each</w>': 206, 'word</w>': 207, 'embedding</w>': 208, 'Number</w>': 209, 'Length</w>': 210, 'sequence</w>': 211, 'Dimension</w>': 212, 'embeddings</w>': 213, 'partitions</w>': 214, 'processing</w>': 215, 'num_classes</w>': 216, '10</w>': 217, 'Instantiate</w>': 218, 'attention_model</w>': 219, 'AttentionClassifier</w>': 220, 'global_pool</w>': 221, 'AdaptiveAvgPool1d</w>': 222, 'Global</w>': 223, 'pooling</w>': 224, 's</w>': 225, 'here</w>': 226, ';</w>': 227, 'adjust</w>': 228, 'necessary</w>': 229, 'classifier</w>': 230, 'Classifier</w>': 231, 'Ensure</w>': 232, 'has</w>': 233, 'num_heads</w>': 234, 'You</w>': 235, 'might</w>': 236, 'need</w>': 237, 'this</w>': 238, 'depending</w>': 239, 'on</w>': 240, 'your</w>': 241, 'exact</w>': 242, 'resulting</w>': 243, 'Pass</w>': 244, 'appropriate</w>': 245, 'Create</w>': 246, 'a</w>': 247, 'pinned</w>': 248, 'memory</w>': 249, 'MyDataset</w>': 250, 'Initialize</w>': 251, '__len__</w>': 252, 'Return</w>': 253, 'dataset</w>': 254, '100</w>': 255, '__getitem__</w>': 256, 'idx</w>': 257, 'Pad</w>': 258, 'length</w>': 259, 'ensure</w>': 260, 'This</w>': 261, 'simplified</w>': 262, 'example</w>': 263, 'according</w>': 264, 'actual</w>': 265, 'Padded</w>': 266, 'modeled</w>': 267, 'expected</w>': 268, 'target</w>': 269, 'randint</w>': 270, 'item</w>': 271, 'Generate</w>': 272, 'index</w>': 273, '9</w>': 274, 'classes</w>': 275, 'loader</w>': 276, '32</w>': 277, 'shuffle</w>': 278, 'pin_memory</w>': 279, 'optimizer</w>': 280, 'parameters</w>': 281, 'lr</w>': 282, '1e</w>': 283, 'scaler</w>': 284, 'epoch</w>': 285, 'epochs</w>': 286, 'inputs</w>': 287, 'targets</w>': 288, 'correct</w>': 289, 'non_blocking</w>': 290, 'zero_grad</w>': 291, 'Forward</w>': 292, 'predictions</w>': 293, 'Calculate</w>': 294, 'loss</w>': 295, 'assuming</w>': 296, 'classification</w>': 297, 'task</w>': 298, 'are</w>': 299, 'indices</w>': 300, 'cross_entropy</w>': 301, 'Backward</w>': 302, 'step</w>': 303, 'using</w>': 304, 'gradient</w>': 305, 'scaling</w>': 306, 'mixed</w>': 307, 'precision</w>': 308, 'scale</w>': 309, 'backward</w>': 310, 'update</w>': 311, 'Epoch</w>': 312, 'Loss</w>': 313, 'Fast</w>': 314, 'WordPiece</w>': 315, 'Tokenization</w>': 316, 'Xinying</w>': 317, 'Song</w>': 318, '</w>': 319, 'Alex</w>': 320, 'Salcianu</w>': 321, 'Yang</w>': 322, '</w>': 323, '</w>': 324, 'Dave</w>': 325, 'Dopson</w>': 326, 'Denny</w>': 327, 'Zhou</w>': 328, 'Google</w>': 329, 'Research</w>': 330, 'Mountain</w>': 331, 'View</w>': 332, 'CA</w>': 333, 'xysong</w>': 334, 'salcianu</w>': 335, 'ddopson</w>': 336, 'dennyzhou</w>': 337, '@</w>': 338, 'google</w>': 339, 'com</w>': 340, 'Kuaishou</w>': 341, 'Technology</w>': 342, 'Beijing</w>': 343, 'China</w>': 344, 'yangsong</w>': 345, 'kuaishou</w>': 346, 'Abstract</w>': 347, 'fundamental</w>': 348, 'preprocessing</w>': 349, 'almost</w>': 350, 'all</w>': 351, 'NLP</w>': 352, 'tasks</w>': 353, 'In</w>': 354, 'paper</w>': 355, 'we</w>': 356, 'propose</w>': 357, 'efficient</w>': 358, 'algorithms</w>': 359, 'tokenization</w>': 360, 'used</w>': 361, 'BERT</w>': 362, 'singleword</w>': 363, 'general</w>': 364, 'text</w>': 365, 'e</w>': 366, 'g</w>': 367, 'sentence</w>': 368, 'When</w>': 369, 'tokenizing</w>': 370, 'single</w>': 371, 'uses</w>': 372, 'longest</w>': 373, 'matchfirst</w>': 374, 'strategy</w>': 375, 'known</w>': 376, 'maximum</w>': 377, 'matching</w>': 378, 'The</w>': 379, 'best</w>': 380, 'so</w>': 381, 'far</w>': 382, '</w>': 383, '</w>': 384, 'where</w>': 385, 'or</w>': 386, '</w>': 387, '</w>': 388, 'vocabulary</w>': 389, 'token</w>': 390, 'We</w>': 391, 'novel</w>': 392, 'algorithm</w>': 393, 'whose</w>': 394, 'complexity</w>': 395, 'strictly</w>': 396, 'Our</w>': 397, 'method</w>': 398, 'inspired</w>': 399, 'by</w>': 400, 'Aho</w>': 401, 'Corasick</w>': 402, 'introduce</w>': 403, 'additional</w>': 404, 'linkages</w>': 405, 'top</w>': 406, 'trie</w>': 407, 'built</w>': 408, 'allowing</w>': 409, 'smart</w>': 410, 'transitions</w>': 411, 'when</w>': 412, 'cannot</w>': 413, 'continue</w>': 414, 'For</w>': 415, 'further</w>': 416, 'that</w>': 417, 'combines</w>': 418, 'pre</w>': 419, 'splitting</w>': 420, 'into</w>': 421, 'words</w>': 422, 'our</w>': 423, 'time</w>': 424, 'Experimental</w>': 425, 'results</w>': 426, 'show</w>': 427, '2x</w>': 428, 'faster</w>': 429, 'than</w>': 430, 'HuggingFace</w>': 431, 'Tokenizers</w>': 432, '5</w>': 433, '1x</w>': 434, 'TensorFlow</w>': 435, 'Text</w>': 436, 'average</w>': 437, 'Introduction</w>': 438, 'process</w>': 439, 'smaller</w>': 440, 'units</w>': 441, 'called</w>': 442, 'tokens</w>': 443, 'It</w>': 444, 'applications</w>': 445, 'sentiment</w>': 446, 'analysis</w>': 447, 'question</w>': 448, 'answering</w>': 449, 'machine</w>': 450, 'translation</w>': 451, 'information</w>': 452, 'retrieval</w>': 453, 'etc</w>': 454, 'Modern</w>': 455, 'models</w>': 456, 'like</w>': 457, 'Devlin</w>': 458, 'et</w>': 459, 'al</w>': 460, '2019</w>': 461, 'GPT</w>': 462, 'Brown</w>': 463, '2020</w>': 464, 'XLNet</w>': 465, 'tokenize</w>': 466, 'subword</w>': 467, 'Schuster</w>': 468, 'Nakajima</w>': 469, '2012</w>': 470, 'Sennrich</w>': 471, '2016</w>': 472, 'Kudo</w>': 473, '2018</w>': 474, 'As</w>': 475, 'midpoint</w>': 476, 'between</w>': 477, 'characters</w>': 478, 'retain</w>': 479, 'linguistic</w>': 480, 'meaning</w>': 481, 'morphemes</w>': 482, 'while</w>': 483, 'alleviating</w>': 484, 'situations</w>': 485, 'even</w>': 486, 'relatively</w>': 487, 'small</w>': 488, 'conducted</w>': 489, 'working</w>': 490, 'at</w>': 491, 'Given</w>': 492, 'Unicode</w>': 493, 'already</w>': 494, 'been</w>': 495, 'cleaned</w>': 496, 'up</w>': 497, 'normalized</w>': 498, 'two</w>': 499, 'steps</w>': 500, 'punctuation</w>': 501, 'whitespaces</w>': 502, 'wordpieces</w>': 503, 'greedy</w>': 504, 'first</w>': 505, 'iteratively</w>': 506, 'pick</w>': 507, 'prefix</w>': 508, 'remaining</w>': 509, 'matches</w>': 510, 'well</w>': 511, 'Maximum</w>': 512, 'Matching</w>': 513, 'MaxMatch</w>': 514, 'Palmer</w>': 515, '2000</w>': 516, 'which</w>': 517, 'also</w>': 518, 'Chinese</w>': 519, 'segmentation</w>': 520, 'since</w>': 521, '1980s</w>': 522, 'Liu</w>': 523, 'Liang</w>': 524, '1986</w>': 525, 'Despite</w>': 526, 'its</w>': 527, 'wide</w>': 528, 'use</w>': 529, 'decades</w>': 530, 'knowledge</w>': 531, 'most</w>': 532, 'see</w>': 533, 'Section</w>': 534, '</w>': 535, 'worth</w>': 536, 'noting</w>': 537, 'latter</w>': 538, 'vocabularyspecific</w>': 539, 'multiplicative</w>': 540, 'factor</w>': 541, 'can</w>': 542, 'be</w>': 543, 'large</w>': 544, 'contains</w>': 545, 'long</w>': 546, 'LinMaxMatch</w>': 547, 'any</w>': 548, 'specific</w>': 549, 'factors</w>': 550, 'Inspired</w>': 551, '1975</w>': 552, 'organize</w>': 553, 'Fredkin</w>': 554, '1960</w>': 555, 'precomputed</w>': 556, 'failure</w>': 557, 'links</w>': 558, 'pops</w>': 559, 'During</w>': 560, 'character</w>': 561, 'does</w>': 562, 'not</w>': 563, 'edge</w>': 564, 'perform</w>': 565, 'avoid</w>': 566, 'backtracking</w>': 567, 'earlier</w>': 568, 'involves</w>': 569, 'collecting</w>': 570, 'recognized</w>': 571, 'moving</w>': 572, 'node</w>': 573, 'via</w>': 574, 'link</w>': 575, 'same</w>': 576, 'referred</w>': 577, 'end</w>': 578, 'E2E</w>': 579, 'arXiv</w>': 580, '15524v3</w>': 581, 'cs</w>': 582, 'CL</w>': 583, 'Oct</w>': 584, '2021</w>': 585, '4</w>': 586, 'Although</w>': 587, 'other</w>': 588, 'it</w>': 589, 'still</w>': 590, 'improving</w>': 591, 'performance</w>': 592, 'prerequisite</w>': 593, 'improvement</w>': 594, 'efficiency</w>': 595, 'helps</w>': 596, 'latency</w>': 597, 'entire</w>': 598, 'inference</w>': 599, 'One</w>': 600, 'potential</w>': 601, 'impact</w>': 602, 'work</w>': 603, 'mobile</w>': 604, 'Ondevice</w>': 605, 'generally</w>': 606, 'highly</w>': 607, 'optimized</w>': 608, 'reducing</w>': 609, 'distilling</w>': 610, 'compressing</w>': 611, 'larger</w>': 612, 'Thus</w>': 613, 'significant</w>': 614, 'Another</w>': 615, 'aggregate</w>': 616, 'computational</w>': 617, 'savings</w>': 618, 'Web</w>': 619, 'services</w>': 620, 'Facebook</w>': 621, 'Twitter</w>': 622, 'power</w>': 623, 'search</w>': 624, 'nowadays</w>': 625, 'serves</w>': 626, 'billions</w>': 627, 'per</w>': 628, 'day</w>': 629, 'processes</w>': 630, 'hundreds</w>': 631, 'trillions</w>': 632, 'pages</w>': 633, 'building</w>': 634, 'By</w>': 635, 'employing</w>': 636, 'system</w>': 637, 'would</w>': 638, 'material</w>': 639, 'benefits</w>': 640, 'environment</w>': 641, 'less</w>': 642, 'consumption</w>': 643, 'makes</w>': 644, 'theoretical</w>': 645, 'contribution</w>': 646, 'proposed</w>': 647, 'solves</w>': 648, 'old</w>': 649, 'problem</w>': 650, 'optimal</w>': 651, 'idea</w>': 652, 'applicable</w>': 653, 'string</w>': 654, 'rewriting</w>': 655, 'problems</w>': 656, '6</w>': 657, 'code</w>': 658, 'will</w>': 659, 'available</w>': 660, 'https</w>': 661, 'www</w>': 662, 'tensorflow</w>': 663, 'org</w>': 664, 'Related</w>': 665, 'Work</w>': 666, 'CWS</w>': 667, 'Recent</w>': 668, 'focuses</w>': 669, 'learning</w>': 670, 'based</w>': 671, 'approaches</w>': 672, 'but</w>': 673, 'remains</w>': 674, 'commonly</w>': 675, 'referenced</w>': 676, 'baseline</w>': 677, 'Chang</w>': 678, '2008</w>': 679, 'More</w>': 680, 'recently</w>': 681, 'techniques</w>': 682, 'have</w>': 683, 'become</w>': 684, 'near</w>': 685, 'universal</w>': 686, 'feature</w>': 687, 'modern</w>': 688, 'including</w>': 689, 'Common</w>': 690, 'include</w>': 691, 'Byte</w>': 692, 'Pair</w>': 693, 'Encoding</w>': 694, 'BPE</w>': 695, 'SentencePiece</w>': 696, 'unigram</w>': 697, 'lan1https</w>': 698, 'blog</w>': 699, 'products</w>': 700, 'language</w>': 701, 'understanding</w>': 702, 'bert</w>': 703, 'guage</w>': 704, 'modeling</w>': 705, 'widely</w>': 706, 'adopted</w>': 707, 'original</w>': 708, 'starts</w>': 709, 'possible</w>': 710, 'decrements</w>': 711, 'Jie</w>': 712, '1989</w>': 713, 'A</w>': 714, 'variant</w>': 715, 'shortest</w>': 716, 'substring</w>': 717, 'increases</w>': 718, 'Webster</w>': 719, 'Kit</w>': 720, '1992</w>': 721, 'Reps</w>': 722, '1998</w>': 723, 'Sassano</w>': 724, '2014</w>': 725, 'worst</w>': 726, 'case</w>': 727, 'previous</w>': 728, 'higher</w>': 729, '23</w>': 730, 'notations</w>': 731, 'Lookup</w>': 732, 't</w>': 733, 'c</w>': 734, 'Figure</w>': 735, 'their</w>': 736, 'may</w>': 737, 'take</w>': 738, 'similar</w>': 739, 'recognizes</w>': 740, 'regular</w>': 741, 'expressions</w>': 742, 'context</w>': 743, 'compilers</w>': 744, '|</w>': 745, '</w>': 746, 'number</w>': 747, 'states</w>': 748, 'automaton</w>': 749, 'grammar</w>': 750, 'If</w>': 751, 'applied</w>': 752, 'finite</w>': 753, 'strings</w>': 754, 'refined</w>': 755, 'designed</w>': 756, 'address</w>': 757, 'different</w>': 758, 'scenario</w>': 759, 'every</w>': 760, 'finds</w>': 761, 'quadratic</w>': 762, 'overall</w>': 763, 'comparison</w>': 764, 'achieves</w>': 765, 'due</w>': 766, 'definition</w>': 767, 'newly</w>': 768, 'introduced</w>': 769, 'way</w>': 770, 'emitting</w>': 771, 'clarifying</w>': 772, 'difference</w>': 773, 'tabular</w>': 774, 'solution</w>': 775, 'table</w>': 776, 'failed_previously</w>': 777, 'store</w>': 778, 'whether</w>': 779, 'state</w>': 780, '<</w>': 781, 'q</w>': 782, 'seen</w>': 783, 'failed</w>': 784, 'attempt</w>': 785, 'position</w>': 786, 'wasteful</w>': 787, 'revisits</w>': 788, 'entries</w>': 789, 'depend</w>': 790, 'both</w>': 791, 'contrast</w>': 792, 'capture</w>': 793, '2The</w>': 794, 'depends</w>': 795, 'implementation</w>': 796, 'details</w>': 797, 'hashes</w>': 798, 'computed</w>': 799, 'scratch</w>': 800, 'incrementally</w>': 801, 'how</w>': 802, 'substrings</w>': 803, 'searched</w>': 804, '3Previous</w>': 805, 'studies</w>': 806, 'usually</w>': 807, 'do</w>': 808, 'explicitly</w>': 809, 'related</w>': 810, 'just</w>': 811, 'treat</w>': 812, 'hidden</w>': 813, 'constant</w>': 814, 'transit</w>': 815, 'Definition</w>': 816, 'they</w>': 817, 'only</w>': 818, 'independent</w>': 819, 'Finally</w>': 820, 'discuss</w>': 821, 'Note</w>': 822, 'topic</w>': 823, 'found</w>': 824, 'Viterbi</w>': 825, '1967</w>': 826, 'implemented</w>': 827, 'ways</w>': 828, 'enumerate</w>': 829, 'symbol</w>': 830, 'pairs</w>': 831, 'order</w>': 832, 'were</w>': 833, 'added</w>': 834, 'phase</w>': 835, 'pair</w>': 836, 'scan</w>': 837, 'current</w>': 838, 'occurrences</w>': 839, 'merged</w>': 840, '</w>': 841, 'approach</w>': 842, 'repeatedly</w>': 843, 'select</w>': 844, 'symbols</w>': 845, 'highest</w>': 846, 'priority</w>': 847, 'frequency</w>': 848, 'Using</w>': 849, 'heap</w>': 850, 'done</w>': 851, '</w>': 852, 'Time</w>': 853, 'Single</w>': 854, 'Word</w>': 855, 'section</w>': 856, 'present</w>': 857, 'Background</w>': 858, 'Notations</w>': 859, 'tokenizes</w>': 860, 'until</w>': 861, 'segmented</w>': 862, 'tokenized</w>': 863, 'mapped</w>': 864, 'special</w>': 865, 'unk</w>': 866, 'distinguishes</w>': 867, 'start</w>': 868, 'starting</w>': 869, 'middle</w>': 870, 'suffix</w>': 871, 'indicator</w>': 872, 'denoted</w>': 873, '</w>': 874, 'works</w>': 875, 'arbitrary</w>': 876, 'empty</w>': 877, 'no</w>': 878, 'distinction</w>': 879, 'kinds</w>': 880, 'johanson</w>': 881, 'johan</w>': 882, 'son</w>': 883, 'running</w>': 884, 'Table</w>': 885, 'summarizes</w>': 886, 'construct</w>': 887, '</w>': 888, '</w>': 889, '</w>': 890, '</w>': 891, 'denote</w>': 892, 'label</w>': 893, 'there</w>': 894, 'outgoing</w>': 895, '4The</w>': 896, 'construction</w>': 897, 'outside</w>': 898, 'scope</w>': 899, 'refer</w>': 900, 'interested</w>': 901, 'reader</w>': 902, '</w>': 903, 'Let</w>': 904, '</w>': 905, 'represented</w>': 906, 'obtained</w>': 907, 'concatenating</w>': 908, 'labels</w>': 909, 'along</w>': 910, 'path</w>': 911, 'root</w>': 912, '</w>': 913, 'Obviously</w>': 914, '</w>': 915, '</w>': 916, 'denotes</w>': 917, 'depth</w>': 918, 'defined</w>': 919, 'excluding</w>': 920, 'Hence</w>': 921, '1a</w>': 922, 'nodes</w>': 923, 'Symbol</w>': 924, 'Meaning</w>': 925, 'unkown</w>': 926, '</w>': 927, '</w>': 928, 'whitespace</w>': 929, 'Trie</w>': 930, 'often</w>': 931, 'parent</w>': 932, 'Null</w>': 933, '</w>': 934, '</w>': 935, 'Failure</w>': 936, '</w>': 937, 'sum</w>': 938, 'lengths</w>': 939, 'Intuition</w>': 940, 'motivate</w>': 941, 'let</w>': 942, 'consider</w>': 943, 'alternative</w>': 944, 'simple</w>': 945, 'searching</w>': 946, 'iterates</w>': 947, 'over</w>': 948, 'left</w>': 949, 'right</w>': 950, 'following</w>': 951, 'find</w>': 952, 'prefixes</w>': 953, 'Consider</w>': 954, 'abcdz</w>': 955, 'dz</w>': 956, 'Starting</w>': 957, 'follow</w>': 958, 'edges</w>': 959, 'd</w>': 960, 'arriving</w>': 961, 'No</w>': 962, 'exits</w>': 963, 'z</w>': 964, '</w>': 965, 'challenge</w>': 966, 'tokenizer</w>': 967, 'detokenizer</w>': 968, 'Neural</w>': 969, 'Processing</w>': 970, 'Taku</w>': 971, 'John</w>': 972, 'Richardson</w>': 973, 'Inc</w>': 974, 'taku</w>': 975, 'johnri</w>': 976, 'describes</w>': 977, 'Machine</w>': 978, 'Translation</w>': 979, 'provides</w>': 980, 'open</w>': 981, 'source</w>': 982, 'Python</w>': 983, 'implementations</w>': 984, 'While</w>': 985, 'existing</w>': 986, 'tools</w>': 987, 'assume</w>': 988, 'sequences</w>': 989, 'train</w>': 990, 'directly</w>': 991, 'raw</w>': 992, 'sentences</w>': 993, 'allows</w>': 994, 'us</w>': 995, 'make</w>': 996, 'purely</w>': 997, 'validation</w>': 998, 'experiment</w>': 999, 'NMT</w>': 1000, 'English</w>': 1001, 'Japanese</w>': 1002, 'achieve</w>': 1003, 'comparable</w>': 1004, 'accuracy</w>': 1005, 'direct</w>': 1006, 'training</w>': 1007, 'compare</w>': 1008, 'various</w>': 1009, 'configurations</w>': 1010, 'under</w>': 1011, 'Apache</w>': 1012, 'license</w>': 1013, 'github</w>': 1014, 'sentencepiece</w>': 1015, 'Deep</w>': 1016, 'neural</w>': 1017, 'networks</w>': 1018, 'demonstrating</w>': 1019, 'Natural</w>': 1020, 'Language</w>': 1021, 'Bahdanau</w>': 1022, 'Luong</w>': 1023, '2015</w>': 1024, 'Wu</w>': 1025, 'Vaswani</w>': 1026, '2017</w>': 1027, 'especially</w>': 1028, 'gained</w>': 1029, 'increasing</w>': 1030, 'popularity</w>': 1031, 'leverage</w>': 1032, 'translations</w>': 1033, 'toend</w>': 1034, 'architecture</w>': 1035, 'shown</w>': 1036, 'remarkable</w>': 1037, 'several</w>': 1038, 'shared</w>': 1039, 'Denkowski</w>': 1040, 'Neubig</w>': 1041, 'Nakazawa</w>': 1042, 'effective</w>': 1043, 'had</w>': 1044, 'strong</w>': 1045, 'influence</w>': 1046, 'such</w>': 1047, 'dialog</w>': 1048, 'generation</w>': 1049, 'Vinyals</w>': 1050, 'Le</w>': 1051, 'automatic</w>': 1052, 'summarization</w>': 1053, 'Rush</w>': 1054, 'potentially</w>': 1055, 'many</w>': 1056, 'systems</w>': 1057, 'relying</w>': 1058, 'dependent</w>': 1059, 'postprocessors</w>': 1060, 'traditional</w>': 1061, 'statistical</w>': 1062, 'SMT</w>': 1063, 'Moses1</w>': 1064, 'de</w>': 1065, 'facto</w>': 1066, 'standard</w>': 1067, 'toolkit</w>': 1068, 'implements</w>': 1069, 'reasonably</w>': 1070, 'useful</w>': 1071, 'postprocessor</w>': 1072, 'However</w>': 1073, 'upon</w>': 1074, 'hand</w>': 1075, 'crafted</w>': 1076, 'rules</w>': 1077, 'effectiveness</w>': 1078, 'proven</w>': 1079, 'addition</w>': 1080, 'these</w>': 1081, 'mainly</w>': 1082, 'European</w>': 1083, 'languages</w>': 1084, 'non</w>': 1085, 'Korean</w>': 1086, 'run</w>': 1087, 'segmenters</w>': 1088, 'independently</w>': 1089, 'Such</w>': 1090, 'languagedependent</w>': 1091, 'hard</w>': 1092, 'multilingual</w>': 1093, 'Johnson</w>': 1094, 'carefully</w>': 1095, 'manage</w>': 1096, 'internal</w>': 1097, 'deep</w>': 1098, 'architectures</w>': 1099, 'languageindependent</w>': 1100, 'standardized</w>': 1101, 'more</w>': 1102, 'agnostic</w>': 1103, 'becoming</w>': 1104, 'important</w>': 1105, 'community</w>': 1106, 'develop</w>': 1107, 'reproducible</w>': 1108, 'easily</w>': 1109, 'integrated</w>': 1110, 'Network</w>': 1111, 'demo</w>': 1112, 'describe</w>': 1113, 'Networkbased</w>': 1114, 'predetermined</w>': 1115, 'prior</w>': 1116, 'byte</w>': 1117, 'pairencoding</w>': 1118, 'extension</w>': 1119, 'enables</w>': 1120, 'languagespecific</w>': 1121, '1http</w>': 1122, 'statmt</w>': 1123, 'moses</w>': 1124, '1808</w>': 1125, '06226v1</w>': 1126, '19</w>': 1127, 'Aug</w>': 1128, '%</w>': 1129, 'spm_train</w>': 1130, '</w>': 1131, 'txt</w>': 1132, 'model_prefix</w>': 1133, 'spm</w>': 1134, 'vocab_size</w>': 1135, '1000</w>': 1136, 'echo</w>': 1137, 'Hello</w>': 1138, 'world</w>': 1139, 'spm_encode</w>': 1140, '_He</w>': 1141, 'll</w>': 1142, 'o</w>': 1143, '_world</w>': 1144, 'output_format</w>': 1145, 'id</w>': 1146, '151</w>': 1147, '88</w>': 1148, '21</w>': 1149, '887</w>': 1150, 'spm_decode</w>': 1151, 'input_format</w>': 1152, 'Commandline</w>': 1153, 'System</w>': 1154, 'Overview</w>': 1155, 'comprises</w>': 1156, 'four</w>': 1157, 'main</w>': 1158, 'components</w>': 1159, 'Normalizer</w>': 1160, 'Trainer</w>': 1161, 'Encoder</w>': 1162, 'Decoder</w>': 1163, 'module</w>': 1164, 'normalize</w>': 1165, 'semanticallyequivalent</w>': 1166, 'canonical</w>': 1167, 'forms</w>': 1168, 'trains</w>': 1169, 'corpus</w>': 1170, 'specify</w>': 1171, 'type</w>': 1172, 'parameter</w>': 1173, 'internally</w>': 1174, 'executes</w>': 1175, 'trained</w>': 1176, 'converts</w>': 1177, 'roles</w>': 1178, 'correspond</w>': 1179, 'postprocessing</w>': 1180, 'detokenization</w>': 1181, 'respectively</w>': 1182, 'call</w>': 1183, 'them</w>': 1184, 'encoding</w>': 1185, 'decoding</w>': 1186, 'manages</w>': 1187, 'mapping</w>': 1188, 'convert</w>': 1189, 'vice</w>': 1190, 'versa</w>': 1191, 'Direct</w>': 1192, '?</w>': 1193, 'presents</w>': 1194, 'reversibly</w>': 1195, 'converted</w>': 1196, 'Library</w>': 1197, 'Design</w>': 1198, 'design</w>': 1199, 'command</w>': 1200, 'line</w>': 1201, 'snippets</w>': 1202, 'Lossless</w>': 1203, '</w>': 1204, 'Raw</w>': 1205, 'Tokenized</w>': 1206, 'observation</w>': 1207, 'convertible</w>': 1208, 'space</w>': 1209, 'exists</w>': 1210, '</w>': 1211, '</w>': 1212, 'kept</w>': 1213, 'Detokenization</w>': 1214, 'restore</w>': 1215, 'irreversible</w>': 1216, 'operations</w>': 1217, 'puts</w>': 1218, 'primitive</w>': 1219, 'spaces</w>': 1220, 'required</w>': 1221, '</w>': 1222, '</w>': 1223, '</w>': 1224, '</w>': 1225, 'manually</w>': 1226, 'expensive</w>': 1227, 'write</w>': 1228, 'maintain</w>': 1229, 'inverse</w>': 1230, 'operation</w>': 1231, 'Decode</w>': 1232, 'Encode</w>': 1233, 'Normalize</w>': 1234, 'lossless</w>': 1235, 'reproduce</w>': 1236, 'preserved</w>': 1237, 'encoder</w>': 1238, 'basic</w>': 1239, 'Even</w>': 1240, 'handled</w>': 1241, 'normal</w>': 1242, 'sake</w>': 1243, 'clarity</w>': 1244, 'escapes</w>': 1245, 'meta</w>': 1246, 'U</w>': 1247, '2581</w>': 1248, 'Hello_world</w>': 1249, '_wor</w>': 1250, 'ld</w>': 1251, 'detokenize</w>': 1252, 'ambiguities</w>': 1253, 'detok</w>': 1254, 'join</w>': 1255, 'should</w>': 1256, 'noted</w>': 1257, 'nmt2</w>': 1258, 'adopts</w>': 1259, 'representation</w>': 1260, 'subwords</w>': 1261, 'intra</w>': 1262, 'boundary</w>': 1263, 'marker</w>': 1264, '2https</w>': 1265, 'rsennrich</w>': 1266, 'nmt</w>': 1267, 'wor</w>': 1268, 'always</w>': 1269, 'ambiguity</w>': 1270, 'treatment</w>': 1271, 'specifically</w>': 1272, 'encode</w>': 1273, 'consecutive</w>': 1274, 'Efficient</w>': 1275, 'Existing</w>': 1276, 'was</w>': 1277, 'pretokenization</w>': 1278, 'difficult</w>': 1279, 'employs</w>': 1280, 'speed</w>': 1281, 'amount</w>': 1282, 'given</w>': 1283, 'requires</w>': 1284, 'O</w>': 1285, 'N2</w>': 1286, 'cost</w>': 1287, 'naively</w>': 1288, 'iteration</w>': 1289, 'log</w>': 1290, 'managed</w>': 1291, 'binary</w>': 1292, 'queue</w>': 1293, 'complexities</w>': 1294, 'Vocabulary</w>': 1295, 'management</w>': 1296, 'specified</w>': 1297, 'flag</w>': 1298, 'specifies</w>': 1299, 'merge</w>': 1300, 'reserves</w>': 1301, 'ids</w>': 1302, 'unknown</w>': 1303, 'BOS</w>': 1304, 'EOS</w>': 1305, 'padding</w>': 1306, 'pad</w>': 1307, 'Their</w>': 1308, 'configured</w>': 1309, 'flags</w>': 1310, 'define</w>': 1311, 'custom</w>': 1312, 'contextual</w>': 1313, 'virtual</w>': 1314, 'Examples</w>': 1315, 'languageindicators</w>': 1316, '2ja</w>': 1317, '2de</w>': 1318, '41</w>': 1319, '302</w>': 1320, '300</w>': 1321, 'tab</w>': 1322, '1EA6</w>': 1323, '301</w>': 1324, '1EA4</w>': 1325, 'Custom</w>': 1326, 'normalization</w>': 1327, 'rule</w>': 1328, 'TSV</w>': 1329, 'Customizable</w>': 1330, 'Character</w>': 1331, 'handling</w>': 1332, 'real</w>': 1333, 'consists</w>': 1334, 'semantically</w>': 1335, 'equivalent</w>': 1336, 'fullwidth</w>': 1337, 'Latin</w>': 1338, 'ASCII</w>': 1339, 'Lowercasing</w>': 1340, 'application</w>': 1341, 'Recently</w>': 1342, 'Normalization</w>': 1343, 'Forms</w>': 1344, 'NFC</w>': 1345, 'NFKC</w>': 1346, 'because</w>': 1347, 'better</w>': 1348, 'reproducibility</w>': 1349, 'support</w>': 1350, 'default</w>': 1351, 'normalizes</w>': 1352, 'normalization_rule_name</w>': 1353, 'nfkc</w>': 1354, 'Sentencepiece</w>': 1355, 'leftmost</w>': 1356, 'compiled</w>': 1357, 'transducer</w>': 1358, 'normalization3</w>': 1359, 'supports</w>': 1360, 'file</w>': 1361, 'shows</w>': 1362, '1EA64</w>': 1363, 'conversion</w>': 1364, 'User</w>': 1365, 'files</w>': 1366, 'normalization_rule_tsv</w>': 1367, 'Task</w>': 1368, 'extending</w>': 1369, 'provided</w>': 1370, 'package</w>': 1371, 'Self</w>': 1372, 'contained</w>': 1373, 'researchers</w>': 1374, 'pretrained</w>': 1375, 'reproduciblity</w>': 1376, '3The</w>': 1377, 'CCC</w>': 1378, 'Canonical</w>': 1379, 'Combining</w>': 1380, 'Class</w>': 1381, 'reordering</w>': 1382, 'handle</w>': 1383, 'full</w>': 1384, 'subset</w>': 1385, '4Note</w>': 1386, 'tabs</w>': 1387, 'delimiter</w>': 1388, 'individual</w>': 1389, 'experimental</w>': 1390, 'stated</w>': 1391, 'preprocessed</w>': 1392, 'Post</w>': 1393, 'reported</w>': 1394, 'subtle</w>': 1395, 'differences</w>': 1396, 'schemes</w>': 1397, 'change</w>': 1398, 'BLEU</w>': 1399, 'scores</w>': 1400, 'Moses</w>': 1401, 'guaranteed</w>': 1402, 'settings</w>': 1403, 'unless</w>': 1404, 'version</w>': 1405, 'clearly</w>': 1406, 'Strictly</w>': 1407, 'speaking</w>': 1408, 'yield</w>': 1409, 'Ideally</w>': 1410, 'must</w>': 1411, 'embedded</w>': 1412, 'manner</w>': 1413, 'setting</w>': 1414, 'includes</w>': 1415, 'behavior</w>': 1416, 'determined</w>': 1417, 'external</w>': 1418, 'dependencies</w>': 1419, 'guarantees</w>': 1420, 'perfect</w>': 1421, 'distribute</w>': 1422, 'part</w>': 1423, 'developers</w>': 1424, 'refine</w>': 1425, 'having</w>': 1426, 'worry</w>': 1427, 'about</w>': 1428, 'breaking</w>': 1429, 'behaviors</w>': 1430, 'stored</w>': 1431, 'wire</w>': 1432, 'format</w>': 1433, 'Protocol</w>': 1434, 'buffer5</w>': 1435, 'platform</w>': 1436, 'neutral</w>': 1437, 'extensible</w>': 1438, 'mechanism</w>': 1439, 'serializing</w>': 1440, 'structured</w>': 1441, 'buffers</w>': 1442, 'help</w>': 1443, 'safely</w>': 1444, 'serialize</w>': 1445, 'keeping</w>': 1446, 'compatibility</w>': 1447, 'extensibility</w>': 1448, 'API</w>': 1449, 'fly</w>': 1450, 'considered</w>': 1451, 'offline</w>': 1452, 'Prior</w>': 1453, 'standalone</w>': 1454, 'preprocessor</w>': 1455, 'off</w>': 1456, 'First</w>': 1457, 'user</w>': 1458, 'facing</w>': 1459, 'preprocess</w>': 1460, 'Second</w>': 1461, 'employ</w>': 1462, 'subsentence</w>': 1463, 'level</w>': 1464, 'augmentation</w>': 1465, 'noise</w>': 1466, 'injection</w>': 1467, 'aim</w>': 1468, 'robustness</w>': 1469, 'There</w>': 1470, 'inject</w>': 1471, 'ran5https</w>': 1472, 'protocol</w>': 1473, 'sentencepiece_processor</w>': 1474, 'sentencepiece_trainer</w>': 1475, 'SentencePieceTrainer</w>': 1476, 'Train</w>': 1477, 'SentencePieceProcessor</w>': 1478, 'sp</w>': 1479, 'Load</w>': 1480, 'std</w>': 1481, 'vector</w>': 1482, 'pieces</w>': 1483, '&</w>': 1484, 'int</w>': 1485, 'params</w>': 1486, 'EncodeAsPieces</w>': 1487, 'EncodeAsIds</w>': 1488, 'DecodeIds</w>': 1489, 'domly</w>': 1490, 'changing</w>': 1491, 'proposes</w>': 1492, 'regularization</w>': 1493, 'randomly</w>': 1494, 'changes</w>': 1495, 'during</w>': 1496, 'Lample</w>': 1497, 'Artetxe</w>': 1498, 'denoising</w>': 1499, 'autoencoder</w>': 1500, 'alter</w>': 1501, 'reconstruct</w>': 1502, 'emulate</w>': 1503, 'dynamic</w>': 1504, 'sampling</w>': 1505, 'tool</w>': 1506, 'Tensorflow</w>': 1507, 'library</w>': 1508, 'frameworks</w>': 1509, 'Figures</w>': 1510, 'usages</w>': 1511, 'API6</w>': 1512, 'one</w>': 1513, 'sampled</w>': 1514, 'New</w>': 1515, 'York</w>': 1516, 'differently</w>': 1517, 'How</w>': 1518, 'transferable</w>': 1519, 'features</w>': 1520, 'Jason</w>': 1521, 'Yosinski</w>': 1522, 'Jeff</w>': 1523, 'Clune</w>': 1524, 'Yoshua</w>': 1525, 'Bengio</w>': 1526, 'Hod</w>': 1527, 'Lipson4</w>': 1528, 'Dept</w>': 1529, 'Computer</w>': 1530, 'Science</w>': 1531, 'Cornell</w>': 1532, 'University</w>': 1533, 'Wyoming</w>': 1534, 'Operations</w>': 1535, 'Montreal</w>': 1536, 'Mechanical</w>': 1537, 'Aerospace</w>': 1538, 'Engineering</w>': 1539, 'Many</w>': 1540, 'natural</w>': 1541, 'images</w>': 1542, 'exhibit</w>': 1543, 'curious</w>': 1544, 'phenomenon</w>': 1545, 'common</w>': 1546, 'learn</w>': 1547, 'Gabor</w>': 1548, 'filters</w>': 1549, 'color</w>': 1550, 'blobs</w>': 1551, 'appear</w>': 1552, 'particular</w>': 1553, 'datasets</w>': 1554, 'Features</w>': 1555, 'eventually</w>': 1556, 'transition</w>': 1557, 'last</w>': 1558, 'network</w>': 1559, 'studied</w>': 1560, 'extensively</w>': 1561, 'experimentally</w>': 1562, 'quantify</w>': 1563, 'generality</w>': 1564, 'versus</w>': 1565, 'specificity</w>': 1566, 'neurons</w>': 1567, 'convolutional</w>': 1568, 'report</w>': 1569, 'few</w>': 1570, 'surprising</w>': 1571, 'Transferability</w>': 1572, 'negatively</w>': 1573, 'affected</w>': 1574, 'distinct</w>': 1575, 'issues</w>': 1576, 'specialization</w>': 1577, 'expense</w>': 1578, 'optimization</w>': 1579, 'difficulties</w>': 1580, 'co</w>': 1581, 'adapted</w>': 1582, 'ImageNet</w>': 1583, 'demonstrate</w>': 1584, 'either</w>': 1585, 'dominate</w>': 1586, 'transferred</w>': 1587, 'bottom</w>': 1588, 'document</w>': 1589, 'transferability</w>': 1590, 'decreases</w>': 1591, 'distance</w>': 1592, 'base</w>': 1593, 'transferring</w>': 1594, 'distant</w>': 1595, 'result</w>': 1596, 'initializing</w>': 1597, 'layers</w>': 1598, 'produce</w>': 1599, 'boost</w>': 1600, 'generalization</w>': 1601, 'lingers</w>': 1602, 'fine</w>': 1603, 'tuning</w>': 1604, 'tend</w>': 1605, 'resemble</w>': 1606, 'appearance</w>': 1607, 'obtaining</w>': 1608, 'anything</w>': 1609, 'image</w>': 1610, 'causes</w>': 1611, 'suspicion</w>': 1612, 'poorly</w>': 1613, 'chosen</w>': 1614, 'hyperparameters</w>': 1615, 'software</w>': 1616, 'bug</w>': 1617, 'occurs</w>': 1618, 'very</w>': 1619, 'objectives</w>': 1620, 'supervised</w>': 1621, 'Krizhevsky</w>': 1622, 'unsupervised</w>': 1623, 'density</w>': 1624, 'Lee</w>': 1625, '2009</w>': 1626, 'sparse</w>': 1627, 'representations</w>': 1628, '2011</w>': 1629, 'Because</w>': 1630, 'finding</w>': 1631, 'seems</w>': 1632, 'occur</w>': 1633, 'regardless</w>': 1634, 'function</w>': 1635, 'On</w>': 1636, 'know</w>': 1637, 'greatly</w>': 1638, 'dimensional</w>': 1639, 'successfully</w>': 1640, 'toward</w>': 1641, 'objective</w>': 1642, 'unit</w>': 1643, 'thus</w>': 1644, 'These</w>': 1645, 'intuitive</w>': 1646, 'notions</w>': 1647, 'provide</w>': 1648, 'rigorous</w>': 1649, 'definitions</w>': 1650, 'below</w>': 1651, '1411</w>': 1652, '1792v1</w>': 1653, 'LG</w>': 1654, 'Nov</w>': 1655, 'somewhere</w>': 1656, 'raises</w>': 1657, 'questions</w>': 1658, 'Can</w>': 1659, 'degree</w>': 1660, 'Does</w>': 1661, 'suddenly</w>': 1662, 'spread</w>': 1663, 'Where</w>': 1664, 'place</w>': 1665, 'answers</w>': 1666, 'extent</w>': 1667, 'within</w>': 1668, 'able</w>': 1669, 'transfer</w>': 1670, 'Caruana</w>': 1671, '1995</w>': 1672, 'repurpose</w>': 1673, 'learned</w>': 1674, 'second</w>': 1675, 'suitable</w>': 1676, 'instead</w>': 1677, 'significantly</w>': 1678, 'powerful</w>': 1679, 'enable</w>': 1680, 'overfitting</w>': 1681, 'taken</w>': 1682, 'advantage</w>': 1683, 'fact</w>': 1684, 'obtain</w>': 1685, 'art</w>': 1686, 'Donahue</w>': 1687, '2013a</w>': 1688, 'Zeiler</w>': 1689, 'Fergus</w>': 1690, '2013</w>': 1691, 'Sermanet</w>': 1692, 'collectively</w>': 1693, 'suggesting</w>': 1694, 'indeed</w>': 1695, 'compute</w>': 1696, 'fairly</w>': 1697, 'emphasize</w>': 1698, 'importance</w>': 1699, 'studying</w>': 1700, 'nature</w>': 1701, 'usual</w>': 1702, 'copy</w>': 1703, 'n</w>': 1704, 'initialized</w>': 1705, 'choose</w>': 1706, 'backpropagate</w>': 1707, 'errors</w>': 1708, 'new</w>': 1709, 'copied</w>': 1710, 'tune</w>': 1711, 'frozen</w>': 1712, 'tuned</w>': 1713, 'improve</w>': 1714, 'Of</w>': 1715, 'course</w>': 1716, 'little</w>': 1717, 'lower</w>': 1718, 'could</w>': 1719, '</w>': 1720, 'sections</w>': 1721, 'contributions</w>': 1722, 'namely</w>': 1723, 'another</w>': 1724, 'characterize</w>': 1725, 'yields</w>': 1726, 'separate</w>': 1727, 'cause</w>': 1728, 'degradation</w>': 1729, 'themselves</w>': 1730, 'ii</w>': 1731, 'neighboring</w>': 1732, 'effects</w>': 1733, 'dissimilar</w>': 1734, 'previously</w>': 1735, 'Jarrett</w>': 1736, 'weights</w>': 1737, 'vs</w>': 1738, 'particularly</w>': 1739, 'effect</w>': 1740, 'persists</w>': 1741, 'extensive</w>': 1742, 'Generality</w>': 1743, 'Specificity</w>': 1744, 'Measured</w>': 1745, 'Transfer</w>': 1746, 'Performance</w>': 1747, 'tendency</w>': 1748, 'study</w>': 1749, 'set</w>': 1750, 'B</w>': 1751, 'note</w>': 1752, 'similarity</w>': 1753, 'create</w>': 1754, 'constructing</w>': 1755, 'overlapping</w>': 1756, 'subsets</w>': 1757, 'split</w>': 1758, 'groups</w>': 1759, 'containing</w>': 1760, '500</w>': 1761, 'approximately</w>': 1762, 'half</w>': 1763, '645</w>': 1764, '000</w>': 1765, 'examples</w>': 1766, 'eight</w>': 1767, 'baseA</w>': 1768, 'baseB</w>': 1769, 'rows</w>': 1770, '7</w>': 1771, 'explanation</w>': 1772, 'selffer</w>': 1773, 'B3B</w>': 1774, 'five</w>': 1775, '</w>': 1776, 'control</w>': 1777, 'row</w>': 1778, 'A3B</w>': 1779, 'Intuitively</w>': 1780, 'classify</w>': 1781, 'performs</w>': 1782, 'evidence</w>': 1783, 'third</w>': 1784, 'least</w>': 1785, 'respect</w>': 1786, 'suffers</w>': 1787, 'repeated</w>': 1788, 'directions</w>': 1789, 'AnB</w>': 1790, 'BnA</w>': 1791, 'above</w>': 1792, 'versions</w>': 1793, 'assign</w>': 1794, 'clusters</w>': 1795, 'dogs</w>': 1796, 'cats</w>': 1797, '13</w>': 1798, 'biological</w>': 1799, 'family</w>': 1800, 'Felidae</w>': 1801, 'tabby</w>': 1802, 'cat</w>': 1803, 'tiger</w>': 1804, 'Persian</w>': 1805, 'Siamese</w>': 1806, 'Egyptian</w>': 1807, 'mountain</w>': 1808, 'lion</w>': 1809, 'lynx</w>': 1810, 'leopard</w>': 1811, 'snow</w>': 1812, 'jaguar</w>': 1813, 'cheetah</w>': 1814, 'contain</w>': 1815, 'felid</w>': 1816, 'levels</w>': 1817, 'some</w>': 1818, 'types</w>': 1819, 'felids</w>': 1820, 'generalizing</w>': 1821, 'expect</w>': 1822, 'high</w>': 1823, 'detectors</w>': 1824, 'low</w>': 1825, 'created</w>': 1826, 'assigning</w>': 1827, 'Fortunately</w>': 1828, 'hierarchy</w>': 1829, 'allowed</w>': 1830, 'halves</w>': 1831, 'man</w>': 1832, 'made</w>': 1833, 'entities</w>': 1834, 'quite</w>': 1835, '551</w>': 1836, 'group</w>': 1837, '449</w>': 1838, 'Further</w>': 1839, 'supplementary</w>': 1840, '1The</w>': 1841, 'released</w>': 1842, 'Large</w>': 1843, 'Scale</w>': 1844, 'Visual</w>': 1845, 'Recognition</w>': 1846, 'Challenge</w>': 1847, 'ILSVRC2012</w>': 1848, 'Deng</w>': 1849, '281</w>': 1850, '167</w>': 1851, 'labeled</w>': 1852, '50</w>': 1853, 'test</w>': 1854, '2Note</w>': 1855, 'doesn</w>': 1856, 'sense</w>': 1857, 'B8B</w>': 1858, 'A8B</w>': 1859, 'never</w>': 1860, 'WA1</w>': 1861, 'WA2</w>': 1862, 'WA3</w>': 1863, 'WA4</w>': 1864, 'WA5</w>': 1865, 'WA6</w>': 1866, 'WA7</w>': 1867, 'WA8</w>': 1868, 'WB1</w>': 1869, 'WB2</w>': 1870, 'WB3</w>': 1871, 'WB4</w>': 1872, 'WB5</w>': 1873, 'WB6</w>': 1874, 'WB7</w>': 1875, 'WB8</w>': 1876, 'treatments</w>': 1877, 'controls</w>': 1878, 'Top</w>': 1879, 'backprop</w>': 1880, 'rectangles</w>': 1881, 'represent</w>': 1882, 'weight</w>': 1883, 'indicating</w>': 1884, 'originally</w>': 1885, 'vertical</w>': 1886, 'ellipsoidal</w>': 1887, 'bars</w>': 1888, 'vectors</w>': 1889, 'activations</w>': 1890, 'Third</w>': 1891, 'upper</w>': 1892, 'locked</w>': 1893, 'reveals</w>': 1894, 'occurrence</w>': 1895, 'fragile</w>': 1896, 'coadaptation</w>': 1897, 'adapt</w>': 1898, 'rediscovered</w>': 1899, 'Fourth</w>': 1900, 'except</w>': 1901, 'tests</w>': 1902, 'Setup</w>': 1903, 'Since</w>': 1904, 'won</w>': 1905, 'competition</w>': 1906, 'much</w>': 1907, 'interest</w>': 1908, 'tweaking</w>': 1909, 'maximize</w>': 1910, 'absolute</w>': 1911, 'rather</w>': 1912, 'reference</w>': 1913, 'Caffe</w>': 1914, 'Jia</w>': 1915, 'setup</w>': 1916, 'rates</w>': 1917, 'experiments</w>': 1918, 'http</w>': 1919, 'yosinski</w>': 1920, 'Results</w>': 1921, 'Discussion</w>': 1922, 'performed</w>': 1923, 'three</w>': 1924, 'sets</w>': 1925, 'splits</w>': 1926, 'discussed</w>': 1927, '52</w>': 1928, '54</w>': 1929, '56</w>': 1930, '58</w>': 1931, '60</w>': 1932, '62</w>': 1933, '64</w>': 1934, '66</w>': 1935, 'BnB</w>': 1936, 'Layer</w>': 1937, 'chopped</w>': 1938, 'retrained</w>': 1939, 'improves</w>': 1940, 'Fine</w>': 1941, 'recovers</w>': 1942, 'interactions</w>': 1943, 'drops</w>': 1944, 'adaptation</w>': 1945, 'Each</w>': 1946, 'figure</w>': 1947, 'represents</w>': 1948, 'white</w>': 1949, 'circles</w>': 1950, 'points</w>': 1951, 'tested</w>': 1952, 'dark</w>': 1953, 'blue</w>': 1954, 'dot</w>': 1955, 'Light</w>': 1956, 'Dark</w>': 1957, 'red</w>': 1958, 'diamonds</w>': 1959, 'light</w>': 1960, 'Points</w>': 1961, 'shifted</w>': 1962, 'slightly</w>': 1963, 'visual</w>': 1964, 'Bottom</w>': 1965, 'Lines</w>': 1966, 'connecting</w>': 1967, 'means</w>': 1968, 'Numbered</w>': 1969, 'descriptions</w>': 1970, 'interpretation</w>': 1971, 'applies</w>': 1972, 'Similar</w>': 1973, 'Datasets</w>': 1974, 'shown3</w>': 1975, 'conclusions</w>': 1976, 'interpretations</w>': 1977, 'dotted</w>': 1978, '3AnA</w>': 1979, 'statistically</w>': 1980, 'cases</w>': 1981, 'simplify</w>': 1982, 'notation</w>': 1983, 'Similarly</w>': 1984, 'aggregated</w>': 1985, 'identical</w>': 1986, 'attains</w>': 1987, '625</w>': 1988, '37</w>': 1989, 'error</w>': 1990, '42</w>': 1991, 'attained</w>': 1992, 'lead</w>': 1993, 'net</w>': 1994, 'mistakes</w>': 1995, 'That</w>': 1996, 'save</w>': 1997, 'reinitialize</w>': 1998, 'whole</w>': 1999, 'retrain</w>': 2000, 'holds</w>': 2001, 'true</w>': 2002, 'worse</w>': 2003, 'drop</w>': 2004, 'successive</w>': 2005, 'interact</w>': 2006, 'complex</w>': 2007, 'relearned</w>': 2008, 'alone</w>': 2009, 'Gradient</w>': 2010, 'descent</w>': 2011, 'good</w>': 2012, 'jointly</w>': 2013, 'nearly</w>': 2014, 'back</w>': 2015, 'get</w>': 2016, 'closer</w>': 2017, 'relearn</w>': 2018, 'apparently</w>': 2019, 'relearning</w>': 2020, 'enough</w>': 2021, 'Alternately</w>': 2022, 'say</w>': 2023, 'observed</w>': 2024, 'literature</w>': 2025, 'prevents</w>': 2026, 'measure</w>': 2027, 'Layers</w>': 2028, 'perfectly</w>': 2029, 'giving</w>': 2030, 'blob</w>': 2031, 'slight</w>': 2032, 'Thanks</w>': 2033, 'tell</w>': 2034, 'combination</w>': 2035, 'lost</w>': 2036, 'dominates</w>': 2037, 'whereas</w>': 2038, 'diminishes</w>': 2039, 'successful</w>': 2040, 'elsewhere</w>': 2041, 'Girshick</w>': 2042, '2013b</w>': 2043, 'limited</w>': 2044, 'noticing</w>': 2045, 'believe</w>': 2046, 'quantified</w>': 2047, 'decoupled</w>': 2048, 'showing</w>': 2049, 'regime</w>': 2050, 'generalize</w>': 2051, 'those</w>': 2052, 'Previously</w>': 2053, 'reason</w>': 2054, 'want</w>': 2055, 'suggests</w>': 2056, 'attributed</w>': 2057, 'longer</w>': 2058, 'total</w>': 2059, '450k</w>': 2060, 'iterations</w>': 2061, 'finetuned</w>': 2062, 'plausible</w>': 2063, 'beginning</w>': 2064, 'completely</w>': 2065, 'linger</w>': 2066, 'boosting</w>': 2067, 'retraining</w>': 2068, 'keep</w>': 2069, 'initialize</w>': 2070, 'anywhere</w>': 2071, 'seven</w>': 2072, 'produces</w>': 2073, 'improved</w>': 2074, 'across</w>': 2075, '4We</w>': 2076, 'point</w>': 2077, 'computationally</w>': 2078, 'days</w>': 2079, 'GPU</w>': 2080, 'publication</w>': 2081, 'aggregation</w>': 2082, 'averaged</w>': 2083, 'ranges</w>': 2084, 'mean</w>': 2085, 'Dissimilar</w>': 2086, 'Splitting</w>': 2087, 'Man</w>': 2088, 'Classes</w>': 2089, 'Into</w>': 2090, 'Separate</w>': 2091, 'mentioned</w>': 2092, 'decline</w>': 2093, 'hypothesis</w>': 2094, 'comparing</w>': 2095, 'object</w>': 2096, 'creates</w>': 2097, 'subplot</w>': 2098, 'orange</w>': 2099, 'hexagons</w>': 2100, 'lines</w>': 2101, 'categories</w>': 2102, 'simply</w>': 2103, 'being</w>': 2104, 'easier</w>': 2105, 'Weights</w>': 2106, 'untrained</w>': 2107, 'showed</w>': 2108, 'strikingly</w>': 2109, 'rectification</w>': 2110, 'local</w>': 2111, 'They</w>': 2112, 'Caltech</w>': 2113, '101</w>': 2114, 'Fei</w>': 2115, '2004</w>': 2116, 'ask</w>': 2117, 'carries</w>': 2118, 'deeper</w>': 2119, 'choices</w>': 2120, 'falls</w>': 2121, 'quickly</w>': 2122, 'chance</w>': 2123, 'getting</w>': 2124, 'straightforward</w>': 2125, 'Whereas</w>': 2126, 'max</w>': 2127, 'did</w>': 2128, 'nonlinearity</w>': 2129, 'relu</w>': 2130, 'abs</w>': 2131, 'tanh</w>': 2132, 'sizes</w>': 2133, 'Additionally</w>': 2134, 'hyperparameter</w>': 2135, 'architectural</w>': 2136, 'datapoint</w>': 2137, 'tweak</w>': 2138, 'initialization</w>': 2139, 'subtracting</w>': 2140, 'performances</w>': 2141, 'plotted</w>': 2142, 'things</w>': 2143, 'apparent</w>': 2144, 'gap</w>': 2145, 'grows</w>': 2146, '25</w>': 2147, 'differ</w>': 2148, 'fully</w>': 2149, 'ours</w>': 2150, 'informative</w>': 2151, 'however</w>': 2152, 'draws</w>': 2153, 'multiple</w>': 2154, 'runs</w>': 2155, 'converge</w>': 2156, 'producing</w>': 2157, 'Much</w>': 2158, 'convergence</w>': 2159, '30</w>': 2160, '20</w>': 2161, '15</w>': 2162, '05</w>': 2163, '00</w>': 2164, 'Relative</w>': 2165, 'm</w>': 2166, 'Degradation</w>': 2167, 'connects</w>': 2168, 'consist</w>': 2169, 'plots</w>': 2170, 'compared</w>': 2171, 'making</w>': 2172, 'extra</w>': 2173, 'overfit</w>': 2174, 'Conclusions</w>': 2175, 'demonstrated</w>': 2176, 'quantifying</w>': 2177, 'fragilely</w>': 2178, 'substantial</w>': 2179, 'technique</w>': 2180, 'Prototypical</w>': 2181, 'Networks</w>': 2182, 'Few</w>': 2183, 'shot</w>': 2184, 'Learning</w>': 2185, 'Jake</w>': 2186, 'Snell</w>': 2187, 'Toronto</w>': 2188, 'Kevin</w>': 2189, 'Swersky</w>': 2190, 'Richard</w>': 2191, 'S</w>': 2192, 'Zemel</w>': 2193, 'Vector</w>': 2194, 'Institute</w>': 2195, 'prototypical</w>': 2196, 'metric</w>': 2197, 'computing</w>': 2198, 'distances</w>': 2199, 'prototype</w>': 2200, 'Compared</w>': 2201, 'recent</w>': 2202, 'reflect</w>': 2203, 'simpler</w>': 2204, 'inductive</w>': 2205, 'beneficial</w>': 2206, 'excellent</w>': 2207, 'decisions</w>': 2208, 'improvements</w>': 2209, 'involving</w>': 2210, 'complicated</w>': 2211, 'extend</w>': 2212, 'zero</w>': 2213, 'theart</w>': 2214, 'CU</w>': 2215, 'Birds</w>': 2216, '16</w>': 2217, 'accommodate</w>': 2218, 'naive</w>': 2219, 're</w>': 2220, 'severely</w>': 2221, 'humans</w>': 2222, 'ability</w>': 2223, 'Two</w>': 2224, 'progress</w>': 2225, '29</w>': 2226, 'predict</w>': 2227, 'unlabeled</w>': 2228, 'query</w>': 2229, 'interpreted</w>': 2230, 'weighted</w>': 2231, 'nearest</w>': 2232, 'neighbor</w>': 2233, 'Notably</w>': 2234, 'utilizes</w>': 2235, 'mini</w>': 2236, 'batches</w>': 2237, 'episodes</w>': 2238, 'episode</w>': 2239, 'mimic</w>': 2240, 'subsampling</w>': 2241, 'faithful</w>': 2242, 'thereby</w>': 2243, 'Ravi</w>': 2244, 'Larochelle</w>': 2245, '22</w>': 2246, 'episodic</w>': 2247, 'LSTM</w>': 2248, 'updates</w>': 2249, 'Here</w>': 2250, 'learner</w>': 2251, 'learns</w>': 2252, 'attack</w>': 2253, 'addressing</w>': 2254, 'key</w>': 2255, 'issue</w>': 2256, 'assumption</w>': 2257, 'cluster</w>': 2258, 'around</w>': 2259, 'Classification</w>': 2260, 'tackle</w>': 2261, 'comes</w>': 2262, 'description</w>': 2263, 'therefore</w>': 2264, 'serve</w>': 2265, 'Initial</w>': 2266, 'author</w>': 2267, '1703</w>': 2268, '05175v2</w>': 2269, 'Jun</w>': 2270, 'c1</w>': 2271, 'c2</w>': 2272, 'c3</w>': 2273, 'v1</w>': 2274, 'v2</w>': 2275, 'v3</w>': 2276, 'Zero</w>': 2277, 'scenarios</w>': 2278, 'Left</w>': 2279, 'prototypes</w>': 2280, 'ck</w>': 2281, 'Right</w>': 2282, 'produced</w>': 2283, 'vk</w>': 2284, 'classified</w>': 2285, 'p</w>': 2286, 'y</w>': 2287, '</w>': 2288, 'exp</w>': 2289, 'f</w>': 2290, 'formulate</w>': 2291, 'draw</w>': 2292, 'connections</w>': 2293, 'analyze</w>': 2294, 'underlying</w>': 2295, 'relate</w>': 2296, 'clustering</w>': 2297, 'justify</w>': 2298, 'Bregman</w>': 2299, 'divergence</w>': 2300, 'squared</w>': 2301, 'Euclidean</w>': 2302, 'empirically</w>': 2303, 'vital</w>': 2304, 'outperforms</w>': 2305, 'cosine</w>': 2306, 'benchmark</w>': 2307, 'appealing</w>': 2308, 'Notation</w>': 2309, 'x1</w>': 2310, 'y1</w>': 2311, 'xN</w>': 2312, 'yN</w>': 2313, 'xi</w>': 2314, '</w>': 2315, 'D</w>': 2316, 'yi</w>': 2317, 'K</w>': 2318, 'corresponding</w>': 2319, 'Sk</w>': 2320, 'Model</w>': 2321, 'M</w>': 2322, 'RM</w>': 2323, '</w>': 2324, 'learnable</w>': 2325, '</w>': 2326, 'belonging</w>': 2327, 'X</w>': 2328, '</w>': 2329, '</w>': 2330, 'distribution</w>': 2331, 'P</w>': 2332, 'k0</w>': 2333, 'ck0</w>': 2334, 'proceeds</w>': 2335, 'minimizing</w>': 2336, 'negative</w>': 2337, 'probability</w>': 2338, 'J</w>': 2339, 'SGD</w>': 2340, 'Training</w>': 2341, 'formed</w>': 2342, 'selecting</w>': 2343, 'choosing</w>': 2344, 'act</w>': 2345, 'remainder</w>': 2346, 'Pseudocode</w>': 2347, 'Algorithm</w>': 2348, 'computation</w>': 2349, 'NC</w>': 2350, '</w>': 2351, 'NS</w>': 2352, 'NQ</w>': 2353, 'RANDOMSAMPLE</w>': 2354, 'elements</w>': 2355, 'uniformly</w>': 2356, 'replacement</w>': 2357, 'Input</w>': 2358, 'Dk</w>': 2359, 'Output</w>': 2360, 'generated</w>': 2361, 'V</w>': 2362, '</w>': 2363, 'Select</w>': 2364, 'DVk</w>': 2365, 'Qk</w>': 2366, '\\\\</w>': 2367, 'Compute</w>': 2368, 'logX</w>': 2369, 'Update</w>': 2370, 'Mixture</w>': 2371, 'Density</w>': 2372, 'Estimation</w>': 2373, 'functions</w>': 2374, 'divergences</w>': 2375, 'performing</w>': 2376, 'mixture</w>': 2377, 'estimation</w>': 2378, 'exponential</w>': 2379, 'd</w>': 2380, '</w>': 2381, 'differentiable</w>': 2382, 'convex</w>': 2383, 'Legendre</w>': 2384, 'kz</w>': 2385, '0k</w>': 2386, 'Mahalanobis</w>': 2387, 'Prototype</w>': 2388, 'viewed</w>': 2389, 'terms</w>': 2390, 'assigned</w>': 2391, 'representative</w>': 2392, 'achieving</w>': 2393, 'minimal</w>': 2394, 'Equation</w>': 2395, 'representatives</w>': 2396, 'Moreover</w>': 2397, 'p</w>': 2398, '</w>': 2399, 'cumulant</w>': 2400, '</w>': 2401, 'written</w>': 2402, 'uniquely</w>': 2403, 'equally</w>': 2404, 'assignment</w>': 2405, 'prediction</w>': 2406, '</w>': 2407, 'k</w>': 2408, 'effectively</w>': 2409, 'assumptions</w>': 2410, 'classconditional</w>': 2411, 'Reinterpretation</w>': 2412, 'gaining</w>': 2413, 'insight</w>': 2414, 'parameterization</w>': 2415, 'term</w>': 2416, 'affect</w>': 2417, 'probabilities</w>': 2418, 'focus</w>': 2419, 'primarily</w>': 2420, 'spherical</w>': 2421, 'Gaussian</w>': 2422, 'densities</w>': 2423, 'indicate</w>': 2424, 'despite</w>': 2425, 'equivalence</w>': 2426, 'hypothesize</w>': 2427, 'linearity</w>': 2428, 'Indeed</w>': 2429, 'currently</w>': 2430, '14</w>': 2431, '28</w>': 2432, 'Comparison</w>': 2433, 'xk</w>': 2434, 'fixed</w>': 2435, 'greater</w>': 2436, 'require</w>': 2437, 'partitioning</w>': 2438, 'scheme</w>': 2439, 'Mensink</w>': 2440, 'Rippel</w>': 2441, 'methods</w>': 2442, 'ordinary</w>': 2443, 'extensions</w>': 2444, 'decoupling</w>': 2445, 'conditional</w>': 2446, 'FCE</w>': 2447, 'takes</w>': 2448, 'account</w>': 2449, 'likewise</w>': 2450, 'incorporated</w>': 2451, 'increase</w>': 2452, 'imposes</w>': 2453, 'ordering</w>': 2454, 'bi</w>': 2455, 'directional</w>': 2456, 'Instead</w>': 2457, 'outline</w>': 2458, 'Choices</w>': 2459, 'Distance</w>': 2460, 'apply</w>': 2461, 'permissible</w>': 2462, 'conjecture</w>': 2463, 'hold</w>': 2464, 'Episode</w>': 2465, 'composition</w>': 2466, 'Nc</w>': 2467, 'situation</w>': 2468, 'comprised</w>': 2469, 'extremely</w>': 2470, 'held</w>': 2471, 'consideration</w>': 2472, 'Shot</w>': 2473, 'differs</w>': 2474, 'accuracies</w>': 2475, 'Omniglot</w>': 2476, 'Acc</w>': 2477, 'Dist</w>': 2478, 'Tune</w>': 2479, 'MATCHING</w>': 2480, 'NETWORKS</w>': 2481, 'Cosine</w>': 2482, '98</w>': 2483, '93</w>': 2484, 'Y</w>': 2485, '97</w>': 2486, 'NEURAL</w>': 2487, 'STATISTICIAN</w>': 2488, '99</w>': 2489, 'PROTOTYPICAL</w>': 2490, 'OURS</w>': 2491, 'Euclid</w>': 2492, '96</w>': 2493, 'advance</w>': 2494, 'Modifying</w>': 2495, 'deal</w>': 2496, 'g</w>': 2497, 'An</w>': 2498, 'illustration</w>': 2499, 'procedure</w>': 2500, 'relates</w>': 2501, 'come</w>': 2502, 'domains</w>': 2503, 'helpful</w>': 2504, 'fix</w>': 2505, 'constrain</w>': 2506, 'Experiments</w>': 2507, 'miniImageNet</w>': 2508, 'ILSVRC</w>': 2509, '26</w>': 2510, 'UCSD</w>': 2511, 'bird</w>': 2512, 'CUB</w>': 2513, '200</w>': 2514, '31</w>': 2515, '1623</w>': 2516, 'handwritten</w>': 2517, 'collected</w>': 2518, 'alphabets</w>': 2519, 'associated</w>': 2520, 'drawn</w>': 2521, 'human</w>': 2522, 'subject</w>': 2523, 'resizing</w>': 2524, 'grayscale</w>': 2525, 'augmenting</w>': 2526, 'rotations</w>': 2527, 'multiples</w>': 2528, '90</w>': 2529, 'degrees</w>': 2530, '1200</w>': 2531, 'plus</w>': 2532, '800</w>': 2533, 'mirrors</w>': 2534, 'composed</w>': 2535, 'blocks</w>': 2536, 'block</w>': 2537, 'filter</w>': 2538, 'convolution</w>': 2539, 'batch</w>': 2540, 'ReLU</w>': 2541, 'All</w>': 2542, '11</w>': 2543, 'initial</w>': 2544, 'rate</w>': 2545, 'cut</w>': 2546, 'advantageous</w>': 2547, 'fewer</w>': 2548, 'against</w>': 2549, 'baselines</w>': 2550, 'statistician</w>': 2551, 'derived</w>': 2552, '12</w>': 2553, '84</w>': 2554, 'divided</w>': 2555, '600</w>': 2556, 'monitoring</w>': 2557, 'though</w>': 2558, '1600</w>': 2559, 'increased</w>': 2560, '95</w>': 2561, 'confidence</w>': 2562, 'intervals</w>': 2563, 'BASELINE</w>': 2564, 'NEAREST</w>': 2565, 'NEIGHBORS</w>': 2566, '86</w>': 2567, '</w>': 2568, '49</w>': 2569, '79</w>': 2570, '43</w>': 2571, '40</w>': 2572, '78</w>': 2573, '51</w>': 2574, '09</w>': 2575, '71</w>': 2576, '55</w>': 2577, '73</w>': 2578, 'META</w>': 2579, 'LEARNER</w>': 2580, '44</w>': 2581, '77</w>': 2582, '68</w>': 2583, '70</w>': 2584, '80</w>': 2585, 'Accuracy</w>': 2586, 'Proto</w>': 2587, 'Nets</w>': 2588, 'axis</w>': 2589, 'indicates</w>': 2590, 'configuration</w>': 2591, 'Error</w>': 2592, 'schedule</w>': 2593, 'stops</w>': 2594, 'variants</w>': 2595, 'Meta</w>': 2596, 'Learner</w>': 2597, 'margin</w>': 2598, 'own</w>': 2599, 'difficulty</w>': 2600, 'forces</w>': 2601, 'grained</w>': 2602, 'Also</w>': 2603, 'substantially</w>': 2604, 'pronounced</w>': 2605, 'naturally</w>': 2606, 'suited</w>': 2607, 'assess</w>': 2608, 'suitability</w>': 2609, '788</w>': 2610, 'species</w>': 2611, 'closely</w>': 2612, 'Reed</w>': 2613, 'preparing</w>': 2614, 'Image</w>': 2615, 'ALE</w>': 2616, 'Fisher</w>': 2617, 'SJE</w>': 2618, 'AlexNet</w>': 2619, 'SAMPLE</w>': 2620, 'CLUSTERING</w>': 2621, '17</w>': 2622, 'GoogLeNet</w>': 2623, 'DS</w>': 2624, 'DA</w>': 2625, 'PROTO</w>': 2626, 'NETS</w>': 2627, 'divide</w>': 2628, '024</w>': 2629, 'extracted</w>': 2630, 'applying</w>': 2631, 'crops</w>': 2632, 'horizontally</w>': 2633, 'flipped</w>': 2634, 'image2</w>': 2635, 'At</w>': 2636, 'crop</w>': 2637, '312</w>': 2638, 'continuous</w>': 2639, 'attribute</w>': 2640, 'attributes</w>': 2641, 'characteristics</w>': 2642, 'feather</w>': 2643, 'patterns</w>': 2644, 'domain</w>': 2645, 'constructed</w>': 2646, 'decay</w>': 2647, 'Early</w>': 2648, 'stopping</w>': 2649, 'utilizing</w>': 2650, 'SVM</w>': 2651, 'relative</w>': 2652, 'vast</w>': 2653, 'summarize</w>': 2654, 'relevant</w>': 2655, 'Neighborhood</w>': 2656, 'Components</w>': 2657, 'Analysis</w>': 2658, 'NCA</w>': 2659, 'KNN</w>': 2660, 'leave</w>': 2661, 'transformed</w>': 2662, 'Salakhutdinov</w>': 2663, 'Hinton</w>': 2664, '27</w>': 2665, 'transformation</w>': 2666, 'LMNN</w>': 2667, 'attempts</w>': 2668, 'optimize</w>': 2669, 'hinge</w>': 2670, 'encourages</w>': 2671, 'neighborhood</w>': 2672, 'DNet</w>': 2673, 'opposed</w>': 2674, 'form</w>': 2675, 'concise</w>': 2676, 'obviates</w>': 2677, 'developed</w>': 2678, 'rapidly</w>': 2679, 'incorporate</w>': 2680, 'relies</w>': 2681, 'downloaded</w>': 2682, 'reedscot</w>': 2683, 'cvpr2016</w>': 2684, 'linearly</w>': 2685, 'embed</w>': 2686, 'couple</w>': 2687, 'multi</w>': 2688, 'modal</w>': 2689, 'generalizes</w>': 2690, 'dynamics</w>': 2691, 'itself</w>': 2692, 'goal</w>': 2693, 'classifiers</w>': 2694, 'dynamically</w>': 2695, 'core</w>': 2696, 'rely</w>': 2697, 'nets</w>': 2698, 'secondary</w>': 2699, 'generative</w>': 2700, 'extends</w>': 2701, 'variational</w>': 2702, '24</w>': 2703, 'component</w>': 2704, 'statistic</w>': 2705, 'taking</w>': 2706, 'sample</w>': 2707, 'post</w>': 2708, 'approximate</w>': 2709, 'posterior</w>': 2710, 'Edwards</w>': 2711, 'Storkey</w>': 2712, 'considering</w>': 2713, 'KL</w>': 2714, 'inferred</w>': 2715, 'Like</w>': 2716, 'summary</w>': 2717, 'discriminative</w>': 2718, 'befits</w>': 2719, 'With</w>': 2720, 'resembles</w>': 2721, 'multimodal</w>': 2722, 'Unlike</w>': 2723, 'empirical</w>': 2724, 'risk</w>': 2725, 'Neither</w>': 2726, 'nor</w>': 2727, 'regularize</w>': 2728, 'Conclusion</w>': 2729, 'sophisticated</w>': 2730, 'although</w>': 2731, 'modifying</w>': 2732, 'direction</w>': 2733, 'future</w>': 2734, 'utilize</w>': 2735, 'distributions</w>': 2736, 'beyond</w>': 2737, 'Gaussians</w>': 2738, 'preliminary</w>': 2739, 'explorations</w>': 2740, 'variance</w>': 2741, 'gains</w>': 2742, 'flexibility</w>': 2743, 'requiring</w>': 2744, 'fitted</w>': 2745, 'Overall</w>': 2746, 'simplicity</w>': 2747, 'promising</w>': 2748}\n",
      "bpe_encoded_text vocabulary: 0\n",
      "WordPiece vocabulary: Counter({'0': 1})\n",
      "Trie Construction Completed Successfully\n",
      "Trie built successfully.\n",
      "Subwords: ['import</w>']\n",
      "BPE Vocabulary: {'import</w>': 0, 'numpy</w>': 1, 'as</w>': 2, 'np</w>': 3, 'torch</w>': 4, '.</w>': 5, 'nn</w>': 6, 'functional</w>': 7, 'F</w>': 8, 'from</w>': 9, 'utils</w>': 10, 'data</w>': 11, 'DataLoader</w>': 12, ',</w>': 13, 'Dataset</w>': 14, 'cuda</w>': 15, 'amp</w>': 16, 'GradScaler</w>': 17, 'autocast</w>': 18, 'optim</w>': 19, 'Adam</w>': 20, 'def</w>': 21, 'random_projection</w>': 22, '(</w>': 23, 'matrix</w>': 24, 'k</w>': 25, ')</w>': 26, ':</w>': 27, '\"</w>': 28, 'Random</w>': 29, 'projection</w>': 30, 'to</w>': 31, 'reduce</w>': 32, 'dimensionality</w>': 33, 'of</w>': 34, 'dimensions</w>': 35, 'random_matrix</w>': 36, '=</w>': 37, 'randn</w>': 38, 'size</w>': 39, '-</w>': 40, '1</w>': 41, 'device</w>': 42, 'return</w>': 43, 'matmul</w>': 44, 'cur_decomposition</w>': 45, 'projection_dim</w>': 46, '#</w>': 47, 'Change</w>': 48, \"'</w>\": 49, 'argument</w>': 50, 'Applies</w>': 51, 'CUR</w>': 52, 'decomposition</w>': 53, 'with</w>': 54, 'C</w>': 55, 'dimension</w>': 56, 'aligned</w>': 57, 'batch_size</w>': 58, 'seq_length</w>': 59, 'heads</w>': 60, 'dim</w>': 61, 'shape</w>': 62, 'min</w>': 63, '/</w>': 64, '2</w>': 65, 'Use</w>': 66, 'determine</w>': 67, 'zeros</w>': 68, 'R</w>': 69, 'for</w>': 70, 'b</w>': 71, 'in</w>': 72, 'range</w>': 73, 'h</w>': 74, 'col_indices</w>': 75, 'random</w>': 76, 'choice</w>': 77, 'replace</w>': 78, 'False</w>': 79, 'row_indices</w>': 80, '[</w>': 81, ']</w>': 82, 'class</w>': 83, 'PartitionedLinformerAttentionACT</w>': 84, 'Module</w>': 85, '__init__</w>': 86, 'self</w>': 87, 'embed_size</w>': 88, 'sequence_length</w>': 89, 'partition_size</w>': 90, 'super</w>': 91, 'head_dim</w>': 92, 'values</w>': 93, 'Linear</w>': 94, 'bias</w>': 95, 'keys</w>': 96, 'queries</w>': 97, 'value_projection</w>': 98, 'To</w>': 99, 'project</w>': 100, 'match</w>': 101, 'ponder</w>': 102, 'True</w>': 103, 'sigmoid</w>': 104, 'Sigmoid</w>': 105, 'forward</w>': 106, 'x</w>': 107, 'print</w>': 108, 'f</w>': 109, 'BEGIN</w>': 110, 'FORWARD</w>': 111, 'input</w>': 112, '{</w>': 113, '}</w>': 114, 'N</w>': 115, '_</w>': 116, 'x_reshaped</w>': 117, 'view</w>': 118, 'attention_scores</w>': 119, 'after</w>': 120, 'i</w>': 121, '0</w>': 122, 'PARTITION</w>': 123, 'START</w>': 124, 'partition_start</w>': 125, 'partition_end</w>': 126, '+</w>': 127, 'keys_part</w>': 128, 'queries_part</w>': 129, 'C_keys</w>': 130, 'R_queries</w>': 131, 'before</w>': 132, 'ponder_scores</w>': 133, 'Partition</w>': 134, 'Start</w>': 135, 'End</w>': 136, 'HEADS</w>': 137, 'head_queries</w>': 138, 'head_ponder_scores</w>': 139, 'squeeze</w>': 140, 'Correctly</w>': 141, 'expand</w>': 142, 'without</w>': 143, 'adding</w>': 144, 'an</w>': 145, 'unnecessary</w>': 146, 'BEFORE</w>': 147, '1ST</w>': 148, 'EINSUM</w>': 149, 'ponder_scores_permuted</w>': 150, 'permute</w>': 151, '3</w>': 152, 'Move</w>': 153, '128</w>': 154, '8</w>': 155, 'ponder_scores_broadcastable</w>': 156, 'Expand</w>': 157, 'energy</w>': 158, 'einsum</w>': 159, 'bnhd</w>': 160, 'bnhk</w>': 161, '></w>': 162, 'attention_weights</w>': 163, 'softmax</w>': 164, 'AFTER</w>': 165, 'attention</w>': 166, '*</w>': 167, 'attention_corrected</w>': 168, 'Swap</w>': 169, 'and</w>': 170, 'bring</w>': 171, 'next</w>': 172, 'reshape</w>': 173, 'Flatten</w>': 174, 'linear</w>': 175, 'layer</w>': 176, 'projected_values</w>': 177, 'Now</w>': 178, '2ND</w>': 179, 'Combine</w>': 180, 'then</w>': 181, 'pass</w>': 182, 'through</w>': 183, 'the</w>': 184, 'final</w>': 185, 'out</w>': 186, 'bnhp</w>': 187, 'bnh</w>': 188, 'Example</w>': 189, 'usage</w>': 190, 'model</w>': 191, '512</w>': 192, '1024</w>': 193, '256</w>': 194, 'input_tensor</w>': 195, 'rand</w>': 196, 'output</w>': 197, 'if</w>': 198, 'is_available</w>': 199, 'else</w>': 200, 'cpu</w>': 201, 'Assuming</w>': 202, 'tensor</w>': 203, 'is</w>': 204, 'Size</w>': 205, 'each</w>': 206, 'word</w>': 207, 'embedding</w>': 208, 'Number</w>': 209, 'Length</w>': 210, 'sequence</w>': 211, 'Dimension</w>': 212, 'embeddings</w>': 213, 'partitions</w>': 214, 'processing</w>': 215, 'num_classes</w>': 216, '10</w>': 217, 'Instantiate</w>': 218, 'attention_model</w>': 219, 'AttentionClassifier</w>': 220, 'global_pool</w>': 221, 'AdaptiveAvgPool1d</w>': 222, 'Global</w>': 223, 'pooling</w>': 224, 's</w>': 225, 'here</w>': 226, ';</w>': 227, 'adjust</w>': 228, 'necessary</w>': 229, 'classifier</w>': 230, 'Classifier</w>': 231, 'Ensure</w>': 232, 'has</w>': 233, 'num_heads</w>': 234, 'You</w>': 235, 'might</w>': 236, 'need</w>': 237, 'this</w>': 238, 'depending</w>': 239, 'on</w>': 240, 'your</w>': 241, 'exact</w>': 242, 'resulting</w>': 243, 'Pass</w>': 244, 'appropriate</w>': 245, 'Create</w>': 246, 'a</w>': 247, 'pinned</w>': 248, 'memory</w>': 249, 'MyDataset</w>': 250, 'Initialize</w>': 251, '__len__</w>': 252, 'Return</w>': 253, 'dataset</w>': 254, '100</w>': 255, '__getitem__</w>': 256, 'idx</w>': 257, 'Pad</w>': 258, 'length</w>': 259, 'ensure</w>': 260, 'This</w>': 261, 'simplified</w>': 262, 'example</w>': 263, 'according</w>': 264, 'actual</w>': 265, 'Padded</w>': 266, 'modeled</w>': 267, 'expected</w>': 268, 'target</w>': 269, 'randint</w>': 270, 'item</w>': 271, 'Generate</w>': 272, 'index</w>': 273, '9</w>': 274, 'classes</w>': 275, 'loader</w>': 276, '32</w>': 277, 'shuffle</w>': 278, 'pin_memory</w>': 279, 'optimizer</w>': 280, 'parameters</w>': 281, 'lr</w>': 282, '1e</w>': 283, 'scaler</w>': 284, 'epoch</w>': 285, 'epochs</w>': 286, 'inputs</w>': 287, 'targets</w>': 288, 'correct</w>': 289, 'non_blocking</w>': 290, 'zero_grad</w>': 291, 'Forward</w>': 292, 'predictions</w>': 293, 'Calculate</w>': 294, 'loss</w>': 295, 'assuming</w>': 296, 'classification</w>': 297, 'task</w>': 298, 'are</w>': 299, 'indices</w>': 300, 'cross_entropy</w>': 301, 'Backward</w>': 302, 'step</w>': 303, 'using</w>': 304, 'gradient</w>': 305, 'scaling</w>': 306, 'mixed</w>': 307, 'precision</w>': 308, 'scale</w>': 309, 'backward</w>': 310, 'update</w>': 311, 'Epoch</w>': 312, 'Loss</w>': 313, 'Fast</w>': 314, 'WordPiece</w>': 315, 'Tokenization</w>': 316, 'Xinying</w>': 317, 'Song</w>': 318, '</w>': 319, 'Alex</w>': 320, 'Salcianu</w>': 321, 'Yang</w>': 322, '</w>': 323, '</w>': 324, 'Dave</w>': 325, 'Dopson</w>': 326, 'Denny</w>': 327, 'Zhou</w>': 328, 'Google</w>': 329, 'Research</w>': 330, 'Mountain</w>': 331, 'View</w>': 332, 'CA</w>': 333, 'xysong</w>': 334, 'salcianu</w>': 335, 'ddopson</w>': 336, 'dennyzhou</w>': 337, '@</w>': 338, 'google</w>': 339, 'com</w>': 340, 'Kuaishou</w>': 341, 'Technology</w>': 342, 'Beijing</w>': 343, 'China</w>': 344, 'yangsong</w>': 345, 'kuaishou</w>': 346, 'Abstract</w>': 347, 'fundamental</w>': 348, 'preprocessing</w>': 349, 'almost</w>': 350, 'all</w>': 351, 'NLP</w>': 352, 'tasks</w>': 353, 'In</w>': 354, 'paper</w>': 355, 'we</w>': 356, 'propose</w>': 357, 'efficient</w>': 358, 'algorithms</w>': 359, 'tokenization</w>': 360, 'used</w>': 361, 'BERT</w>': 362, 'singleword</w>': 363, 'general</w>': 364, 'text</w>': 365, 'e</w>': 366, 'g</w>': 367, 'sentence</w>': 368, 'When</w>': 369, 'tokenizing</w>': 370, 'single</w>': 371, 'uses</w>': 372, 'longest</w>': 373, 'matchfirst</w>': 374, 'strategy</w>': 375, 'known</w>': 376, 'maximum</w>': 377, 'matching</w>': 378, 'The</w>': 379, 'best</w>': 380, 'so</w>': 381, 'far</w>': 382, '</w>': 383, '</w>': 384, 'where</w>': 385, 'or</w>': 386, '</w>': 387, '</w>': 388, 'vocabulary</w>': 389, 'token</w>': 390, 'We</w>': 391, 'novel</w>': 392, 'algorithm</w>': 393, 'whose</w>': 394, 'complexity</w>': 395, 'strictly</w>': 396, 'Our</w>': 397, 'method</w>': 398, 'inspired</w>': 399, 'by</w>': 400, 'Aho</w>': 401, 'Corasick</w>': 402, 'introduce</w>': 403, 'additional</w>': 404, 'linkages</w>': 405, 'top</w>': 406, 'trie</w>': 407, 'built</w>': 408, 'allowing</w>': 409, 'smart</w>': 410, 'transitions</w>': 411, 'when</w>': 412, 'cannot</w>': 413, 'continue</w>': 414, 'For</w>': 415, 'further</w>': 416, 'that</w>': 417, 'combines</w>': 418, 'pre</w>': 419, 'splitting</w>': 420, 'into</w>': 421, 'words</w>': 422, 'our</w>': 423, 'time</w>': 424, 'Experimental</w>': 425, 'results</w>': 426, 'show</w>': 427, '2x</w>': 428, 'faster</w>': 429, 'than</w>': 430, 'HuggingFace</w>': 431, 'Tokenizers</w>': 432, '5</w>': 433, '1x</w>': 434, 'TensorFlow</w>': 435, 'Text</w>': 436, 'average</w>': 437, 'Introduction</w>': 438, 'process</w>': 439, 'smaller</w>': 440, 'units</w>': 441, 'called</w>': 442, 'tokens</w>': 443, 'It</w>': 444, 'applications</w>': 445, 'sentiment</w>': 446, 'analysis</w>': 447, 'question</w>': 448, 'answering</w>': 449, 'machine</w>': 450, 'translation</w>': 451, 'information</w>': 452, 'retrieval</w>': 453, 'etc</w>': 454, 'Modern</w>': 455, 'models</w>': 456, 'like</w>': 457, 'Devlin</w>': 458, 'et</w>': 459, 'al</w>': 460, '2019</w>': 461, 'GPT</w>': 462, 'Brown</w>': 463, '2020</w>': 464, 'XLNet</w>': 465, 'tokenize</w>': 466, 'subword</w>': 467, 'Schuster</w>': 468, 'Nakajima</w>': 469, '2012</w>': 470, 'Sennrich</w>': 471, '2016</w>': 472, 'Kudo</w>': 473, '2018</w>': 474, 'As</w>': 475, 'midpoint</w>': 476, 'between</w>': 477, 'characters</w>': 478, 'retain</w>': 479, 'linguistic</w>': 480, 'meaning</w>': 481, 'morphemes</w>': 482, 'while</w>': 483, 'alleviating</w>': 484, 'situations</w>': 485, 'even</w>': 486, 'relatively</w>': 487, 'small</w>': 488, 'conducted</w>': 489, 'working</w>': 490, 'at</w>': 491, 'Given</w>': 492, 'Unicode</w>': 493, 'already</w>': 494, 'been</w>': 495, 'cleaned</w>': 496, 'up</w>': 497, 'normalized</w>': 498, 'two</w>': 499, 'steps</w>': 500, 'punctuation</w>': 501, 'whitespaces</w>': 502, 'wordpieces</w>': 503, 'greedy</w>': 504, 'first</w>': 505, 'iteratively</w>': 506, 'pick</w>': 507, 'prefix</w>': 508, 'remaining</w>': 509, 'matches</w>': 510, 'well</w>': 511, 'Maximum</w>': 512, 'Matching</w>': 513, 'MaxMatch</w>': 514, 'Palmer</w>': 515, '2000</w>': 516, 'which</w>': 517, 'also</w>': 518, 'Chinese</w>': 519, 'segmentation</w>': 520, 'since</w>': 521, '1980s</w>': 522, 'Liu</w>': 523, 'Liang</w>': 524, '1986</w>': 525, 'Despite</w>': 526, 'its</w>': 527, 'wide</w>': 528, 'use</w>': 529, 'decades</w>': 530, 'knowledge</w>': 531, 'most</w>': 532, 'see</w>': 533, 'Section</w>': 534, '</w>': 535, 'worth</w>': 536, 'noting</w>': 537, 'latter</w>': 538, 'vocabularyspecific</w>': 539, 'multiplicative</w>': 540, 'factor</w>': 541, 'can</w>': 542, 'be</w>': 543, 'large</w>': 544, 'contains</w>': 545, 'long</w>': 546, 'LinMaxMatch</w>': 547, 'any</w>': 548, 'specific</w>': 549, 'factors</w>': 550, 'Inspired</w>': 551, '1975</w>': 552, 'organize</w>': 553, 'Fredkin</w>': 554, '1960</w>': 555, 'precomputed</w>': 556, 'failure</w>': 557, 'links</w>': 558, 'pops</w>': 559, 'During</w>': 560, 'character</w>': 561, 'does</w>': 562, 'not</w>': 563, 'edge</w>': 564, 'perform</w>': 565, 'avoid</w>': 566, 'backtracking</w>': 567, 'earlier</w>': 568, 'involves</w>': 569, 'collecting</w>': 570, 'recognized</w>': 571, 'moving</w>': 572, 'node</w>': 573, 'via</w>': 574, 'link</w>': 575, 'same</w>': 576, 'referred</w>': 577, 'end</w>': 578, 'E2E</w>': 579, 'arXiv</w>': 580, '15524v3</w>': 581, 'cs</w>': 582, 'CL</w>': 583, 'Oct</w>': 584, '2021</w>': 585, '4</w>': 586, 'Although</w>': 587, 'other</w>': 588, 'it</w>': 589, 'still</w>': 590, 'improving</w>': 591, 'performance</w>': 592, 'prerequisite</w>': 593, 'improvement</w>': 594, 'efficiency</w>': 595, 'helps</w>': 596, 'latency</w>': 597, 'entire</w>': 598, 'inference</w>': 599, 'One</w>': 600, 'potential</w>': 601, 'impact</w>': 602, 'work</w>': 603, 'mobile</w>': 604, 'Ondevice</w>': 605, 'generally</w>': 606, 'highly</w>': 607, 'optimized</w>': 608, 'reducing</w>': 609, 'distilling</w>': 610, 'compressing</w>': 611, 'larger</w>': 612, 'Thus</w>': 613, 'significant</w>': 614, 'Another</w>': 615, 'aggregate</w>': 616, 'computational</w>': 617, 'savings</w>': 618, 'Web</w>': 619, 'services</w>': 620, 'Facebook</w>': 621, 'Twitter</w>': 622, 'power</w>': 623, 'search</w>': 624, 'nowadays</w>': 625, 'serves</w>': 626, 'billions</w>': 627, 'per</w>': 628, 'day</w>': 629, 'processes</w>': 630, 'hundreds</w>': 631, 'trillions</w>': 632, 'pages</w>': 633, 'building</w>': 634, 'By</w>': 635, 'employing</w>': 636, 'system</w>': 637, 'would</w>': 638, 'material</w>': 639, 'benefits</w>': 640, 'environment</w>': 641, 'less</w>': 642, 'consumption</w>': 643, 'makes</w>': 644, 'theoretical</w>': 645, 'contribution</w>': 646, 'proposed</w>': 647, 'solves</w>': 648, 'old</w>': 649, 'problem</w>': 650, 'optimal</w>': 651, 'idea</w>': 652, 'applicable</w>': 653, 'string</w>': 654, 'rewriting</w>': 655, 'problems</w>': 656, '6</w>': 657, 'code</w>': 658, 'will</w>': 659, 'available</w>': 660, 'https</w>': 661, 'www</w>': 662, 'tensorflow</w>': 663, 'org</w>': 664, 'Related</w>': 665, 'Work</w>': 666, 'CWS</w>': 667, 'Recent</w>': 668, 'focuses</w>': 669, 'learning</w>': 670, 'based</w>': 671, 'approaches</w>': 672, 'but</w>': 673, 'remains</w>': 674, 'commonly</w>': 675, 'referenced</w>': 676, 'baseline</w>': 677, 'Chang</w>': 678, '2008</w>': 679, 'More</w>': 680, 'recently</w>': 681, 'techniques</w>': 682, 'have</w>': 683, 'become</w>': 684, 'near</w>': 685, 'universal</w>': 686, 'feature</w>': 687, 'modern</w>': 688, 'including</w>': 689, 'Common</w>': 690, 'include</w>': 691, 'Byte</w>': 692, 'Pair</w>': 693, 'Encoding</w>': 694, 'BPE</w>': 695, 'SentencePiece</w>': 696, 'unigram</w>': 697, 'lan1https</w>': 698, 'blog</w>': 699, 'products</w>': 700, 'language</w>': 701, 'understanding</w>': 702, 'bert</w>': 703, 'guage</w>': 704, 'modeling</w>': 705, 'widely</w>': 706, 'adopted</w>': 707, 'original</w>': 708, 'starts</w>': 709, 'possible</w>': 710, 'decrements</w>': 711, 'Jie</w>': 712, '1989</w>': 713, 'A</w>': 714, 'variant</w>': 715, 'shortest</w>': 716, 'substring</w>': 717, 'increases</w>': 718, 'Webster</w>': 719, 'Kit</w>': 720, '1992</w>': 721, 'Reps</w>': 722, '1998</w>': 723, 'Sassano</w>': 724, '2014</w>': 725, 'worst</w>': 726, 'case</w>': 727, 'previous</w>': 728, 'higher</w>': 729, '23</w>': 730, 'notations</w>': 731, 'Lookup</w>': 732, 't</w>': 733, 'c</w>': 734, 'Figure</w>': 735, 'their</w>': 736, 'may</w>': 737, 'take</w>': 738, 'similar</w>': 739, 'recognizes</w>': 740, 'regular</w>': 741, 'expressions</w>': 742, 'context</w>': 743, 'compilers</w>': 744, '|</w>': 745, '</w>': 746, 'number</w>': 747, 'states</w>': 748, 'automaton</w>': 749, 'grammar</w>': 750, 'If</w>': 751, 'applied</w>': 752, 'finite</w>': 753, 'strings</w>': 754, 'refined</w>': 755, 'designed</w>': 756, 'address</w>': 757, 'different</w>': 758, 'scenario</w>': 759, 'every</w>': 760, 'finds</w>': 761, 'quadratic</w>': 762, 'overall</w>': 763, 'comparison</w>': 764, 'achieves</w>': 765, 'due</w>': 766, 'definition</w>': 767, 'newly</w>': 768, 'introduced</w>': 769, 'way</w>': 770, 'emitting</w>': 771, 'clarifying</w>': 772, 'difference</w>': 773, 'tabular</w>': 774, 'solution</w>': 775, 'table</w>': 776, 'failed_previously</w>': 777, 'store</w>': 778, 'whether</w>': 779, 'state</w>': 780, '<</w>': 781, 'q</w>': 782, 'seen</w>': 783, 'failed</w>': 784, 'attempt</w>': 785, 'position</w>': 786, 'wasteful</w>': 787, 'revisits</w>': 788, 'entries</w>': 789, 'depend</w>': 790, 'both</w>': 791, 'contrast</w>': 792, 'capture</w>': 793, '2The</w>': 794, 'depends</w>': 795, 'implementation</w>': 796, 'details</w>': 797, 'hashes</w>': 798, 'computed</w>': 799, 'scratch</w>': 800, 'incrementally</w>': 801, 'how</w>': 802, 'substrings</w>': 803, 'searched</w>': 804, '3Previous</w>': 805, 'studies</w>': 806, 'usually</w>': 807, 'do</w>': 808, 'explicitly</w>': 809, 'related</w>': 810, 'just</w>': 811, 'treat</w>': 812, 'hidden</w>': 813, 'constant</w>': 814, 'transit</w>': 815, 'Definition</w>': 816, 'they</w>': 817, 'only</w>': 818, 'independent</w>': 819, 'Finally</w>': 820, 'discuss</w>': 821, 'Note</w>': 822, 'topic</w>': 823, 'found</w>': 824, 'Viterbi</w>': 825, '1967</w>': 826, 'implemented</w>': 827, 'ways</w>': 828, 'enumerate</w>': 829, 'symbol</w>': 830, 'pairs</w>': 831, 'order</w>': 832, 'were</w>': 833, 'added</w>': 834, 'phase</w>': 835, 'pair</w>': 836, 'scan</w>': 837, 'current</w>': 838, 'occurrences</w>': 839, 'merged</w>': 840, '</w>': 841, 'approach</w>': 842, 'repeatedly</w>': 843, 'select</w>': 844, 'symbols</w>': 845, 'highest</w>': 846, 'priority</w>': 847, 'frequency</w>': 848, 'Using</w>': 849, 'heap</w>': 850, 'done</w>': 851, '</w>': 852, 'Time</w>': 853, 'Single</w>': 854, 'Word</w>': 855, 'section</w>': 856, 'present</w>': 857, 'Background</w>': 858, 'Notations</w>': 859, 'tokenizes</w>': 860, 'until</w>': 861, 'segmented</w>': 862, 'tokenized</w>': 863, 'mapped</w>': 864, 'special</w>': 865, 'unk</w>': 866, 'distinguishes</w>': 867, 'start</w>': 868, 'starting</w>': 869, 'middle</w>': 870, 'suffix</w>': 871, 'indicator</w>': 872, 'denoted</w>': 873, '</w>': 874, 'works</w>': 875, 'arbitrary</w>': 876, 'empty</w>': 877, 'no</w>': 878, 'distinction</w>': 879, 'kinds</w>': 880, 'johanson</w>': 881, 'johan</w>': 882, 'son</w>': 883, 'running</w>': 884, 'Table</w>': 885, 'summarizes</w>': 886, 'construct</w>': 887, '</w>': 888, '</w>': 889, '</w>': 890, '</w>': 891, 'denote</w>': 892, 'label</w>': 893, 'there</w>': 894, 'outgoing</w>': 895, '4The</w>': 896, 'construction</w>': 897, 'outside</w>': 898, 'scope</w>': 899, 'refer</w>': 900, 'interested</w>': 901, 'reader</w>': 902, '</w>': 903, 'Let</w>': 904, '</w>': 905, 'represented</w>': 906, 'obtained</w>': 907, 'concatenating</w>': 908, 'labels</w>': 909, 'along</w>': 910, 'path</w>': 911, 'root</w>': 912, '</w>': 913, 'Obviously</w>': 914, '</w>': 915, '</w>': 916, 'denotes</w>': 917, 'depth</w>': 918, 'defined</w>': 919, 'excluding</w>': 920, 'Hence</w>': 921, '1a</w>': 922, 'nodes</w>': 923, 'Symbol</w>': 924, 'Meaning</w>': 925, 'unkown</w>': 926, '</w>': 927, '</w>': 928, 'whitespace</w>': 929, 'Trie</w>': 930, 'often</w>': 931, 'parent</w>': 932, 'Null</w>': 933, '</w>': 934, '</w>': 935, 'Failure</w>': 936, '</w>': 937, 'sum</w>': 938, 'lengths</w>': 939, 'Intuition</w>': 940, 'motivate</w>': 941, 'let</w>': 942, 'consider</w>': 943, 'alternative</w>': 944, 'simple</w>': 945, 'searching</w>': 946, 'iterates</w>': 947, 'over</w>': 948, 'left</w>': 949, 'right</w>': 950, 'following</w>': 951, 'find</w>': 952, 'prefixes</w>': 953, 'Consider</w>': 954, 'abcdz</w>': 955, 'dz</w>': 956, 'Starting</w>': 957, 'follow</w>': 958, 'edges</w>': 959, 'd</w>': 960, 'arriving</w>': 961, 'No</w>': 962, 'exits</w>': 963, 'z</w>': 964, '</w>': 965, 'challenge</w>': 966, 'tokenizer</w>': 967, 'detokenizer</w>': 968, 'Neural</w>': 969, 'Processing</w>': 970, 'Taku</w>': 971, 'John</w>': 972, 'Richardson</w>': 973, 'Inc</w>': 974, 'taku</w>': 975, 'johnri</w>': 976, 'describes</w>': 977, 'Machine</w>': 978, 'Translation</w>': 979, 'provides</w>': 980, 'open</w>': 981, 'source</w>': 982, 'Python</w>': 983, 'implementations</w>': 984, 'While</w>': 985, 'existing</w>': 986, 'tools</w>': 987, 'assume</w>': 988, 'sequences</w>': 989, 'train</w>': 990, 'directly</w>': 991, 'raw</w>': 992, 'sentences</w>': 993, 'allows</w>': 994, 'us</w>': 995, 'make</w>': 996, 'purely</w>': 997, 'validation</w>': 998, 'experiment</w>': 999, 'NMT</w>': 1000, 'English</w>': 1001, 'Japanese</w>': 1002, 'achieve</w>': 1003, 'comparable</w>': 1004, 'accuracy</w>': 1005, 'direct</w>': 1006, 'training</w>': 1007, 'compare</w>': 1008, 'various</w>': 1009, 'configurations</w>': 1010, 'under</w>': 1011, 'Apache</w>': 1012, 'license</w>': 1013, 'github</w>': 1014, 'sentencepiece</w>': 1015, 'Deep</w>': 1016, 'neural</w>': 1017, 'networks</w>': 1018, 'demonstrating</w>': 1019, 'Natural</w>': 1020, 'Language</w>': 1021, 'Bahdanau</w>': 1022, 'Luong</w>': 1023, '2015</w>': 1024, 'Wu</w>': 1025, 'Vaswani</w>': 1026, '2017</w>': 1027, 'especially</w>': 1028, 'gained</w>': 1029, 'increasing</w>': 1030, 'popularity</w>': 1031, 'leverage</w>': 1032, 'translations</w>': 1033, 'toend</w>': 1034, 'architecture</w>': 1035, 'shown</w>': 1036, 'remarkable</w>': 1037, 'several</w>': 1038, 'shared</w>': 1039, 'Denkowski</w>': 1040, 'Neubig</w>': 1041, 'Nakazawa</w>': 1042, 'effective</w>': 1043, 'had</w>': 1044, 'strong</w>': 1045, 'influence</w>': 1046, 'such</w>': 1047, 'dialog</w>': 1048, 'generation</w>': 1049, 'Vinyals</w>': 1050, 'Le</w>': 1051, 'automatic</w>': 1052, 'summarization</w>': 1053, 'Rush</w>': 1054, 'potentially</w>': 1055, 'many</w>': 1056, 'systems</w>': 1057, 'relying</w>': 1058, 'dependent</w>': 1059, 'postprocessors</w>': 1060, 'traditional</w>': 1061, 'statistical</w>': 1062, 'SMT</w>': 1063, 'Moses1</w>': 1064, 'de</w>': 1065, 'facto</w>': 1066, 'standard</w>': 1067, 'toolkit</w>': 1068, 'implements</w>': 1069, 'reasonably</w>': 1070, 'useful</w>': 1071, 'postprocessor</w>': 1072, 'However</w>': 1073, 'upon</w>': 1074, 'hand</w>': 1075, 'crafted</w>': 1076, 'rules</w>': 1077, 'effectiveness</w>': 1078, 'proven</w>': 1079, 'addition</w>': 1080, 'these</w>': 1081, 'mainly</w>': 1082, 'European</w>': 1083, 'languages</w>': 1084, 'non</w>': 1085, 'Korean</w>': 1086, 'run</w>': 1087, 'segmenters</w>': 1088, 'independently</w>': 1089, 'Such</w>': 1090, 'languagedependent</w>': 1091, 'hard</w>': 1092, 'multilingual</w>': 1093, 'Johnson</w>': 1094, 'carefully</w>': 1095, 'manage</w>': 1096, 'internal</w>': 1097, 'deep</w>': 1098, 'architectures</w>': 1099, 'languageindependent</w>': 1100, 'standardized</w>': 1101, 'more</w>': 1102, 'agnostic</w>': 1103, 'becoming</w>': 1104, 'important</w>': 1105, 'community</w>': 1106, 'develop</w>': 1107, 'reproducible</w>': 1108, 'easily</w>': 1109, 'integrated</w>': 1110, 'Network</w>': 1111, 'demo</w>': 1112, 'describe</w>': 1113, 'Networkbased</w>': 1114, 'predetermined</w>': 1115, 'prior</w>': 1116, 'byte</w>': 1117, 'pairencoding</w>': 1118, 'extension</w>': 1119, 'enables</w>': 1120, 'languagespecific</w>': 1121, '1http</w>': 1122, 'statmt</w>': 1123, 'moses</w>': 1124, '1808</w>': 1125, '06226v1</w>': 1126, '19</w>': 1127, 'Aug</w>': 1128, '%</w>': 1129, 'spm_train</w>': 1130, '</w>': 1131, 'txt</w>': 1132, 'model_prefix</w>': 1133, 'spm</w>': 1134, 'vocab_size</w>': 1135, '1000</w>': 1136, 'echo</w>': 1137, 'Hello</w>': 1138, 'world</w>': 1139, 'spm_encode</w>': 1140, '_He</w>': 1141, 'll</w>': 1142, 'o</w>': 1143, '_world</w>': 1144, 'output_format</w>': 1145, 'id</w>': 1146, '151</w>': 1147, '88</w>': 1148, '21</w>': 1149, '887</w>': 1150, 'spm_decode</w>': 1151, 'input_format</w>': 1152, 'Commandline</w>': 1153, 'System</w>': 1154, 'Overview</w>': 1155, 'comprises</w>': 1156, 'four</w>': 1157, 'main</w>': 1158, 'components</w>': 1159, 'Normalizer</w>': 1160, 'Trainer</w>': 1161, 'Encoder</w>': 1162, 'Decoder</w>': 1163, 'module</w>': 1164, 'normalize</w>': 1165, 'semanticallyequivalent</w>': 1166, 'canonical</w>': 1167, 'forms</w>': 1168, 'trains</w>': 1169, 'corpus</w>': 1170, 'specify</w>': 1171, 'type</w>': 1172, 'parameter</w>': 1173, 'internally</w>': 1174, 'executes</w>': 1175, 'trained</w>': 1176, 'converts</w>': 1177, 'roles</w>': 1178, 'correspond</w>': 1179, 'postprocessing</w>': 1180, 'detokenization</w>': 1181, 'respectively</w>': 1182, 'call</w>': 1183, 'them</w>': 1184, 'encoding</w>': 1185, 'decoding</w>': 1186, 'manages</w>': 1187, 'mapping</w>': 1188, 'convert</w>': 1189, 'vice</w>': 1190, 'versa</w>': 1191, 'Direct</w>': 1192, '?</w>': 1193, 'presents</w>': 1194, 'reversibly</w>': 1195, 'converted</w>': 1196, 'Library</w>': 1197, 'Design</w>': 1198, 'design</w>': 1199, 'command</w>': 1200, 'line</w>': 1201, 'snippets</w>': 1202, 'Lossless</w>': 1203, '</w>': 1204, 'Raw</w>': 1205, 'Tokenized</w>': 1206, 'observation</w>': 1207, 'convertible</w>': 1208, 'space</w>': 1209, 'exists</w>': 1210, '</w>': 1211, '</w>': 1212, 'kept</w>': 1213, 'Detokenization</w>': 1214, 'restore</w>': 1215, 'irreversible</w>': 1216, 'operations</w>': 1217, 'puts</w>': 1218, 'primitive</w>': 1219, 'spaces</w>': 1220, 'required</w>': 1221, '</w>': 1222, '</w>': 1223, '</w>': 1224, '</w>': 1225, 'manually</w>': 1226, 'expensive</w>': 1227, 'write</w>': 1228, 'maintain</w>': 1229, 'inverse</w>': 1230, 'operation</w>': 1231, 'Decode</w>': 1232, 'Encode</w>': 1233, 'Normalize</w>': 1234, 'lossless</w>': 1235, 'reproduce</w>': 1236, 'preserved</w>': 1237, 'encoder</w>': 1238, 'basic</w>': 1239, 'Even</w>': 1240, 'handled</w>': 1241, 'normal</w>': 1242, 'sake</w>': 1243, 'clarity</w>': 1244, 'escapes</w>': 1245, 'meta</w>': 1246, 'U</w>': 1247, '2581</w>': 1248, 'Hello_world</w>': 1249, '_wor</w>': 1250, 'ld</w>': 1251, 'detokenize</w>': 1252, 'ambiguities</w>': 1253, 'detok</w>': 1254, 'join</w>': 1255, 'should</w>': 1256, 'noted</w>': 1257, 'nmt2</w>': 1258, 'adopts</w>': 1259, 'representation</w>': 1260, 'subwords</w>': 1261, 'intra</w>': 1262, 'boundary</w>': 1263, 'marker</w>': 1264, '2https</w>': 1265, 'rsennrich</w>': 1266, 'nmt</w>': 1267, 'wor</w>': 1268, 'always</w>': 1269, 'ambiguity</w>': 1270, 'treatment</w>': 1271, 'specifically</w>': 1272, 'encode</w>': 1273, 'consecutive</w>': 1274, 'Efficient</w>': 1275, 'Existing</w>': 1276, 'was</w>': 1277, 'pretokenization</w>': 1278, 'difficult</w>': 1279, 'employs</w>': 1280, 'speed</w>': 1281, 'amount</w>': 1282, 'given</w>': 1283, 'requires</w>': 1284, 'O</w>': 1285, 'N2</w>': 1286, 'cost</w>': 1287, 'naively</w>': 1288, 'iteration</w>': 1289, 'log</w>': 1290, 'managed</w>': 1291, 'binary</w>': 1292, 'queue</w>': 1293, 'complexities</w>': 1294, 'Vocabulary</w>': 1295, 'management</w>': 1296, 'specified</w>': 1297, 'flag</w>': 1298, 'specifies</w>': 1299, 'merge</w>': 1300, 'reserves</w>': 1301, 'ids</w>': 1302, 'unknown</w>': 1303, 'BOS</w>': 1304, 'EOS</w>': 1305, 'padding</w>': 1306, 'pad</w>': 1307, 'Their</w>': 1308, 'configured</w>': 1309, 'flags</w>': 1310, 'define</w>': 1311, 'custom</w>': 1312, 'contextual</w>': 1313, 'virtual</w>': 1314, 'Examples</w>': 1315, 'languageindicators</w>': 1316, '2ja</w>': 1317, '2de</w>': 1318, '41</w>': 1319, '302</w>': 1320, '300</w>': 1321, 'tab</w>': 1322, '1EA6</w>': 1323, '301</w>': 1324, '1EA4</w>': 1325, 'Custom</w>': 1326, 'normalization</w>': 1327, 'rule</w>': 1328, 'TSV</w>': 1329, 'Customizable</w>': 1330, 'Character</w>': 1331, 'handling</w>': 1332, 'real</w>': 1333, 'consists</w>': 1334, 'semantically</w>': 1335, 'equivalent</w>': 1336, 'fullwidth</w>': 1337, 'Latin</w>': 1338, 'ASCII</w>': 1339, 'Lowercasing</w>': 1340, 'application</w>': 1341, 'Recently</w>': 1342, 'Normalization</w>': 1343, 'Forms</w>': 1344, 'NFC</w>': 1345, 'NFKC</w>': 1346, 'because</w>': 1347, 'better</w>': 1348, 'reproducibility</w>': 1349, 'support</w>': 1350, 'default</w>': 1351, 'normalizes</w>': 1352, 'normalization_rule_name</w>': 1353, 'nfkc</w>': 1354, 'Sentencepiece</w>': 1355, 'leftmost</w>': 1356, 'compiled</w>': 1357, 'transducer</w>': 1358, 'normalization3</w>': 1359, 'supports</w>': 1360, 'file</w>': 1361, 'shows</w>': 1362, '1EA64</w>': 1363, 'conversion</w>': 1364, 'User</w>': 1365, 'files</w>': 1366, 'normalization_rule_tsv</w>': 1367, 'Task</w>': 1368, 'extending</w>': 1369, 'provided</w>': 1370, 'package</w>': 1371, 'Self</w>': 1372, 'contained</w>': 1373, 'researchers</w>': 1374, 'pretrained</w>': 1375, 'reproduciblity</w>': 1376, '3The</w>': 1377, 'CCC</w>': 1378, 'Canonical</w>': 1379, 'Combining</w>': 1380, 'Class</w>': 1381, 'reordering</w>': 1382, 'handle</w>': 1383, 'full</w>': 1384, 'subset</w>': 1385, '4Note</w>': 1386, 'tabs</w>': 1387, 'delimiter</w>': 1388, 'individual</w>': 1389, 'experimental</w>': 1390, 'stated</w>': 1391, 'preprocessed</w>': 1392, 'Post</w>': 1393, 'reported</w>': 1394, 'subtle</w>': 1395, 'differences</w>': 1396, 'schemes</w>': 1397, 'change</w>': 1398, 'BLEU</w>': 1399, 'scores</w>': 1400, 'Moses</w>': 1401, 'guaranteed</w>': 1402, 'settings</w>': 1403, 'unless</w>': 1404, 'version</w>': 1405, 'clearly</w>': 1406, 'Strictly</w>': 1407, 'speaking</w>': 1408, 'yield</w>': 1409, 'Ideally</w>': 1410, 'must</w>': 1411, 'embedded</w>': 1412, 'manner</w>': 1413, 'setting</w>': 1414, 'includes</w>': 1415, 'behavior</w>': 1416, 'determined</w>': 1417, 'external</w>': 1418, 'dependencies</w>': 1419, 'guarantees</w>': 1420, 'perfect</w>': 1421, 'distribute</w>': 1422, 'part</w>': 1423, 'developers</w>': 1424, 'refine</w>': 1425, 'having</w>': 1426, 'worry</w>': 1427, 'about</w>': 1428, 'breaking</w>': 1429, 'behaviors</w>': 1430, 'stored</w>': 1431, 'wire</w>': 1432, 'format</w>': 1433, 'Protocol</w>': 1434, 'buffer5</w>': 1435, 'platform</w>': 1436, 'neutral</w>': 1437, 'extensible</w>': 1438, 'mechanism</w>': 1439, 'serializing</w>': 1440, 'structured</w>': 1441, 'buffers</w>': 1442, 'help</w>': 1443, 'safely</w>': 1444, 'serialize</w>': 1445, 'keeping</w>': 1446, 'compatibility</w>': 1447, 'extensibility</w>': 1448, 'API</w>': 1449, 'fly</w>': 1450, 'considered</w>': 1451, 'offline</w>': 1452, 'Prior</w>': 1453, 'standalone</w>': 1454, 'preprocessor</w>': 1455, 'off</w>': 1456, 'First</w>': 1457, 'user</w>': 1458, 'facing</w>': 1459, 'preprocess</w>': 1460, 'Second</w>': 1461, 'employ</w>': 1462, 'subsentence</w>': 1463, 'level</w>': 1464, 'augmentation</w>': 1465, 'noise</w>': 1466, 'injection</w>': 1467, 'aim</w>': 1468, 'robustness</w>': 1469, 'There</w>': 1470, 'inject</w>': 1471, 'ran5https</w>': 1472, 'protocol</w>': 1473, 'sentencepiece_processor</w>': 1474, 'sentencepiece_trainer</w>': 1475, 'SentencePieceTrainer</w>': 1476, 'Train</w>': 1477, 'SentencePieceProcessor</w>': 1478, 'sp</w>': 1479, 'Load</w>': 1480, 'std</w>': 1481, 'vector</w>': 1482, 'pieces</w>': 1483, '&</w>': 1484, 'int</w>': 1485, 'params</w>': 1486, 'EncodeAsPieces</w>': 1487, 'EncodeAsIds</w>': 1488, 'DecodeIds</w>': 1489, 'domly</w>': 1490, 'changing</w>': 1491, 'proposes</w>': 1492, 'regularization</w>': 1493, 'randomly</w>': 1494, 'changes</w>': 1495, 'during</w>': 1496, 'Lample</w>': 1497, 'Artetxe</w>': 1498, 'denoising</w>': 1499, 'autoencoder</w>': 1500, 'alter</w>': 1501, 'reconstruct</w>': 1502, 'emulate</w>': 1503, 'dynamic</w>': 1504, 'sampling</w>': 1505, 'tool</w>': 1506, 'Tensorflow</w>': 1507, 'library</w>': 1508, 'frameworks</w>': 1509, 'Figures</w>': 1510, 'usages</w>': 1511, 'API6</w>': 1512, 'one</w>': 1513, 'sampled</w>': 1514, 'New</w>': 1515, 'York</w>': 1516, 'differently</w>': 1517, 'How</w>': 1518, 'transferable</w>': 1519, 'features</w>': 1520, 'Jason</w>': 1521, 'Yosinski</w>': 1522, 'Jeff</w>': 1523, 'Clune</w>': 1524, 'Yoshua</w>': 1525, 'Bengio</w>': 1526, 'Hod</w>': 1527, 'Lipson4</w>': 1528, 'Dept</w>': 1529, 'Computer</w>': 1530, 'Science</w>': 1531, 'Cornell</w>': 1532, 'University</w>': 1533, 'Wyoming</w>': 1534, 'Operations</w>': 1535, 'Montreal</w>': 1536, 'Mechanical</w>': 1537, 'Aerospace</w>': 1538, 'Engineering</w>': 1539, 'Many</w>': 1540, 'natural</w>': 1541, 'images</w>': 1542, 'exhibit</w>': 1543, 'curious</w>': 1544, 'phenomenon</w>': 1545, 'common</w>': 1546, 'learn</w>': 1547, 'Gabor</w>': 1548, 'filters</w>': 1549, 'color</w>': 1550, 'blobs</w>': 1551, 'appear</w>': 1552, 'particular</w>': 1553, 'datasets</w>': 1554, 'Features</w>': 1555, 'eventually</w>': 1556, 'transition</w>': 1557, 'last</w>': 1558, 'network</w>': 1559, 'studied</w>': 1560, 'extensively</w>': 1561, 'experimentally</w>': 1562, 'quantify</w>': 1563, 'generality</w>': 1564, 'versus</w>': 1565, 'specificity</w>': 1566, 'neurons</w>': 1567, 'convolutional</w>': 1568, 'report</w>': 1569, 'few</w>': 1570, 'surprising</w>': 1571, 'Transferability</w>': 1572, 'negatively</w>': 1573, 'affected</w>': 1574, 'distinct</w>': 1575, 'issues</w>': 1576, 'specialization</w>': 1577, 'expense</w>': 1578, 'optimization</w>': 1579, 'difficulties</w>': 1580, 'co</w>': 1581, 'adapted</w>': 1582, 'ImageNet</w>': 1583, 'demonstrate</w>': 1584, 'either</w>': 1585, 'dominate</w>': 1586, 'transferred</w>': 1587, 'bottom</w>': 1588, 'document</w>': 1589, 'transferability</w>': 1590, 'decreases</w>': 1591, 'distance</w>': 1592, 'base</w>': 1593, 'transferring</w>': 1594, 'distant</w>': 1595, 'result</w>': 1596, 'initializing</w>': 1597, 'layers</w>': 1598, 'produce</w>': 1599, 'boost</w>': 1600, 'generalization</w>': 1601, 'lingers</w>': 1602, 'fine</w>': 1603, 'tuning</w>': 1604, 'tend</w>': 1605, 'resemble</w>': 1606, 'appearance</w>': 1607, 'obtaining</w>': 1608, 'anything</w>': 1609, 'image</w>': 1610, 'causes</w>': 1611, 'suspicion</w>': 1612, 'poorly</w>': 1613, 'chosen</w>': 1614, 'hyperparameters</w>': 1615, 'software</w>': 1616, 'bug</w>': 1617, 'occurs</w>': 1618, 'very</w>': 1619, 'objectives</w>': 1620, 'supervised</w>': 1621, 'Krizhevsky</w>': 1622, 'unsupervised</w>': 1623, 'density</w>': 1624, 'Lee</w>': 1625, '2009</w>': 1626, 'sparse</w>': 1627, 'representations</w>': 1628, '2011</w>': 1629, 'Because</w>': 1630, 'finding</w>': 1631, 'seems</w>': 1632, 'occur</w>': 1633, 'regardless</w>': 1634, 'function</w>': 1635, 'On</w>': 1636, 'know</w>': 1637, 'greatly</w>': 1638, 'dimensional</w>': 1639, 'successfully</w>': 1640, 'toward</w>': 1641, 'objective</w>': 1642, 'unit</w>': 1643, 'thus</w>': 1644, 'These</w>': 1645, 'intuitive</w>': 1646, 'notions</w>': 1647, 'provide</w>': 1648, 'rigorous</w>': 1649, 'definitions</w>': 1650, 'below</w>': 1651, '1411</w>': 1652, '1792v1</w>': 1653, 'LG</w>': 1654, 'Nov</w>': 1655, 'somewhere</w>': 1656, 'raises</w>': 1657, 'questions</w>': 1658, 'Can</w>': 1659, 'degree</w>': 1660, 'Does</w>': 1661, 'suddenly</w>': 1662, 'spread</w>': 1663, 'Where</w>': 1664, 'place</w>': 1665, 'answers</w>': 1666, 'extent</w>': 1667, 'within</w>': 1668, 'able</w>': 1669, 'transfer</w>': 1670, 'Caruana</w>': 1671, '1995</w>': 1672, 'repurpose</w>': 1673, 'learned</w>': 1674, 'second</w>': 1675, 'suitable</w>': 1676, 'instead</w>': 1677, 'significantly</w>': 1678, 'powerful</w>': 1679, 'enable</w>': 1680, 'overfitting</w>': 1681, 'taken</w>': 1682, 'advantage</w>': 1683, 'fact</w>': 1684, 'obtain</w>': 1685, 'art</w>': 1686, 'Donahue</w>': 1687, '2013a</w>': 1688, 'Zeiler</w>': 1689, 'Fergus</w>': 1690, '2013</w>': 1691, 'Sermanet</w>': 1692, 'collectively</w>': 1693, 'suggesting</w>': 1694, 'indeed</w>': 1695, 'compute</w>': 1696, 'fairly</w>': 1697, 'emphasize</w>': 1698, 'importance</w>': 1699, 'studying</w>': 1700, 'nature</w>': 1701, 'usual</w>': 1702, 'copy</w>': 1703, 'n</w>': 1704, 'initialized</w>': 1705, 'choose</w>': 1706, 'backpropagate</w>': 1707, 'errors</w>': 1708, 'new</w>': 1709, 'copied</w>': 1710, 'tune</w>': 1711, 'frozen</w>': 1712, 'tuned</w>': 1713, 'improve</w>': 1714, 'Of</w>': 1715, 'course</w>': 1716, 'little</w>': 1717, 'lower</w>': 1718, 'could</w>': 1719, '</w>': 1720, 'sections</w>': 1721, 'contributions</w>': 1722, 'namely</w>': 1723, 'another</w>': 1724, 'characterize</w>': 1725, 'yields</w>': 1726, 'separate</w>': 1727, 'cause</w>': 1728, 'degradation</w>': 1729, 'themselves</w>': 1730, 'ii</w>': 1731, 'neighboring</w>': 1732, 'effects</w>': 1733, 'dissimilar</w>': 1734, 'previously</w>': 1735, 'Jarrett</w>': 1736, 'weights</w>': 1737, 'vs</w>': 1738, 'particularly</w>': 1739, 'effect</w>': 1740, 'persists</w>': 1741, 'extensive</w>': 1742, 'Generality</w>': 1743, 'Specificity</w>': 1744, 'Measured</w>': 1745, 'Transfer</w>': 1746, 'Performance</w>': 1747, 'tendency</w>': 1748, 'study</w>': 1749, 'set</w>': 1750, 'B</w>': 1751, 'note</w>': 1752, 'similarity</w>': 1753, 'create</w>': 1754, 'constructing</w>': 1755, 'overlapping</w>': 1756, 'subsets</w>': 1757, 'split</w>': 1758, 'groups</w>': 1759, 'containing</w>': 1760, '500</w>': 1761, 'approximately</w>': 1762, 'half</w>': 1763, '645</w>': 1764, '000</w>': 1765, 'examples</w>': 1766, 'eight</w>': 1767, 'baseA</w>': 1768, 'baseB</w>': 1769, 'rows</w>': 1770, '7</w>': 1771, 'explanation</w>': 1772, 'selffer</w>': 1773, 'B3B</w>': 1774, 'five</w>': 1775, '</w>': 1776, 'control</w>': 1777, 'row</w>': 1778, 'A3B</w>': 1779, 'Intuitively</w>': 1780, 'classify</w>': 1781, 'performs</w>': 1782, 'evidence</w>': 1783, 'third</w>': 1784, 'least</w>': 1785, 'respect</w>': 1786, 'suffers</w>': 1787, 'repeated</w>': 1788, 'directions</w>': 1789, 'AnB</w>': 1790, 'BnA</w>': 1791, 'above</w>': 1792, 'versions</w>': 1793, 'assign</w>': 1794, 'clusters</w>': 1795, 'dogs</w>': 1796, 'cats</w>': 1797, '13</w>': 1798, 'biological</w>': 1799, 'family</w>': 1800, 'Felidae</w>': 1801, 'tabby</w>': 1802, 'cat</w>': 1803, 'tiger</w>': 1804, 'Persian</w>': 1805, 'Siamese</w>': 1806, 'Egyptian</w>': 1807, 'mountain</w>': 1808, 'lion</w>': 1809, 'lynx</w>': 1810, 'leopard</w>': 1811, 'snow</w>': 1812, 'jaguar</w>': 1813, 'cheetah</w>': 1814, 'contain</w>': 1815, 'felid</w>': 1816, 'levels</w>': 1817, 'some</w>': 1818, 'types</w>': 1819, 'felids</w>': 1820, 'generalizing</w>': 1821, 'expect</w>': 1822, 'high</w>': 1823, 'detectors</w>': 1824, 'low</w>': 1825, 'created</w>': 1826, 'assigning</w>': 1827, 'Fortunately</w>': 1828, 'hierarchy</w>': 1829, 'allowed</w>': 1830, 'halves</w>': 1831, 'man</w>': 1832, 'made</w>': 1833, 'entities</w>': 1834, 'quite</w>': 1835, '551</w>': 1836, 'group</w>': 1837, '449</w>': 1838, 'Further</w>': 1839, 'supplementary</w>': 1840, '1The</w>': 1841, 'released</w>': 1842, 'Large</w>': 1843, 'Scale</w>': 1844, 'Visual</w>': 1845, 'Recognition</w>': 1846, 'Challenge</w>': 1847, 'ILSVRC2012</w>': 1848, 'Deng</w>': 1849, '281</w>': 1850, '167</w>': 1851, 'labeled</w>': 1852, '50</w>': 1853, 'test</w>': 1854, '2Note</w>': 1855, 'doesn</w>': 1856, 'sense</w>': 1857, 'B8B</w>': 1858, 'A8B</w>': 1859, 'never</w>': 1860, 'WA1</w>': 1861, 'WA2</w>': 1862, 'WA3</w>': 1863, 'WA4</w>': 1864, 'WA5</w>': 1865, 'WA6</w>': 1866, 'WA7</w>': 1867, 'WA8</w>': 1868, 'WB1</w>': 1869, 'WB2</w>': 1870, 'WB3</w>': 1871, 'WB4</w>': 1872, 'WB5</w>': 1873, 'WB6</w>': 1874, 'WB7</w>': 1875, 'WB8</w>': 1876, 'treatments</w>': 1877, 'controls</w>': 1878, 'Top</w>': 1879, 'backprop</w>': 1880, 'rectangles</w>': 1881, 'represent</w>': 1882, 'weight</w>': 1883, 'indicating</w>': 1884, 'originally</w>': 1885, 'vertical</w>': 1886, 'ellipsoidal</w>': 1887, 'bars</w>': 1888, 'vectors</w>': 1889, 'activations</w>': 1890, 'Third</w>': 1891, 'upper</w>': 1892, 'locked</w>': 1893, 'reveals</w>': 1894, 'occurrence</w>': 1895, 'fragile</w>': 1896, 'coadaptation</w>': 1897, 'adapt</w>': 1898, 'rediscovered</w>': 1899, 'Fourth</w>': 1900, 'except</w>': 1901, 'tests</w>': 1902, 'Setup</w>': 1903, 'Since</w>': 1904, 'won</w>': 1905, 'competition</w>': 1906, 'much</w>': 1907, 'interest</w>': 1908, 'tweaking</w>': 1909, 'maximize</w>': 1910, 'absolute</w>': 1911, 'rather</w>': 1912, 'reference</w>': 1913, 'Caffe</w>': 1914, 'Jia</w>': 1915, 'setup</w>': 1916, 'rates</w>': 1917, 'experiments</w>': 1918, 'http</w>': 1919, 'yosinski</w>': 1920, 'Results</w>': 1921, 'Discussion</w>': 1922, 'performed</w>': 1923, 'three</w>': 1924, 'sets</w>': 1925, 'splits</w>': 1926, 'discussed</w>': 1927, '52</w>': 1928, '54</w>': 1929, '56</w>': 1930, '58</w>': 1931, '60</w>': 1932, '62</w>': 1933, '64</w>': 1934, '66</w>': 1935, 'BnB</w>': 1936, 'Layer</w>': 1937, 'chopped</w>': 1938, 'retrained</w>': 1939, 'improves</w>': 1940, 'Fine</w>': 1941, 'recovers</w>': 1942, 'interactions</w>': 1943, 'drops</w>': 1944, 'adaptation</w>': 1945, 'Each</w>': 1946, 'figure</w>': 1947, 'represents</w>': 1948, 'white</w>': 1949, 'circles</w>': 1950, 'points</w>': 1951, 'tested</w>': 1952, 'dark</w>': 1953, 'blue</w>': 1954, 'dot</w>': 1955, 'Light</w>': 1956, 'Dark</w>': 1957, 'red</w>': 1958, 'diamonds</w>': 1959, 'light</w>': 1960, 'Points</w>': 1961, 'shifted</w>': 1962, 'slightly</w>': 1963, 'visual</w>': 1964, 'Bottom</w>': 1965, 'Lines</w>': 1966, 'connecting</w>': 1967, 'means</w>': 1968, 'Numbered</w>': 1969, 'descriptions</w>': 1970, 'interpretation</w>': 1971, 'applies</w>': 1972, 'Similar</w>': 1973, 'Datasets</w>': 1974, 'shown3</w>': 1975, 'conclusions</w>': 1976, 'interpretations</w>': 1977, 'dotted</w>': 1978, '3AnA</w>': 1979, 'statistically</w>': 1980, 'cases</w>': 1981, 'simplify</w>': 1982, 'notation</w>': 1983, 'Similarly</w>': 1984, 'aggregated</w>': 1985, 'identical</w>': 1986, 'attains</w>': 1987, '625</w>': 1988, '37</w>': 1989, 'error</w>': 1990, '42</w>': 1991, 'attained</w>': 1992, 'lead</w>': 1993, 'net</w>': 1994, 'mistakes</w>': 1995, 'That</w>': 1996, 'save</w>': 1997, 'reinitialize</w>': 1998, 'whole</w>': 1999, 'retrain</w>': 2000, 'holds</w>': 2001, 'true</w>': 2002, 'worse</w>': 2003, 'drop</w>': 2004, 'successive</w>': 2005, 'interact</w>': 2006, 'complex</w>': 2007, 'relearned</w>': 2008, 'alone</w>': 2009, 'Gradient</w>': 2010, 'descent</w>': 2011, 'good</w>': 2012, 'jointly</w>': 2013, 'nearly</w>': 2014, 'back</w>': 2015, 'get</w>': 2016, 'closer</w>': 2017, 'relearn</w>': 2018, 'apparently</w>': 2019, 'relearning</w>': 2020, 'enough</w>': 2021, 'Alternately</w>': 2022, 'say</w>': 2023, 'observed</w>': 2024, 'literature</w>': 2025, 'prevents</w>': 2026, 'measure</w>': 2027, 'Layers</w>': 2028, 'perfectly</w>': 2029, 'giving</w>': 2030, 'blob</w>': 2031, 'slight</w>': 2032, 'Thanks</w>': 2033, 'tell</w>': 2034, 'combination</w>': 2035, 'lost</w>': 2036, 'dominates</w>': 2037, 'whereas</w>': 2038, 'diminishes</w>': 2039, 'successful</w>': 2040, 'elsewhere</w>': 2041, 'Girshick</w>': 2042, '2013b</w>': 2043, 'limited</w>': 2044, 'noticing</w>': 2045, 'believe</w>': 2046, 'quantified</w>': 2047, 'decoupled</w>': 2048, 'showing</w>': 2049, 'regime</w>': 2050, 'generalize</w>': 2051, 'those</w>': 2052, 'Previously</w>': 2053, 'reason</w>': 2054, 'want</w>': 2055, 'suggests</w>': 2056, 'attributed</w>': 2057, 'longer</w>': 2058, 'total</w>': 2059, '450k</w>': 2060, 'iterations</w>': 2061, 'finetuned</w>': 2062, 'plausible</w>': 2063, 'beginning</w>': 2064, 'completely</w>': 2065, 'linger</w>': 2066, 'boosting</w>': 2067, 'retraining</w>': 2068, 'keep</w>': 2069, 'initialize</w>': 2070, 'anywhere</w>': 2071, 'seven</w>': 2072, 'produces</w>': 2073, 'improved</w>': 2074, 'across</w>': 2075, '4We</w>': 2076, 'point</w>': 2077, 'computationally</w>': 2078, 'days</w>': 2079, 'GPU</w>': 2080, 'publication</w>': 2081, 'aggregation</w>': 2082, 'averaged</w>': 2083, 'ranges</w>': 2084, 'mean</w>': 2085, 'Dissimilar</w>': 2086, 'Splitting</w>': 2087, 'Man</w>': 2088, 'Classes</w>': 2089, 'Into</w>': 2090, 'Separate</w>': 2091, 'mentioned</w>': 2092, 'decline</w>': 2093, 'hypothesis</w>': 2094, 'comparing</w>': 2095, 'object</w>': 2096, 'creates</w>': 2097, 'subplot</w>': 2098, 'orange</w>': 2099, 'hexagons</w>': 2100, 'lines</w>': 2101, 'categories</w>': 2102, 'simply</w>': 2103, 'being</w>': 2104, 'easier</w>': 2105, 'Weights</w>': 2106, 'untrained</w>': 2107, 'showed</w>': 2108, 'strikingly</w>': 2109, 'rectification</w>': 2110, 'local</w>': 2111, 'They</w>': 2112, 'Caltech</w>': 2113, '101</w>': 2114, 'Fei</w>': 2115, '2004</w>': 2116, 'ask</w>': 2117, 'carries</w>': 2118, 'deeper</w>': 2119, 'choices</w>': 2120, 'falls</w>': 2121, 'quickly</w>': 2122, 'chance</w>': 2123, 'getting</w>': 2124, 'straightforward</w>': 2125, 'Whereas</w>': 2126, 'max</w>': 2127, 'did</w>': 2128, 'nonlinearity</w>': 2129, 'relu</w>': 2130, 'abs</w>': 2131, 'tanh</w>': 2132, 'sizes</w>': 2133, 'Additionally</w>': 2134, 'hyperparameter</w>': 2135, 'architectural</w>': 2136, 'datapoint</w>': 2137, 'tweak</w>': 2138, 'initialization</w>': 2139, 'subtracting</w>': 2140, 'performances</w>': 2141, 'plotted</w>': 2142, 'things</w>': 2143, 'apparent</w>': 2144, 'gap</w>': 2145, 'grows</w>': 2146, '25</w>': 2147, 'differ</w>': 2148, 'fully</w>': 2149, 'ours</w>': 2150, 'informative</w>': 2151, 'however</w>': 2152, 'draws</w>': 2153, 'multiple</w>': 2154, 'runs</w>': 2155, 'converge</w>': 2156, 'producing</w>': 2157, 'Much</w>': 2158, 'convergence</w>': 2159, '30</w>': 2160, '20</w>': 2161, '15</w>': 2162, '05</w>': 2163, '00</w>': 2164, 'Relative</w>': 2165, 'm</w>': 2166, 'Degradation</w>': 2167, 'connects</w>': 2168, 'consist</w>': 2169, 'plots</w>': 2170, 'compared</w>': 2171, 'making</w>': 2172, 'extra</w>': 2173, 'overfit</w>': 2174, 'Conclusions</w>': 2175, 'demonstrated</w>': 2176, 'quantifying</w>': 2177, 'fragilely</w>': 2178, 'substantial</w>': 2179, 'technique</w>': 2180, 'Prototypical</w>': 2181, 'Networks</w>': 2182, 'Few</w>': 2183, 'shot</w>': 2184, 'Learning</w>': 2185, 'Jake</w>': 2186, 'Snell</w>': 2187, 'Toronto</w>': 2188, 'Kevin</w>': 2189, 'Swersky</w>': 2190, 'Richard</w>': 2191, 'S</w>': 2192, 'Zemel</w>': 2193, 'Vector</w>': 2194, 'Institute</w>': 2195, 'prototypical</w>': 2196, 'metric</w>': 2197, 'computing</w>': 2198, 'distances</w>': 2199, 'prototype</w>': 2200, 'Compared</w>': 2201, 'recent</w>': 2202, 'reflect</w>': 2203, 'simpler</w>': 2204, 'inductive</w>': 2205, 'beneficial</w>': 2206, 'excellent</w>': 2207, 'decisions</w>': 2208, 'improvements</w>': 2209, 'involving</w>': 2210, 'complicated</w>': 2211, 'extend</w>': 2212, 'zero</w>': 2213, 'theart</w>': 2214, 'CU</w>': 2215, 'Birds</w>': 2216, '16</w>': 2217, 'accommodate</w>': 2218, 'naive</w>': 2219, 're</w>': 2220, 'severely</w>': 2221, 'humans</w>': 2222, 'ability</w>': 2223, 'Two</w>': 2224, 'progress</w>': 2225, '29</w>': 2226, 'predict</w>': 2227, 'unlabeled</w>': 2228, 'query</w>': 2229, 'interpreted</w>': 2230, 'weighted</w>': 2231, 'nearest</w>': 2232, 'neighbor</w>': 2233, 'Notably</w>': 2234, 'utilizes</w>': 2235, 'mini</w>': 2236, 'batches</w>': 2237, 'episodes</w>': 2238, 'episode</w>': 2239, 'mimic</w>': 2240, 'subsampling</w>': 2241, 'faithful</w>': 2242, 'thereby</w>': 2243, 'Ravi</w>': 2244, 'Larochelle</w>': 2245, '22</w>': 2246, 'episodic</w>': 2247, 'LSTM</w>': 2248, 'updates</w>': 2249, 'Here</w>': 2250, 'learner</w>': 2251, 'learns</w>': 2252, 'attack</w>': 2253, 'addressing</w>': 2254, 'key</w>': 2255, 'issue</w>': 2256, 'assumption</w>': 2257, 'cluster</w>': 2258, 'around</w>': 2259, 'Classification</w>': 2260, 'tackle</w>': 2261, 'comes</w>': 2262, 'description</w>': 2263, 'therefore</w>': 2264, 'serve</w>': 2265, 'Initial</w>': 2266, 'author</w>': 2267, '1703</w>': 2268, '05175v2</w>': 2269, 'Jun</w>': 2270, 'c1</w>': 2271, 'c2</w>': 2272, 'c3</w>': 2273, 'v1</w>': 2274, 'v2</w>': 2275, 'v3</w>': 2276, 'Zero</w>': 2277, 'scenarios</w>': 2278, 'Left</w>': 2279, 'prototypes</w>': 2280, 'ck</w>': 2281, 'Right</w>': 2282, 'produced</w>': 2283, 'vk</w>': 2284, 'classified</w>': 2285, 'p</w>': 2286, 'y</w>': 2287, '</w>': 2288, 'exp</w>': 2289, 'f</w>': 2290, 'formulate</w>': 2291, 'draw</w>': 2292, 'connections</w>': 2293, 'analyze</w>': 2294, 'underlying</w>': 2295, 'relate</w>': 2296, 'clustering</w>': 2297, 'justify</w>': 2298, 'Bregman</w>': 2299, 'divergence</w>': 2300, 'squared</w>': 2301, 'Euclidean</w>': 2302, 'empirically</w>': 2303, 'vital</w>': 2304, 'outperforms</w>': 2305, 'cosine</w>': 2306, 'benchmark</w>': 2307, 'appealing</w>': 2308, 'Notation</w>': 2309, 'x1</w>': 2310, 'y1</w>': 2311, 'xN</w>': 2312, 'yN</w>': 2313, 'xi</w>': 2314, '</w>': 2315, 'D</w>': 2316, 'yi</w>': 2317, 'K</w>': 2318, 'corresponding</w>': 2319, 'Sk</w>': 2320, 'Model</w>': 2321, 'M</w>': 2322, 'RM</w>': 2323, '</w>': 2324, 'learnable</w>': 2325, '</w>': 2326, 'belonging</w>': 2327, 'X</w>': 2328, '</w>': 2329, '</w>': 2330, 'distribution</w>': 2331, 'P</w>': 2332, 'k0</w>': 2333, 'ck0</w>': 2334, 'proceeds</w>': 2335, 'minimizing</w>': 2336, 'negative</w>': 2337, 'probability</w>': 2338, 'J</w>': 2339, 'SGD</w>': 2340, 'Training</w>': 2341, 'formed</w>': 2342, 'selecting</w>': 2343, 'choosing</w>': 2344, 'act</w>': 2345, 'remainder</w>': 2346, 'Pseudocode</w>': 2347, 'Algorithm</w>': 2348, 'computation</w>': 2349, 'NC</w>': 2350, '</w>': 2351, 'NS</w>': 2352, 'NQ</w>': 2353, 'RANDOMSAMPLE</w>': 2354, 'elements</w>': 2355, 'uniformly</w>': 2356, 'replacement</w>': 2357, 'Input</w>': 2358, 'Dk</w>': 2359, 'Output</w>': 2360, 'generated</w>': 2361, 'V</w>': 2362, '</w>': 2363, 'Select</w>': 2364, 'DVk</w>': 2365, 'Qk</w>': 2366, '\\\\</w>': 2367, 'Compute</w>': 2368, 'logX</w>': 2369, 'Update</w>': 2370, 'Mixture</w>': 2371, 'Density</w>': 2372, 'Estimation</w>': 2373, 'functions</w>': 2374, 'divergences</w>': 2375, 'performing</w>': 2376, 'mixture</w>': 2377, 'estimation</w>': 2378, 'exponential</w>': 2379, 'd</w>': 2380, '</w>': 2381, 'differentiable</w>': 2382, 'convex</w>': 2383, 'Legendre</w>': 2384, 'kz</w>': 2385, '0k</w>': 2386, 'Mahalanobis</w>': 2387, 'Prototype</w>': 2388, 'viewed</w>': 2389, 'terms</w>': 2390, 'assigned</w>': 2391, 'representative</w>': 2392, 'achieving</w>': 2393, 'minimal</w>': 2394, 'Equation</w>': 2395, 'representatives</w>': 2396, 'Moreover</w>': 2397, 'p</w>': 2398, '</w>': 2399, 'cumulant</w>': 2400, '</w>': 2401, 'written</w>': 2402, 'uniquely</w>': 2403, 'equally</w>': 2404, 'assignment</w>': 2405, 'prediction</w>': 2406, '</w>': 2407, 'k</w>': 2408, 'effectively</w>': 2409, 'assumptions</w>': 2410, 'classconditional</w>': 2411, 'Reinterpretation</w>': 2412, 'gaining</w>': 2413, 'insight</w>': 2414, 'parameterization</w>': 2415, 'term</w>': 2416, 'affect</w>': 2417, 'probabilities</w>': 2418, 'focus</w>': 2419, 'primarily</w>': 2420, 'spherical</w>': 2421, 'Gaussian</w>': 2422, 'densities</w>': 2423, 'indicate</w>': 2424, 'despite</w>': 2425, 'equivalence</w>': 2426, 'hypothesize</w>': 2427, 'linearity</w>': 2428, 'Indeed</w>': 2429, 'currently</w>': 2430, '14</w>': 2431, '28</w>': 2432, 'Comparison</w>': 2433, 'xk</w>': 2434, 'fixed</w>': 2435, 'greater</w>': 2436, 'require</w>': 2437, 'partitioning</w>': 2438, 'scheme</w>': 2439, 'Mensink</w>': 2440, 'Rippel</w>': 2441, 'methods</w>': 2442, 'ordinary</w>': 2443, 'extensions</w>': 2444, 'decoupling</w>': 2445, 'conditional</w>': 2446, 'FCE</w>': 2447, 'takes</w>': 2448, 'account</w>': 2449, 'likewise</w>': 2450, 'incorporated</w>': 2451, 'increase</w>': 2452, 'imposes</w>': 2453, 'ordering</w>': 2454, 'bi</w>': 2455, 'directional</w>': 2456, 'Instead</w>': 2457, 'outline</w>': 2458, 'Choices</w>': 2459, 'Distance</w>': 2460, 'apply</w>': 2461, 'permissible</w>': 2462, 'conjecture</w>': 2463, 'hold</w>': 2464, 'Episode</w>': 2465, 'composition</w>': 2466, 'Nc</w>': 2467, 'situation</w>': 2468, 'comprised</w>': 2469, 'extremely</w>': 2470, 'held</w>': 2471, 'consideration</w>': 2472, 'Shot</w>': 2473, 'differs</w>': 2474, 'accuracies</w>': 2475, 'Omniglot</w>': 2476, 'Acc</w>': 2477, 'Dist</w>': 2478, 'Tune</w>': 2479, 'MATCHING</w>': 2480, 'NETWORKS</w>': 2481, 'Cosine</w>': 2482, '98</w>': 2483, '93</w>': 2484, 'Y</w>': 2485, '97</w>': 2486, 'NEURAL</w>': 2487, 'STATISTICIAN</w>': 2488, '99</w>': 2489, 'PROTOTYPICAL</w>': 2490, 'OURS</w>': 2491, 'Euclid</w>': 2492, '96</w>': 2493, 'advance</w>': 2494, 'Modifying</w>': 2495, 'deal</w>': 2496, 'g</w>': 2497, 'An</w>': 2498, 'illustration</w>': 2499, 'procedure</w>': 2500, 'relates</w>': 2501, 'come</w>': 2502, 'domains</w>': 2503, 'helpful</w>': 2504, 'fix</w>': 2505, 'constrain</w>': 2506, 'Experiments</w>': 2507, 'miniImageNet</w>': 2508, 'ILSVRC</w>': 2509, '26</w>': 2510, 'UCSD</w>': 2511, 'bird</w>': 2512, 'CUB</w>': 2513, '200</w>': 2514, '31</w>': 2515, '1623</w>': 2516, 'handwritten</w>': 2517, 'collected</w>': 2518, 'alphabets</w>': 2519, 'associated</w>': 2520, 'drawn</w>': 2521, 'human</w>': 2522, 'subject</w>': 2523, 'resizing</w>': 2524, 'grayscale</w>': 2525, 'augmenting</w>': 2526, 'rotations</w>': 2527, 'multiples</w>': 2528, '90</w>': 2529, 'degrees</w>': 2530, '1200</w>': 2531, 'plus</w>': 2532, '800</w>': 2533, 'mirrors</w>': 2534, 'composed</w>': 2535, 'blocks</w>': 2536, 'block</w>': 2537, 'filter</w>': 2538, 'convolution</w>': 2539, 'batch</w>': 2540, 'ReLU</w>': 2541, 'All</w>': 2542, '11</w>': 2543, 'initial</w>': 2544, 'rate</w>': 2545, 'cut</w>': 2546, 'advantageous</w>': 2547, 'fewer</w>': 2548, 'against</w>': 2549, 'baselines</w>': 2550, 'statistician</w>': 2551, 'derived</w>': 2552, '12</w>': 2553, '84</w>': 2554, 'divided</w>': 2555, '600</w>': 2556, 'monitoring</w>': 2557, 'though</w>': 2558, '1600</w>': 2559, 'increased</w>': 2560, '95</w>': 2561, 'confidence</w>': 2562, 'intervals</w>': 2563, 'BASELINE</w>': 2564, 'NEAREST</w>': 2565, 'NEIGHBORS</w>': 2566, '86</w>': 2567, '</w>': 2568, '49</w>': 2569, '79</w>': 2570, '43</w>': 2571, '40</w>': 2572, '78</w>': 2573, '51</w>': 2574, '09</w>': 2575, '71</w>': 2576, '55</w>': 2577, '73</w>': 2578, 'META</w>': 2579, 'LEARNER</w>': 2580, '44</w>': 2581, '77</w>': 2582, '68</w>': 2583, '70</w>': 2584, '80</w>': 2585, 'Accuracy</w>': 2586, 'Proto</w>': 2587, 'Nets</w>': 2588, 'axis</w>': 2589, 'indicates</w>': 2590, 'configuration</w>': 2591, 'Error</w>': 2592, 'schedule</w>': 2593, 'stops</w>': 2594, 'variants</w>': 2595, 'Meta</w>': 2596, 'Learner</w>': 2597, 'margin</w>': 2598, 'own</w>': 2599, 'difficulty</w>': 2600, 'forces</w>': 2601, 'grained</w>': 2602, 'Also</w>': 2603, 'substantially</w>': 2604, 'pronounced</w>': 2605, 'naturally</w>': 2606, 'suited</w>': 2607, 'assess</w>': 2608, 'suitability</w>': 2609, '788</w>': 2610, 'species</w>': 2611, 'closely</w>': 2612, 'Reed</w>': 2613, 'preparing</w>': 2614, 'Image</w>': 2615, 'ALE</w>': 2616, 'Fisher</w>': 2617, 'SJE</w>': 2618, 'AlexNet</w>': 2619, 'SAMPLE</w>': 2620, 'CLUSTERING</w>': 2621, '17</w>': 2622, 'GoogLeNet</w>': 2623, 'DS</w>': 2624, 'DA</w>': 2625, 'PROTO</w>': 2626, 'NETS</w>': 2627, 'divide</w>': 2628, '024</w>': 2629, 'extracted</w>': 2630, 'applying</w>': 2631, 'crops</w>': 2632, 'horizontally</w>': 2633, 'flipped</w>': 2634, 'image2</w>': 2635, 'At</w>': 2636, 'crop</w>': 2637, '312</w>': 2638, 'continuous</w>': 2639, 'attribute</w>': 2640, 'attributes</w>': 2641, 'characteristics</w>': 2642, 'feather</w>': 2643, 'patterns</w>': 2644, 'domain</w>': 2645, 'constructed</w>': 2646, 'decay</w>': 2647, 'Early</w>': 2648, 'stopping</w>': 2649, 'utilizing</w>': 2650, 'SVM</w>': 2651, 'relative</w>': 2652, 'vast</w>': 2653, 'summarize</w>': 2654, 'relevant</w>': 2655, 'Neighborhood</w>': 2656, 'Components</w>': 2657, 'Analysis</w>': 2658, 'NCA</w>': 2659, 'KNN</w>': 2660, 'leave</w>': 2661, 'transformed</w>': 2662, 'Salakhutdinov</w>': 2663, 'Hinton</w>': 2664, '27</w>': 2665, 'transformation</w>': 2666, 'LMNN</w>': 2667, 'attempts</w>': 2668, 'optimize</w>': 2669, 'hinge</w>': 2670, 'encourages</w>': 2671, 'neighborhood</w>': 2672, 'DNet</w>': 2673, 'opposed</w>': 2674, 'form</w>': 2675, 'concise</w>': 2676, 'obviates</w>': 2677, 'developed</w>': 2678, 'rapidly</w>': 2679, 'incorporate</w>': 2680, 'relies</w>': 2681, 'downloaded</w>': 2682, 'reedscot</w>': 2683, 'cvpr2016</w>': 2684, 'linearly</w>': 2685, 'embed</w>': 2686, 'couple</w>': 2687, 'multi</w>': 2688, 'modal</w>': 2689, 'generalizes</w>': 2690, 'dynamics</w>': 2691, 'itself</w>': 2692, 'goal</w>': 2693, 'classifiers</w>': 2694, 'dynamically</w>': 2695, 'core</w>': 2696, 'rely</w>': 2697, 'nets</w>': 2698, 'secondary</w>': 2699, 'generative</w>': 2700, 'extends</w>': 2701, 'variational</w>': 2702, '24</w>': 2703, 'component</w>': 2704, 'statistic</w>': 2705, 'taking</w>': 2706, 'sample</w>': 2707, 'post</w>': 2708, 'approximate</w>': 2709, 'posterior</w>': 2710, 'Edwards</w>': 2711, 'Storkey</w>': 2712, 'considering</w>': 2713, 'KL</w>': 2714, 'inferred</w>': 2715, 'Like</w>': 2716, 'summary</w>': 2717, 'discriminative</w>': 2718, 'befits</w>': 2719, 'With</w>': 2720, 'resembles</w>': 2721, 'multimodal</w>': 2722, 'Unlike</w>': 2723, 'empirical</w>': 2724, 'risk</w>': 2725, 'Neither</w>': 2726, 'nor</w>': 2727, 'regularize</w>': 2728, 'Conclusion</w>': 2729, 'sophisticated</w>': 2730, 'although</w>': 2731, 'modifying</w>': 2732, 'direction</w>': 2733, 'future</w>': 2734, 'utilize</w>': 2735, 'distributions</w>': 2736, 'beyond</w>': 2737, 'Gaussians</w>': 2738, 'preliminary</w>': 2739, 'explorations</w>': 2740, 'variance</w>': 2741, 'gains</w>': 2742, 'flexibility</w>': 2743, 'requiring</w>': 2744, 'fitted</w>': 2745, 'Overall</w>': 2746, 'simplicity</w>': 2747, 'promising</w>': 2748}\n",
      "Preprocessed Text: [ u n k ]\n",
      "Current Node: None\n",
      "Token IDs: [0]\n",
      "Current Node: None\n",
      "Token IDs: [0, 0]\n",
      "Current Node: None\n",
      "Token IDs: [0, 0, 0]\n",
      "Current Node: None\n",
      "Token IDs: [0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import collections\n",
    "\n",
    "class Tokenizer:\n",
    "    class TrieNode:\n",
    "        def __init__(self):\n",
    "            self.children = {}\n",
    "            self.token_id = None\n",
    "            self.frequency = 0\n",
    "            self.failure_link = None\n",
    "            self.is_end = False  \n",
    "            self.token = None  \n",
    "\n",
    "    class Tokenize:  \n",
    "        def __init__(self, bpe_vocab_size=30522, wordpiece_vocab_size=30522, unk_token=\"[UNK]\", unk_token_id=0, num_merges=100):\n",
    "            self.bpe_model = None  # Initialize BPE model\n",
    "            self.wordpiece_model = None  # Initialize WordPiece model\n",
    "            self.bpe_vocab_size = bpe_vocab_size\n",
    "            self.wordpiece_vocab_size = wordpiece_vocab_size\n",
    "            self.unk_token = unk_token\n",
    "            self.unk_token_id = unk_token_id\n",
    "            self.num_merges = num_merges\n",
    "\n",
    "        def train(self, text):\n",
    "            # Step 1: Train the BPE model\n",
    "            self.bpe_model = Tokenizer.BPE(self.bpe_vocab_size, self.unk_token_id) \n",
    "            self.bpe_model.train(text)\n",
    "\n",
    "            # Step 2: Apply BPE encoding to the text\n",
    "            bpe_encoded_text = \" \".join(str(token) for token in self.bpe_model.encode(text))\n",
    "            print(\"bpe_encoded_text vocabulary:\", bpe_encoded_text)  # Inspect contents\n",
    "\n",
    "            # Step 3: Build vocabulary for WordPiece from BPE-encoded text\n",
    "            wordpiece_vocab = self.build_wordpiece_vocab(bpe_encoded_text)\n",
    "            print(\"WordPiece vocabulary:\", wordpiece_vocab)  # Inspect contents\n",
    "\n",
    "            # Step 4: Initialize WordPiece model dynamically using vocabulary\n",
    "            self.wordpiece_model = Tokenizer.WordPiece(wordpiece_vocab, unk_token_id=self.unk_token_id) \n",
    "\n",
    "        def encode(self, text):\n",
    "            \"\"\"\n",
    "            Encodes text using the trained BPE and WordPiece models.\n",
    "            \"\"\"\n",
    "            if not self.bpe_model or not self.wordpiece_model:\n",
    "                raise ValueError(\"Tokenizer has not been trained yet.\")\n",
    "\n",
    "            # Step 1: Apply BPE encoding\n",
    "            bpe_tokens = self.bpe_model.encode(text)\n",
    "\n",
    "            # Step 2: Convert BPE token IDs to tokens\n",
    "            bpe_text = \" \".join(self.bpe_model.decode(bpe_tokens))\n",
    "\n",
    "            # Step 3: Apply WordPiece encoding\n",
    "            wordpiece_ids = self.wordpiece_model.tokenize(bpe_text)\n",
    "\n",
    "            return wordpiece_ids\n",
    "\n",
    "        def decode(self, ids):\n",
    "            if not self.bpe_model or not self.wordpiece_model:\n",
    "                raise ValueError(\"Tokenizer has not been trained yet.\")\n",
    "\n",
    "            # Step 1: WordPiece decoding \n",
    "            wordpiece_text = self.wordpiece_model.decode(ids)\n",
    "\n",
    "            # Step 2: Convert WordPiece text to BPE tokens\n",
    "            bpe_tokens = wordpiece_text.split(\" \")\n",
    "\n",
    "            # Step 3: Decode BPE tokens to original text\n",
    "            bpe_token_ids = [self.bpe_model.vocab.get(token, self.unk_token_id) for token in bpe_tokens]\n",
    "            decoded_text = self.bpe_model.decode(bpe_token_ids)\n",
    "\n",
    "            return decoded_text\n",
    "        \n",
    "        def build_wordpiece_vocab(self, text):\n",
    "            \"\"\"\n",
    "            Builds a vocabulary for WordPiece from BPE-encoded text.\n",
    "            \"\"\"\n",
    "            words = text.split(\" \")  # Split the BPE-encoded text into words\n",
    "            vocab = collections.Counter(words) \n",
    "            return vocab\n",
    "\n",
    "    class BPE:\n",
    "        def __init__(self, num_merges, unk_token_id=0):  # Accept unk_token_id parameter\n",
    "            self.vocab = {}\n",
    "            self.merges = []\n",
    "            self.num_merges = num_merges\n",
    "            self.unk_token_id = unk_token_id  # Store the unknown token ID\n",
    "            self.unk_token = \"[UNK]\"  # Add this line to define the unknown token\n",
    "\n",
    "        def train(self, text):\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "            vocab = collections.Counter(words)\n",
    "            vocab = {word + '</w>': count for word, count in vocab.items()}\n",
    "            print(\"Initial vocabulary:\", vocab)  # Check the initial vocabulary\n",
    "\n",
    "            for _ in range(self.num_merges):  # Use the num_merges from the instance variable\n",
    "                pairs = self.get_stats(vocab)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                best = max(pairs, key=pairs.get)\n",
    "                vocab = self.merge_vocab(best, vocab)\n",
    "                self.merges.append(best)\n",
    "                print(\"Vocabulary after merge:\", vocab)  # Check how it evolves\n",
    "\n",
    "            self.vocab = {word: i for i, word in enumerate(vocab.keys())}\n",
    "\n",
    "        @staticmethod\n",
    "        def get_stats(vocab):\n",
    "            pairs = collections.defaultdict(int)\n",
    "            for word, freq in vocab.items():\n",
    "                symbols = word.split()\n",
    "                for i in range(len(symbols)-1):\n",
    "                    pairs[symbols[i], symbols[i+1]] += freq\n",
    "            return pairs\n",
    "\n",
    "        @staticmethod\n",
    "        def merge_vocab(pair, vocab):\n",
    "            v_out = {}\n",
    "            bigram = re.escape(' '.join(pair))\n",
    "            p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "            for word in vocab:\n",
    "                w_out = p.sub(''.join(pair), word)\n",
    "                v_out[w_out] = vocab[word]\n",
    "            return v_out\n",
    "\n",
    "        def encode(self, text):\n",
    "            \"\"\"Encode text into subwords using learned BPE merges.\"\"\"\n",
    "            encoded_tokens = []\n",
    "            for word in re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE):\n",
    "                word += '</w>'\n",
    "                subwords = [word]  # Start with the entire word as one subword\n",
    "                for merge in self.merges:\n",
    "                    new_subwords = []\n",
    "                    for subword in subwords:\n",
    "                        # If the merge is in subword, split it; otherwise, keep it as is\n",
    "                        if ' '.join(merge) in subword:\n",
    "                            new_subwords.extend(subword.replace(' '.join(merge), ''.join(merge)).split(' '))\n",
    "                        else:\n",
    "                            new_subwords.append(subword)\n",
    "                    subwords = new_subwords\n",
    "                encoded_tokens.extend(subwords)\n",
    "                print(\"Subwords:\", subwords)   # Check subword generation\n",
    "                print(\"BPE Vocabulary:\", self.vocab) # Ensure the vocabulary is populated\n",
    "                return [str(self.vocab.get(token, self.unk_token_id)) for token in encoded_tokens]\n",
    "        \n",
    "            # New method to save trained model\n",
    "        def save_model(self, filepath):\n",
    "            bpe_data = {\n",
    "                'merges': self.merges,\n",
    "                'vocab': self.vocab,\n",
    "                'num_merges': self.num_merges,\n",
    "                # Include other attributes as needed\n",
    "            }\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(bpe_data, f)\n",
    "\n",
    "        def load_model(self, filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                bpe_data = json.load(f)\n",
    "            \n",
    "            self.merges = bpe_data['merges']\n",
    "            self.vocab = bpe_data['vocab']\n",
    "            self.num_merges = bpe_data['num_merges']\n",
    "\n",
    "        def decode(self, ids):\n",
    "            \"\"\"Decode a list of BPE token IDs back into the original text.\"\"\"\n",
    "            decoded_text = \"\"\n",
    "            for id_ in ids:\n",
    "                token = self.vocab.get(id_, self.unk_token)\n",
    "                decoded_text += token.replace(\"</w>\", \"\")  # Remove the '</w>' marker\n",
    "            return decoded_text\n",
    "\n",
    "    class WordPiece:\n",
    "        def __init__(self, vocab, unk_token_id=0, unk_token=\"[UNK]\"):\n",
    "            self.vocab = vocab\n",
    "            self.unk_token_id = unk_token_id\n",
    "            self.unk_token = unk_token  # Define the unknown token\n",
    "            self.root = self.build_trie(vocab)\n",
    "            self.id_to_token = {id_: token for token, id_ in vocab.items()}  # Inverse mapping\n",
    "            self.compute_failure_links(self.root)\n",
    "            print(\"Trie built successfully.\")\n",
    "\n",
    "        def convert_ids_to_tokens(self, ids):\n",
    "            \"\"\"\n",
    "            Convert a list of token ids back to their string token representations.\n",
    "            \"\"\"\n",
    "            return [self.id_to_token.get(id_, self.unk_token) for id_ in ids]\n",
    "\n",
    "        # Add debug prints to build_trie to confirm structure\n",
    "        def build_trie(self, vocab):\n",
    "            root = Tokenizer.TrieNode()\n",
    "            for token in vocab:\n",
    "                node = root\n",
    "                for char in token:\n",
    "                    if char not in node.children:\n",
    "                        node.children[char] = Tokenizer.TrieNode()\n",
    "                    node = node.children[char]\n",
    "                node.is_end = True\n",
    "                node.token = token\n",
    "            print(\"Trie Construction Completed Successfully\")\n",
    "            return root\n",
    "\n",
    "\n",
    "        def compute_failure_links(self, root):\n",
    "            queue = [root]\n",
    "            while queue:\n",
    "                current_node = queue.pop(0)\n",
    "                for char, child_node in current_node.children.items():\n",
    "                    failure_node = current_node.failure_link\n",
    "                    while failure_node and char not in failure_node.children:\n",
    "                        failure_node = failure_node.failure_link\n",
    "                    child_node.failure_link = failure_node.children[char] if failure_node else root\n",
    "                    queue.append(child_node)\n",
    "\n",
    "        # Improved debug prints in tokenize method\n",
    "                    \n",
    "        def tokenize(self, text):\n",
    "            # Preprocess input text\n",
    "            text = self.preprocess_text(text)\n",
    "            node = self.root\n",
    "            token_ids = []  # Will store token IDs instead of tokens\n",
    "            i = 0\n",
    "            print(\"Preprocessed Text:\", text)\n",
    "            while i < len(text):\n",
    "                char = text[i]\n",
    "                if char == ' ':\n",
    "                    node = self.root\n",
    "                    i += 1\n",
    "                    print(\"Current Node:\", node.token)  # Track how the trie is traversed\n",
    "                    print(\"Token IDs:\", token_ids)   \n",
    "                    continue\n",
    "\n",
    "                if char not in node.children:\n",
    "                    if node != self.root and node.token is not None:\n",
    "                        # Convert found token to its ID\n",
    "                        token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                        token_ids.append(token_id)\n",
    "                        node = self.root  # Reset to root\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Append unknown token ID\n",
    "                        token_ids.append(self.unk_token_id)\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                node = node.children[char]\n",
    "                if node.is_end:\n",
    "                    if i + 1 == len(text) or text[i + 1] == ' ':\n",
    "                        # Convert found token to its ID\n",
    "                        token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                        token_ids.append(token_id)\n",
    "                        node = self.root\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            #print(f\"Token IDs: {token_ids[:10]}\")\n",
    "            return token_ids\n",
    "\n",
    "        def preprocess_text(self, text):\n",
    "            # Convert text to lowercase to ensure case insensitivity\n",
    "            text = text.lower()\n",
    "\n",
    "            # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "            # This depends on how your vocabulary handles punctuation\n",
    "            text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "\n",
    "            # Replace multiple spaces with a single space\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "            # Trim leading and trailing spaces\n",
    "            text = text.strip()\n",
    "\n",
    "            return text\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "tokenizer = Tokenizer.Tokenize(bpe_vocab_size=30522, wordpiece_vocab_size=30522, num_merges=200) \n",
    "with open(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\", 'r', encoding='utf-8') as f: \n",
    "    text = f.read()\n",
    "tokenizer.train(text) \n",
    "encoded_text = tokenizer.encode(text)\n",
    "print(encoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "#############################################\n",
    "# Dataset class\n",
    "video_folder_path = \"D:\\MMM_Data\\Video\"\n",
    "image_folder_path = \"D:\\MMM_Data\\Image\"\n",
    "text_folder_path = \"D:\\MMM_Data\\Text\"\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, image_paths, video_paths, texts, tokenizer, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.video_paths = video_paths\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load video (simplified: loading only the first frame for example)\n",
    "        video_cap = cv2.VideoCapture(self.video_paths[idx])\n",
    "        ret, frame = video_cap.read()\n",
    "        if ret:\n",
    "            # Convert BGR to RGB and apply same transformations as for the image\n",
    "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            frame = self.transform(frame)\n",
    "        video_cap.release()\n",
    "\n",
    "        # Process text\n",
    "        text = self.tokenizer(self.texts[idx], return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "        return image, frame, text.input_ids.squeeze(), text.attention_mask.squeeze()\n",
    "\n",
    "def multimodal_collate_fn(batch):\n",
    "    images, frames, texts, attention_masks = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    frames = torch.stack(frames)\n",
    "    texts = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    return images, frames, texts, attention_masks\n",
    "\n",
    "\n",
    "# Assuming vocab is your vocabulary dictionary where keys are tokens and values are token IDs\n",
    "vocab = {...}  # Your vocabulary here\n",
    "\n",
    "# Instantiate your WordPiece tokenizer\n",
    "wordpiece_tokenizer = WordPiece(vocab, unk_token_id=vocab.get(\"[UNK]\", 0), unk_token=\"[UNK]\")\n",
    "\n",
    "# Example paths and texts remain the same\n",
    "image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\"]\n",
    "video_paths = [\"path/to/video1.mp4\", \"path/to/video2.mp4\"]\n",
    "texts = [\"This is a description for the first item\", \"This is a description for the second item\"]\n",
    "\n",
    "# Create dataset with custom tokenizer\n",
    "dataset = MultimodalDataset(image_paths, video_paths, texts, wordpiece_tokenizer)\n",
    "\n",
    "# Create DataLoader as before\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=multimodal_collate_fn)\n",
    "\n",
    "# Example usage\n",
    "for images, frames, texts, attention_masks in dataloader:\n",
    "    print(images.shape, frames.shape, texts.shape, attention_masks.shape)\n",
    "    # Route each modality input to the relevant model part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
