{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision (Image and Video) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIT\n",
    "class FlexiblePatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_size=768, temporal_patch_size=1, is_3d=False):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_size = embed_size\n",
    "        self.is_3d = is_3d\n",
    "\n",
    "        if is_3d:\n",
    "            self.num_patches = int(((img_size // patch_size) ** 2) * temporal_patch_size)\n",
    "            self.projection = nn.Conv3d(in_channels, embed_size, kernel_size=(temporal_patch_size, patch_size, patch_size), \n",
    "\n",
    "                                        stride=(temporal_patch_size, patch_size, patch_size))\n",
    "\n",
    "        else:\n",
    "            self.num_patches = (img_size // patch_size) ** 2\n",
    "            self.projection = nn.Conv2d(in_channels, embed_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)  # [B, E, T/P, H/P, W/P] or [B, E, H/P, W/P]\n",
    "        x = x.flatten(2)  # Flatten spatial and temporal dimensions\n",
    "        x = x.transpose(1, 2)  # [B, N, E]\n",
    "        return x\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_patches, embed_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, embed_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        cls_token = torch.zeros(batch_size, 1, x.shape[-1], device=x.device)\n",
    "        x = torch.cat([cls_token, x], dim=1)  # [B, 1+N, E]\n",
    "        x += self.positional_embedding\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        # Feedforward network\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, src):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "class FlexibleVisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_size=768, num_heads=12, num_layers=12, num_classes=1000, temporal_patch_size=1, is_3d=False):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = FlexiblePatchEmbedding(img_size, patch_size, in_channels, embed_size, temporal_patch_size, is_3d)\n",
    "        self.positional_embedding = PositionalEmbedding(self.patch_embedding.num_patches, embed_size)\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_size),\n",
    "            nn.Linear(embed_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = self.positional_embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        if return_embeddings:\n",
    "            return x  # Return the sequence of embeddings directly\n",
    "        cls_token = x[:, 0]\n",
    "        x = self.mlp_head(cls_token)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trie Construction Completed Successfully\n",
      "Trie built successfully.\n",
      "Configured vocab size: 2749\n",
      "Tokenizer vocab size: 2749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot locate reference to <class '__main__.LanguageExpert.WordPiece'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "c:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot pickle <class '__main__.LanguageExpert.WordPiece'>: __main__.LanguageExpert.WordPiece has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Parameter 'function'=<function tokenize_and_prepare_labels at 0x0000020CEC43E480> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf6a237347147afb1cc73989b79064f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All token IDs are within the expected range.\n",
      "All token IDs are within the expected range.\n",
      "Skipping empty batch at index 0\n",
      "All token IDs are within the expected range.\n",
      "All token IDs are within the expected range.\n",
      "Max input_id in SPLASH: 2611, vocab_size=2749\n",
      "x_reshaped shape: torch.Size([2, 345, 8, 64])\n",
      "values shape: torch.Size([2, 345, 8, 64]), queries shape: torch.Size([2, 345, 8, 128]), keys shape: torch.Size([2, 345, 8, 128])\n",
      "attention_scores shape: torch.Size([2, 8, 345, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 0, Partition End 128 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([2, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([2, 128, 8, 128])\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([2, 128, 8, 128])\n",
      "attention shape: torch.Size([2, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 128, Partition End 256 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([2, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([2, 128, 8, 128])\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([2, 128, 8, 128])\n",
      "attention shape: torch.Size([2, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 89, 8, 89])\n",
      "Partition Start 256, Partition End 345 , ponder_scores: torch.Size([2, 8, 89, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([2, 89, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([2, 89, 8, 128])\n",
      "queries_part shape: torch.Size([2, 89, 8, 128])\n",
      "C_keys shape: torch.Size([2, 89, 8, 89])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([2, 89, 8, 128])\n",
      "attention_weights shape: torch.Size([2, 89, 8, 128])\n",
      "attention shape: torch.Size([2, 89, 8, 128])\n",
      "Output before final_projection: shape=torch.Size([690, 1024])\n",
      "Output after final_projection: shape=torch.Size([690, 512])\n",
      "Before QLORA: SPLASH output shape=torch.Size([690, 512])\n",
      "After QLORA, embeddings shape: torch.Size([690, 512])\n",
      "Embeddings passed to decoder: shape=torch.Size([690, 512])\n",
      "Max input_id in SPLASH: 5, vocab_size=2749\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1328\u001b[0m\n\u001b[0;32m   1325\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/EXPERT_WEIGHTS/lmt_expert_trained_custom_tokenizer.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;66;03m# Train the LMT sub-model within the Expert system\u001b[39;00m\n\u001b[1;32m-> 1328\u001b[0m trained_model, average_loss \u001b[38;5;241m=\u001b[39m \u001b[43mlanguage_expert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_language_model_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete. Model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 1182\u001b[0m, in \u001b[0;36mLanguageExpert.train_language_model_transformer\u001b[1;34m(self, train_loader, device, vocab_size, save_path)\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput ID out of range: Max ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exceeds vocab size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1181\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m-> 1182\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1183\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1184\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 384\u001b[0m, in \u001b[0;36mLanguageExpert.LanguageModelTransformer.forward\u001b[1;34m(self, input_ids, embeddings, attention_mask)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither input_ids or embeddings must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings passed to decoder: shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 384\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m    385\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoder output: shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 328\u001b[0m, in \u001b[0;36mLanguageExpert.LanguageModelDecoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Note: No need to apply dropout here as x are token IDs\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 328\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# SPLASH is called within TransformerBlock\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# Apply dropout after all transformer blocks and before the final projection layer\u001b[39;00m\n\u001b[0;32m    331\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 353\u001b[0m, in \u001b[0;36mLanguageExpert.TransformerBlock.forward\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids):\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;66;03m# SPLASH already expects token IDs and handles embedding\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplash\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(attention_output))\n\u001b[0;32m    355\u001b[0m     forward_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(attention_output)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 120\u001b[0m, in \u001b[0;36mLanguageExpert.SPLASH.forward\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Max token ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exceeds vocab size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m            \n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m#print(f\"After embedding: {x.shape}\")\u001b[39;00m\n\u001b[0;32m    122\u001b[0m N, seq_length, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import re\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "# Test on larger text\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "\n",
    "# Set the HF_HOME environment variable to a new cache directory on the D drive\n",
    "os.environ['HF_HOME'] = 'D:/hf_datasets_cache'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = wordpiece_tokenizer\n",
    "        self.vocab_size = len(wordpiece_tokenizer.vocab)\n",
    "        self.embed_size = 512  # Size of each embedding vector\n",
    "        self.heads = 8  # Number of attention heads\n",
    "        self.num_layers = 2  # Number of transformer blocks\n",
    "        self.forward_expansion = 4  # Expansion size for the feedforward layer\n",
    "        self.dropout = 0.1\n",
    "        self.max_length = 1024  # Maximum length of the input sequences\n",
    "        self.rank = 64  # Rank for LORA adjustments\n",
    "        self.sequence_length = 1024  # Input sequence length for SPLASH\n",
    "        self.projection_dim = 256  # Projection dimension for SPLASH\n",
    "        self.partition_size = 128  # Partition size for SPLASH processing\n",
    "        self.device = 'cpu'\n",
    "        #self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.alpha = 2  # Alpha parameter for LORA layers\n",
    "        self.quantization_bits = 4  # Bit size for quantization in QLORA\n",
    "        self.freq_threshold = 1000  # Frequency threshold for adaptive embeddings\n",
    "        self.large_embed_dim = 512  # Embedding dimension for frequent tokens\n",
    "        self.small_embed_dim = 128  # Embedding dimension for infrequent tokens\n",
    "        self.head_dim = self.embed_size // self.heads\n",
    "\n",
    "  # Language Model Transformer\n",
    "class LanguageExpert:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # Initialize sub-models on the correct device at creation\n",
    "        self.language_model_transformer = self.LanguageModelTransformer(config, tokenizer=config.tokenizer).to(config.device)\n",
    "        self.splash = self.SPLASH(config).to(config.device)\n",
    "        self.dpo = self.DPO(config, self.language_model_transformer).to(config.device)\n",
    "\n",
    "    ###############################\n",
    "\n",
    "    # SPLASH\n",
    "    class SPLASH(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.vocab_size = config.vocab_size\n",
    "            self.embed_size = config.embed_size\n",
    "            self.heads = config.heads\n",
    "            self.sequence_length = config.sequence_length\n",
    "            self.projection_dim = config.projection_dim\n",
    "            self.partition_size = config.partition_size\n",
    "            self.head_dim = self.embed_size // self.heads\n",
    "\n",
    "            self.embedding = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.embed_size)\n",
    "            self.values = nn.Linear(config.head_dim, config.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(config.head_dim, config.projection_dim, bias=False)\n",
    "            self.queries = nn.Linear(config.head_dim, config.projection_dim, bias=False)\n",
    "            self.value_projection = nn.Linear(config.head_dim, config.projection_dim//2)  # To project values to match dimensions\n",
    "            self.ponder = nn.Linear(config.partition_size, 1, bias=True)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            #self.final_projection = nn.Linear(config.heads * (config.projection_dim // 2), config.vocab_size)\n",
    "            self.final_projection = nn.Linear(config.heads * (config.projection_dim // 2), config.embed_size)\n",
    "\n",
    "        def random_projection(self, matrix, k):\n",
    "            \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "            random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "            return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "        def cur_decomposition(self, matrix, projection_dim):\n",
    "            \"\"\"Applies CUR decomposition with C matrix dimension aligned to projection dimension.\"\"\"\n",
    "            batch_size, seq_length, heads, dim = matrix.shape\n",
    "            k = min(projection_dim // 2, dim, seq_length)\n",
    "            C = torch.zeros(batch_size, seq_length, heads, k, device=matrix.device)\n",
    "            R = torch.zeros(batch_size, k, heads, dim, device=matrix.device)\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                for h in range(heads):\n",
    "                    # Using torch.randperm for random indices in PyTorch\n",
    "                    col_indices = torch.randperm(dim, device=matrix.device)[:k]\n",
    "                    row_indices = torch.randperm(seq_length, device=matrix.device)[:k]\n",
    "                    C[b, :, h] = matrix[b, :, h, col_indices]\n",
    "                    R[b, :, h] = matrix[b, row_indices, h]\n",
    "            return C, R\n",
    "\n",
    "        def forward(self, input_ids):\n",
    "            if input_ids.dim() != 2 or input_ids.dtype != torch.long:\n",
    "                raise ValueError(f\"input_ids must be a 2D tensor of long integers, got shape {input_ids.shape} and dtype {input_ids.dtype}\")\n",
    "\n",
    "            # Debug print to check max token ID\n",
    "            print(f\"Max input_id in SPLASH: {input_ids.max().item()}, vocab_size={self.vocab_size}\")\n",
    "            assert input_ids.max() < self.vocab_size, f\"Max token ID {input_ids.max().item()} exceeds vocab size {self.vocab_size - 1}\"\n",
    "\n",
    "            # Check for out-of-range token IDs right before embedding call\n",
    "            if input_ids.max() >= self.vocab_size:\n",
    "                raise ValueError(f\"Error: Max token ID {input_ids.max().item()} exceeds vocab size {self.vocab_size}\")\n",
    "\n",
    "            x = self.embedding(input_ids.long())            \n",
    "            #print(f\"After embedding: {x.shape}\")\n",
    "            N, seq_length, _ = x.shape\n",
    "            x_reshaped = x.view(N, seq_length, self.heads, self.head_dim)\n",
    "            #print(f\"x_reshaped shape: {x_reshaped.shape}\")\n",
    "\n",
    "            values = self.values(x_reshaped)\n",
    "            queries = self.random_projection(self.queries(x_reshaped), self.projection_dim // 2)\n",
    "            keys = self.random_projection(self.keys(x_reshaped), self.projection_dim // 2 )\n",
    "            print(f\"x_reshaped shape: {x_reshaped.shape}\")\n",
    "            print(f\"values shape: {values.shape}, queries shape: {queries.shape}, keys shape: {keys.shape}\")\n",
    "\n",
    "\n",
    "            attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "            print(f\"attention_scores shape: {attention_scores.shape}\")\n",
    "\n",
    "            for i in range(0, seq_length, self.partition_size):\n",
    "                print(f\"PARTITION START\")\n",
    "                partition_start = i\n",
    "                partition_end = min(i + self.partition_size, seq_length)\n",
    "                keys_part = keys[:, partition_start:partition_end, :, :]\n",
    "                queries_part = queries[:, partition_start:partition_end, :, :]\n",
    "\n",
    "                C_keys, R_queries = self.cur_decomposition(keys_part, self.projection_dim)\n",
    "                print(\"C_keys shape before return:\", C_keys.shape)\n",
    "\n",
    "                ponder_scores = torch.zeros(N, self.heads, partition_end - partition_start, 1, device=x.device)\n",
    "                print(f\"Partition Start {i}, Partition End {partition_end} , ponder_scores: {ponder_scores.shape}\")\n",
    "\n",
    "                for h in range(self.heads):\n",
    "                    #print(f\"HEADS START\")\n",
    "                    head_queries = queries_part[:, :, h, :]\n",
    "                    #print(f\"head_queries: {head_queries.shape}\")\n",
    "                    head_ponder_scores = self.sigmoid(self.ponder(head_queries))\n",
    "                    #print(f\"head_ponder_scores: {head_ponder_scores.shape}\")\n",
    "                    ponder_scores[:, h, :, 0] = head_ponder_scores.squeeze(-1)\n",
    "\n",
    "                # Correctly expand ponder_scores without adding an unnecessary dimension\n",
    "                print(\"BEFORE 1ST EINSUM:\")\n",
    "                ponder_scores_permuted = ponder_scores.permute(0, 2, 1, 3)  # Move to [2, 128, 8, 1]\n",
    "                print(\"ponder_scores_permuted shape:\", ponder_scores_permuted.shape) \n",
    "                ponder_scores_broadcastable = ponder_scores_permuted.expand(-1, -1, -1, 128)  # Expand to [2, 128, 8, 128]            \n",
    "                print(\"ponder_scores_broadcastable shape:\", ponder_scores_broadcastable.shape) \n",
    "                print(\"queries_part shape:\", queries_part.shape) \n",
    "                print(\"C_keys shape:\", C_keys.shape)\n",
    "                energy = torch.einsum('bnhd,bnhk->bnhd', queries_part, C_keys)\n",
    "                attention_weights = F.softmax(energy, dim=-1)\n",
    "                print(\"AFTER 1ST EINSUM:\")\n",
    "                print(\"energy shape:\", energy.shape) \n",
    "                print(\"attention_weights shape:\", attention_weights.shape)\n",
    "                attention = attention_weights * ponder_scores_broadcastable\n",
    "                print(\"attention shape:\", attention.shape)\n",
    "                attention_corrected = attention.permute(0, 2, 1, 3)\n",
    "                attention_scores[:, :, partition_start:partition_end, :] = attention_corrected\n",
    "\n",
    "            values = values.permute(0, 2, 1, 3)  # Swap heads and seq_length to bring heads next to head_dim\n",
    "            #print(\"values shape:\", values.shape)\n",
    "            values = values.reshape(-1, self.head_dim)  # Flatten to [N*heads*seq_length, head_dim] for linear layer\n",
    "            #print(\"values.reshape(-1, self.head_dim) shape:\", values.shape)\n",
    "            projected_values = self.value_projection(values)  # Now [N*heads*seq_length, projection_dim / 2]\n",
    "            #print(\"self.value_projection(values) shape:\", projected_values.shape)\n",
    "            projected_values = projected_values.view(N, self.heads, seq_length, self.projection_dim // 2)\n",
    "            #print(\"projected_values shape:\", projected_values.shape)\n",
    "\n",
    "            #print(f\"2ND EINSUM\")\n",
    "            # Combine attention_scores and projected_values then pass through the final linear layer\n",
    "            out = torch.einsum('bnhp,bnhp->bnhp', attention_scores, projected_values)\n",
    "            #print(\"out shape after einsum:\", out.shape)\n",
    "\n",
    "            # Correct reshaping: Flatten batch and sequence length dimensions, keep the last two dimensions for projection\n",
    "            out = out.reshape(-1, self.heads * (self.projection_dim // 2))\n",
    "            #print(\"out reshaped for projection:\", out.shape)\n",
    "\n",
    "            # Ensure the final_projection layer matches the flattened shape expected after reshaping\n",
    "            # Assuming final_projection is defined as nn.Linear(self.heads * (self.projection_dim // 2), vocab_size)\n",
    "            print(f\"Output before final_projection: shape={out.shape}\")\n",
    "\n",
    "            out = self.final_projection(out)\n",
    "\n",
    "            print(f\"Output after final_projection: shape={out.shape}\")\n",
    "\n",
    "            # At this point, out should have a shape of [batch_size * sequence_length, vocab_size], ready for loss calculation\n",
    "            return out\n",
    "\n",
    "\n",
    "    # LORA\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, config, input_dim, output_dim):\n",
    "            super(LanguageExpert.LORALayer, self).__init__()\n",
    "            self.rank = config.rank\n",
    "            self.alpha = config.alpha\n",
    "\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, self.rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(self.rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "    # QLORA\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.QLORALayer, self).__init__()\n",
    "            self.rank = config.rank\n",
    "            self.alpha = config.alpha\n",
    "            self.quantization_bits = config.quantization_bits\n",
    "            self.dropout = nn.Dropout(config.dropout)\n",
    "            self.initialized = False\n",
    "            # Parameters are declared here but not initialized with specific dimensions\n",
    "\n",
    "        def initialize_weights(self, input_dim, output_dim):\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, self.rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(self.rank, output_dim))\n",
    "            self.layer_norm = nn.LayerNorm(output_dim).to(config.device)\n",
    "            # Initialize parameters\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "            self.initialized = True\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            if not self.initialized:\n",
    "                input_dim = x.size(-1)\n",
    "                output_dim = input_dim  # This could be adapted based on your specific needs\n",
    "                self.initialize_weights(input_dim, output_dim)\n",
    "\n",
    "            # Instead of moving self.A and self.B directly, use temporary variables for operations\n",
    "            A_temp = self.A.to(x.device)\n",
    "            B_temp = self.B.to(x.device)\n",
    "\n",
    "            # Proceed with quantization using A_temp and B_temp which are on the correct device\n",
    "            A_quantized, scale_A = self.quantize(A_temp, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(B_temp, self.quantization_bits)\n",
    "\n",
    "            lora_adjustment = self.alpha * (x @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            x_transformed = F.linear(x.view(-1, x.size(-1)), self.weight.to(x.device), self.bias.to(x.device)).view(x.size())\n",
    "\n",
    "            x_adjusted = x_transformed + lora_adjustment\n",
    "            x_adjusted = self.dropout(x_adjusted)\n",
    "            x_normalized = self.layer_norm(x_adjusted.to(x.device))\n",
    "\n",
    "            return x_normalized.squeeze(0) if x_normalized.size(0) == 1 else x_normalized\n",
    "\n",
    "\n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config  # Already stored\n",
    "            self.layers = nn.ModuleList([\n",
    "                LanguageExpert.TransformerBlock(config) for _ in range(config.num_layers)\n",
    "            ])\n",
    "            self.fc_out = nn.Linear(config.embed_size, config.vocab_size)\n",
    "            self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Ensure x is token IDs for SPLASH\n",
    "            x = x.to(dtype=torch.long)\n",
    "            # Note: No need to apply dropout here as x are token IDs\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)  # SPLASH is called within TransformerBlock\n",
    "\n",
    "            # Apply dropout after all transformer blocks and before the final projection layer\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            out = self.fc_out(x)  # Final projection from transformer block output to vocab size\n",
    "\n",
    "            return out\n",
    "\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.splash = LanguageExpert.SPLASH(config=config)\n",
    "            self.norm1 = nn.LayerNorm(config.embed_size)\n",
    "            self.norm2 = nn.LayerNorm(config.embed_size)\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                nn.Linear(config.embed_size, config.forward_expansion * config.embed_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(config.forward_expansion * config.embed_size, config.embed_size),\n",
    "            )\n",
    "            self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        def forward(self, input_ids):\n",
    "            # SPLASH already expects token IDs and handles embedding\n",
    "            attention_output = self.splash(input_ids.long())\n",
    "            attention_output = self.dropout(self.norm1(attention_output))\n",
    "            forward_output = self.feed_forward(attention_output)\n",
    "            output = self.dropout(self.norm2(forward_output + attention_output))\n",
    "            return output\n",
    "        \n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, config, tokenizer):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.tokenizer = tokenizer\n",
    "            self.splash = LanguageExpert.SPLASH(config).to(config.device)\n",
    "            self.decoder = LanguageExpert.LanguageModelDecoder(config).to(config.device)\n",
    "            self.qlora_layer = LanguageExpert.QLORALayer(config).to(config.device)\n",
    "            self.vocab_size = config.vocab_size\n",
    "\n",
    "        def forward(self, input_ids=None, embeddings=None, attention_mask=None):\n",
    "            if input_ids.max() >= self.vocab_size:\n",
    "                raise ValueError(f\"Error: Max token ID {input_ids.max().item()} exceeds vocab size {self.vocab_size}\")\n",
    "\n",
    "            if input_ids is not None:\n",
    "                input_ids = input_ids.long()\n",
    "                embeddings = self.splash(input_ids)\n",
    "                print(f\"Before QLORA: SPLASH output shape={embeddings.shape}\")\n",
    "\n",
    "                embeddings = self.qlora_layer(embeddings)  # Apply QLORA\n",
    "                print(f\"After QLORA, embeddings shape: {embeddings.shape}\")\n",
    "            elif embeddings is None:\n",
    "                raise ValueError(\"Either input_ids or embeddings must be provided.\")\n",
    "            \n",
    "            print(f\"Embeddings passed to decoder: shape={embeddings.shape}\")\n",
    "            out = self.decoder(embeddings)  \n",
    "            print(f\"Decoder output: shape={out.shape}\")\n",
    "\n",
    "            return out\n",
    "    \n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len).to(trg.device)\n",
    "            return trg_mask\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = [self.tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            return response\n",
    "\n",
    "    \n",
    "    ##################################################\n",
    "    # Tokenizer\n",
    "\n",
    "    class TrieNode:\n",
    "        def __init__(self):\n",
    "            self.children = {}\n",
    "            self.token_id = None\n",
    "            self.frequency = 0\n",
    "            self.failure_link = None\n",
    "            self.is_end = False  # Add is_end attribute to mark the end of a word\n",
    "            self.token = None  # Add token attribute to store the token associated with the node\n",
    "\n",
    "\n",
    "    class Trie:\n",
    "        def __init__(self, unk_token_id=0):\n",
    "            self.root = LanguageExpert.TrieNode()\n",
    "            self.unk_token_id = unk_token_id\n",
    "\n",
    "        def insert(self, token, token_id, frequency):\n",
    "            node = self.root\n",
    "            for char in token:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = LanguageExpert.TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.token_id = token_id\n",
    "            node.frequency = frequency\n",
    "\n",
    "        def find_subwords(self, token):\n",
    "            \"\"\"Finds the most probable subwords based on frequency.\"\"\"\n",
    "            node = self.root\n",
    "            best_subwords = []\n",
    "\n",
    "            def dfs(current_node, subword='', collected_subwords=[]):\n",
    "                if current_node.token_id is not None:\n",
    "                    # Update to correctly calculate total_frequency based on the structure of collected_subwords\n",
    "                    total_frequency = sum(n.frequency for _, _, n in collected_subwords) + current_node.frequency\n",
    "                    probability = current_node.frequency / total_frequency if total_frequency else 0\n",
    "                    collected_subwords.append((subword, probability, current_node))\n",
    "\n",
    "                for char, next_node in current_node.children.items():\n",
    "                    dfs(next_node, subword + char, list(collected_subwords))  # Create a copy of the list to avoid shared state\n",
    "\n",
    "            dfs(node)\n",
    "            best_subwords = sorted(best_subwords, key=lambda x: x[1], reverse=True)\n",
    "            return [subword for subword, _, _ in best_subwords][:5] or [self.unk_token_id]\n",
    "\n",
    "\n",
    "        def compute_failure_links(self):\n",
    "            root = self.root\n",
    "            root.failure_link = root  # Root's failure link points to itself\n",
    "            queue = [root]\n",
    "\n",
    "            while queue:\n",
    "                current_node = queue.pop(0)\n",
    "\n",
    "                for char, child_node in current_node.children.items():\n",
    "                    queue.append(child_node)\n",
    "\n",
    "                    # Follow failure link to find the longest suffix for the child_node\n",
    "                    failure_candidate = current_node.failure_link\n",
    "                    while failure_candidate != root and char not in failure_candidate.children:\n",
    "                        failure_candidate = failure_candidate.failure_link\n",
    "                    child_node.failure_link = failure_candidate.children.get(char, root)\n",
    "\n",
    "\n",
    "    class SimpleSentencePiece:\n",
    "        def __init__(self, model_type=\"bpe\", vocab_size=30522):\n",
    "            self.vocab = {}\n",
    "            self.id_to_subword = {}\n",
    "            self.unk_token = \"[UNK]\"\n",
    "            self.unk_token_id = 0\n",
    "            self.vocab_size = vocab_size\n",
    "            self.model = None if model_type == \"bpe\" else None\n",
    "            self.model_type = model_type\n",
    "\n",
    "        def train(self, text):\n",
    "            if self.model_type == \"bpe\":\n",
    "                self.model = LanguageExpert.BPE(num_merges=self.vocab_size, unk_token_id=self.unk_token_id)\n",
    "                self.model.train(text)\n",
    "                self.vocab = self.model.vocab\n",
    "                self.id_to_subword = {i: word for word, i in self.vocab.items()}\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Model type {self.model_type} not supported yet.\")\n",
    "\n",
    "        def encode(self, text):\n",
    "            text = self.preprocess_text(text)  # Preprocess text before encoding\n",
    "            if not self.model:\n",
    "                raise ValueError(\"Model has not been trained yet.\")\n",
    "            encoded = self.model.encode(text)\n",
    "            #print(f\"Encoded: {encoded[:10]}\")  # Print first 10 encoded tokens\n",
    "            return encoded\n",
    "\n",
    "        def decode(self, ids):\n",
    "            if not self.id_to_subword:\n",
    "                raise ValueError(\"Vocabulary is empty. Ensure the model is trained first.\")\n",
    "            text = \" \".join([self.id_to_subword.get(id_, self.unk_token) for id_ in ids])\n",
    "            text = text.replace(\" </w>\", \"\").replace(\"</w>\", \" \").strip()\n",
    "            return text\n",
    "\n",
    "        def preprocess_text(self, text):\n",
    "            # Convert text to lowercase to ensure case insensitivity\n",
    "            text = text.lower()\n",
    "            # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "            text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "            # Replace multiple spaces with a single space\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            # Trim leading and trailing spaces\n",
    "            text = text.strip()\n",
    "            return text\n",
    "        \n",
    "        def save_model(self, filepath):\n",
    "            model_data = {\n",
    "                'vocab': self.vocab,\n",
    "                'id_to_subword': self.id_to_subword,\n",
    "                'model_type': self.model_type,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                # Potentially include other relevant attributes\n",
    "            }\n",
    "            # Save the high-level tokenizer settings\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(model_data, f)\n",
    "            \n",
    "            # Now save the BPE model specifically\n",
    "            if self.model_type == \"bpe\" and self.model:\n",
    "                self.model.save_model(filepath + \"_bpe\")\n",
    "\n",
    "        def load_model(self, filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                model_data = json.load(f)\n",
    "            \n",
    "            self.vocab = model_data['vocab']\n",
    "            self.id_to_subword = model_data['id_to_subword']\n",
    "            self.model_type = model_data['model_type']\n",
    "            self.vocab_size = model_data['vocab_size']\n",
    "            \n",
    "            # Assuming model_type is still \"bpe\", we now load the BPE model\n",
    "            if self.model_type == \"bpe\":\n",
    "                self.model = LanguageExpert.BPE(self.vocab_size, self.unk_token_id)\n",
    "                self.model.load_model(filepath + \"_bpe\")\n",
    "\n",
    "    class BPE:\n",
    "        def __init__(self, num_merges=100, unk_token_id=0):  # Accept unk_token_id parameter\n",
    "            self.vocab = {}\n",
    "            self.merges = []\n",
    "            self.num_merges = num_merges\n",
    "            self.unk_token_id = unk_token_id  # Store the unknown token ID\n",
    "\n",
    "        def train(self, text):\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "            vocab = collections.Counter(words)\n",
    "            vocab = {word + '</w>': count for word, count in vocab.items()}\n",
    "            \n",
    "            for _ in range(self.num_merges):  # Use the num_merges from the instance variable\n",
    "                pairs = self.get_stats(vocab)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                best = max(pairs, key=pairs.get)\n",
    "                vocab = self.merge_vocab(best, vocab)\n",
    "                self.merges.append(best)\n",
    "\n",
    "            self.vocab = {word: i for i, word in enumerate(vocab.keys())}\n",
    "\n",
    "        @staticmethod\n",
    "        def get_stats(vocab):\n",
    "            pairs = collections.defaultdict(int)\n",
    "            for word, freq in vocab.items():\n",
    "                symbols = word.split()\n",
    "                for i in range(len(symbols)-1):\n",
    "                    pairs[symbols[i], symbols[i+1]] += freq\n",
    "            return pairs\n",
    "\n",
    "        @staticmethod\n",
    "        def merge_vocab(pair, vocab):\n",
    "            v_out = {}\n",
    "            bigram = re.escape(' '.join(pair))\n",
    "            p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "            for word in vocab:\n",
    "                w_out = p.sub(''.join(pair), word)\n",
    "                v_out[w_out] = vocab[word]\n",
    "            return v_out\n",
    "\n",
    "        def encode(self, text):\n",
    "            \"\"\"Encode text into subwords using learned BPE merges.\"\"\"\n",
    "            encoded_tokens = []\n",
    "            for word in re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE):\n",
    "                word += '</w>'\n",
    "                subwords = [word]  # Start with the entire word as one subword\n",
    "                for merge in self.merges:\n",
    "                    new_subwords = []\n",
    "                    for subword in subwords:\n",
    "                        # If the merge is in subword, split it; otherwise, keep it as is\n",
    "                        if ' '.join(merge) in subword:\n",
    "                            new_subwords.extend(subword.replace(' '.join(merge), ''.join(merge)).split(' '))\n",
    "                        else:\n",
    "                            new_subwords.append(subword)\n",
    "                    subwords = new_subwords\n",
    "                encoded_tokens.extend(subwords)\n",
    "            return [self.vocab.get(token, self.unk_token_id) for token in encoded_tokens]\n",
    "        \n",
    "            # New method to save trained model\n",
    "        def save_model(self, filepath):\n",
    "            bpe_data = {\n",
    "                'merges': self.merges,\n",
    "                'vocab': self.vocab,\n",
    "                'num_merges': self.num_merges,\n",
    "                # Include other attributes as needed\n",
    "            }\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(bpe_data, f)\n",
    "\n",
    "        def load_model(self, filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                bpe_data = json.load(f)\n",
    "            \n",
    "            self.merges = bpe_data['merges']\n",
    "            self.vocab = bpe_data['vocab']\n",
    "            self.num_merges = bpe_data['num_merges']\n",
    "\n",
    "\n",
    "    class WordPiece:\n",
    "        def __init__(self, vocab, unk_token_id=0, unk_token=\"[UNK]\"):\n",
    "            self.vocab = vocab\n",
    "            self.unk_token_id = unk_token_id\n",
    "            self.unk_token = unk_token  # Define the unknown token\n",
    "            self.root = self.build_trie(vocab)\n",
    "            self.id_to_token = {id_: token for token, id_ in vocab.items()}  # Inverse mapping\n",
    "            self.compute_failure_links(self.root)\n",
    "            print(\"Trie built successfully.\")\n",
    "\n",
    "        def convert_ids_to_tokens(self, ids):\n",
    "            \"\"\"\n",
    "            Convert a list of token ids back to their string token representations.\n",
    "            \"\"\"\n",
    "            return [self.id_to_token.get(id_, self.unk_token) for id_ in ids]\n",
    "\n",
    "        # Add debug prints to build_trie to confirm structure\n",
    "        def build_trie(self, vocab):\n",
    "            root = LanguageExpert.TrieNode()\n",
    "            for token in vocab:\n",
    "                node = root\n",
    "                for char in token:\n",
    "                    if char not in node.children:\n",
    "                        node.children[char] = LanguageExpert.TrieNode()\n",
    "                    node = node.children[char]\n",
    "                node.is_end = True\n",
    "                node.token = token\n",
    "            print(\"Trie Construction Completed Successfully\")\n",
    "            return root\n",
    "\n",
    "\n",
    "        def compute_failure_links(self, root):\n",
    "            queue = [root]\n",
    "            while queue:\n",
    "                current_node = queue.pop(0)\n",
    "                for char, child_node in current_node.children.items():\n",
    "                    failure_node = current_node.failure_link\n",
    "                    while failure_node and char not in failure_node.children:\n",
    "                        failure_node = failure_node.failure_link\n",
    "                    child_node.failure_link = failure_node.children[char] if failure_node else root\n",
    "                    queue.append(child_node)\n",
    "\n",
    "        # Improved debug prints in tokenize method\n",
    "                    \n",
    "        def tokenize(self, text):\n",
    "            # Preprocess input text\n",
    "            text = self.preprocess_text(text)\n",
    "            node = self.root\n",
    "            token_ids = []  # Will store token IDs instead of tokens\n",
    "            i = 0\n",
    "\n",
    "            while i < len(text):\n",
    "                char = text[i]\n",
    "                if char == ' ':\n",
    "                    node = self.root\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                if char not in node.children:\n",
    "                    if node != self.root and node.token is not None:\n",
    "                        # Convert found token to its ID\n",
    "                        token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                        token_ids.append(token_id)\n",
    "                        node = self.root  # Reset to root\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Append unknown token ID\n",
    "                        token_ids.append(self.unk_token_id)\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                node = node.children[char]\n",
    "                if node.is_end:\n",
    "                    if i + 1 == len(text) or text[i + 1] == ' ':\n",
    "                        # Convert found token to its ID\n",
    "                        token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                        token_ids.append(token_id)\n",
    "                        node = self.root\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            #print(f\"Token IDs: {token_ids[:10]}\")\n",
    "            return token_ids\n",
    "\n",
    "        def preprocess_text(self, text):\n",
    "            # Convert text to lowercase to ensure case insensitivity\n",
    "            text = text.lower()\n",
    "\n",
    "            # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "            # This depends on how your vocabulary handles punctuation\n",
    "            text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "\n",
    "            # Replace multiple spaces with a single space\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "            # Trim leading and trailing spaces\n",
    "            text = text.strip()\n",
    "\n",
    "            return text\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "    # DPO\n",
    "    class DPO(nn.Module):\n",
    "        def __init__(self, config, language_model):\n",
    "            super(LanguageExpert.DPO, self).__init__()\n",
    "            self.language_model = language_model\n",
    "            self.device = config.device\n",
    "            self.projection = nn.Linear(config.vocab_size, config.embed_size)\n",
    "            self.classifier = nn.Linear(config.embed_size, 2)\n",
    "\n",
    "        def forward(self, input_ids_question, input_ids_chosen=None, input_ids_rejected=None, labels=None):\n",
    "            combined_input_ids = torch.cat((input_ids_question, input_ids_chosen, input_ids_rejected), dim=1)\n",
    "\n",
    "            # Assuming combined_input_ids has shape [batch_size, sequence_length]\n",
    "            logits = self.language_model(combined_input_ids)  # Output shape: [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "            # Project logits to embedding space before pooling\n",
    "            projected_logits = self.projection(logits)  # New shape: [batch_size, sequence_length, embed_size]\n",
    "            \n",
    "            # Apply global mean pooling across the sequence length dimension\n",
    "            pooled_logits = projected_logits.mean(dim=1)  # New shape: [batch_size, embed_size]\n",
    "\n",
    "            # Pass the pooled representation through the classifier\n",
    "            predictions = self.classifier(pooled_logits)  # New shape: [batch_size, 2]\n",
    "\n",
    "            # Calculate loss if labels are provided\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(predictions, labels)\n",
    "\n",
    "            return predictions, loss\n",
    "\n",
    "\n",
    "    ###############################\n",
    "    # RAG\n",
    "    \n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.PositionalEncoding, self).__init__()\n",
    "            self.d_model = config.embed_size\n",
    "            self.max_len = config.max_length\n",
    "            pe = torch.zeros(self.max_len, self.d_model)\n",
    "            position = torch.arange(0, self.max_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * -(math.log(10000.0) / self.d_model))\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "            pe = pe.unsqueeze(0)\n",
    "            self.register_buffer('pe', pe)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # x: Tensor of shape [Batch Size, Sequence Length, Embedding Dimension]\n",
    "            # Adjust positional encoding to match the input size and device\n",
    "            pe = self.pe[:, :x.size(1)]\n",
    "            # Assuming x is on the correct device, pe will be automatically aligned to the same device\n",
    "            return pe\n",
    "\n",
    "\n",
    "    class AdaptiveDropoutLayer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.AdaptiveDropoutLayer, self).__init__()\n",
    "            self.log_alpha = nn.Parameter(torch.tensor(math.log(config.dropout / (1 - config.dropout))).float())\n",
    "\n",
    "        def forward(self, x):\n",
    "            p = torch.sigmoid(self.log_alpha)\n",
    "            # Convert p from a tensor to a float\n",
    "            p_value = p.item()  # This extracts the scalar value as a Python float\n",
    "            return nn.functional.dropout(x, p=p_value, training=self.training)\n",
    "\n",
    "\n",
    "    class AdaptiveEmbeddingLayer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.AdaptiveEmbeddingLayer, self).__init__()\n",
    "            self.vocab = config.wordpiece_vocab  # Ensure this is part of your config\n",
    "            self.vocab_size = config.vocab_size\n",
    "            self.freq_threshold = config.freq_threshold\n",
    "            self.large_embed_dim = config.large_embed_dim\n",
    "            self.small_embed_dim = config.small_embed_dim\n",
    "            self.max_seq_len = config.max_length\n",
    "            self.split_vocab(self.vocab, self.freq_threshold)\n",
    "            self.frequent_embeddings = nn.Embedding(len(self.frequent_vocab), self.large_embed_dim)\n",
    "            self.infrequent_embeddings = nn.Embedding(len(self.infrequent_vocab), self.small_embed_dim)\n",
    "            self.infrequent_projection = nn.Linear(self.small_embed_dim, self.large_embed_dim)\n",
    "            self.positional_embeddings = LanguageExpert.PositionalEncoding(config)\n",
    "\n",
    "\n",
    "\n",
    "        def split_vocab(self, vocab, freq_threshold):\n",
    "            token_counts = [(token, count) for token, count in vocab.items()]\n",
    "            token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "            split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "            \n",
    "            self.frequent_vocab = dict(token_counts[:split_point])\n",
    "            self.infrequent_vocab = dict(token_counts[split_point:])\n",
    "\n",
    "        def forward(self, token_ids):\n",
    "            device = token_ids.device\n",
    "            seq_len = token_ids.size(1)\n",
    "            batch_size = token_ids.size(0)  # Obtain batch size from token_ids tensor\n",
    "\n",
    "            # Initialize embeddings tensor\n",
    "            embeddings = torch.zeros(token_ids.shape[0], seq_len, self.large_embed_dim, device=device)\n",
    "\n",
    "            # Map token_ids to indices for frequent and infrequent vocab\n",
    "            frequent_indices = torch.zeros_like(token_ids)\n",
    "            infrequent_indices = torch.zeros_like(token_ids)\n",
    "            \n",
    "            for token_id, index in self.vocab.items():\n",
    "                mask = token_ids == token_id\n",
    "                if token_id in self.frequent_vocab:\n",
    "                    # Map to index in frequent_vocab\n",
    "                    frequent_indices[mask] = self.frequent_vocab[token_id]\n",
    "                elif token_id in self.infrequent_vocab:\n",
    "                    # Map to index in infrequent_vocab\n",
    "                    infrequent_indices[mask] = self.infrequent_vocab[token_id]\n",
    "\n",
    "            # Create masks for frequent and infrequent tokens\n",
    "            frequent_mask = frequent_indices > 0\n",
    "            infrequent_mask = infrequent_indices > 0\n",
    "\n",
    "            # Embed frequent tokens\n",
    "            if frequent_mask.any():\n",
    "                frequent_embeddings = self.frequent_embeddings(frequent_indices[frequent_mask])\n",
    "                embeddings[frequent_mask] = frequent_embeddings\n",
    "\n",
    "            # Embed and project infrequent tokens\n",
    "            if infrequent_mask.any():\n",
    "                infrequent_embeddings = self.infrequent_embeddings(infrequent_indices[infrequent_mask])\n",
    "                infrequent_embeddings_projected = self.infrequent_projection(infrequent_embeddings)\n",
    "                embeddings[infrequent_mask] = infrequent_embeddings_projected\n",
    "\n",
    "            # Apply positional embeddings\n",
    "            position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            position_embeddings = self.positional_embeddings(position_ids)  # Generate for seq_len\n",
    "\n",
    "            # Ensure positional embeddings are broadcastable to the embeddings tensor\n",
    "            if position_embeddings.size(0) != batch_size:\n",
    "                position_embeddings = position_embeddings.expand(batch_size, -1, -1)\n",
    "\n",
    "            print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "            print(f\"Positional embeddings shape: {position_embeddings.shape}\")\n",
    "            embeddings += position_embeddings\n",
    "\n",
    "            return embeddings\n",
    "\n",
    "\n",
    "    class DPRContextEncoder(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.DPRContextEncoder, self).__init__()\n",
    "            self.wordpiece_tokenizer = config.wordpiece_tokenizer\n",
    "            self.embedding_layer = LanguageExpert.AdaptiveEmbeddingLayer(config)\n",
    "            self.attention_layer = LanguageExpert.SPLASH(config=config).to(config.device)\n",
    "            self.dropout = LanguageExpert.AdaptiveDropoutLayer(config)\n",
    " \n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            embeddings = self.embedding_layer(input_ids)\n",
    "            attention_output = self.attention_layer(embeddings, attention_mask=attention_mask)\n",
    "            attention_output = self.dropout(attention_output)\n",
    "\n",
    "            # Mean pooling across the sequence length dimension\n",
    "            pooled_output = attention_output.mean(dim=1)\n",
    "\n",
    "            return pooled_output\n",
    "\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.DPRQuestionEncoder, self).__init__()\n",
    "            self.wordpiece_tokenizer = config.wordpiece_tokenizer\n",
    "            self.embedding_layer = LanguageExpert.AdaptiveEmbeddingLayer(config)\n",
    "            self.attention_layer = LanguageExpert.SPLASH(config=config).to(config.device)\n",
    "            self.dropout = LanguageExpert.AdaptiveDropoutLayer(config)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            embeddings = self.embedding_layer(input_ids)\n",
    "            attention_output = self.attention_layer(embeddings, attention_mask=attention_mask)\n",
    "            attention_output = self.dropout(attention_output)\n",
    "\n",
    "            # Mean pooling across the sequence length dimension\n",
    "            pooled_output = attention_output.mean(dim=1)\n",
    "\n",
    "            return pooled_output\n",
    "\n",
    "\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(LanguageExpert.TransformerRAG, self).__init__()\n",
    "            self.config = config\n",
    "            self.context_encoder = LanguageExpert.DPRContextEncoder(config).to(config.device)\n",
    "            self.language_model = LanguageExpert.LanguageModelTransformer(config, config.wordpiece_tokenizer).to(config.device)\n",
    "            self.question_encoder = LanguageExpert.DPRQuestionEncoder(config).to(config.device)\n",
    "            self.tokenizer = config.wordpiece_tokenizer\n",
    "\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= config.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "            \n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "                \n",
    "                aggregated_context_embedding = torch.zeros(config.embedding_dim, device=config.device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = torch.tensor(context['input_ids']).unsqueeze(0).to(config.device)  # Add unsqueeze(0) for batch dimension\n",
    "                    context_attention_mask = torch.tensor(context['attention_mask']).unsqueeze(0).to(config.device)  # Add unsqueeze(0) for batch dimension\n",
    "                    print(f\"context_input_ids shape: {context_input_ids.shape}\")\n",
    "                    print(f\"context_attention_mask shape: {context_attention_mask.shape}\")\n",
    "\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "                \n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "            \n",
    "            question_input_ids = question_input_ids.to(config.device).long()\n",
    "            question_attention_mask = question_attention_mask.to(config.device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "            \n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            #similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            similarities = [cos_sim(question_embeddings.unsqueeze(0), context_emb.unsqueeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=config.device))\n",
    "            \n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(config.device) for k, v in tokenized_combined_input.items()}\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            #response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            response = \" \".join(predicted_tokens).replace(\" </w>\", \"\").replace(\"</w>\", \" \").strip()\n",
    "            \n",
    "            return response\n",
    "\n",
    "        @staticmethod\n",
    "        def extract_text_from_pdf(file_path):\n",
    "            text = \"\"\n",
    "            with fitz.open(file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            return text\n",
    "\n",
    "        @staticmethod\n",
    "        def split_into_chunks(text, chunk_size):\n",
    "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        @staticmethod\n",
    "        def preprocess_text(text, tokenizer, max_length=512):\n",
    "            chunk_size = max_length - 50\n",
    "            text_chunks = LanguageExpert.TransformerRAG.split_into_chunks(text, chunk_size)\n",
    "            processed_chunks = []\n",
    "            for chunk in text_chunks:\n",
    "                # Tokenize the chunk using the WordPiece tokenizer\n",
    "                token_ids = tokenizer.tokenize(chunk)\n",
    "                \n",
    "                # Manual padding and attention mask creation\n",
    "                attention_mask = [1] * len(token_ids)\n",
    "                # Padding: Extend token_ids and attention_mask to max_length\n",
    "                while len(token_ids) < max_length:\n",
    "                    token_ids.append(tokenizer.unk_token_id)  # Use unk_token_id for padding\n",
    "                    attention_mask.append(0)  # Padding token has attention mask 0\n",
    "                \n",
    "                # Ensure token_ids and attention_mask are not longer than max_length\n",
    "                token_ids = token_ids[:max_length]\n",
    "                attention_mask = attention_mask[:max_length]\n",
    "                \n",
    "                processed_chunk = {\n",
    "                    'input_ids': token_ids,\n",
    "                    'attention_mask': attention_mask\n",
    "                }\n",
    "                processed_chunks.append(processed_chunk)\n",
    "            return processed_chunks\n",
    "\n",
    "        @staticmethod\n",
    "        def create_dataset_from_pdfs(pdf_file_paths, tokenizer):\n",
    "            dataset = []\n",
    "            for file_path in pdf_file_paths:\n",
    "                text = LanguageExpert.TransformerRAG.extract_text_from_pdf(file_path)\n",
    "                processed_text = LanguageExpert.TransformerRAG.preprocess_text(text, tokenizer)                \n",
    "                dataset.append(processed_text)\n",
    "            return dataset\n",
    "\n",
    "        def retrieve_contexts(self, dataset, query_embedding, top_k=5):\n",
    "            similarity_scores = []\n",
    "            for context in dataset:\n",
    "                context_input_ids = context['input_ids'].to(self.config.device)\n",
    "                context_attention_mask = context['attention_mask'].to(self.config.device)\n",
    "                # Use the class's context_encoder\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "                similarity_scores.append(similarity.squeeze().item())\n",
    "            top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "            top_contexts = [dataset[i] for i in top_k_indices]\n",
    "            return top_contexts\n",
    "\n",
    "        def rag_retrieve_and_generate(self, dataset, query):\n",
    "            # Tokenize the query using the class's tokenizer\n",
    "            tokenized_query = self.tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
    "            input_ids = tokenized_query['input_ids']\n",
    "            attention_mask = tokenized_query['attention_mask']\n",
    "            # Use the class's question_encoder\n",
    "            encoded_query = self.question_encoder(input_ids, attention_mask)\n",
    "            relevant_contexts = self.retrieve_contexts(dataset, encoded_query)\n",
    "            # Assuming generate_response is a method of LanguageModelTransformer that accepts tokenized contexts and generates a response\n",
    "            response = self.language_model.generate_response(relevant_contexts)\n",
    "            return response\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "        \"\"\"\n",
    "        Calculate a new alpha value based on the current loss.\n",
    "        \"\"\"\n",
    "        if current_loss >= initial_loss:\n",
    "            return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "        loss_ratio = current_loss / initial_loss\n",
    "        alpha_range = initial_alpha - final_alpha\n",
    "        new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "        return new_alpha  \n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Linear warmup with cosine decay\n",
    "        scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "        return optimizer, scheduler\n",
    "        \n",
    "    # DPO Training\n",
    "    def train_dpo(self, train_loader, optimizer, config, save_path):\n",
    "            self.train()  # Set the model to training mode\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                input_ids_question = batch['input_ids_question'].to(config.device)\n",
    "                input_ids_chosen = batch['input_ids_chosen'].to(config.device)\n",
    "                input_ids_rejected = batch['input_ids_rejected'].to(config.device)\n",
    "                labels = batch['labels'].to(config.device)\n",
    "                print(f\"train_dpo input_ids_question: {input_ids_question.shape}\")\n",
    "                print(f\"train_dpo input_ids_chosen: {input_ids_chosen.shape}\")\n",
    "                print(f\"train_dpo input_ids_rejected: {input_ids_rejected.shape}\")\n",
    "                print(f\"train_dpo labels: {labels.shape}\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                logit, loss = self.transformer_dpo(input_ids_question, input_ids_chosen, input_ids_rejected, labels)\n",
    "                print(f\"Logits shape: {logit.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            print(f\"Training complete. Average Loss: {average_loss}\")\n",
    "            \n",
    "            # Save the model\n",
    "            torch.save(self.transformer_dpo.state_dict(), save_path)\n",
    "\n",
    "            return average_loss\n",
    "\n",
    "    # DPR Training\n",
    "    def train_dpr_encoders(self, train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs , context_save_path, question_save_path):\n",
    "        loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(train_data[\"queries\"])):\n",
    "                query = train_data[\"queries\"][i]\n",
    "                context = train_data[\"contexts\"][i]\n",
    "\n",
    "                # Ensure query is a string\n",
    "                if not isinstance(query, str):\n",
    "                    raise ValueError(\"Query must be a string.\")\n",
    "                tokenized_query = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                # Ensure context is a string\n",
    "                if isinstance(context, dict):\n",
    "                    # If context is a dictionary, extract the text content. This is a placeholder and might need adjustment.\n",
    "                    context_text = context.get(\"text\", \"\")\n",
    "                elif isinstance(context, str):\n",
    "                    context_text = context\n",
    "                else:\n",
    "                    raise ValueError(\"Context must be a string or a dictionary containing a text field.\")\n",
    "                tokenized_context = tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                question_embeddings = question_encoder(input_ids=tokenized_query['input_ids'], attention_mask=tokenized_query['attention_mask'])\n",
    "                context_embeddings = context_encoder(input_ids=tokenized_context['input_ids'], attention_mask=tokenized_context['attention_mask'])\n",
    "\n",
    "                # The labels tensor should have the same first dimension size as the input tensors\n",
    "                labels = torch.tensor([1.0] * question_embeddings.size(0), dtype=torch.float).to(question_embeddings.device)\n",
    "\n",
    "                loss = loss_function(question_embeddings, context_embeddings, labels)\n",
    "\n",
    "                optimizer_context.zero_grad()\n",
    "                optimizer_question.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_context.step()\n",
    "                optimizer_question.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_data['queries'])}\")\n",
    "        average_loss = total_loss / len(train_data['queries'])\n",
    "        torch.save(context_encoder.state_dict(), context_save_path)\n",
    "        torch.save(question_encoder.state_dict(), question_save_path)\n",
    "        return (context_encoder, question_encoder), average_loss\n",
    "       \n",
    "    # LMT Training\n",
    "    # Define the function to check token IDs\n",
    "    @staticmethod\n",
    "    def check_token_ids_for_embedding(token_ids, vocab_size):\n",
    "        if token_ids.numel() == 0:\n",
    "            print(\"Warning: Received empty token_ids tensor.\")\n",
    "            return False  # or consider what makes sense for your context\n",
    "        if token_ids.max() >= vocab_size:\n",
    "            print(f\"Out-of-range token ID found: {token_ids.max()}. Max allowed: {vocab_size - 1}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"All token IDs are within the expected range.\")\n",
    "            return True\n",
    "\n",
    "    def train_language_model_transformer(self, train_loader, device, vocab_size, save_path):\n",
    "        model = self.language_model_transformer.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.98)\n",
    "        num_epochs = 1\n",
    "        initial_loss = None\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs, targets = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "\n",
    "                # Check if the batch is empty and skip if true\n",
    "                if inputs.numel() == 0 or targets.numel() == 0:\n",
    "                    print(f\"Skipping empty batch at index {batch_idx}\")\n",
    "                    continue\n",
    "\n",
    "                # Check for out-of-range token IDs\n",
    "                if inputs.max() >= vocab_size:\n",
    "                    raise ValueError(f\"Input ID out of range: Max ID {inputs.max()} exceeds vocab size {vocab_size - 1}\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss, if your model uses QLORA or a similar mechanism\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        return model, average_loss\n",
    "\n",
    "\n",
    "##################################################################\n",
    "    \n",
    "# Test Language Expert\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = [line.strip() for line in file.readlines()]\n",
    "    return texts\n",
    "\n",
    "texts = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "\n",
    "num_merges = 100\n",
    "def adapt_vocab_for_wordpiece(ssp_vocab):\n",
    "    adapted_vocab = {}\n",
    "    for token, id_or_freq in ssp_vocab.items():\n",
    "        if not token.startswith(\" \") and not token.endswith(\"</w>\"):\n",
    "            adapted_token = \"##\" + token.replace(\"</w>\", \"\")\n",
    "        else:\n",
    "            adapted_token = token.replace(\"</w>\", \"\")\n",
    "        adapted_vocab[adapted_token] = id_or_freq\n",
    "    return adapted_vocab\n",
    "\n",
    "# Initialize and train the SimpleSentencePiece model with BPE\n",
    "ssp = LanguageExpert.SimpleSentencePiece(model_type=\"bpe\", vocab_size=30522)\n",
    "# Assume `texts` is a list of text to train the tokenizer\n",
    "ssp.train('\\n'.join(texts))\n",
    "wordpiece_vocab = adapt_vocab_for_wordpiece(ssp.vocab)\n",
    "# Debugging step to ensure vocabulary completeness\n",
    "def debug_vocab(adapted_vocab):\n",
    "    print(\"Sample Vocabulary Check:\")\n",
    "    # Iterate over the first 10 key-value pairs in the adapted vocabulary\n",
    "    for i, (token, id_or_freq) in enumerate(adapted_vocab.items()):\n",
    "        print(f\"{token}: {id_or_freq}\")\n",
    "        if i >= 9:  # Stop after printing 10 entries\n",
    "            break\n",
    "    # Specifically check for subtokens if your tokenizer expects them\n",
    "    subtokens = [token for token in adapted_vocab.keys() if token.startswith(\"##\")]\n",
    "    print(f\"Found {len(subtokens)} subtokens in vocabulary.\")\n",
    "\n",
    "# Ensure wordpiece_vocab is a list of vocabulary tokens\n",
    "# debug_vocab(wordpiece_vocab)  # Call this after initializing wordpiece_vocab\n",
    "\n",
    "# Initialize WordPiece with the adapted vocabulary\n",
    "wordpiece_tokenizer = LanguageExpert.WordPiece(wordpiece_vocab, unk_token_id=0, unk_token=\"[UNK]\")\n",
    "\n",
    "\n",
    "class WikiTextDatasetForLM(Dataset):\n",
    "    def __init__(self, texts, tokenizer, sequence_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sequence_length = sequence_length\n",
    "        self.inputs, self.labels = self.process_texts(texts)\n",
    "        \n",
    "    def process_texts(self, texts):\n",
    "        inputs, labels = [], []\n",
    "        step_size = 256  # Example step size for overlapping sequences\n",
    "        for text in texts:\n",
    "            token_ids = self.tokenizer.tokenize(text)\n",
    "            for i in range(0, len(token_ids) - self.sequence_length, step_size):  # Adjust step_size for overlap\n",
    "                inputs.append(token_ids[i:i+self.sequence_length])\n",
    "                labels.append(token_ids[i+1:i+self.sequence_length+1])\n",
    "        return torch.tensor(inputs, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "# Initialize configuration\n",
    "config = ModelConfig()\n",
    "language_expert = LanguageExpert(config)\n",
    "vocab_size = len(wordpiece_tokenizer.vocab)\n",
    "print(f\"Configured vocab size: {config.vocab_size}\")\n",
    "print(f\"Tokenizer vocab size: {vocab_size}\")\n",
    "####################################################################################\n",
    "# LMT Training\n",
    "# Load the wikitext-2 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\n",
    "\n",
    "def generate_attention_mask(token_ids):\n",
    "    \"\"\"Generate an attention mask for the given token IDs.\"\"\"\n",
    "    return [1 if token_id != 0 else 0 for token_id in token_ids]\n",
    "\n",
    "def tokenize_and_prepare_labels(examples):\n",
    "    token_ids = [wordpiece_tokenizer.tokenize(text) for text in examples[\"text\"]]\n",
    "    labels = [[wordpiece_tokenizer.unk_token_id] + ids[:-1] for ids in token_ids]  # Shift labels\n",
    "    return {\"input_ids\": token_ids, \"labels\": labels}\n",
    "\n",
    "# Load dataset and apply tokenization\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")\n",
    "tokenized_datasets = dataset.map(tokenize_and_prepare_labels, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "def verify_token_ids(tokenized_texts, vocab_size):\n",
    "    # Flatten the list of token IDs to easily iterate through it\n",
    "    flat_token_ids = [token_id for sublist in tokenized_texts for token_id in sublist]\n",
    "\n",
    "    # Check if any token ID exceeds the vocabulary size\n",
    "    out_of_range_tokens = [token_id for token_id in flat_token_ids if token_id >= vocab_size]\n",
    "\n",
    "    if out_of_range_tokens:\n",
    "        print(f\"Warning: Found token IDs out of range: {set(out_of_range_tokens)}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"All token IDs are within the expected range.\")\n",
    "        return True\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch_input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    batch_labels = [torch.tensor(item['labels']) for item in batch]\n",
    "\n",
    "    # Verify token IDs for each item in the batch\n",
    "    for item_input_ids in batch_input_ids:\n",
    "        if not verify_token_ids([item_input_ids.tolist()], vocab_size):\n",
    "            raise ValueError(\"Found out-of-range token IDs, please check the tokenization process.\")\n",
    "\n",
    "    input_ids_padded = pad_sequence(batch_input_ids, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(batch_labels, batch_first=True, padding_value=-100)\n",
    "    attention_masks_padded = (input_ids_padded != 0).long()\n",
    "    return {'input_ids': input_ids_padded, 'attention_mask': attention_masks_padded, 'labels': labels_padded}\n",
    "\n",
    "\n",
    "# Initialize DataLoader with the custom collate function\n",
    "train_loader = DataLoader(tokenized_datasets, batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n",
    "# Define save path for the trained model\n",
    "save_path = 'D:/EXPERT_WEIGHTS/lmt_expert_trained_custom_tokenizer.pth'\n",
    "\n",
    "# Train the LMT sub-model within the Expert system\n",
    "trained_model, average_loss = language_expert.train_language_model_transformer(\n",
    "    train_loader=train_loader, \n",
    "    device=config.device, \n",
    "    vocab_size=config.vocab_size, \n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "print(f\"Training complete. Model saved to {save_path}. Average Loss: {average_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import collections\n",
    "\n",
    "class Tokenizer:\n",
    "    class TrieNode:\n",
    "        def __init__(self):\n",
    "            self.children = {}\n",
    "            self.token_id = None\n",
    "            self.frequency = 0\n",
    "            self.failure_link = None\n",
    "            self.is_end = False  \n",
    "            self.token = None  \n",
    "\n",
    "    class Tokenize:  \n",
    "        def __init__(self, bpe_vocab_size=30522, wordpiece_vocab_size=30522, unk_token=\"[UNK]\", unk_token_id=0, num_merges=100):\n",
    "            self.bpe_model = None  # Initialize BPE model\n",
    "            self.wordpiece_model = None  # Initialize WordPiece model\n",
    "            self.bpe_vocab_size = bpe_vocab_size\n",
    "            self.wordpiece_vocab_size = wordpiece_vocab_size\n",
    "            self.unk_token = unk_token\n",
    "            self.unk_token_id = unk_token_id\n",
    "            self.num_merges = num_merges\n",
    "\n",
    "        def train(self, text):\n",
    "            # Step 1: Train the BPE model\n",
    "            self.bpe_model = Tokenizer.BPE(self.bpe_vocab_size, self.unk_token_id) \n",
    "            self.bpe_model.train(text)\n",
    "\n",
    "            # Step 2: Apply BPE encoding to the text\n",
    "            bpe_encoded_text = \" \".join(str(token) for token in self.bpe_model.encode(text))\n",
    "            print(\"bpe_encoded_text vocabulary:\", bpe_encoded_text)  # Inspect contents\n",
    "\n",
    "            # Step 3: Build vocabulary for WordPiece from BPE-encoded text\n",
    "            wordpiece_vocab = self.build_wordpiece_vocab(bpe_encoded_text)\n",
    "            print(\"WordPiece vocabulary:\", wordpiece_vocab)  # Inspect contents\n",
    "\n",
    "            # Step 4: Initialize WordPiece model dynamically using vocabulary\n",
    "            self.wordpiece_model = Tokenizer.WordPiece(wordpiece_vocab, unk_token_id=self.unk_token_id) \n",
    "\n",
    "        def encode(self, text):\n",
    "            \"\"\"\n",
    "            Encodes text using the trained BPE and WordPiece models.\n",
    "            \"\"\"\n",
    "            if not self.bpe_model or not self.wordpiece_model:\n",
    "                raise ValueError(\"Tokenizer has not been trained yet.\")\n",
    "\n",
    "            # Step 1: Apply BPE encoding\n",
    "            bpe_tokens = self.bpe_model.encode(text)\n",
    "\n",
    "            # Step 2: Convert BPE token IDs to tokens\n",
    "            bpe_text = \" \".join(self.bpe_model.decode(bpe_tokens))\n",
    "\n",
    "            # Step 3: Apply WordPiece encoding\n",
    "            wordpiece_ids = self.wordpiece_model.tokenize(bpe_text)\n",
    "\n",
    "            return wordpiece_ids\n",
    "\n",
    "        def decode(self, ids):\n",
    "            if not self.bpe_model or not self.wordpiece_model:\n",
    "                raise ValueError(\"Tokenizer has not been trained yet.\")\n",
    "\n",
    "            # Step 1: WordPiece decoding \n",
    "            wordpiece_text = self.wordpiece_model.decode(ids)\n",
    "\n",
    "            # Step 2: Convert WordPiece text to BPE tokens\n",
    "            bpe_tokens = wordpiece_text.split(\" \")\n",
    "\n",
    "            # Step 3: Decode BPE tokens to original text\n",
    "            bpe_token_ids = [self.bpe_model.vocab.get(token, self.unk_token_id) for token in bpe_tokens]\n",
    "            decoded_text = self.bpe_model.decode(bpe_token_ids)\n",
    "\n",
    "            return decoded_text\n",
    "        \n",
    "        def build_wordpiece_vocab(self, text):\n",
    "            \"\"\"\n",
    "            Builds a vocabulary for WordPiece from BPE-encoded text.\n",
    "            \"\"\"\n",
    "            words = text.split(\" \")  # Split the BPE-encoded text into words\n",
    "            vocab = collections.Counter(words) \n",
    "            return vocab\n",
    "\n",
    "    class BPE:\n",
    "        def __init__(self, num_merges, unk_token_id=0):  # Accept unk_token_id parameter\n",
    "            self.vocab = {}\n",
    "            self.merges = []\n",
    "            self.num_merges = num_merges\n",
    "            self.unk_token_id = unk_token_id  # Store the unknown token ID\n",
    "            self.unk_token = \"[UNK]\"  # Add this line to define the unknown token\n",
    "\n",
    "        def train(self, text):\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "            vocab = collections.Counter(words)\n",
    "            vocab = {word + '</w>': count for word, count in vocab.items()}\n",
    "            print(\"Initial vocabulary:\", vocab)  # Check the initial vocabulary\n",
    "\n",
    "            for _ in range(self.num_merges):  # Use the num_merges from the instance variable\n",
    "                pairs = self.get_stats(vocab)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                best = max(pairs, key=pairs.get)\n",
    "                vocab = self.merge_vocab(best, vocab)\n",
    "                self.merges.append(best)\n",
    "                print(\"Vocabulary after merge:\", vocab)  # Check how it evolves\n",
    "\n",
    "            self.vocab = {word: i for i, word in enumerate(vocab.keys())}\n",
    "\n",
    "        @staticmethod\n",
    "        def get_stats(vocab):\n",
    "            pairs = collections.defaultdict(int)\n",
    "            for word, freq in vocab.items():\n",
    "                symbols = word.split()\n",
    "                for i in range(len(symbols)-1):\n",
    "                    pairs[symbols[i], symbols[i+1]] += freq\n",
    "            return pairs\n",
    "\n",
    "        @staticmethod\n",
    "        def merge_vocab(pair, vocab):\n",
    "            v_out = {}\n",
    "            bigram = re.escape(' '.join(pair))\n",
    "            p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "            for word in vocab:\n",
    "                w_out = p.sub(''.join(pair), word)\n",
    "                v_out[w_out] = vocab[word]\n",
    "            return v_out\n",
    "\n",
    "        def encode(self, text):\n",
    "            \"\"\"Encode text into subwords using learned BPE merges.\"\"\"\n",
    "            encoded_tokens = []\n",
    "            for word in re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE):\n",
    "                word += '</w>'\n",
    "                subwords = [word]  # Start with the entire word as one subword\n",
    "                for merge in self.merges:\n",
    "                    new_subwords = []\n",
    "                    for subword in subwords:\n",
    "                        # If the merge is in subword, split it; otherwise, keep it as is\n",
    "                        if ' '.join(merge) in subword:\n",
    "                            new_subwords.extend(subword.replace(' '.join(merge), ''.join(merge)).split(' '))\n",
    "                        else:\n",
    "                            new_subwords.append(subword)\n",
    "                    subwords = new_subwords\n",
    "                encoded_tokens.extend(subwords)\n",
    "                print(\"Subwords:\", subwords)   # Check subword generation\n",
    "                print(\"BPE Vocabulary:\", self.vocab) # Ensure the vocabulary is populated\n",
    "                return [str(self.vocab.get(token, self.unk_token_id)) for token in encoded_tokens]\n",
    "        \n",
    "            # New method to save trained model\n",
    "        def save_model(self, filepath):\n",
    "            bpe_data = {\n",
    "                'merges': self.merges,\n",
    "                'vocab': self.vocab,\n",
    "                'num_merges': self.num_merges,\n",
    "                # Include other attributes as needed\n",
    "            }\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(bpe_data, f)\n",
    "\n",
    "        def load_model(self, filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                bpe_data = json.load(f)\n",
    "            \n",
    "            self.merges = bpe_data['merges']\n",
    "            self.vocab = bpe_data['vocab']\n",
    "            self.num_merges = bpe_data['num_merges']\n",
    "\n",
    "        def decode(self, ids):\n",
    "            \"\"\"Decode a list of BPE token IDs back into the original text.\"\"\"\n",
    "            decoded_text = \"\"\n",
    "            for id_ in ids:\n",
    "                token = self.vocab.get(id_, self.unk_token)\n",
    "                decoded_text += token.replace(\"</w>\", \"\")  # Remove the '</w>' marker\n",
    "            return decoded_text\n",
    "\n",
    "    class WordPiece:\n",
    "        def __init__(self, vocab, unk_token_id=0, unk_token=\"[UNK]\"):\n",
    "            self.vocab = vocab\n",
    "            self.unk_token_id = unk_token_id\n",
    "            self.unk_token = unk_token  # Define the unknown token\n",
    "            self.root = self.build_trie(vocab)\n",
    "            self.id_to_token = {id_: token for token, id_ in vocab.items()}  # Inverse mapping\n",
    "            self.compute_failure_links(self.root)\n",
    "            print(\"Trie built successfully.\")\n",
    "\n",
    "        def convert_ids_to_tokens(self, ids):\n",
    "            \"\"\"\n",
    "            Convert a list of token ids back to their string token representations.\n",
    "            \"\"\"\n",
    "            return [self.id_to_token.get(id_, self.unk_token) for id_ in ids]\n",
    "\n",
    "        # Add debug prints to build_trie to confirm structure\n",
    "        def build_trie(self, vocab):\n",
    "            root = Tokenizer.TrieNode()\n",
    "            for token in vocab:\n",
    "                node = root\n",
    "                for char in token:\n",
    "                    if char not in node.children:\n",
    "                        node.children[char] = Tokenizer.TrieNode()\n",
    "                    node = node.children[char]\n",
    "                node.is_end = True\n",
    "                node.token = token\n",
    "            print(\"Trie Construction Completed Successfully\")\n",
    "            return root\n",
    "\n",
    "\n",
    "        def compute_failure_links(self, root):\n",
    "            queue = [root]\n",
    "            while queue:\n",
    "                current_node = queue.pop(0)\n",
    "                for char, child_node in current_node.children.items():\n",
    "                    failure_node = current_node.failure_link\n",
    "                    while failure_node and char not in failure_node.children:\n",
    "                        failure_node = failure_node.failure_link\n",
    "                    child_node.failure_link = failure_node.children[char] if failure_node else root\n",
    "                    queue.append(child_node)\n",
    "\n",
    "        # Improved debug prints in tokenize method\n",
    "                    \n",
    "        def tokenize(self, text):\n",
    "            # Preprocess input text\n",
    "            text = self.preprocess_text(text)\n",
    "            node = self.root\n",
    "            token_ids = []  # Will store token IDs instead of tokens\n",
    "            i = 0\n",
    "            print(\"Preprocessed Text:\", text)\n",
    "            while i < len(text):\n",
    "                char = text[i]\n",
    "                if char == ' ':\n",
    "                    node = self.root\n",
    "                    i += 1\n",
    "                    print(\"Current Node:\", node.token)  # Track how the trie is traversed\n",
    "                    print(\"Token IDs:\", token_ids)   \n",
    "                    continue\n",
    "\n",
    "                if char not in node.children:\n",
    "                    if node != self.root and node.token is not None:\n",
    "                        # Convert found token to its ID\n",
    "                        token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                        token_ids.append(token_id)\n",
    "                        node = self.root  # Reset to root\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Append unknown token ID\n",
    "                        token_ids.append(self.unk_token_id)\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                node = node.children[char]\n",
    "                if node.is_end:\n",
    "                    if i + 1 == len(text) or text[i + 1] == ' ':\n",
    "                        # Convert found token to its ID\n",
    "                        token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                        token_ids.append(token_id)\n",
    "                        node = self.root\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            #print(f\"Token IDs: {token_ids[:10]}\")\n",
    "            return token_ids\n",
    "\n",
    "        def preprocess_text(self, text):\n",
    "            # Convert text to lowercase to ensure case insensitivity\n",
    "            text = text.lower()\n",
    "\n",
    "            # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "            # This depends on how your vocabulary handles punctuation\n",
    "            text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "\n",
    "            # Replace multiple spaces with a single space\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "            # Trim leading and trailing spaces\n",
    "            text = text.strip()\n",
    "\n",
    "            return text\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "tokenizer = Tokenizer.Tokenize(bpe_vocab_size=30522, wordpiece_vocab_size=30522, num_merges=200) \n",
    "with open(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\", 'r', encoding='utf-8') as f: \n",
    "    text = f.read()\n",
    "tokenizer.train(text) \n",
    "encoded_text = tokenizer.encode(text)\n",
    "print(encoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "#############################################\n",
    "# Dataset class\n",
    "video_folder_path = \"D:\\MMM_Data\\Video\"\n",
    "image_folder_path = \"D:\\MMM_Data\\Image\"\n",
    "text_folder_path = \"D:\\MMM_Data\\Text\"\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, image_paths, video_paths, texts, tokenizer, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.video_paths = video_paths\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load video (simplified: loading only the first frame for example)\n",
    "        video_cap = cv2.VideoCapture(self.video_paths[idx])\n",
    "        ret, frame = video_cap.read()\n",
    "        if ret:\n",
    "            # Convert BGR to RGB and apply same transformations as for the image\n",
    "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            frame = self.transform(frame)\n",
    "        video_cap.release()\n",
    "\n",
    "        # Process text\n",
    "        text = self.tokenizer(self.texts[idx], return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "        return image, frame, text.input_ids.squeeze(), text.attention_mask.squeeze()\n",
    "\n",
    "def multimodal_collate_fn(batch):\n",
    "    images, frames, texts, attention_masks = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    frames = torch.stack(frames)\n",
    "    texts = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    return images, frames, texts, attention_masks\n",
    "\n",
    "\n",
    "# Assuming vocab is your vocabulary dictionary where keys are tokens and values are token IDs\n",
    "vocab = {...}  # Your vocabulary here\n",
    "\n",
    "# Instantiate your WordPiece tokenizer\n",
    "wordpiece_tokenizer = WordPiece(vocab, unk_token_id=vocab.get(\"[UNK]\", 0), unk_token=\"[UNK]\")\n",
    "\n",
    "# Example paths and texts remain the same\n",
    "image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\"]\n",
    "video_paths = [\"path/to/video1.mp4\", \"path/to/video2.mp4\"]\n",
    "texts = [\"This is a description for the first item\", \"This is a description for the second item\"]\n",
    "\n",
    "# Create dataset with custom tokenizer\n",
    "dataset = MultimodalDataset(image_paths, video_paths, texts, wordpiece_tokenizer)\n",
    "\n",
    "# Create DataLoader as before\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=multimodal_collate_fn)\n",
    "\n",
    "# Example usage\n",
    "for images, frames, texts, attention_masks in dataloader:\n",
    "    print(images.shape, frames.shape, texts.shape, attention_masks.shape)\n",
    "    # Route each modality input to the relevant model part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
