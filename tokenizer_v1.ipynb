{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "\n",
    "# Assuming you have a way to load your vocab mapping (word to index)\n",
    "# For simplicity, let's pretend we have a vocab dictionary and a reverse_vocab for encoding and decoding\n",
    "vocab = {\"[PAD]\": 0, \"[UNK]\": 1}  # Add the rest of your vocabulary here\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "pad_token_id = vocab[\"[PAD]\"]\n",
    "unk_token_id = vocab[\"[UNK]\"]\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.pattern = re.compile(r'[\\w]+|[^\\w\\s]')  # Regex to split words and punctuation\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.pattern.findall(text)  # Improved tokenization\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            subwords = self.find_subwords(token)\n",
    "            token_ids.extend(subwords)\n",
    "        return token_ids\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        subwords = []\n",
    "        i = 0\n",
    "        while i < len(token):\n",
    "            found_subword = False\n",
    "            for j in range(len(token), i, -1):\n",
    "                subword = token[i:j]\n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(self.vocab[subword])\n",
    "                    i = j\n",
    "                    found_subword = True\n",
    "                    break\n",
    "            if not found_subword:\n",
    "                subwords.append(unk_token_id)  # Fallback to UNK\n",
    "                i += 1  # Move to the next character\n",
    "        return subwords\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        if padding_length > 0:\n",
    "            encoded_text += [pad_token_id] * padding_length\n",
    "        else:\n",
    "            encoded_text = encoded_text[:self.max_seq_len]\n",
    "\n",
    "        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Adjusting the EmbeddingLayer to not use the Tokenizer's non-existent vocab attribute\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        return self.embedding(token_ids)\n",
    "\n",
    "# Correcting TransformerEncoderLayer's forward method to properly use MultiHeadAttention\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.norm1(src)\n",
    "        q, _ = self.self_attn(src2, src2, src2)\n",
    "        src = src + self.dropout(q)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "# Correction: Pooler squeezes the wrong dimension; it should squeeze dimension 0 (batch dimension is assumed to be 1 here)\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "# Load pre-trained tokenizer and adjust vocab_size accordingly\n",
    "tokenizer = Tokenizer(vocab=vocab)\n",
    "\n",
    "# Assuming vocab_size is the length of your vocab dictionary\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    EmbeddingLayer(vocab_size=vocab_size, embedding_dim=embedding_dim),\n",
    "    TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dim_feedforward=2048, dropout=0.1),\n",
    "    Pooler(d_model=embedding_dim)\n",
    ")\n",
    "\n",
    "# Correcting the training and evaluation loop\n",
    "# Load and preprocess data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_texts = train_data[\"text\"].tolist()\n",
    "train_labels = train_data[\"label\"].tolist()\n",
    "\n",
    "# Convert texts and labels into a Dataset and DataLoader\n",
    "max_seq_len = 512\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop corrected for proper input handling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for token_ids, labels in train_dataloader:\n",
    "        token_ids, labels = token_ids.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.positional_embeddings = PositionalEncoding(max_seq_len, embedding_dim)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        token_embeds = self.token_embeddings(token_ids)  # [batch_size, seq_len, embedding_dim]\n",
    "        position_embeds = self.positional_embeddings(token_embeds)  # [seq_len, embedding_dim]\n",
    "        return token_embeds + position_embeds\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\robbi\\Expert\n",
      "Working Directory After change WD: D:\\EXPERT_WEIGHTS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "os.chdir('D:\\EXPERT_WEIGHTS')\n",
    "\n",
    "print(\"Working Directory After change WD:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer unk_token_id: 1\n",
      "Tokenizer Vocabulary: {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, ' ': 4, ',': 5, 'the': 6, 'her': 7, 'of': 8, 'and': 9, '\\n': 10, 'a': 11, 'or': 12, 'she': 13, 'was': 14, 'Alice': 15, 'as': 16, 'book': 17, 'by': 18, 'conversations': 19, 'had': 20, 'in': 21, 'it': 22, 'pictures': 23, 'sister': 24, 'to': 25, 'very': 26, '.': 27, '?': 28, 'Rabbit': 29, 'So': 30, 'White': 31, 'bank': 32, 'be': 33, 'beginning': 34, 'but': 35, 'chain': 36, 'close': 37, 'considering': 38, 'could': 39, 'daisies': 40, 'daisy': 41, 'day': 42, 'do': 43, 'eyes': 44, 'feel': 45, 'for': 46, 'get': 47, 'getting': 48, 'having': 49, 'hot': 50, 'into': 51, 'is': 52, 'made': 53, 'making': 54, 'mind': 55, 'no': 56, 'nothing': 57, 'on': 58, 'once': 59, 'own': 60, 'peeped': 61, 'picking': 62, 'pink': 63, 'pleasure': 64, 'ran': 65, 'reading': 66, 'sitting': 67, 'sleepy': 68, 'stupid': 69, 'suddenly': 70, 'thought': 71, 'tired': 72, 'trouble': 73, 'twice': 74, 'up': 75, 'use': 76, 'well': 77, 'what': 78, 'when': 79, 'whether': 80, 'with': 81, 'without': 82, 'worth': 83, 'would': 84}\n",
      "Original Sentence: This is a sample sentence.\n",
      "Tokenized Output: [2, 1, 1, 52, 4, 52, 4, 11, 4, 1, 11, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 27, 3]\n",
      "--------------------------------------------------\n",
      "Original Sentence: Let's tokenize some unusual words with punctuation, shall we?\n",
      "Tokenized Output: [2, 1, 1, 1, 1, 4, 25, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 11, 1, 4, 1, 12, 1, 1, 4, 81, 4, 1, 1, 1, 1, 1, 1, 11, 1, 1, 58, 5, 4, 1, 1, 11, 1, 1, 4, 1, 1, 28, 3]\n",
      "--------------------------------------------------\n",
      "Original Sentence: 1234 numbers or combinations? How does the tokenizer handle this?\n",
      "Tokenized Output: [2, 1, 1, 1, 1, 4, 1, 1, 1, 33, 1, 1, 4, 12, 4, 1, 1, 1, 1, 21, 11, 1, 1, 58, 1, 28, 4, 1, 1, 1, 4, 43, 4, 6, 4, 25, 4, 1, 9, 1, 1, 4, 1, 1, 52, 28, 3]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def combined_loss(output, target, model, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)  \n",
    "    regularization_loss = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if isinstance(param, nn.Parameter):  \n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1\n",
    "\n",
    "    return task_loss + regularization_loss\n",
    "\n",
    "class AdaptiveDropoutLayer(nn.Module):\n",
    "    def __init__(self, init_dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())  # Use logit transformation for stability\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.log_alpha) \n",
    "        return nn.functional.dropout(x, p=p, training=self.training) \n",
    "\n",
    "class AdaptiveWeightDecayOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, init_l2_strength=0.01):\n",
    "        super().__init__(params, {'lr': lr})\n",
    "        self.log_l2_strength = nn.Parameter(torch.tensor(math.log(init_l2_strength)).float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = torch.exp(self.log_l2_strength)  \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay) \n",
    "                p.update(d_p, group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.positional_embeddings = nn.Parameter(torch.zeros(max_seq_len, embedding_dim), requires_grad=False)\n",
    "        \n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(math.log(10000.0) / embedding_dim))\n",
    "        \n",
    "        self.positional_embeddings[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_embeddings[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.positional_embeddings[:x.size(1), :]\n",
    "\n",
    "class MultiHeadLinformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, k=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k if k is not None else embed_dim // num_heads  # Projection dimension\n",
    "\n",
    "        # Separate projections for each head\n",
    "        self.key_projections = nn.ModuleList([nn.Linear(embed_dim, self.k) for _ in range(num_heads)])\n",
    "        self.value_projections = nn.ModuleList([nn.Linear(embed_dim, self.k) for _ in range(num_heads)]) \n",
    "\n",
    "        self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query):\n",
    "        seq_len, batch_size, _ = query.size()\n",
    "        heads = []  # Store output from each head\n",
    "\n",
    "        for head_idx in range(self.num_heads):\n",
    "            projected_keys = self.key_projections[head_idx](query)\n",
    "            projected_values = self.value_projections[head_idx](query)\n",
    "\n",
    "            # Calculate attention using projected keys and values\n",
    "            attention = torch.softmax(projected_keys.transpose(2, 3) @ projected_values, dim=-1) \n",
    "\n",
    "            out = attention @ projected_values.view(batch_size, seq_len, self.num_heads, self.k)\n",
    "            out = out.transpose(1, 2).contiguous().view(seq_len, batch_size, self.embed_dim)\n",
    "\n",
    "            heads.append(out)\n",
    "\n",
    "        # Concatenate outputs from all heads\n",
    "        out = torch.cat(heads, dim=-1) \n",
    "        out = self.out_projection(out) \n",
    "        return out\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len):  # Removed labels \n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "        encoded_text = self.dynamic_masking(encoded_text)\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def dynamic_masking(self, encoded_text):\n",
    "        for i in range(len(encoded_text)):\n",
    "            if np.random.rand() < 0.15:  # 15% chance like BERT\n",
    "                encoded_text[i] = tokenizer.vocab[\"[MASK]\"] \n",
    "        return encoded_text\n",
    "\n",
    "class AdaptiveEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.split_vocab(vocab, freq_threshold)\n",
    "\n",
    "        self.frequent_embeddings = nn.Embedding(len(self.frequent_vocab), large_embed_dim)\n",
    "        self.infrequent_embeddings = nn.Embedding(len(self.infrequent_vocab), small_embed_dim)\n",
    "\n",
    "        self.positional_embeddings = PositionalEncoding(max_seq_len, large_embed_dim)  \n",
    "\n",
    "\n",
    "    def split_vocab(self, vocab, freq_threshold):\n",
    "        token_counts = [(token, count) for token, count in vocab.items()] \n",
    "        token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "\n",
    "        self.frequent_vocab = {token: i for i, (token, _) in enumerate(token_counts[:split_point])}\n",
    "        self.infrequent_vocab = {token: i for i, (token, _) in enumerate(token_counts[split_point:])}\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        token_embeds = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.frequent_vocab:\n",
    "                embed = self.frequent_embeddings(torch.tensor(token_id).long())\n",
    "            else:\n",
    "                embed = self.infrequent_embeddings(torch.tensor(token_id).long())\n",
    "            token_embeds.append(embed)\n",
    "\n",
    "        token_embeds = torch.stack(token_embeds)\n",
    "        position_embeds = self.positional_embeddings(token_embeds)\n",
    "        return token_embeds + position_embeds\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn_layer = MultiHeadLinformerAttention(embed_dim=d_model, num_heads=nhead) \n",
    "        self.dropout1 = AdaptiveDropoutLayer()  # After self-attention\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        ) \n",
    "\n",
    "        # Instantiate adaptive dropout layers\n",
    "        self.dropout2 = AdaptiveDropoutLayer()  # After feed-forward\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src1 = self.norm1(src)  # Changed variable names for clarity\n",
    "        attn_output, _ = self.self_attn(src1, src1, src1, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(attn_output) \n",
    "\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout2(self.ffnn(src2))  \n",
    "        return src\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, nhead, dim_feedforward, dropout, \n",
    "                 freq_threshold, smaller_embed_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab, \n",
    "            freq_threshold=freq_threshold,  \n",
    "            large_embed_dim=embedding_dim,       \n",
    "            small_embed_dim=smaller_embed_dim,   \n",
    "            max_seq_len=max_seq_len\n",
    "        )        \n",
    "        self.encoder = TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward, dropout)\n",
    "        self.pooler = Pooler(embedding_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        encoded = self.encoder(embedded, src_mask=attention_mask)\n",
    "        pooled = self.pooler(encoded)\n",
    "        return pooled\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        node = self.root\n",
    "        subword_ids = []\n",
    "        for char in token:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "                if node.token_id is not None:\n",
    "                    subword_ids.append(node.token_id)\n",
    "                    break  # Assuming one token maps to one subword for simplicity\n",
    "            else:\n",
    "                break  # No further subword match found\n",
    "        if not subword_ids:  # If no subword was found\n",
    "            subword_ids.append(self.unk_token_id if hasattr(self, 'unk_token_id') else 1) \n",
    "        return subword_ids\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token_id = self.vocab.get(\"[UNK]\", 1)  # Get ID of [UNK] \n",
    "        self.max_subword_length = max(len(token) for token in vocab.keys())\n",
    "        self.pattern = re.compile(r'\\b\\w+\\b|[\\s\\.,!?;]')\n",
    "\n",
    "        # Build the Trie\n",
    "        self.trie = Trie()\n",
    "        for token, token_id in vocab.items():\n",
    "            self.trie.insert(token, token_id)\n",
    "\n",
    "    def _find_subwords(self, word):\n",
    "        # 1. Trie Lookup \n",
    "        subword_ids = self.trie.find_subwords(word)\n",
    "\n",
    "        # 2. Fallback to Original Logic \n",
    "        if len(subword_ids) == 1 and subword_ids[0] == self.unk_token_id:  \n",
    "            # ... Your existing WordPiece-like tokenization below ...\n",
    "            subwords = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                for j in range(self.max_subword_length, 0, -1):\n",
    "                    subword = word[i:i+j]\n",
    "                    if subword in self.vocab:\n",
    "                        subwords.append(self.vocab[subword])\n",
    "                        i += j\n",
    "                        break \n",
    "                else: \n",
    "                    subwords.append(self.vocab[\"[UNK]\"])\n",
    "                    i += 1\n",
    "            subword_ids = subwords  # Replace with token IDs\n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Add [CLS] token at the beginning\n",
    "        token_ids = [self.vocab[\"[CLS]\"]]\n",
    "        tokens = self.pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            subword_ids = self._find_subwords(token)\n",
    "            token_ids.extend(subword_ids)\n",
    "        # Add [SEP] token at the end\n",
    "        token_ids.append(self.vocab[\"[SEP]\"])\n",
    "        return token_ids\n",
    "\n",
    "# Load corpus and build vocab\n",
    "corpus = load_corpus(\"sample.txt\")\n",
    "tokens = tokenize(corpus)\n",
    "vocab = build_vocab(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(vocab=vocab)\n",
    "\n",
    "# Data Loading (Illustrative)\n",
    "train_texts = [corpus]  # Treat your whole sample as one \"document\"\n",
    "train_dataset = TextDataset(train_texts, tokenizer, max_seq_len=512)\n",
    "\n",
    "print(\"Tokenizer unk_token_id:\", tokenizer.unk_token_id) \n",
    "print(\"Tokenizer Vocabulary:\", tokenizer.vocab)\n",
    "\n",
    "\n",
    "# Sample text for testing your tokenizer\n",
    "test_sentences = [\n",
    "    \"This is a sample sentence.\",\n",
    "    \"Let's tokenize some unusual words with punctuation, shall we?\",\n",
    "    \"1234 numbers or combinations? How does the tokenizer handle this?\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    tokenized_output = tokenizer.encode(sentence)\n",
    "    print(f\"Original Sentence: {sentence}\")\n",
    "    print(f\"Tokenized Output: {tokenized_output}\")\n",
    "    print(\"-\" * 50)  # A separator for visual clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unk_token_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m test_sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a sample sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms tokenize some unusual words with punctuation, shall we?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1234 numbers or combinations? How does the tokenizer handle this?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m ]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m test_sentences:\n\u001b[1;32m----> 9\u001b[0m     tokenized_output \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Sentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized Output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenized_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 321\u001b[0m, in \u001b[0;36mTokenizer.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    319\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern\u001b[38;5;241m.\u001b[39mfindall(text)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m--> 321\u001b[0m     subword_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_subwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     token_ids\u001b[38;5;241m.\u001b[39mextend(subword_ids)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# Add [SEP] token at the end\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 295\u001b[0m, in \u001b[0;36mTokenizer._find_subwords\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_find_subwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# 1. Trie Lookup \u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m     subword_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_subwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;66;03m# 2. Fallback to Original Logic \u001b[39;00m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subword_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m subword_ids[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_token_id:  \n\u001b[0;32m    299\u001b[0m         \u001b[38;5;66;03m# ... Your existing WordPiece-like tokenization below ...\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 278\u001b[0m, in \u001b[0;36mTrie.find_subwords\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# No further subword match found\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subword_ids:  \u001b[38;5;66;03m# If no subword was found\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m     subword_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[43munk_token_id\u001b[49m)\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subword_ids\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unk_token_id' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_threshold_values = [10, 50, 100, 200, 500]  \n",
    "best_validation_accuracy = 0.0 \n",
    "\n",
    "for freq_threshold in freq_threshold_values:\n",
    "\n",
    "    # Model instantiation and training setup\n",
    "    model = TransformerModel(vocab_size=len(vocab), \n",
    "                             embedding_dim=128, \n",
    "                             max_seq_len=512, \n",
    "                             nhead=8, \n",
    "                             dim_feedforward=2048,\n",
    "                             freq_threshold=freq_threshold, \n",
    "                             smaller_embed_dim=64).to(device) \n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4) \n",
    "    meta_optimizer = AdaptiveWeightDecayOptimizer(model.parameters(), lr=1e-5) \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    meta_update_freq = 5\n",
    "\n",
    "    # Training loop corrected for model architecture\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad() \n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\n",
    "            # Meta-update occasionally \n",
    "            if (i + 1) % meta_update_freq == 0:\n",
    "                meta_optimizer.zero_grad() \n",
    "                loss = combined_loss(output, labels, model) \n",
    "                loss.backward()\n",
    "                meta_optimizer.step()  \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
