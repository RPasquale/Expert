{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "\n",
    "# Assuming you have a way to load your vocab mapping (word to index)\n",
    "# For simplicity, let's pretend we have a vocab dictionary and a reverse_vocab for encoding and decoding\n",
    "vocab = {\"[PAD]\": 0, \"[UNK]\": 1}  # Add the rest of your vocabulary here\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "pad_token_id = vocab[\"[PAD]\"]\n",
    "unk_token_id = vocab[\"[UNK]\"]\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.pattern = re.compile(r'[\\w]+|[^\\w\\s]')  # Regex to split words and punctuation\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.pattern.findall(text)  # Improved tokenization\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            subwords = self.find_subwords(token)\n",
    "            token_ids.extend(subwords)\n",
    "        return token_ids\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        subwords = []\n",
    "        i = 0\n",
    "        while i < len(token):\n",
    "            found_subword = False\n",
    "            for j in range(len(token), i, -1):\n",
    "                subword = token[i:j]\n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(self.vocab[subword])\n",
    "                    i = j\n",
    "                    found_subword = True\n",
    "                    break\n",
    "            if not found_subword:\n",
    "                subwords.append(unk_token_id)  # Fallback to UNK\n",
    "                i += 1  # Move to the next character\n",
    "        return subwords\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        if padding_length > 0:\n",
    "            encoded_text += [pad_token_id] * padding_length\n",
    "        else:\n",
    "            encoded_text = encoded_text[:self.max_seq_len]\n",
    "\n",
    "        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Adjusting the EmbeddingLayer to not use the Tokenizer's non-existent vocab attribute\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        return self.embedding(token_ids)\n",
    "\n",
    "# Correcting TransformerEncoderLayer's forward method to properly use MultiHeadAttention\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.norm1(src)\n",
    "        q, _ = self.self_attn(src2, src2, src2)\n",
    "        src = src + self.dropout(q)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "# Correction: Pooler squeezes the wrong dimension; it should squeeze dimension 0 (batch dimension is assumed to be 1 here)\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "# Load pre-trained tokenizer and adjust vocab_size accordingly\n",
    "tokenizer = Tokenizer(vocab=vocab)\n",
    "\n",
    "# Assuming vocab_size is the length of your vocab dictionary\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    EmbeddingLayer(vocab_size=vocab_size, embedding_dim=embedding_dim),\n",
    "    TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dim_feedforward=2048, dropout=0.1),\n",
    "    Pooler(d_model=embedding_dim)\n",
    ")\n",
    "\n",
    "# Correcting the training and evaluation loop\n",
    "# Load and preprocess data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_texts = train_data[\"text\"].tolist()\n",
    "train_labels = train_data[\"label\"].tolist()\n",
    "\n",
    "# Convert texts and labels into a Dataset and DataLoader\n",
    "max_seq_len = 512\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop corrected for proper input handling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for token_ids, labels in train_dataloader:\n",
    "        token_ids, labels = token_ids.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "class AdaptiveDropoutLayer(nn.Module):\n",
    "    def __init__(self, init_dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())  # Use logit transformation for stability\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.log_alpha) \n",
    "        return nn.functional.dropout(x, p=p, training=self.training) \n",
    "\n",
    "\n",
    "\n",
    "def combined_loss(output, target, model, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)  \n",
    "    regularization_loss = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if isinstance(param, nn.Parameter):  \n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1\n",
    "\n",
    "    return task_loss + regularization_loss\n",
    "\n",
    "class AdaptiveWeightDecayOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, init_l2_strength=0.01):\n",
    "        super().__init__(params, {'lr': lr})\n",
    "        self.log_l2_strength = nn.Parameter(torch.tensor(math.log(init_l2_strength)).float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = torch.exp(self.log_l2_strength)  \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay) \n",
    "                p.update(d_p, group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.positional_embeddings = nn.Parameter(torch.zeros(max_seq_len, embedding_dim), requires_grad=False)\n",
    "        \n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(math.log(10000.0) / embedding_dim))\n",
    "        \n",
    "        self.positional_embeddings[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_embeddings[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.positional_embeddings[:x.size(1), :]\n",
    "\n",
    "class MultiHeadLinformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, k=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k if k is not None else embed_dim // num_heads  # Projection dimension\n",
    "\n",
    "        # Separate projections for each head\n",
    "        self.key_projections = nn.ModuleList([nn.Linear(embed_dim, self.k) for _ in range(num_heads)])\n",
    "        self.value_projections = nn.ModuleList([nn.Linear(embed_dim, self.k) for _ in range(num_heads)]) \n",
    "\n",
    "        self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query):\n",
    "        seq_len, batch_size, _ = query.size()\n",
    "        heads = []  # Store output from each head\n",
    "\n",
    "        for head_idx in range(self.num_heads):\n",
    "            projected_keys = self.key_projections[head_idx](query)\n",
    "            projected_values = self.value_projections[head_idx](query)\n",
    "\n",
    "            # Calculate attention using projected keys and values\n",
    "            attention = torch.softmax(projected_keys.transpose(2, 3) @ projected_values, dim=-1) \n",
    "\n",
    "            out = attention @ projected_values.view(batch_size, seq_len, self.num_heads, self.k)\n",
    "            out = out.transpose(1, 2).contiguous().view(seq_len, batch_size, self.embed_dim)\n",
    "\n",
    "            heads.append(out)\n",
    "\n",
    "        # Concatenate outputs from all heads\n",
    "        out = torch.cat(heads, dim=-1) \n",
    "        out = self.out_projection(out) \n",
    "        return out\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "        encoded_text = self.dynamic_masking(encoded_text)  # Add this masking call\n",
    "\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    def dynamic_masking(self, encoded_text):\n",
    "        for i in range(len(encoded_text)):\n",
    "            if np.random.rand() < 0.15:  # 15% chance like BERT\n",
    "                encoded_text[i] = tokenizer.vocab[\"[MASK]\"] \n",
    "        return encoded_text\n",
    "\n",
    "'''\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.positional_embeddings = PositionalEncoding(max_seq_len, embedding_dim)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        token_embeds = self.token_embeddings(token_ids)  # [batch_size, seq_len, embedding_dim]\n",
    "        position_embeds = self.positional_embeddings(token_embeds)  # [seq_len, embedding_dim]\n",
    "        return token_embeds + position_embeds\n",
    "'''\n",
    "\n",
    "\n",
    "class AdaptiveEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.split_vocab(vocab, freq_threshold)\n",
    "\n",
    "        self.frequent_embeddings = nn.Embedding(len(self.frequent_vocab), large_embed_dim)\n",
    "        self.infrequent_embeddings = nn.Embedding(len(self.infrequent_vocab), small_embed_dim)\n",
    "\n",
    "        self.positional_embeddings = PositionalEncoding(max_seq_len, large_embed_dim)  \n",
    "\n",
    "\n",
    "    def split_vocab(self, vocab, freq_threshold):\n",
    "        token_counts = [(token, count) for token, count in vocab.items()] \n",
    "        token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "\n",
    "        self.frequent_vocab = {token: i for i, (token, _) in enumerate(token_counts[:split_point])}\n",
    "        self.infrequent_vocab = {token: i for i, (token, _) in enumerate(token_counts[split_point:])}\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        token_embeds = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.frequent_vocab:\n",
    "                embed = self.frequent_embeddings(torch.tensor(token_id).long())\n",
    "            else:\n",
    "                embed = self.infrequent_embeddings(torch.tensor(token_id).long())\n",
    "            token_embeds.append(embed)\n",
    "\n",
    "        token_embeds = torch.stack(token_embeds)\n",
    "        position_embeds = self.positional_embeddings(token_embeds)\n",
    "        return token_embeds + position_embeds\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn_layer = MultiHeadLinformerAttention(embed_dim=d_model, num_heads=nhead) \n",
    "        self.dropout1 = AdaptiveDropoutLayer()  # After self-attention\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        ) \n",
    "\n",
    "        # Instantiate adaptive dropout layers\n",
    "        self.dropout2 = AdaptiveDropoutLayer()  # After feed-forward\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src1 = self.norm1(src)  # Changed variable names for clarity\n",
    "        attn_output, _ = self.self_attn(src1, src1, src1, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(attn_output) \n",
    "\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout2(self.ffnn(src2))  \n",
    "        return src\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, nhead, dim_feedforward, dropout, \n",
    "                 freq_threshold, smaller_embed_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab, \n",
    "            freq_threshold=freq_threshold,  # You'll need to set this\n",
    "            large_embed_dim=embedding_dim,       # Or potentially a different size\n",
    "            small_embed_dim=smaller_embed_dim,   # Smaller dimension for infrequent words\n",
    "            max_seq_len=max_seq_len\n",
    "        )        \n",
    "        self.encoder = TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward, dropout)\n",
    "        self.pooler = Pooler(embedding_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        encoded = self.encoder(embedded, src_mask=attention_mask)\n",
    "        pooled = self.pooler(encoded)\n",
    "        return pooled\n",
    "\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        node = self.root\n",
    "        subword_ids = []\n",
    "        for char in token:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "                if node.token_id is not None:\n",
    "                    subword_ids.append(node.token_id)\n",
    "                    break  # Assuming one token maps to one subword for simplicity\n",
    "            else:\n",
    "                break  # No further subword match found\n",
    "        if not subword_ids:  # If no subword was found\n",
    "            subword_ids.append(unk_token_id)\n",
    "        return subword_ids\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.max_subword_length = max(len(token) for token in vocab.keys())\n",
    "        # Adjust regex to better match BERT's tokenization (e.g., including apostrophes)\n",
    "        self.pattern = re.compile(r'\\b\\w+\\b|[\\s\\.,!?;]')\n",
    "\n",
    "    def _find_subwords(self, word):\n",
    "        # Simplified simulation of WordPiece tokenization\n",
    "        subwords = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            for j in range(self.max_subword_length, 0, -1):\n",
    "                subword = word[i:i+j]\n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(self.vocab[subword])\n",
    "                    i += j\n",
    "                    break\n",
    "            else:\n",
    "                # If no subword is found, use [UNK]\n",
    "                subwords.append(self.vocab[\"[UNK]\"])\n",
    "                i += 1\n",
    "        return subwords\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Add [CLS] token at the beginning\n",
    "        token_ids = [self.vocab[\"[CLS]\"]]\n",
    "        tokens = self.pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            subword_ids = self._find_subwords(token)\n",
    "            token_ids.extend(subword_ids)\n",
    "        # Add [SEP] token at the end\n",
    "        token_ids.append(self.vocab[\"[SEP]\"])\n",
    "        return token_ids\n",
    "\n",
    "# Load corpus and build vocab\n",
    "corpus = load_corpus(\"corpus.txt\")\n",
    "tokens = tokenize(corpus)\n",
    "vocab = build_vocab(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(vocab=vocab)\n",
    "\n",
    "# Assuming train_texts and train_labels are defined\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_seq_len=512)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "freq_threshold_values = [10, 50, 100, 200, 500]  \n",
    "best_validation_accuracy = 0.0 \n",
    "\n",
    "for freq_threshold in freq_threshold_values:\n",
    "\n",
    "    # Model instantiation and training setup\n",
    "    model = TransformerModel(vocab_size=len(vocab), \n",
    "                             embedding_dim=128, \n",
    "                             max_seq_len=512, \n",
    "                             nhead=8, \n",
    "                             dim_feedforward=2048,\n",
    "                             freq_threshold=freq_threshold, \n",
    "                             smaller_embed_dim=64).to(device) \n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4) \n",
    "    meta_optimizer = AdaptiveWeightDecayOptimizer(model.parameters(), lr=1e-5) \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    meta_update_freq = 5\n",
    "    # Training loop corrected for model architecture\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad() \n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\n",
    "            # Meta-update occasionally \n",
    "            if (i + 1) % meta_update_freq == 0:\n",
    "                meta_optimizer.zero_grad() \n",
    "                loss = combined_loss(output, labels, model) \n",
    "                loss.backward()\n",
    "                meta_optimizer.step()  \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
