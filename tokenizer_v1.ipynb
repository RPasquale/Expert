{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "\n",
    "# Assuming you have a way to load your vocab mapping (word to index)\n",
    "# For simplicity, let's pretend we have a vocab dictionary and a reverse_vocab for encoding and decoding\n",
    "vocab = {\"[PAD]\": 0, \"[UNK]\": 1}  # Add the rest of your vocabulary here\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "pad_token_id = vocab[\"[PAD]\"]\n",
    "unk_token_id = vocab[\"[UNK]\"]\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.pattern = re.compile(r'[\\w]+|[^\\w\\s]')  # Regex to split words and punctuation\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.pattern.findall(text)  # Improved tokenization\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            subwords = self.find_subwords(token)\n",
    "            token_ids.extend(subwords)\n",
    "        return token_ids\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        subwords = []\n",
    "        i = 0\n",
    "        while i < len(token):\n",
    "            found_subword = False\n",
    "            for j in range(len(token), i, -1):\n",
    "                subword = token[i:j]\n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(self.vocab[subword])\n",
    "                    i = j\n",
    "                    found_subword = True\n",
    "                    break\n",
    "            if not found_subword:\n",
    "                subwords.append(unk_token_id)  # Fallback to UNK\n",
    "                i += 1  # Move to the next character\n",
    "        return subwords\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        if padding_length > 0:\n",
    "            encoded_text += [pad_token_id] * padding_length\n",
    "        else:\n",
    "            encoded_text = encoded_text[:self.max_seq_len]\n",
    "\n",
    "        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Adjusting the EmbeddingLayer to not use the Tokenizer's non-existent vocab attribute\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        return self.embedding(token_ids)\n",
    "\n",
    "# Correcting TransformerEncoderLayer's forward method to properly use MultiHeadAttention\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.norm1(src)\n",
    "        q, _ = self.self_attn(src2, src2, src2)\n",
    "        src = src + self.dropout(q)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "# Correction: Pooler squeezes the wrong dimension; it should squeeze dimension 0 (batch dimension is assumed to be 1 here)\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "# Load pre-trained tokenizer and adjust vocab_size accordingly\n",
    "tokenizer = Tokenizer(vocab=vocab)\n",
    "\n",
    "# Assuming vocab_size is the length of your vocab dictionary\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    EmbeddingLayer(vocab_size=vocab_size, embedding_dim=embedding_dim),\n",
    "    TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dim_feedforward=2048, dropout=0.1),\n",
    "    Pooler(d_model=embedding_dim)\n",
    ")\n",
    "\n",
    "# Correcting the training and evaluation loop\n",
    "# Load and preprocess data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_texts = train_data[\"text\"].tolist()\n",
    "train_labels = train_data[\"label\"].tolist()\n",
    "\n",
    "# Convert texts and labels into a Dataset and DataLoader\n",
    "max_seq_len = 512\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop corrected for proper input handling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for token_ids, labels in train_dataloader:\n",
    "        token_ids, labels = token_ids.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.positional_embeddings = PositionalEncoding(max_seq_len, embedding_dim)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        token_embeds = self.token_embeddings(token_ids)  # [batch_size, seq_len, embedding_dim]\n",
    "        position_embeds = self.positional_embeddings(token_embeds)  # [seq_len, embedding_dim]\n",
    "        return token_embeds + position_embeds\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: D:\\EXPERT_WEIGHTS\n",
      "Working Directory After change WD: D:\\EXPERT_WEIGHTS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "os.chdir('D:\\EXPERT_WEIGHTS')\n",
    "\n",
    "print(\"Working Directory After change WD:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual vocabulary size (including special tokens): 1743\n",
      "Tokenizer unk_token_id: 1\n",
      "Tokenizer Vocabulary: {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, ' ': 5, '\\n': 6, '.': 7, 'the': 8, ',': 9, 'of': 10, 'to': 11, 'and': 12, 'a': 13, 'is': 14, 'in': 15, 'that': 16, 'on': 17, '1': 18, 'networks': 19, 'shot': 20, 'for': 21, 'are': 22, 'we': 23, 'network': 24, 'We': 25, 'features': 26, 'dataset': 27, 'layer': 28, 'layers': 29, 'as': 30, '2': 31, 'from': 32, 'class': 33, 'with': 34, '0': 35, 'by': 36, 'each': 37, '5': 38, 'The': 39, 'classes': 40, 'training': 41, '3': 42, '4': 43, 'be': 44, 'performance': 45, 'or': 46, 'this': 47, 'learning': 48, 'trained': 49, 'A': 50, 'first': 51, 'points': 52, 'way': 53, 'distance': 54, 'task': 55, 'which': 56, 'embedding': 57, 'random': 58, 'set': 59, 'target': 60, 'an': 61, 'B': 62, 'al': 63, 'et': 64, 'prototypical': 65, '7': 66, 'can': 67, 'than': 68, 'few': 69, 'not': 70, 'at': 71, 'base': 72, 'two': 73, 'our': 74, 'results': 75, '6': 76, 'approach': 77, 'classification': 78, 'fine': 79, 'number': 80, 'one': 81, 'support': 82, 'transfer': 83, 'In': 84, 'data': 85, 'n': 86, 'use': 87, 'it': 88, 'over': 89, 'these': 90, 'using': 91, 'examples': 92, 'neural': 93, 'per': 94, 'test': 95, 'when': 96, 'This': 97, 'have': 98, 'only': 99, 'then': 100, 'also': 101, 'general': 102, 'new': 103, 'query': 104, 'accuracy': 105, 'because': 106, 'better': 107, 'different': 108, 'episodes': 109, 'meta': 110, 'more': 111, '20': 112, 'AnB': 113, 'Figure': 114, 'episode': 115, 'learn': 116, 'learned': 117, 'other': 118, 'same': 119, 'similar': 120, 'space': 121, 'tasks': 122, 'x': 123, 'both': 124, 'images': 125, 'natural': 126, 'specific': 127, 'train': 128, 'well': 129, '29': 130, 'Euclidean': 131, 'ImageNet': 132, 'but': 133, 'even': 134, 'given': 135, 'higher': 136, 'linear': 137, 'matching': 138, 'model': 139, 'prototype': 140, 'they': 141, 'weights': 142, 'BnB': 143, 'N': 144, 'Section': 145, 'baseB': 146, 'been': 147, 'into': 148, 'mean': 149, 'perform': 150, 'their': 151, 'top': 152, 'transferred': 153, 'used': 154, 'ck': 155, 'has': 156, 'non': 157, 'show': 158, 'simple': 159, 'tuning': 160, 'where': 161, 'will': 162, 'work': 163, 'zero': 164, '50': 165, '8': 166, 'Bregman': 167, 'between': 168, 'case': 169, 'classifier': 170, 'datasets': 171, 'effect': 172, 'experiments': 173, 'filters': 174, 'loss': 175, 'made': 176, 'randomly': 177, 'so': 178, 'there': 179, 'was': 180, ';': 181, 'For': 182, 'Prototypical': 183, 'These': 184, 'all': 185, 'co': 186, 'compare': 187, 'dimensional': 188, 'e': 189, 'frozen': 190, 'half': 191, 'k': 192, 'large': 193, 'may': 194, 'point': 195, 'prototypes': 196, 'split': 197, 'splits': 198, 'such': 199, 'time': 200, 'tuned': 201, '22': 202, '98': 203, 'Cosine': 204, 'SJE': 205, 'cluster': 206, 'do': 207, 'drop': 208, 'example': 209, 'fφ': 210, 'generalization': 211, 'level': 212, 'lower': 213, 'produce': 214, 'result': 215, 'selffer': 216, 'state': 217, 'upper': 218, 'validation': 219, 'vector': 220, '60': 221, 'Few': 222, 'Omniglot': 223, 'Table': 224, 'Vinyals': 225, 'art': 226, 'boost': 227, 'color': 228, 'could': 229, 'embedded': 230, 'found': 231, 'how': 232, 'its': 233, 'just': 234, 'man': 235, 'method': 236, 'nearest': 237, 'small': 238, 'toward': 239, 'transferring': 240, 'without': 241, '2009': 242, '28': 243, '64': 244, '9': 245, 'B3B': 246, 'CUB': 247, 'LSTM': 248, 'Larochelle': 249, 'Performance': 250, 'Ravi': 251, 'Top': 252, 'after': 253, 'based': 254, 'computed': 255, 'convolutional': 256, 'distances': 257, 'divergence': 258, 'either': 259, 'find': 260, 'function': 261, 'g': 262, 'here': 263, 'however': 264, 'if': 265, 'labeled': 266, 'left': 267, 'less': 268, 'middle': 269, 'much': 270, 'overfitting': 271, 'parameters': 272, 'particular': 273, 'possible': 274, 'proposed': 275, 'representation': 276, 'separate': 277, 'softmax': 278, 'them': 279, 'treatment': 280, 'vs': 281, 'within': 282, 'would': 283, '10': 284, '16': 285, '2011': 286, '23': 287, 'A3B': 288, 'Euclid': 289, 'However': 290, 'J': 291, 'K': 292, 'Matching': 293, 'NC': 294, 'NETWORKS': 295, 'On': 296, 'Sk': 297, 'University': 298, 'Zero': 299, 'achieve': 300, 'adapted': 301, 'chosen': 302, 'containing': 303, 'copied': 304, 'cosine': 305, 'd': 306, 'deep': 307, 'diamonds': 308, 'dissimilar': 309, 'does': 310, 'due': 311, 'experiment': 312, 'extent': 313, 'further': 314, 'image': 315, 'instead': 316, 'make': 317, 'metric': 318, 'order': 319, 'output': 320, 'particularly': 321, 'procedure': 322, 'rather': 323, 'recent': 324, 'reported': 325, 'represent': 326, 'row': 327, 'seen': 328, 'smaller': 329, 'squared': 330, 'transferability': 331, 'transition': 332, 'were': 333, 'whether': 334, 'xi': 335, 'yi': 336, '1000': 337, '200': 338, '2012': 339, '30': 340, '500': 341, 'Classification': 342, 'D': 343, 'FCE': 344, 'Gabor': 345, 'GoogLeNet': 346, 'If': 347, 'It': 348, 'Jarrett': 349, 'Model': 350, 'NS': 351, 'To': 352, 'above': 353, 'almost': 354, 'another': 355, 'any': 356, 'approaches': 357, 'architecture': 358, 'cat': 359, 'contains': 360, 'corresponding': 361, 'create': 362, 'degree': 363, 'density': 364, 'divergences': 365, 'end': 366, 'episodic': 367, 'equivalent': 368, 'error': 369, 'expected': 370, 'feature': 371, 'following': 372, 'generalize': 373, 'i': 374, 'including': 375, 'input': 376, 'issues': 377, 'last': 378, 'margin': 379, 'miniImageNet': 380, 'must': 381, 'neighbor': 382, 'neurons': 383, 'original': 384, 'performed': 385, 'problem': 386, 'red': 387, 'right': 388, 'second': 389, 'several': 390, 'shown': 391, 'shows': 392, 'specificity': 393, 'statistic': 394, 'subset': 395, 'surprising': 396, 'vectors': 397, 'weight': 398, 'y': 399, '15': 400, '19': 401, '40': 402, '450k': 403, '54': 404, '?': 405, 'Acc': 406, 'As': 407, 'Caltech': 408, 'Dept': 409, 'Each': 410, 'MATCHING': 411, 'NCA': 412, 'Nc': 413, 'Networks': 414, 'Our': 415, 'RANDOMSAMPLE': 416, 'RM': 417, 'S': 418, 'Thus': 419, 'Training': 420, 'When': 421, 'adaptation': 422, 'applied': 423, 'attribute': 424, 'average': 425, 'baseA': 426, 'blobs': 427, 'blue': 428, 'bottom': 429, 'call': 430, 'choice': 431, 'choices': 432, 'circles': 433, 'curious': 434, 'define': 435, 'descent': 436, 'difficulties': 437, 'distribution': 438, 'during': 439, 'effects': 440, 'evidence': 441, 'exhibit': 442, 'exp': 443, 'family': 444, 'follow': 445, 'four': 446, 'fragile': 447, 'functions': 448, 'generality': 449, 'greatly': 450, 'improves': 451, 'initialized': 452, 'line': 453, 'literature': 454, 'makes': 455, 'match': 456, 'methods': 457, 'mixture': 458, 'models': 459, 'multiple': 460, 'normalization': 461, 'optimization': 462, 'out': 463, 'paper': 464, 'provide': 465, 'provided': 466, 'quantify': 467, 'rate': 468, 'regular': 469, 's': 470, 'scenario': 471, 'single': 472, 'size': 473, 'statistician': 474, 'straightforward': 475, 'those': 476, 'three': 477, 'versions': 478, 'via': 479, 'white': 480, '000': 481, '100': 482, '2014': 483, '25': 484, '26': 485, '31': 486, '43': 487, '56': 488, '600': 489, '84': 490, '93': 491, 'AlexNet': 492, 'Bengio': 493, 'BnA': 494, 'Computer': 495, 'DA': 496, 'DS': 497, 'Datasets': 498, 'Equation': 499, 'Features': 500, 'Fine': 501, 'KNN': 502, 'Layer': 503, 'Learning': 504, 'NQ': 505, 'Nets': 506, 'OURS': 507, 'One': 508, 'Random': 509, 'SGD': 510, 'Science': 511, 'Select': 512, 'Since': 513, 'WA1': 514, 'accuracies': 515, 'across': 516, 'analysis': 517, 'attributes': 518, 'averaged': 519, 'baselines': 520, 'being': 521, 'bias': 522, 'bird': 523, 'character': 524, 'choose': 525, 'classify': 526, 'clustering': 527, 'come': 528, 'common': 529, 'comparison': 530, 'computation': 531, 'compute': 532, 'dark': 533, 'demonstrate': 534, 'denotes': 535, 'depends': 536, 'details': 537, 'determined': 538, 'directly': 539, 'discussed': 540, 'distant': 541, 'dominate': 542, 'dominates': 543, 'drops': 544, 'eight': 545, 'enable': 546, 'enough': 547, 'entire': 548, 'equivalence': 549, 'estimation': 550, 'expect': 551, 'exponential': 552, 'extend': 553, 'felid': 554, 'final': 555, 'finding': 556, 'five': 557, 'fixed': 558, 'gradient': 559, 'hand': 560, 'having': 561, 'high': 562, 'hyperparameters': 563, 'idea': 564, 'identical': 565, 'improve': 566, 'increases': 567, 'individual': 568, 'inductive': 569, 'initializing': 570, 'iterations': 571, 'keep': 572, 'key': 573, 'knowledge': 574, 'label': 575, 'labels': 576, 'larger': 577, 'learns': 578, 'least': 579, 'length': 580, 'light': 581, 'like': 582, 'limited': 583, 'local': 584, 'making': 585, 'many': 586, 'material': 587, 'meaning': 588, 'means': 589, 'near': 590, 'need': 591, 'observed': 592, 'obtain': 593, 'optimal': 594, 'ours': 595, 'pairs': 596, 'phenomenon': 597, 'pooling': 598, 'posterior': 599, 'previously': 600, 'processing': 601, 'propose': 602, 'pφ': 603, 'quite': 604, 'related': 605, 'respect': 606, 'retraining': 607, 'rotations': 608, 'scenarios': 609, 'seems': 610, 'sense': 611, 'setting': 612, 'showing': 613, 'simpler': 614, 'simply': 615, 'since': 616, 'some': 617, 'splitting': 618, 'study': 619, 'subplot': 620, 'suggests': 621, 'supervised': 622, 'supplementary': 623, 'take': 624, 'thus': 625, 'tune': 626, 'unit': 627, 'untrained': 628, 'useful': 629, 'utilizes': 630, 'various': 631, 'very': 632, 'vk': 633, 'weighted': 634, 'while': 635, 'z': 636, 'φ': 637, '024': 638, '101': 639, '11': 640, '12': 641, '13': 642, '14': 643, '17': 644, '2013': 645, '27': 646, '312': 647, '42': 648, '44': 649, '449': 650, '49': 651, '551': 652, '58': 653, '62': 654, '66': 655, '70': 656, '71': 657, '78': 658, '79': 659, '80': 660, '95': 661, '99': 662, 'ALE': 663, 'Abstract': 664, 'Accuracy': 665, 'Adam': 666, 'Algorithm': 667, 'All': 668, 'An': 669, 'Another': 670, 'Birds': 671, 'Bottom': 672, 'Comparison': 673, 'Cornell': 674, 'DVk': 675, 'Dist': 676, 'Donahue': 677, 'Fei': 678, 'Finally': 679, 'First': 680, 'Further': 681, 'ILSVRC': 682, 'Introduction': 683, 'Krizhevsky': 684, 'LG': 685, 'LMNN': 686, 'Large': 687, 'Lines': 688, 'Mahalanobis': 689, 'Man': 690, 'Mensink': 691, 'Natural': 692, 'Note': 693, 'Of': 694, 'PROTOTYPICAL': 695, 'Proto': 696, 'Qk': 697, 'R': 698, 'Results': 699, 'Such': 700, 'That': 701, 'Their': 702, 'There': 703, 'They': 704, 'Toronto': 705, 'Transfer': 706, 'Tune': 707, 'Twitter': 708, 'UCSD': 709, 'WA2': 710, 'WA3': 711, 'WB1': 712, 'WB2': 713, 'WB3': 714, 'While': 715, 'X': 716, 'able': 717, 'about': 718, 'achieves': 719, 'affected': 720, 'aggregated': 721, 'algorithms': 722, 'allowed': 723, 'allows': 724, 'applying': 725, 'approximate': 726, 'approximately': 727, 'arXiv': 728, 'architectural': 729, 'assigned': 730, 'assigning': 731, 'axis': 732, 'bars': 733, 'batch': 734, 'become': 735, 'beneficial': 736, 'block': 737, 'c1': 738, 'c2': 739, 'c3': 740, 'called': 741, 'carefully': 742, 'cases': 743, 'categories': 744, 'chance': 745, 'characters': 746, 'chopped': 747, 'closer': 748, 'collectively': 749, 'com': 750, 'combination': 751, 'comparable': 752, 'compared': 753, 'computing': 754, 'conditional': 755, 'conducted': 756, 'confidence': 757, 'conjecture': 758, 'connects': 759, 'considering': 760, 'consist': 761, 'contain': 762, 'control': 763, 'controls': 764, 'copy': 765, 'created': 766, 'cs': 767, 'custom': 768, 'decisions': 769, 'decoupled': 770, 'decreases': 771, 'degradation': 772, 'demonstrated': 773, 'depend': 774, 'depending': 775, 'design': 776, 'designed': 777, 'detectors': 778, 'determine': 779, 'developed': 780, 'did': 781, 'differ': 782, 'discriminative': 783, 'distinct': 784, 'divided': 785, 'domain': 786, 'draws': 787, 'dϕ': 788, 'effectively': 789, 'effectiveness': 790, 'efficient': 791, 'elements': 792, 'embeddings': 793, 'empirical': 794, 'empirically': 795, 'entities': 796, 'exact': 797, 'expense': 798, 'experimental': 799, 'experimentally': 800, 'explanation': 801, 'extension': 802, 'extensions': 803, 'fact': 804, 'fully': 805, 'gap': 806, 'generalizing': 807, 'generated': 808, 'generative': 809, 'giving': 810, 'good': 811, 'group': 812, 'grows': 813, 'handle': 814, 'help': 815, 'helpful': 816, 'hexagons': 817, 'implementation': 818, 'improved': 819, 'improvement': 820, 'improving': 821, 'include': 822, 'increased': 823, 'indicate': 824, 'indicates': 825, 'indicating': 826, 'initialization': 827, 'insight': 828, 'intervals': 829, 'involves': 830, 'k0': 831, 'known': 832, 'lead': 833, 'learnable': 834, 'leopard': 835, 'levels': 836, 'lingers': 837, 'lion': 838, 'log': 839, 'longer': 840, 'main': 841, 'mapping': 842, 'max': 843, 'maximize': 844, 'might': 845, 'minimal': 846, 'modeling': 847, 'most': 848, 'naturally': 849, 'nature': 850, 'nearly': 851, 'negatively': 852, 'neighboring': 853, 'nets': 854, 'next': 855, 'nonlinearity': 856, 'normalized': 857, 'note': 858, 'noticing': 859, 'object': 860, 'obtained': 861, 'occur': 862, 'optimize': 863, 'ordinary': 864, 'originally': 865, 'overfit': 866, 'own': 867, 'partitioning': 868, 'performing': 869, 'place': 870, 'plus': 871, 'poorly': 872, 'pre': 873, 'predict': 874, 'predictions': 875, 'previous': 876, 'primarily': 877, 'process': 878, 'produces': 879, 'producing': 880, 'quantified': 881, 'questions': 882, 'quickly': 883, 'reason': 884, 'reference': 885, 'regime': 886, 'relatively': 887, 'relevant': 888, 'remaining': 889, 'report': 890, 'representations': 891, 'represents': 892, 'require': 893, 'retrained': 894, 'reveals': 895, 'rows': 896, 'scratch': 897, 'sections': 898, 'serve': 899, 'severely': 900, 'should': 901, 'showed': 902, 'significant': 903, 'similarity': 904, 'sizes': 905, 'slightly': 906, 'solution': 907, 'specialization': 908, 'species': 909, 'spherical': 910, 'standard': 911, 'statistically': 912, 'still': 913, 'strictly': 914, 'subsets': 915, 'substantial': 916, 'subtracting': 917, 'successful': 918, 'suggesting': 919, 'tend': 920, 'terms': 921, 'therefore': 922, 'third': 923, 'through': 924, 'tiger': 925, 'total': 926, 'transformation': 927, 'transformed': 928, 'true': 929, 'unsupervised': 930, 'up': 931, 'updates': 932, 'us': 933, 'uses': 934, 'utilizing': 935, 'version': 936, 'worse': 937, 'written': 938, 'x1': 939, 'xN': 940, 'y1': 941, 'yN': 942, 'yield': 943, 'yields': 944, 'θ': 945, '00': 946, '05': 947, '05175v2': 948, '09': 949, '0k': 950, '1024': 951, '1200': 952, '1411': 953, '1600': 954, '1623': 955, '167': 956, '1703': 957, '1792v1': 958, '1995': 959, '1The': 960, '2000': 961, '2004': 962, '2013a': 963, '2013b': 964, '2017': 965, '21': 966, '24': 967, '281': 968, '2Note': 969, '37': 970, '3AnA': 971, '4We': 972, '51': 973, '52': 974, '55': 975, '625': 976, '645': 977, '68': 978, '73': 979, '77': 980, '788': 981, '800': 982, '86': 983, '90': 984, '96': 985, '97': 986, 'A8B': 987, 'Additionally': 988, 'Aerospace': 989, 'Also': 990, 'Alternately': 991, 'Although': 992, 'Analysis': 993, 'At': 994, 'B8B': 995, 'BASELINE': 996, 'Because': 997, 'By': 998, 'CLUSTERING': 999, 'CU': 1000, 'Caffe': 1001, 'Can': 1002, 'Caruana': 1003, 'Challenge': 1004, 'Choices': 1005, 'Classes': 1006, 'Clune': 1007, 'Compared': 1008, 'Components': 1009, 'Compute': 1010, 'Conclusion': 1011, 'Conclusions': 1012, 'DNet': 1013, 'Dark': 1014, 'Degradation': 1015, 'Deng': 1016, 'Density': 1017, 'Design': 1018, 'Discussion': 1019, 'Dissimilar': 1020, 'Distance': 1021, 'Dk': 1022, 'Does': 1023, 'Early': 1024, 'Edwards': 1025, 'Egyptian': 1026, 'Engineering': 1027, 'Episode': 1028, 'Error': 1029, 'Estimation': 1030, 'Examples': 1031, 'Experimental': 1032, 'Experiments': 1033, 'Felidae': 1034, 'Fergus': 1035, 'Fisher': 1036, 'Fortunately': 1037, 'Fourth': 1038, 'GPU': 1039, 'Gaussian': 1040, 'Gaussians': 1041, 'Generality': 1042, 'Girshick': 1043, 'Given': 1044, 'Gradient': 1045, 'Here': 1046, 'Hinton': 1047, 'Hod': 1048, 'How': 1049, 'ILSVRC2012': 1050, 'Image': 1051, 'Indeed': 1052, 'Initial': 1053, 'Initialize': 1054, 'Input': 1055, 'Instead': 1056, 'Institute': 1057, 'Into': 1058, 'Intuitively': 1059, 'Jake': 1060, 'Jason': 1061, 'Jeff': 1062, 'Jia': 1063, 'Jun': 1064, 'KL': 1065, 'Kevin': 1066, 'LEARNER': 1067, 'Layers': 1068, 'Le': 1069, 'Learner': 1070, 'Lee': 1071, 'Left': 1072, 'Legendre': 1073, 'Light': 1074, 'Like': 1075, 'Linear': 1076, 'Lipson4': 1077, 'M': 1078, 'META': 1079, 'Many': 1080, 'Measured': 1081, 'Mechanical': 1082, 'Meta': 1083, 'Mixture': 1084, 'Modern': 1085, 'Modifying': 1086, 'Montreal': 1087, 'Moreover': 1088, 'Much': 1089, 'NEAREST': 1090, 'NEIGHBORS': 1091, 'NETS': 1092, 'NEURAL': 1093, 'Neighborhood': 1094, 'Neither': 1095, 'No': 1096, 'Notably': 1097, 'Notation': 1098, 'Nov': 1099, 'Numbered': 1100, 'Operations': 1101, 'Output': 1102, 'Overall': 1103, 'Overview': 1104, 'P': 1105, 'PROTO': 1106, 'Persian': 1107, 'Points': 1108, 'Previously': 1109, 'Prototype': 1110, 'Pseudocode': 1111, 'ReLU': 1112, 'Recent': 1113, 'Recognition': 1114, 'Reed': 1115, 'Reinterpretation': 1116, 'Related': 1117, 'Relative': 1118, 'Research': 1119, 'Richard': 1120, 'Right': 1121, 'Rippel': 1122, 'SAMPLE': 1123, 'STATISTICIAN': 1124, 'SVM': 1125, 'Salakhutdinov': 1126, 'Scale': 1127, 'Second': 1128, 'Separate': 1129, 'Sermanet': 1130, 'Setup': 1131, 'Shot': 1132, 'Siamese': 1133, 'Similar': 1134, 'Similarly': 1135, 'Snell': 1136, 'Specificity': 1137, 'Splitting': 1138, 'Storkey': 1139, 'Swersky': 1140, 'Thanks': 1141, 'Third': 1142, 'Transferability': 1143, 'Two': 1144, 'Unlike': 1145, 'Update': 1146, 'V': 1147, 'Vector': 1148, 'Visual': 1149, 'WA4': 1150, 'WA5': 1151, 'WA6': 1152, 'WA7': 1153, 'WA8': 1154, 'WB4': 1155, 'WB5': 1156, 'WB6': 1157, 'WB7': 1158, 'WB8': 1159, 'Weights': 1160, 'Where': 1161, 'Whereas': 1162, 'With': 1163, 'Work': 1164, 'Wyoming': 1165, 'Y': 1166, 'Yoshua': 1167, 'Yosinski': 1168, 'Zeiler': 1169, 'Zemel': 1170, 'ability': 1171, 'abs': 1172, 'absolute': 1173, 'accommodate': 1174, 'account': 1175, 'achieving': 1176, 'act': 1177, 'activations': 1178, 'adapt': 1179, 'addition': 1180, 'additional': 1181, 'addressing': 1182, 'advance': 1183, 'advantage': 1184, 'advantageous': 1185, 'affect': 1186, 'against': 1187, 'aggregate': 1188, 'aggregation': 1189, 'aim': 1190, 'algorithm': 1191, 'allowing': 1192, 'alone': 1193, 'alphabets': 1194, 'alternative': 1195, 'although': 1196, 'amount': 1197, 'analyze': 1198, 'answers': 1199, 'anything': 1200, 'anywhere': 1201, 'apparent': 1202, 'apparently': 1203, 'appealing': 1204, 'appear': 1205, 'appearance': 1206, 'applicable': 1207, 'applies': 1208, 'apply': 1209, 'arbitrary': 1210, 'around': 1211, 'ask': 1212, 'assess': 1213, 'assign': 1214, 'assignment': 1215, 'associated': 1216, 'assumption': 1217, 'assumptions': 1218, 'attack': 1219, 'attained': 1220, 'attains': 1221, 'attempt': 1222, 'attempts': 1223, 'attention': 1224, 'attributed': 1225, 'augmenting': 1226, 'author': 1227, 'autoencoder': 1228, 'available': 1229, 'b': 1230, 'back': 1231, 'backprop': 1232, 'backpropagate': 1233, 'batches': 1234, 'befits': 1235, 'beginning': 1236, 'behavior': 1237, 'believe': 1238, 'belonging': 1239, 'below': 1240, 'benchmark': 1241, 'benefits': 1242, 'best': 1243, 'beyond': 1244, 'bi': 1245, 'biological': 1246, 'blob': 1247, 'blocks': 1248, 'boosting': 1249, 'bug': 1250, 'cannot': 1251, 'carries': 1252, 'cats': 1253, 'cause': 1254, 'causes': 1255, 'change': 1256, 'characteristics': 1257, 'characterize': 1258, 'cheetah': 1259, 'choosing': 1260, 'ck0': 1261, 'clarity': 1262, 'classconditional': 1263, 'classified': 1264, 'classifiers': 1265, 'closely': 1266, 'clusters': 1267, 'coadaptation': 1268, 'code': 1269, 'collected': 1270, 'comes': 1271, 'commonly': 1272, 'comparing': 1273, 'competition': 1274, 'completely': 1275, 'complex': 1276, 'complicated': 1277, 'component': 1278, 'composed': 1279, 'composition': 1280, 'comprised': 1281, 'comprises': 1282, 'computationally': 1283, 'concise': 1284, 'conclusions': 1285, 'configuration': 1286, 'connecting': 1287, 'connections': 1288, 'consideration': 1289, 'considered': 1290, 'constant': 1291, 'constrain': 1292, 'construct': 1293, 'constructed': 1294, 'constructing': 1295, 'contained': 1296, 'continuous': 1297, 'contrast': 1298, 'contributions': 1299, 'converge': 1300, 'convergence': 1301, 'convex': 1302, 'convolution': 1303, 'core': 1304, 'cost': 1305, 'couple': 1306, 'course': 1307, 'creates': 1308, 'crop': 1309, 'crops': 1310, 'cumulant': 1311, 'currently': 1312, 'cut': 1313, 'cvpr2016': 1314, 'datapoint': 1315, 'days': 1316, 'deal': 1317, 'decay': 1318, 'decline': 1319, 'decoupling': 1320, 'deeper': 1321, 'defined': 1322, 'definition': 1323, 'definitions': 1324, 'degrees': 1325, 'densities': 1326, 'derived': 1327, 'describes': 1328, 'description': 1329, 'descriptions': 1330, 'despite': 1331, 'differences': 1332, 'differentiable': 1333, 'differs': 1334, 'difficult': 1335, 'difficulty': 1336, 'dimension': 1337, 'diminishes': 1338, 'direction': 1339, 'directional': 1340, 'directions': 1341, 'distinction': 1342, 'distributions': 1343, 'divide': 1344, 'document': 1345, 'doesn': 1346, 'dogs': 1347, 'domains': 1348, 'done': 1349, 'dot': 1350, 'dotted': 1351, 'downloaded': 1352, 'draw': 1353, 'drawn': 1354, 'dynamically': 1355, 'dynamics': 1356, 'easier': 1357, 'effective': 1358, 'ellipsoidal': 1359, 'else': 1360, 'elsewhere': 1361, 'embed': 1362, 'emphasize': 1363, 'encode': 1364, 'encoder': 1365, 'encoding': 1366, 'encourages': 1367, 'environment': 1368, 'epochs': 1369, 'equally': 1370, 'errors': 1371, 'etc': 1372, 'eventually': 1373, 'every': 1374, 'excellent': 1375, 'except': 1376, 'exists': 1377, 'expensive': 1378, 'explorations': 1379, 'extends': 1380, 'extensible': 1381, 'extensive': 1382, 'extensively': 1383, 'extra': 1384, 'extracted': 1385, 'extremely': 1386, 'f': 1387, 'failed': 1388, 'fairly': 1389, 'faithful': 1390, 'falls': 1391, 'far': 1392, 'feather': 1393, 'felids': 1394, 'fewer': 1395, 'figure': 1396, 'files': 1397, 'filter': 1398, 'finetuned': 1399, 'fitted': 1400, 'fix': 1401, 'flexibility': 1402, 'flipped': 1403, 'focus': 1404, 'forces': 1405, 'form': 1406, 'formed': 1407, 'forms': 1408, 'formulate': 1409, 'fragilely': 1410, 'future': 1411, 'gaining': 1412, 'gains': 1413, 'generalizes': 1414, 'generally': 1415, 'get': 1416, 'getting': 1417, 'github': 1418, 'goal': 1419, 'grained': 1420, 'grayscale': 1421, 'greater': 1422, 'groups': 1423, 'gϑ': 1424, 'halves': 1425, 'handwritten': 1426, 'hard': 1427, 'held': 1428, 'helps': 1429, 'hierarchy': 1430, 'hinge': 1431, 'hold': 1432, 'holds': 1433, 'horizontally': 1434, 'http': 1435, 'https': 1436, 'human': 1437, 'humans': 1438, 'hyperparameter': 1439, 'hypothesis': 1440, 'hypothesize': 1441, 'ii': 1442, 'illustration': 1443, 'image2': 1444, 'importance': 1445, 'important': 1446, 'imposes': 1447, 'improvements': 1448, 'incorporate': 1449, 'incorporated': 1450, 'increase': 1451, 'indeed': 1452, 'independent': 1453, 'indices': 1454, 'inference': 1455, 'inferred': 1456, 'information': 1457, 'informative': 1458, 'initial': 1459, 'initialize': 1460, 'interact': 1461, 'interactions': 1462, 'interest': 1463, 'interested': 1464, 'interpretation': 1465, 'interpretations': 1466, 'interpreted': 1467, 'introduced': 1468, 'intuitive': 1469, 'involving': 1470, 'issue': 1471, 'itself': 1472, 'jaguar': 1473, 'join': 1474, 'jointly': 1475, 'justify': 1476, 'keeping': 1477, 'know': 1478, 'kz': 1479, 'latter': 1480, 'learner': 1481, 'leave': 1482, 'likewise': 1483, 'linearity': 1484, 'linearly': 1485, 'lines': 1486, 'linger': 1487, 'little': 1488, 'locked': 1489, 'logX': 1490, 'lost': 1491, 'low': 1492, 'lynx': 1493, 'm': 1494, 'manner': 1495, 'marker': 1496, 'measure': 1497, 'mechanism': 1498, 'mentioned': 1499, 'mimic': 1500, 'mini': 1501, 'minimizing': 1502, 'mirrors': 1503, 'mistakes': 1504, 'modal': 1505, 'modern': 1506, 'modifying': 1507, 'monitoring': 1508, 'mountain': 1509, 'multi': 1510, 'multimodal': 1511, 'multiples': 1512, 'naive': 1513, 'namely': 1514, 'negative': 1515, 'neighborhood': 1516, 'net': 1517, 'never': 1518, 'no': 1519, 'nor': 1520, 'normalize': 1521, 'notation': 1522, 'noted': 1523, 'notions': 1524, 'novel': 1525, 'objective': 1526, 'objectives': 1527, 'observation': 1528, 'obtaining': 1529, 'obviates': 1530, 'occurrence': 1531, 'occurs': 1532, 'off': 1533, 'often': 1534, 'old': 1535, 'opposed': 1536, 'optimized': 1537, 'orange': 1538, 'ordering': 1539, 'outline': 1540, 'outperforms': 1541, 'overlapping': 1542, 'parameter': 1543, 'parameterization': 1544, 'parent': 1545, 'part': 1546, 'patterns': 1547, 'perfectly': 1548, 'performances': 1549, 'performs': 1550, 'permissible': 1551, 'persists': 1552, 'phase': 1553, 'plausible': 1554, 'plots': 1555, 'plotted': 1556, 'post': 1557, 'powerful': 1558, 'prediction': 1559, 'preliminary': 1560, 'preparing': 1561, 'presents': 1562, 'prevents': 1563, 'probabilities': 1564, 'probability': 1565, 'proceeds': 1566, 'produced': 1567, 'progress': 1568, 'promising': 1569, 'pronounced': 1570, 'publication': 1571, 'pψ': 1572, 'quantifying': 1573, 'question': 1574, 'raises': 1575, 'ranges': 1576, 'rapidly': 1577, 'rates': 1578, 'raw': 1579, 're': 1580, 'recovers': 1581, 'rectangles': 1582, 'rectification': 1583, 'rediscovered': 1584, 'reedscot': 1585, 'refer': 1586, 'reflect': 1587, 'regardless': 1588, 'regularization': 1589, 'regularize': 1590, 'reinitialize': 1591, 'relate': 1592, 'relates': 1593, 'relative': 1594, 'relearn': 1595, 'relearned': 1596, 'relearning': 1597, 'released': 1598, 'relies': 1599, 'relu': 1600, 'rely': 1601, 'remainder': 1602, 'repeated': 1603, 'replacement': 1604, 'representative': 1605, 'representatives': 1606, 'represented': 1607, 'reproduce': 1608, 'repurpose': 1609, 'required': 1610, 'requires': 1611, 'requiring': 1612, 'researchers': 1613, 'resemble': 1614, 'resembles': 1615, 'resizing': 1616, 'retrain': 1617, 'rigorous': 1618, 'risk': 1619, 'run': 1620, 'runs': 1621, 'sample': 1622, 'sampled': 1623, 'save': 1624, 'say': 1625, 'schedule': 1626, 'scheme': 1627, 'secondary': 1628, 'selecting': 1629, 'semantically': 1630, 'sets': 1631, 'settings': 1632, 'setup': 1633, 'seven': 1634, 'shape': 1635, 'shared': 1636, 'shifted': 1637, 'shown3': 1638, 'significantly': 1639, 'simplicity': 1640, 'simplify': 1641, 'situation': 1642, 'slight': 1643, 'snow': 1644, 'software': 1645, 'somewhere': 1646, 'sophisticated': 1647, 'sparse': 1648, 'special': 1649, 'specifically': 1650, 'specifies': 1651, 'speed': 1652, 'spread': 1653, 'step': 1654, 'stopping': 1655, 'stops': 1656, 'store': 1657, 'strikingly': 1658, 'studied': 1659, 'studies': 1660, 'studying': 1661, 'subject': 1662, 'subsampling': 1663, 'substantially': 1664, 'successfully': 1665, 'successive': 1666, 'suddenly': 1667, 'suffers': 1668, 'suitability': 1669, 'suitable': 1670, 'suited': 1671, 'summarize': 1672, 'summarizes': 1673, 'summary': 1674, 'suspicion': 1675, 'systems': 1676, 't': 1677, 'tabby': 1678, 'tackle': 1679, 'taken': 1680, 'takes': 1681, 'taking': 1682, 'tanh': 1683, 'technique': 1684, 'techniques': 1685, 'tell': 1686, 'tendency': 1687, 'term': 1688, 'tested': 1689, 'tests': 1690, 'text': 1691, 'theart': 1692, 'themselves': 1693, 'thereby': 1694, 'things': 1695, 'though': 1696, 'tool': 1697, 'trains': 1698, 'transferable': 1699, 'treatments': 1700, 'tweak': 1701, 'tweaking': 1702, 'type': 1703, 'types': 1704, 'under': 1705, 'underlying': 1706, 'uniformly': 1707, 'uniquely': 1708, 'unlabeled': 1709, 'until': 1710, 'upon': 1711, 'usual': 1712, 'usually': 1713, 'utilize': 1714, 'v1': 1715, 'v2': 1716, 'v3': 1717, 'variance': 1718, 'variant': 1719, 'variants': 1720, 'variational': 1721, 'vast': 1722, 'versa': 1723, 'versus': 1724, 'vertical': 1725, 'vice': 1726, 'viewed': 1727, 'visual': 1728, 'vital': 1729, 'want': 1730, 'ways': 1731, 'whereas': 1732, 'whole': 1733, 'whose': 1734, 'wide': 1735, 'won': 1736, 'xk': 1737, 'yosinski': 1738, 'µ': 1739, 'θk': 1740, 'ψ': 1741, 'ϕ': 1742}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robbi\\AppData\\Local\\Temp\\ipykernel_35548\\377544582.py:192: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embed = self.infrequent_embeddings(torch.tensor(token_id).long())\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 612\u001b[0m\n\u001b[0;32m    610\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m    611\u001b[0m input_ids, attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 612\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, labels)\n\u001b[0;32m    614\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 252\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 252\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(embedded, src_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m    254\u001b[0m     pooled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoded)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 192\u001b[0m, in \u001b[0;36mAdaptiveEmbeddingLayer.forward\u001b[1;34m(self, token_ids)\u001b[0m\n\u001b[0;32m    190\u001b[0m         embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrequent_embeddings(torch\u001b[38;5;241m.\u001b[39mtensor(token_id)\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m         embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfrequent_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     token_embeds\u001b[38;5;241m.\u001b[39mappend(embed)\n\u001b[0;32m    195\u001b[0m token_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(token_embeds)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # Ensure special tokens are included\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def combined_loss(output, target, model, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)  \n",
    "    regularization_loss = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if isinstance(param, nn.Parameter):  \n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1\n",
    "\n",
    "    return task_loss + regularization_loss\n",
    "\n",
    "# Encoding Transformer Code\n",
    "\n",
    "class AdaptiveDropoutLayer(nn.Module):\n",
    "    def __init__(self, init_dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())  # Use logit transformation for stability\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.log_alpha) \n",
    "        return nn.functional.dropout(x, p=p, training=self.training) \n",
    "\n",
    "class AdaptiveWeightDecayOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, init_l2_strength=0.01):\n",
    "        super().__init__(params, {'lr': lr})\n",
    "        self.log_l2_strength = nn.Parameter(torch.tensor(math.log(init_l2_strength)).float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = torch.exp(self.log_l2_strength)  \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay) \n",
    "                p.update(d_p, group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.positional_embeddings = nn.Parameter(torch.zeros(max_seq_len, embedding_dim), requires_grad=False)\n",
    "        \n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(math.log(10000.0) / embedding_dim))\n",
    "        \n",
    "        self.positional_embeddings[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_embeddings[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.positional_embeddings[:x.size(1), :]\n",
    "\n",
    "class MultiHeadLinformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, k=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k if k is not None else embed_dim // num_heads  # Projection dimension\n",
    "\n",
    "        # Separate projections for each head\n",
    "        self.key_projections = nn.ModuleList([nn.Linear(embed_dim, self.k) for _ in range(num_heads)])\n",
    "        self.value_projections = nn.ModuleList([nn.Linear(embed_dim, self.k) for _ in range(num_heads)]) \n",
    "\n",
    "        self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query):\n",
    "        seq_len, batch_size, _ = query.size()\n",
    "        heads = []  # Store output from each head\n",
    "\n",
    "        for head_idx in range(self.num_heads):\n",
    "            projected_keys = self.key_projections[head_idx](query)\n",
    "            projected_values = self.value_projections[head_idx](query)\n",
    "\n",
    "            # Calculate attention using projected keys and values\n",
    "            attention = torch.softmax(projected_keys.transpose(2, 3) @ projected_values, dim=-1) \n",
    "\n",
    "            out = attention @ projected_values.view(batch_size, seq_len, self.num_heads, self.k)\n",
    "            out = out.transpose(1, 2).contiguous().view(seq_len, batch_size, self.embed_dim)\n",
    "\n",
    "            heads.append(out)\n",
    "\n",
    "        # Concatenate outputs from all heads\n",
    "        out = torch.cat(heads, dim=-1) \n",
    "        out = self.out_projection(out) \n",
    "        return out\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "        labels = encoded_text.copy()  # Copy encoded text for labels\n",
    "\n",
    "        # Apply dynamic masking\n",
    "        masked_indices = np.random.rand(len(encoded_text)) < 0.15\n",
    "        for i in range(len(encoded_text)):\n",
    "            if masked_indices[i]:\n",
    "                encoded_text[i] = self.tokenizer.vocab[\"[MASK]\"]\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "        labels += [-100] * padding_length  # Use -100 for padding positions, ignored by nn.CrossEntropyLoss\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class AdaptiveEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.split_vocab(vocab, freq_threshold)\n",
    "\n",
    "        # Initialize embeddings with the correct size\n",
    "        self.frequent_embeddings = nn.Embedding(num_embeddings=len(self.frequent_vocab), embedding_dim=large_embed_dim)\n",
    "        self.infrequent_embeddings = nn.Embedding(num_embeddings=len(self.infrequent_vocab), embedding_dim=small_embed_dim)\n",
    "\n",
    "\n",
    "        self.positional_embeddings = PositionalEncoding(max_seq_len, large_embed_dim)  \n",
    "\n",
    "\n",
    "    def split_vocab(self, vocab, freq_threshold):\n",
    "        token_counts = [(token, count) for token, count in vocab.items()] \n",
    "        token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "\n",
    "        self.frequent_vocab = {token: i for i, (token, _) in enumerate(token_counts[:split_point])}\n",
    "        self.infrequent_vocab = {token: i for i, (token, _) in enumerate(token_counts[split_point:])}\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        token_embeds = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.frequent_vocab:\n",
    "                embed = self.frequent_embeddings(torch.tensor(token_id).long())\n",
    "            else:\n",
    "                embed = self.infrequent_embeddings(torch.tensor(token_id).long())\n",
    "            token_embeds.append(embed)\n",
    "\n",
    "        token_embeds = torch.stack(token_embeds)\n",
    "        position_embeds = self.positional_embeddings(token_embeds)\n",
    "        return token_embeds + position_embeds\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadLinformerAttention(embed_dim=d_model, num_heads=nhead)\n",
    "        self.dropout1 = AdaptiveDropoutLayer()  # Use AdaptiveDropoutLayer\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            AdaptiveDropoutLayer(),  # Use AdaptiveDropoutLayer here as well\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.dropout2 = AdaptiveDropoutLayer()  # And here\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2 = self.norm1(src)\n",
    "        attn_output = self.attn(src2)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout2(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, nhead, dim_feedforward, \n",
    "                 freq_threshold, smaller_embed_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab, \n",
    "            freq_threshold=freq_threshold,  \n",
    "            large_embed_dim=embedding_dim,       \n",
    "            small_embed_dim=smaller_embed_dim,   \n",
    "            max_seq_len=max_seq_len\n",
    "        )        \n",
    "        self.encoder = TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward)\n",
    "        self.pooler = Pooler(embedding_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        encoded = self.encoder(embedded, src_mask=attention_mask)\n",
    "        pooled = self.pooler(encoded)\n",
    "        return pooled\n",
    "\n",
    "# Tokenizing Code\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        node = self.root\n",
    "        subword_ids = []\n",
    "        for char in token:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "                if node.token_id is not None:\n",
    "                    subword_ids.append(node.token_id)\n",
    "                    break  # Assuming one token maps to one subword for simplicity\n",
    "            else:\n",
    "                break  # No further subword match found\n",
    "        if not subword_ids:  # If no subword was found\n",
    "            subword_ids.append(self.unk_token_id if hasattr(self, 'unk_token_id') else 1) \n",
    "        return subword_ids\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(best)  # Track most frequent pair in each iteration\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.split()  # Assuming simple word splitting\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None \n",
    "        self.failure_pops = None\n",
    "\n",
    "    def train(self, corpus):\n",
    "        vocabulary = self._build_vocabulary(corpus)  # Build the word list\n",
    "        self.trie, self.failure_links, self.failure_pops = self._precompute(vocabulary)\n",
    "\n",
    "    def _encode(self, word):\n",
    "        subword_ids = []\n",
    "        current_node = self.trie.root\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "            if char in current_node.children:\n",
    "                current_node = current_node.children[char]\n",
    "                if current_node.token_id:\n",
    "                    subword_ids.append(current_node.token_id)\n",
    "            else:  # No direct match. Implement failure logic\n",
    "                while True:\n",
    "                    if current_node.failure_link is not None:\n",
    "                        current_node = current_node.failure_link\n",
    "\n",
    "                        # Check for pops to skip further backtracking\n",
    "                        for _ in range(current_node.failure_pop):\n",
    "                            current_node = current_node.failure_link\n",
    "\n",
    "                        if char in current_node.children:\n",
    "                            current_node = current_node.children[char]\n",
    "                            if current_node.token_id:\n",
    "                                subword_ids.append(current_node.token_id)\n",
    "                            break  # Successful subword transition \n",
    "\n",
    "                    else:  # No failure link, we're back at root level\n",
    "                        # Handle situation based on your application\n",
    "                        # e.g., append an unknown token \n",
    "                        subword_ids.append(self.unk_token_id)  \n",
    "                        break  \n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        if model_type == \"unigram\":\n",
    "            tokens = self._unigram_tokenize(corpus)  #  Hypothetical tokenize function\n",
    "            vocabulary = self._build_unigram_vocab(tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        # Count token frequencies\n",
    "        token_freqs = collections.Counter(tokens)\n",
    "        # Select the most frequent tokens up to vocab_size\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(token_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction\n",
    "        self.trie = Trie()  # Assuming you have a Trie class as well\n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token)  \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "    def _unigram_tokenize(self, corpus):\n",
    "        \"\"\"\n",
    "        Tokenizes the given corpus into unigram tokens, checking against the built vocabulary.\n",
    "        Tokens not found in the vocabulary are treated as unknowns.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): The text corpus to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list of str: A list of tokens extracted from the corpus, adjusted to fit the vocabulary.\n",
    "        \"\"\"\n",
    "        # Initial tokenization based on word boundaries and punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', corpus)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, actual_vocab_size):\n",
    "        self.vocab = vocab\n",
    "        self.actual_vocab_size = actual_vocab_size\n",
    "        self.unk_token_id = self.vocab.get(\"[UNK]\", 1)  # Get ID of [UNK] \n",
    "        self.max_subword_length = max(len(token) for token in vocab.keys())\n",
    "        self.pattern = re.compile(r'\\b\\w+\\b|[\\s\\.,!?;]')\n",
    "\n",
    "        # Build the Trie\n",
    "        self.trie = Trie()\n",
    "        for token, token_id in vocab.items():\n",
    "            self.trie.insert(token, token_id)\n",
    "\n",
    "    def _find_subwords(self, word):\n",
    "        # 1. Trie Lookup \n",
    "        subword_ids = self.trie.find_subwords(word)\n",
    "\n",
    "        # 2. Fallback to Original Logic \n",
    "        if len(subword_ids) == 1 and subword_ids[0] == self.unk_token_id:  \n",
    "            subwords = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                for j in range(self.max_subword_length, 0, -1):\n",
    "                    subword = word[i:i+j]\n",
    "                    if subword in self.vocab:\n",
    "                        subwords.append(self.vocab[subword])\n",
    "                        i += j\n",
    "                        break \n",
    "                else: \n",
    "                    subwords.append(self.vocab[\"[UNK]\"])\n",
    "                    i += 1\n",
    "            subword_ids = subwords  # Replace with token IDs\n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Add [CLS] token at the beginning\n",
    "        token_ids = [self.vocab[\"[CLS]\"]]\n",
    "        tokens = self.pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            subword_ids = self._find_subwords(token)\n",
    "            # Ensure all token IDs are within the actual vocabulary size\n",
    "            subword_ids = [id if id < self.actual_vocab_size else self.vocab[\"[UNK]\"] for id in subword_ids]\n",
    "            token_ids.extend(subword_ids)\n",
    "        # Add [SEP] token at the end\n",
    "        token_ids.append(self.vocab[\"[SEP]\"])\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "# Training Code\n",
    "\n",
    "# Load corpus and build vocab\n",
    "corpus = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "tokens = tokenize(corpus)\n",
    "vocab = build_vocab(tokens)\n",
    "actual_vocab_size = len(vocab)  # This includes [PAD], [UNK], [CLS], [SEP], [MASK]\n",
    "\n",
    "print(f\"Actual vocabulary size (including special tokens): {actual_vocab_size}\")\n",
    "\n",
    "tokenizer = Tokenizer(vocab=vocab, actual_vocab_size=actual_vocab_size)\n",
    "\n",
    "# Data Loading (Illustrative)\n",
    "train_texts = [corpus]  # Treat your whole sample as one \"document\"\n",
    "train_dataset = TextDataset(train_texts, tokenizer, max_seq_len=512)\n",
    "\n",
    "print(\"Tokenizer unk_token_id:\", tokenizer.unk_token_id) \n",
    "print(\"Tokenizer Vocabulary:\", tokenizer.vocab)\n",
    "\n",
    "'''\n",
    "# Sample text for testing your tokenizer\n",
    "test_sentences = [\n",
    "    \"This is a sample sentence.\",\n",
    "    \"Let's tokenize some unusual words with punctuation, shall we?\",\n",
    "    \"1234 numbers or combinations? How does the tokenizer handle this?\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    tokenized_output = tokenizer.encode(sentence)\n",
    "    print(f\"Original Sentence: {sentence}\")\n",
    "    print(f\"Tokenized Output: {tokenized_output}\")\n",
    "    print(\"-\" * 50)  # A separator for visual clarity\n",
    "\n",
    "'''\n",
    "\n",
    "device='cpu'\n",
    "freq_threshold_values = [10, 50, 100, 200, 500]  \n",
    "best_validation_accuracy = 0.0 \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for freq_threshold in freq_threshold_values:\n",
    "\n",
    "    # Model instantiation and training setup\n",
    "    model = TransformerModel(\n",
    "        vocab_size=actual_vocab_size,\n",
    "        embedding_dim=128,\n",
    "        max_seq_len=512,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        freq_threshold=freq_threshold,  # Define your frequency threshold for splitting vocab\n",
    "        smaller_embed_dim=64\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4) \n",
    "    meta_optimizer = AdaptiveWeightDecayOptimizer(model.parameters(), lr=1e-5) \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    meta_update_freq = 5\n",
    "\n",
    "    # Training loop corrected for model architecture\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad() \n",
    "            input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\n",
    "            # Meta-update occasionally \n",
    "            if (i + 1) % meta_update_freq == 0:\n",
    "                meta_optimizer.zero_grad() \n",
    "                loss = combined_loss(output, labels, model) \n",
    "                loss.backward()\n",
    "                meta_optimizer.step()  \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual vocabulary size (including special tokens): 1743\n",
      "Tokenizer unk_token_id: 1\n",
      "Tokenizer Vocabulary: {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, ' ': 5, '\\n': 6, '.': 7, 'the': 8, ',': 9, 'of': 10, 'to': 11, 'and': 12, 'a': 13, 'is': 14, 'in': 15, 'that': 16, 'on': 17, '1': 18, 'networks': 19, 'shot': 20, 'for': 21, 'are': 22, 'we': 23, 'network': 24, 'We': 25, 'features': 26, 'dataset': 27, 'layer': 28, 'layers': 29, 'as': 30, '2': 31, 'from': 32, 'class': 33, 'with': 34, '0': 35, 'by': 36, 'each': 37, '5': 38, 'The': 39, 'classes': 40, 'training': 41, '3': 42, '4': 43, 'be': 44, 'performance': 45, 'or': 46, 'this': 47, 'learning': 48, 'trained': 49, 'A': 50, 'first': 51, 'points': 52, 'way': 53, 'distance': 54, 'task': 55, 'which': 56, 'embedding': 57, 'random': 58, 'set': 59, 'target': 60, 'an': 61, 'B': 62, 'al': 63, 'et': 64, 'prototypical': 65, '7': 66, 'can': 67, 'than': 68, 'few': 69, 'not': 70, 'at': 71, 'base': 72, 'two': 73, 'our': 74, 'results': 75, '6': 76, 'approach': 77, 'classification': 78, 'fine': 79, 'number': 80, 'one': 81, 'support': 82, 'transfer': 83, 'In': 84, 'data': 85, 'n': 86, 'use': 87, 'it': 88, 'over': 89, 'these': 90, 'using': 91, 'examples': 92, 'neural': 93, 'per': 94, 'test': 95, 'when': 96, 'This': 97, 'have': 98, 'only': 99, 'then': 100, 'also': 101, 'general': 102, 'new': 103, 'query': 104, 'accuracy': 105, 'because': 106, 'better': 107, 'different': 108, 'episodes': 109, 'meta': 110, 'more': 111, '20': 112, 'AnB': 113, 'Figure': 114, 'episode': 115, 'learn': 116, 'learned': 117, 'other': 118, 'same': 119, 'similar': 120, 'space': 121, 'tasks': 122, 'x': 123, 'both': 124, 'images': 125, 'natural': 126, 'specific': 127, 'train': 128, 'well': 129, '29': 130, 'Euclidean': 131, 'ImageNet': 132, 'but': 133, 'even': 134, 'given': 135, 'higher': 136, 'linear': 137, 'matching': 138, 'model': 139, 'prototype': 140, 'they': 141, 'weights': 142, 'BnB': 143, 'N': 144, 'Section': 145, 'baseB': 146, 'been': 147, 'into': 148, 'mean': 149, 'perform': 150, 'their': 151, 'top': 152, 'transferred': 153, 'used': 154, 'ck': 155, 'has': 156, 'non': 157, 'show': 158, 'simple': 159, 'tuning': 160, 'where': 161, 'will': 162, 'work': 163, 'zero': 164, '50': 165, '8': 166, 'Bregman': 167, 'between': 168, 'case': 169, 'classifier': 170, 'datasets': 171, 'effect': 172, 'experiments': 173, 'filters': 174, 'loss': 175, 'made': 176, 'randomly': 177, 'so': 178, 'there': 179, 'was': 180, ';': 181, 'For': 182, 'Prototypical': 183, 'These': 184, 'all': 185, 'co': 186, 'compare': 187, 'dimensional': 188, 'e': 189, 'frozen': 190, 'half': 191, 'k': 192, 'large': 193, 'may': 194, 'point': 195, 'prototypes': 196, 'split': 197, 'splits': 198, 'such': 199, 'time': 200, 'tuned': 201, '22': 202, '98': 203, 'Cosine': 204, 'SJE': 205, 'cluster': 206, 'do': 207, 'drop': 208, 'example': 209, 'fφ': 210, 'generalization': 211, 'level': 212, 'lower': 213, 'produce': 214, 'result': 215, 'selffer': 216, 'state': 217, 'upper': 218, 'validation': 219, 'vector': 220, '60': 221, 'Few': 222, 'Omniglot': 223, 'Table': 224, 'Vinyals': 225, 'art': 226, 'boost': 227, 'color': 228, 'could': 229, 'embedded': 230, 'found': 231, 'how': 232, 'its': 233, 'just': 234, 'man': 235, 'method': 236, 'nearest': 237, 'small': 238, 'toward': 239, 'transferring': 240, 'without': 241, '2009': 242, '28': 243, '64': 244, '9': 245, 'B3B': 246, 'CUB': 247, 'LSTM': 248, 'Larochelle': 249, 'Performance': 250, 'Ravi': 251, 'Top': 252, 'after': 253, 'based': 254, 'computed': 255, 'convolutional': 256, 'distances': 257, 'divergence': 258, 'either': 259, 'find': 260, 'function': 261, 'g': 262, 'here': 263, 'however': 264, 'if': 265, 'labeled': 266, 'left': 267, 'less': 268, 'middle': 269, 'much': 270, 'overfitting': 271, 'parameters': 272, 'particular': 273, 'possible': 274, 'proposed': 275, 'representation': 276, 'separate': 277, 'softmax': 278, 'them': 279, 'treatment': 280, 'vs': 281, 'within': 282, 'would': 283, '10': 284, '16': 285, '2011': 286, '23': 287, 'A3B': 288, 'Euclid': 289, 'However': 290, 'J': 291, 'K': 292, 'Matching': 293, 'NC': 294, 'NETWORKS': 295, 'On': 296, 'Sk': 297, 'University': 298, 'Zero': 299, 'achieve': 300, 'adapted': 301, 'chosen': 302, 'containing': 303, 'copied': 304, 'cosine': 305, 'd': 306, 'deep': 307, 'diamonds': 308, 'dissimilar': 309, 'does': 310, 'due': 311, 'experiment': 312, 'extent': 313, 'further': 314, 'image': 315, 'instead': 316, 'make': 317, 'metric': 318, 'order': 319, 'output': 320, 'particularly': 321, 'procedure': 322, 'rather': 323, 'recent': 324, 'reported': 325, 'represent': 326, 'row': 327, 'seen': 328, 'smaller': 329, 'squared': 330, 'transferability': 331, 'transition': 332, 'were': 333, 'whether': 334, 'xi': 335, 'yi': 336, '1000': 337, '200': 338, '2012': 339, '30': 340, '500': 341, 'Classification': 342, 'D': 343, 'FCE': 344, 'Gabor': 345, 'GoogLeNet': 346, 'If': 347, 'It': 348, 'Jarrett': 349, 'Model': 350, 'NS': 351, 'To': 352, 'above': 353, 'almost': 354, 'another': 355, 'any': 356, 'approaches': 357, 'architecture': 358, 'cat': 359, 'contains': 360, 'corresponding': 361, 'create': 362, 'degree': 363, 'density': 364, 'divergences': 365, 'end': 366, 'episodic': 367, 'equivalent': 368, 'error': 369, 'expected': 370, 'feature': 371, 'following': 372, 'generalize': 373, 'i': 374, 'including': 375, 'input': 376, 'issues': 377, 'last': 378, 'margin': 379, 'miniImageNet': 380, 'must': 381, 'neighbor': 382, 'neurons': 383, 'original': 384, 'performed': 385, 'problem': 386, 'red': 387, 'right': 388, 'second': 389, 'several': 390, 'shown': 391, 'shows': 392, 'specificity': 393, 'statistic': 394, 'subset': 395, 'surprising': 396, 'vectors': 397, 'weight': 398, 'y': 399, '15': 400, '19': 401, '40': 402, '450k': 403, '54': 404, '?': 405, 'Acc': 406, 'As': 407, 'Caltech': 408, 'Dept': 409, 'Each': 410, 'MATCHING': 411, 'NCA': 412, 'Nc': 413, 'Networks': 414, 'Our': 415, 'RANDOMSAMPLE': 416, 'RM': 417, 'S': 418, 'Thus': 419, 'Training': 420, 'When': 421, 'adaptation': 422, 'applied': 423, 'attribute': 424, 'average': 425, 'baseA': 426, 'blobs': 427, 'blue': 428, 'bottom': 429, 'call': 430, 'choice': 431, 'choices': 432, 'circles': 433, 'curious': 434, 'define': 435, 'descent': 436, 'difficulties': 437, 'distribution': 438, 'during': 439, 'effects': 440, 'evidence': 441, 'exhibit': 442, 'exp': 443, 'family': 444, 'follow': 445, 'four': 446, 'fragile': 447, 'functions': 448, 'generality': 449, 'greatly': 450, 'improves': 451, 'initialized': 452, 'line': 453, 'literature': 454, 'makes': 455, 'match': 456, 'methods': 457, 'mixture': 458, 'models': 459, 'multiple': 460, 'normalization': 461, 'optimization': 462, 'out': 463, 'paper': 464, 'provide': 465, 'provided': 466, 'quantify': 467, 'rate': 468, 'regular': 469, 's': 470, 'scenario': 471, 'single': 472, 'size': 473, 'statistician': 474, 'straightforward': 475, 'those': 476, 'three': 477, 'versions': 478, 'via': 479, 'white': 480, '000': 481, '100': 482, '2014': 483, '25': 484, '26': 485, '31': 486, '43': 487, '56': 488, '600': 489, '84': 490, '93': 491, 'AlexNet': 492, 'Bengio': 493, 'BnA': 494, 'Computer': 495, 'DA': 496, 'DS': 497, 'Datasets': 498, 'Equation': 499, 'Features': 500, 'Fine': 501, 'KNN': 502, 'Layer': 503, 'Learning': 504, 'NQ': 505, 'Nets': 506, 'OURS': 507, 'One': 508, 'Random': 509, 'SGD': 510, 'Science': 511, 'Select': 512, 'Since': 513, 'WA1': 514, 'accuracies': 515, 'across': 516, 'analysis': 517, 'attributes': 518, 'averaged': 519, 'baselines': 520, 'being': 521, 'bias': 522, 'bird': 523, 'character': 524, 'choose': 525, 'classify': 526, 'clustering': 527, 'come': 528, 'common': 529, 'comparison': 530, 'computation': 531, 'compute': 532, 'dark': 533, 'demonstrate': 534, 'denotes': 535, 'depends': 536, 'details': 537, 'determined': 538, 'directly': 539, 'discussed': 540, 'distant': 541, 'dominate': 542, 'dominates': 543, 'drops': 544, 'eight': 545, 'enable': 546, 'enough': 547, 'entire': 548, 'equivalence': 549, 'estimation': 550, 'expect': 551, 'exponential': 552, 'extend': 553, 'felid': 554, 'final': 555, 'finding': 556, 'five': 557, 'fixed': 558, 'gradient': 559, 'hand': 560, 'having': 561, 'high': 562, 'hyperparameters': 563, 'idea': 564, 'identical': 565, 'improve': 566, 'increases': 567, 'individual': 568, 'inductive': 569, 'initializing': 570, 'iterations': 571, 'keep': 572, 'key': 573, 'knowledge': 574, 'label': 575, 'labels': 576, 'larger': 577, 'learns': 578, 'least': 579, 'length': 580, 'light': 581, 'like': 582, 'limited': 583, 'local': 584, 'making': 585, 'many': 586, 'material': 587, 'meaning': 588, 'means': 589, 'near': 590, 'need': 591, 'observed': 592, 'obtain': 593, 'optimal': 594, 'ours': 595, 'pairs': 596, 'phenomenon': 597, 'pooling': 598, 'posterior': 599, 'previously': 600, 'processing': 601, 'propose': 602, 'pφ': 603, 'quite': 604, 'related': 605, 'respect': 606, 'retraining': 607, 'rotations': 608, 'scenarios': 609, 'seems': 610, 'sense': 611, 'setting': 612, 'showing': 613, 'simpler': 614, 'simply': 615, 'since': 616, 'some': 617, 'splitting': 618, 'study': 619, 'subplot': 620, 'suggests': 621, 'supervised': 622, 'supplementary': 623, 'take': 624, 'thus': 625, 'tune': 626, 'unit': 627, 'untrained': 628, 'useful': 629, 'utilizes': 630, 'various': 631, 'very': 632, 'vk': 633, 'weighted': 634, 'while': 635, 'z': 636, 'φ': 637, '024': 638, '101': 639, '11': 640, '12': 641, '13': 642, '14': 643, '17': 644, '2013': 645, '27': 646, '312': 647, '42': 648, '44': 649, '449': 650, '49': 651, '551': 652, '58': 653, '62': 654, '66': 655, '70': 656, '71': 657, '78': 658, '79': 659, '80': 660, '95': 661, '99': 662, 'ALE': 663, 'Abstract': 664, 'Accuracy': 665, 'Adam': 666, 'Algorithm': 667, 'All': 668, 'An': 669, 'Another': 670, 'Birds': 671, 'Bottom': 672, 'Comparison': 673, 'Cornell': 674, 'DVk': 675, 'Dist': 676, 'Donahue': 677, 'Fei': 678, 'Finally': 679, 'First': 680, 'Further': 681, 'ILSVRC': 682, 'Introduction': 683, 'Krizhevsky': 684, 'LG': 685, 'LMNN': 686, 'Large': 687, 'Lines': 688, 'Mahalanobis': 689, 'Man': 690, 'Mensink': 691, 'Natural': 692, 'Note': 693, 'Of': 694, 'PROTOTYPICAL': 695, 'Proto': 696, 'Qk': 697, 'R': 698, 'Results': 699, 'Such': 700, 'That': 701, 'Their': 702, 'There': 703, 'They': 704, 'Toronto': 705, 'Transfer': 706, 'Tune': 707, 'Twitter': 708, 'UCSD': 709, 'WA2': 710, 'WA3': 711, 'WB1': 712, 'WB2': 713, 'WB3': 714, 'While': 715, 'X': 716, 'able': 717, 'about': 718, 'achieves': 719, 'affected': 720, 'aggregated': 721, 'algorithms': 722, 'allowed': 723, 'allows': 724, 'applying': 725, 'approximate': 726, 'approximately': 727, 'arXiv': 728, 'architectural': 729, 'assigned': 730, 'assigning': 731, 'axis': 732, 'bars': 733, 'batch': 734, 'become': 735, 'beneficial': 736, 'block': 737, 'c1': 738, 'c2': 739, 'c3': 740, 'called': 741, 'carefully': 742, 'cases': 743, 'categories': 744, 'chance': 745, 'characters': 746, 'chopped': 747, 'closer': 748, 'collectively': 749, 'com': 750, 'combination': 751, 'comparable': 752, 'compared': 753, 'computing': 754, 'conditional': 755, 'conducted': 756, 'confidence': 757, 'conjecture': 758, 'connects': 759, 'considering': 760, 'consist': 761, 'contain': 762, 'control': 763, 'controls': 764, 'copy': 765, 'created': 766, 'cs': 767, 'custom': 768, 'decisions': 769, 'decoupled': 770, 'decreases': 771, 'degradation': 772, 'demonstrated': 773, 'depend': 774, 'depending': 775, 'design': 776, 'designed': 777, 'detectors': 778, 'determine': 779, 'developed': 780, 'did': 781, 'differ': 782, 'discriminative': 783, 'distinct': 784, 'divided': 785, 'domain': 786, 'draws': 787, 'dϕ': 788, 'effectively': 789, 'effectiveness': 790, 'efficient': 791, 'elements': 792, 'embeddings': 793, 'empirical': 794, 'empirically': 795, 'entities': 796, 'exact': 797, 'expense': 798, 'experimental': 799, 'experimentally': 800, 'explanation': 801, 'extension': 802, 'extensions': 803, 'fact': 804, 'fully': 805, 'gap': 806, 'generalizing': 807, 'generated': 808, 'generative': 809, 'giving': 810, 'good': 811, 'group': 812, 'grows': 813, 'handle': 814, 'help': 815, 'helpful': 816, 'hexagons': 817, 'implementation': 818, 'improved': 819, 'improvement': 820, 'improving': 821, 'include': 822, 'increased': 823, 'indicate': 824, 'indicates': 825, 'indicating': 826, 'initialization': 827, 'insight': 828, 'intervals': 829, 'involves': 830, 'k0': 831, 'known': 832, 'lead': 833, 'learnable': 834, 'leopard': 835, 'levels': 836, 'lingers': 837, 'lion': 838, 'log': 839, 'longer': 840, 'main': 841, 'mapping': 842, 'max': 843, 'maximize': 844, 'might': 845, 'minimal': 846, 'modeling': 847, 'most': 848, 'naturally': 849, 'nature': 850, 'nearly': 851, 'negatively': 852, 'neighboring': 853, 'nets': 854, 'next': 855, 'nonlinearity': 856, 'normalized': 857, 'note': 858, 'noticing': 859, 'object': 860, 'obtained': 861, 'occur': 862, 'optimize': 863, 'ordinary': 864, 'originally': 865, 'overfit': 866, 'own': 867, 'partitioning': 868, 'performing': 869, 'place': 870, 'plus': 871, 'poorly': 872, 'pre': 873, 'predict': 874, 'predictions': 875, 'previous': 876, 'primarily': 877, 'process': 878, 'produces': 879, 'producing': 880, 'quantified': 881, 'questions': 882, 'quickly': 883, 'reason': 884, 'reference': 885, 'regime': 886, 'relatively': 887, 'relevant': 888, 'remaining': 889, 'report': 890, 'representations': 891, 'represents': 892, 'require': 893, 'retrained': 894, 'reveals': 895, 'rows': 896, 'scratch': 897, 'sections': 898, 'serve': 899, 'severely': 900, 'should': 901, 'showed': 902, 'significant': 903, 'similarity': 904, 'sizes': 905, 'slightly': 906, 'solution': 907, 'specialization': 908, 'species': 909, 'spherical': 910, 'standard': 911, 'statistically': 912, 'still': 913, 'strictly': 914, 'subsets': 915, 'substantial': 916, 'subtracting': 917, 'successful': 918, 'suggesting': 919, 'tend': 920, 'terms': 921, 'therefore': 922, 'third': 923, 'through': 924, 'tiger': 925, 'total': 926, 'transformation': 927, 'transformed': 928, 'true': 929, 'unsupervised': 930, 'up': 931, 'updates': 932, 'us': 933, 'uses': 934, 'utilizing': 935, 'version': 936, 'worse': 937, 'written': 938, 'x1': 939, 'xN': 940, 'y1': 941, 'yN': 942, 'yield': 943, 'yields': 944, 'θ': 945, '00': 946, '05': 947, '05175v2': 948, '09': 949, '0k': 950, '1024': 951, '1200': 952, '1411': 953, '1600': 954, '1623': 955, '167': 956, '1703': 957, '1792v1': 958, '1995': 959, '1The': 960, '2000': 961, '2004': 962, '2013a': 963, '2013b': 964, '2017': 965, '21': 966, '24': 967, '281': 968, '2Note': 969, '37': 970, '3AnA': 971, '4We': 972, '51': 973, '52': 974, '55': 975, '625': 976, '645': 977, '68': 978, '73': 979, '77': 980, '788': 981, '800': 982, '86': 983, '90': 984, '96': 985, '97': 986, 'A8B': 987, 'Additionally': 988, 'Aerospace': 989, 'Also': 990, 'Alternately': 991, 'Although': 992, 'Analysis': 993, 'At': 994, 'B8B': 995, 'BASELINE': 996, 'Because': 997, 'By': 998, 'CLUSTERING': 999, 'CU': 1000, 'Caffe': 1001, 'Can': 1002, 'Caruana': 1003, 'Challenge': 1004, 'Choices': 1005, 'Classes': 1006, 'Clune': 1007, 'Compared': 1008, 'Components': 1009, 'Compute': 1010, 'Conclusion': 1011, 'Conclusions': 1012, 'DNet': 1013, 'Dark': 1014, 'Degradation': 1015, 'Deng': 1016, 'Density': 1017, 'Design': 1018, 'Discussion': 1019, 'Dissimilar': 1020, 'Distance': 1021, 'Dk': 1022, 'Does': 1023, 'Early': 1024, 'Edwards': 1025, 'Egyptian': 1026, 'Engineering': 1027, 'Episode': 1028, 'Error': 1029, 'Estimation': 1030, 'Examples': 1031, 'Experimental': 1032, 'Experiments': 1033, 'Felidae': 1034, 'Fergus': 1035, 'Fisher': 1036, 'Fortunately': 1037, 'Fourth': 1038, 'GPU': 1039, 'Gaussian': 1040, 'Gaussians': 1041, 'Generality': 1042, 'Girshick': 1043, 'Given': 1044, 'Gradient': 1045, 'Here': 1046, 'Hinton': 1047, 'Hod': 1048, 'How': 1049, 'ILSVRC2012': 1050, 'Image': 1051, 'Indeed': 1052, 'Initial': 1053, 'Initialize': 1054, 'Input': 1055, 'Instead': 1056, 'Institute': 1057, 'Into': 1058, 'Intuitively': 1059, 'Jake': 1060, 'Jason': 1061, 'Jeff': 1062, 'Jia': 1063, 'Jun': 1064, 'KL': 1065, 'Kevin': 1066, 'LEARNER': 1067, 'Layers': 1068, 'Le': 1069, 'Learner': 1070, 'Lee': 1071, 'Left': 1072, 'Legendre': 1073, 'Light': 1074, 'Like': 1075, 'Linear': 1076, 'Lipson4': 1077, 'M': 1078, 'META': 1079, 'Many': 1080, 'Measured': 1081, 'Mechanical': 1082, 'Meta': 1083, 'Mixture': 1084, 'Modern': 1085, 'Modifying': 1086, 'Montreal': 1087, 'Moreover': 1088, 'Much': 1089, 'NEAREST': 1090, 'NEIGHBORS': 1091, 'NETS': 1092, 'NEURAL': 1093, 'Neighborhood': 1094, 'Neither': 1095, 'No': 1096, 'Notably': 1097, 'Notation': 1098, 'Nov': 1099, 'Numbered': 1100, 'Operations': 1101, 'Output': 1102, 'Overall': 1103, 'Overview': 1104, 'P': 1105, 'PROTO': 1106, 'Persian': 1107, 'Points': 1108, 'Previously': 1109, 'Prototype': 1110, 'Pseudocode': 1111, 'ReLU': 1112, 'Recent': 1113, 'Recognition': 1114, 'Reed': 1115, 'Reinterpretation': 1116, 'Related': 1117, 'Relative': 1118, 'Research': 1119, 'Richard': 1120, 'Right': 1121, 'Rippel': 1122, 'SAMPLE': 1123, 'STATISTICIAN': 1124, 'SVM': 1125, 'Salakhutdinov': 1126, 'Scale': 1127, 'Second': 1128, 'Separate': 1129, 'Sermanet': 1130, 'Setup': 1131, 'Shot': 1132, 'Siamese': 1133, 'Similar': 1134, 'Similarly': 1135, 'Snell': 1136, 'Specificity': 1137, 'Splitting': 1138, 'Storkey': 1139, 'Swersky': 1140, 'Thanks': 1141, 'Third': 1142, 'Transferability': 1143, 'Two': 1144, 'Unlike': 1145, 'Update': 1146, 'V': 1147, 'Vector': 1148, 'Visual': 1149, 'WA4': 1150, 'WA5': 1151, 'WA6': 1152, 'WA7': 1153, 'WA8': 1154, 'WB4': 1155, 'WB5': 1156, 'WB6': 1157, 'WB7': 1158, 'WB8': 1159, 'Weights': 1160, 'Where': 1161, 'Whereas': 1162, 'With': 1163, 'Work': 1164, 'Wyoming': 1165, 'Y': 1166, 'Yoshua': 1167, 'Yosinski': 1168, 'Zeiler': 1169, 'Zemel': 1170, 'ability': 1171, 'abs': 1172, 'absolute': 1173, 'accommodate': 1174, 'account': 1175, 'achieving': 1176, 'act': 1177, 'activations': 1178, 'adapt': 1179, 'addition': 1180, 'additional': 1181, 'addressing': 1182, 'advance': 1183, 'advantage': 1184, 'advantageous': 1185, 'affect': 1186, 'against': 1187, 'aggregate': 1188, 'aggregation': 1189, 'aim': 1190, 'algorithm': 1191, 'allowing': 1192, 'alone': 1193, 'alphabets': 1194, 'alternative': 1195, 'although': 1196, 'amount': 1197, 'analyze': 1198, 'answers': 1199, 'anything': 1200, 'anywhere': 1201, 'apparent': 1202, 'apparently': 1203, 'appealing': 1204, 'appear': 1205, 'appearance': 1206, 'applicable': 1207, 'applies': 1208, 'apply': 1209, 'arbitrary': 1210, 'around': 1211, 'ask': 1212, 'assess': 1213, 'assign': 1214, 'assignment': 1215, 'associated': 1216, 'assumption': 1217, 'assumptions': 1218, 'attack': 1219, 'attained': 1220, 'attains': 1221, 'attempt': 1222, 'attempts': 1223, 'attention': 1224, 'attributed': 1225, 'augmenting': 1226, 'author': 1227, 'autoencoder': 1228, 'available': 1229, 'b': 1230, 'back': 1231, 'backprop': 1232, 'backpropagate': 1233, 'batches': 1234, 'befits': 1235, 'beginning': 1236, 'behavior': 1237, 'believe': 1238, 'belonging': 1239, 'below': 1240, 'benchmark': 1241, 'benefits': 1242, 'best': 1243, 'beyond': 1244, 'bi': 1245, 'biological': 1246, 'blob': 1247, 'blocks': 1248, 'boosting': 1249, 'bug': 1250, 'cannot': 1251, 'carries': 1252, 'cats': 1253, 'cause': 1254, 'causes': 1255, 'change': 1256, 'characteristics': 1257, 'characterize': 1258, 'cheetah': 1259, 'choosing': 1260, 'ck0': 1261, 'clarity': 1262, 'classconditional': 1263, 'classified': 1264, 'classifiers': 1265, 'closely': 1266, 'clusters': 1267, 'coadaptation': 1268, 'code': 1269, 'collected': 1270, 'comes': 1271, 'commonly': 1272, 'comparing': 1273, 'competition': 1274, 'completely': 1275, 'complex': 1276, 'complicated': 1277, 'component': 1278, 'composed': 1279, 'composition': 1280, 'comprised': 1281, 'comprises': 1282, 'computationally': 1283, 'concise': 1284, 'conclusions': 1285, 'configuration': 1286, 'connecting': 1287, 'connections': 1288, 'consideration': 1289, 'considered': 1290, 'constant': 1291, 'constrain': 1292, 'construct': 1293, 'constructed': 1294, 'constructing': 1295, 'contained': 1296, 'continuous': 1297, 'contrast': 1298, 'contributions': 1299, 'converge': 1300, 'convergence': 1301, 'convex': 1302, 'convolution': 1303, 'core': 1304, 'cost': 1305, 'couple': 1306, 'course': 1307, 'creates': 1308, 'crop': 1309, 'crops': 1310, 'cumulant': 1311, 'currently': 1312, 'cut': 1313, 'cvpr2016': 1314, 'datapoint': 1315, 'days': 1316, 'deal': 1317, 'decay': 1318, 'decline': 1319, 'decoupling': 1320, 'deeper': 1321, 'defined': 1322, 'definition': 1323, 'definitions': 1324, 'degrees': 1325, 'densities': 1326, 'derived': 1327, 'describes': 1328, 'description': 1329, 'descriptions': 1330, 'despite': 1331, 'differences': 1332, 'differentiable': 1333, 'differs': 1334, 'difficult': 1335, 'difficulty': 1336, 'dimension': 1337, 'diminishes': 1338, 'direction': 1339, 'directional': 1340, 'directions': 1341, 'distinction': 1342, 'distributions': 1343, 'divide': 1344, 'document': 1345, 'doesn': 1346, 'dogs': 1347, 'domains': 1348, 'done': 1349, 'dot': 1350, 'dotted': 1351, 'downloaded': 1352, 'draw': 1353, 'drawn': 1354, 'dynamically': 1355, 'dynamics': 1356, 'easier': 1357, 'effective': 1358, 'ellipsoidal': 1359, 'else': 1360, 'elsewhere': 1361, 'embed': 1362, 'emphasize': 1363, 'encode': 1364, 'encoder': 1365, 'encoding': 1366, 'encourages': 1367, 'environment': 1368, 'epochs': 1369, 'equally': 1370, 'errors': 1371, 'etc': 1372, 'eventually': 1373, 'every': 1374, 'excellent': 1375, 'except': 1376, 'exists': 1377, 'expensive': 1378, 'explorations': 1379, 'extends': 1380, 'extensible': 1381, 'extensive': 1382, 'extensively': 1383, 'extra': 1384, 'extracted': 1385, 'extremely': 1386, 'f': 1387, 'failed': 1388, 'fairly': 1389, 'faithful': 1390, 'falls': 1391, 'far': 1392, 'feather': 1393, 'felids': 1394, 'fewer': 1395, 'figure': 1396, 'files': 1397, 'filter': 1398, 'finetuned': 1399, 'fitted': 1400, 'fix': 1401, 'flexibility': 1402, 'flipped': 1403, 'focus': 1404, 'forces': 1405, 'form': 1406, 'formed': 1407, 'forms': 1408, 'formulate': 1409, 'fragilely': 1410, 'future': 1411, 'gaining': 1412, 'gains': 1413, 'generalizes': 1414, 'generally': 1415, 'get': 1416, 'getting': 1417, 'github': 1418, 'goal': 1419, 'grained': 1420, 'grayscale': 1421, 'greater': 1422, 'groups': 1423, 'gϑ': 1424, 'halves': 1425, 'handwritten': 1426, 'hard': 1427, 'held': 1428, 'helps': 1429, 'hierarchy': 1430, 'hinge': 1431, 'hold': 1432, 'holds': 1433, 'horizontally': 1434, 'http': 1435, 'https': 1436, 'human': 1437, 'humans': 1438, 'hyperparameter': 1439, 'hypothesis': 1440, 'hypothesize': 1441, 'ii': 1442, 'illustration': 1443, 'image2': 1444, 'importance': 1445, 'important': 1446, 'imposes': 1447, 'improvements': 1448, 'incorporate': 1449, 'incorporated': 1450, 'increase': 1451, 'indeed': 1452, 'independent': 1453, 'indices': 1454, 'inference': 1455, 'inferred': 1456, 'information': 1457, 'informative': 1458, 'initial': 1459, 'initialize': 1460, 'interact': 1461, 'interactions': 1462, 'interest': 1463, 'interested': 1464, 'interpretation': 1465, 'interpretations': 1466, 'interpreted': 1467, 'introduced': 1468, 'intuitive': 1469, 'involving': 1470, 'issue': 1471, 'itself': 1472, 'jaguar': 1473, 'join': 1474, 'jointly': 1475, 'justify': 1476, 'keeping': 1477, 'know': 1478, 'kz': 1479, 'latter': 1480, 'learner': 1481, 'leave': 1482, 'likewise': 1483, 'linearity': 1484, 'linearly': 1485, 'lines': 1486, 'linger': 1487, 'little': 1488, 'locked': 1489, 'logX': 1490, 'lost': 1491, 'low': 1492, 'lynx': 1493, 'm': 1494, 'manner': 1495, 'marker': 1496, 'measure': 1497, 'mechanism': 1498, 'mentioned': 1499, 'mimic': 1500, 'mini': 1501, 'minimizing': 1502, 'mirrors': 1503, 'mistakes': 1504, 'modal': 1505, 'modern': 1506, 'modifying': 1507, 'monitoring': 1508, 'mountain': 1509, 'multi': 1510, 'multimodal': 1511, 'multiples': 1512, 'naive': 1513, 'namely': 1514, 'negative': 1515, 'neighborhood': 1516, 'net': 1517, 'never': 1518, 'no': 1519, 'nor': 1520, 'normalize': 1521, 'notation': 1522, 'noted': 1523, 'notions': 1524, 'novel': 1525, 'objective': 1526, 'objectives': 1527, 'observation': 1528, 'obtaining': 1529, 'obviates': 1530, 'occurrence': 1531, 'occurs': 1532, 'off': 1533, 'often': 1534, 'old': 1535, 'opposed': 1536, 'optimized': 1537, 'orange': 1538, 'ordering': 1539, 'outline': 1540, 'outperforms': 1541, 'overlapping': 1542, 'parameter': 1543, 'parameterization': 1544, 'parent': 1545, 'part': 1546, 'patterns': 1547, 'perfectly': 1548, 'performances': 1549, 'performs': 1550, 'permissible': 1551, 'persists': 1552, 'phase': 1553, 'plausible': 1554, 'plots': 1555, 'plotted': 1556, 'post': 1557, 'powerful': 1558, 'prediction': 1559, 'preliminary': 1560, 'preparing': 1561, 'presents': 1562, 'prevents': 1563, 'probabilities': 1564, 'probability': 1565, 'proceeds': 1566, 'produced': 1567, 'progress': 1568, 'promising': 1569, 'pronounced': 1570, 'publication': 1571, 'pψ': 1572, 'quantifying': 1573, 'question': 1574, 'raises': 1575, 'ranges': 1576, 'rapidly': 1577, 'rates': 1578, 'raw': 1579, 're': 1580, 'recovers': 1581, 'rectangles': 1582, 'rectification': 1583, 'rediscovered': 1584, 'reedscot': 1585, 'refer': 1586, 'reflect': 1587, 'regardless': 1588, 'regularization': 1589, 'regularize': 1590, 'reinitialize': 1591, 'relate': 1592, 'relates': 1593, 'relative': 1594, 'relearn': 1595, 'relearned': 1596, 'relearning': 1597, 'released': 1598, 'relies': 1599, 'relu': 1600, 'rely': 1601, 'remainder': 1602, 'repeated': 1603, 'replacement': 1604, 'representative': 1605, 'representatives': 1606, 'represented': 1607, 'reproduce': 1608, 'repurpose': 1609, 'required': 1610, 'requires': 1611, 'requiring': 1612, 'researchers': 1613, 'resemble': 1614, 'resembles': 1615, 'resizing': 1616, 'retrain': 1617, 'rigorous': 1618, 'risk': 1619, 'run': 1620, 'runs': 1621, 'sample': 1622, 'sampled': 1623, 'save': 1624, 'say': 1625, 'schedule': 1626, 'scheme': 1627, 'secondary': 1628, 'selecting': 1629, 'semantically': 1630, 'sets': 1631, 'settings': 1632, 'setup': 1633, 'seven': 1634, 'shape': 1635, 'shared': 1636, 'shifted': 1637, 'shown3': 1638, 'significantly': 1639, 'simplicity': 1640, 'simplify': 1641, 'situation': 1642, 'slight': 1643, 'snow': 1644, 'software': 1645, 'somewhere': 1646, 'sophisticated': 1647, 'sparse': 1648, 'special': 1649, 'specifically': 1650, 'specifies': 1651, 'speed': 1652, 'spread': 1653, 'step': 1654, 'stopping': 1655, 'stops': 1656, 'store': 1657, 'strikingly': 1658, 'studied': 1659, 'studies': 1660, 'studying': 1661, 'subject': 1662, 'subsampling': 1663, 'substantially': 1664, 'successfully': 1665, 'successive': 1666, 'suddenly': 1667, 'suffers': 1668, 'suitability': 1669, 'suitable': 1670, 'suited': 1671, 'summarize': 1672, 'summarizes': 1673, 'summary': 1674, 'suspicion': 1675, 'systems': 1676, 't': 1677, 'tabby': 1678, 'tackle': 1679, 'taken': 1680, 'takes': 1681, 'taking': 1682, 'tanh': 1683, 'technique': 1684, 'techniques': 1685, 'tell': 1686, 'tendency': 1687, 'term': 1688, 'tested': 1689, 'tests': 1690, 'text': 1691, 'theart': 1692, 'themselves': 1693, 'thereby': 1694, 'things': 1695, 'though': 1696, 'tool': 1697, 'trains': 1698, 'transferable': 1699, 'treatments': 1700, 'tweak': 1701, 'tweaking': 1702, 'type': 1703, 'types': 1704, 'under': 1705, 'underlying': 1706, 'uniformly': 1707, 'uniquely': 1708, 'unlabeled': 1709, 'until': 1710, 'upon': 1711, 'usual': 1712, 'usually': 1713, 'utilize': 1714, 'v1': 1715, 'v2': 1716, 'v3': 1717, 'variance': 1718, 'variant': 1719, 'variants': 1720, 'variational': 1721, 'vast': 1722, 'versa': 1723, 'versus': 1724, 'vertical': 1725, 'vice': 1726, 'viewed': 1727, 'visual': 1728, 'vital': 1729, 'want': 1730, 'ways': 1731, 'whereas': 1732, 'whole': 1733, 'whose': 1734, 'wide': 1735, 'won': 1736, 'xk': 1737, 'yosinski': 1738, 'µ': 1739, 'θk': 1740, 'ψ': 1741, 'ϕ': 1742}\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 1, Loss: 7.287820339202881\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 2, Loss: 7.128766059875488\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 3, Loss: 6.983467102050781\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 4, Loss: 6.835550785064697\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 5, Loss: 6.691709995269775\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 1, Loss: 7.748692035675049\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 2, Loss: 7.5821919441223145\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 3, Loss: 7.4232611656188965\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 4, Loss: 7.259585857391357\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 5, Loss: 7.110004901885986\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 1, Loss: 7.563697338104248\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 2, Loss: 7.401281833648682\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 3, Loss: 7.235201358795166\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 4, Loss: 7.081790447235107\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 5, Loss: 6.9188008308410645\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 1, Loss: 7.607326030731201\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 2, Loss: 7.4694132804870605\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 3, Loss: 7.329788684844971\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 4, Loss: 7.192044258117676\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 5, Loss: 7.063083171844482\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 1, Loss: 7.559328079223633\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 2, Loss: 7.395040035247803\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 3, Loss: 7.239288330078125\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 4, Loss: 7.082148551940918\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Epoch 5, Loss: 6.9263014793396\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # Ensure special tokens are included\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def combined_loss(output, target, model, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)  \n",
    "    regularization_loss = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if isinstance(param, nn.Parameter):  \n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1\n",
    "\n",
    "    return task_loss + regularization_loss\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len= 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)[:self.max_seq_len]         \n",
    "        labels = encoded_text.copy()  # Copy encoded text for labels\n",
    "\n",
    "        # Apply dynamic masking\n",
    "        masked_indices = np.random.rand(len(encoded_text)) < 0.15\n",
    "        for i in range(len(encoded_text)):\n",
    "            if masked_indices[i]:\n",
    "                encoded_text[i] = self.tokenizer.vocab[\"[MASK]\"]\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "        labels += [-100] * padding_length  # Use -100 for padding positions\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Encoding Transformer Code\n",
    "\n",
    "class AdaptiveDropoutLayer(nn.Module):\n",
    "    def __init__(self, init_dropout_rate=0.1):\n",
    "        super(AdaptiveDropoutLayer, self).__init__()\n",
    "        # Use logit transformation for stability\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.log_alpha)\n",
    "        # Convert p from a tensor to a float\n",
    "        p_value = p.item()  # This extracts the scalar value as a Python float\n",
    "        return nn.functional.dropout(x, p=p_value, training=self.training)\n",
    "\n",
    "\n",
    "class AdaptiveWeightDecayOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, init_l2_strength=0.01):\n",
    "        super().__init__(params, {'lr': lr})\n",
    "        self.log_l2_strength = nn.Parameter(torch.tensor(math.log(init_l2_strength)).float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = torch.exp(self.log_l2_strength)  \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay) \n",
    "                p.update(d_p, group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Create positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension (B x T x C)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Tensor of shape [Batch Size, Sequence Length, Embedding Dimension]\n",
    "        # Adjust positional encoding to match the input size and device\n",
    "        pe = self.pe[:, :x.size(1)]\n",
    "        # Assuming x is on the correct device, pe will be automatically aligned to the same device\n",
    "        return pe\n",
    "\n",
    "\n",
    "class MultiHeadLinformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, k=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k if k is not None else embed_dim // num_heads  # Projection dimension per head\n",
    "\n",
    "        self.key_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.value_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        # Project keys and values\n",
    "        keys = self.key_projections(query)\n",
    "        values = self.value_projections(query)\n",
    "        \n",
    "        # Reshape into [batch_size, num_heads, seq_len, k]\n",
    "        keys = keys.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        values = values.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention (scaled dot-product attention)\n",
    "        # Scaling by the square root of the depth of the key vectors to prevent large values in the dot product\n",
    "        # which could push the softmax function into regions where it has extremely small gradients\n",
    "        keys = keys / (self.k ** 0.5)\n",
    "        attention_scores = torch.softmax(torch.matmul(keys, values.transpose(-2, -1)), dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention_scores, values)\n",
    "        \n",
    "        # Concatenate heads and project back to original embedding dimension\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, self.num_heads * self.k)\n",
    "        out = self.out_projection(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class AdaptiveEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab,  vocab_size, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "        super(AdaptiveEmbeddingLayer, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab_size\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.large_embed_dim = large_embed_dim\n",
    "        self.small_embed_dim = small_embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.split_vocab(vocab, freq_threshold)  \n",
    "\n",
    "        self.frequent_embeddings = nn.Embedding(num_embeddings=len(self.frequent_vocab), embedding_dim=large_embed_dim)\n",
    "        self.infrequent_embeddings = nn.Embedding(num_embeddings=len(self.infrequent_vocab), embedding_dim=small_embed_dim)\n",
    "        self.infrequent_projection = nn.Linear(small_embed_dim, large_embed_dim)\n",
    "        self.positional_embeddings = PositionalEncoding(large_embed_dim, max_seq_len)\n",
    "\n",
    "\n",
    "    def split_vocab(self, vocab, freq_threshold):\n",
    "        token_counts = [(token, count) for token, count in vocab.items()]\n",
    "        token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "        \n",
    "        self.frequent_vocab = dict(token_counts[:split_point])\n",
    "        self.infrequent_vocab = dict(token_counts[split_point:])\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        device = token_ids.device\n",
    "        seq_len = token_ids.size(1)\n",
    "        batch_size = token_ids.size(0)  # Obtain batch size from token_ids tensor\n",
    "\n",
    "        # Initialize embeddings tensor\n",
    "        embeddings = torch.zeros(token_ids.shape[0], seq_len, self.large_embed_dim, device=device)\n",
    "\n",
    "        # Map token_ids to indices for frequent and infrequent vocab\n",
    "        frequent_indices = torch.zeros_like(token_ids)\n",
    "        infrequent_indices = torch.zeros_like(token_ids)\n",
    "        \n",
    "        for token_id, index in self.vocab.items():\n",
    "            mask = token_ids == token_id\n",
    "            if token_id in self.frequent_vocab:\n",
    "                # Map to index in frequent_vocab\n",
    "                frequent_indices[mask] = self.frequent_vocab[token_id]\n",
    "            elif token_id in self.infrequent_vocab:\n",
    "                # Map to index in infrequent_vocab\n",
    "                infrequent_indices[mask] = self.infrequent_vocab[token_id]\n",
    "\n",
    "        # Create masks for frequent and infrequent tokens\n",
    "        frequent_mask = frequent_indices > 0\n",
    "        infrequent_mask = infrequent_indices > 0\n",
    "\n",
    "        # Embed frequent tokens\n",
    "        if frequent_mask.any():\n",
    "            frequent_embeddings = self.frequent_embeddings(frequent_indices[frequent_mask])\n",
    "            embeddings[frequent_mask] = frequent_embeddings\n",
    "\n",
    "        # Embed and project infrequent tokens\n",
    "        if infrequent_mask.any():\n",
    "            infrequent_embeddings = self.infrequent_embeddings(infrequent_indices[infrequent_mask])\n",
    "            infrequent_embeddings_projected = self.infrequent_projection(infrequent_embeddings)\n",
    "            embeddings[infrequent_mask] = infrequent_embeddings_projected\n",
    "\n",
    "        # Apply positional embeddings\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        position_embeddings = self.positional_embeddings(position_ids)  # Generate for seq_len\n",
    "\n",
    "        # Ensure positional embeddings are broadcastable to the embeddings tensor\n",
    "        # This step may not be necessary if your positional embeddings are already correctly shaped\n",
    "        if position_embeddings.size(0) != batch_size:\n",
    "            position_embeddings = position_embeddings.expand(batch_size, -1, -1)\n",
    "\n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"Positional embeddings shape: {position_embeddings.shape}\")\n",
    "        embeddings += position_embeddings\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadLinformerAttention(embed_dim=d_model, num_heads=nhead)\n",
    "        self.dropout1 = AdaptiveDropoutLayer()  # Use AdaptiveDropoutLayer\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            AdaptiveDropoutLayer(),  # Use AdaptiveDropoutLayer here as well\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.dropout2 = AdaptiveDropoutLayer()  # And here\n",
    " \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2 = self.norm1(src)\n",
    "        attn_output = self.attn(src2)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout2(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab, vocab_size, embedding_dim, max_seq_len, nhead, dim_feedforward, \n",
    "                 freq_threshold, smaller_embed_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab,\n",
    "            vocab_size=vocab_size, \n",
    "            freq_threshold=freq_threshold,  \n",
    "            large_embed_dim=embedding_dim,       \n",
    "            small_embed_dim=smaller_embed_dim,   \n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.encoder = TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward)\n",
    "        self.pooler = Pooler(embedding_dim)  # Retain Pooler for sentence-level representation\n",
    "        # Add an output projection layer for token-level predictions\n",
    "        self.output_projection = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        encoded = self.encoder(embedded, src_mask=attention_mask)\n",
    "        # Get pooled output for sentence-level tasks\n",
    "        pooled_output = self.pooler(encoded)\n",
    "        # Project encoded output to vocabulary size for token-level predictions\n",
    "        logits = self.output_projection(encoded)\n",
    "        return logits, pooled_output\n",
    "\n",
    "# Tokenizing Code\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        node = self.root\n",
    "        subword_ids = []\n",
    "        for char in token:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "                if node.token_id is not None:\n",
    "                    subword_ids.append(node.token_id)\n",
    "                    break  # Assuming one token maps to one subword for simplicity\n",
    "            else:\n",
    "                break  # No further subword match found\n",
    "        if not subword_ids:  # If no subword was found\n",
    "            subword_ids.append(self.unk_token_id if hasattr(self, 'unk_token_id') else 1) \n",
    "        return subword_ids\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(best)  # Track most frequent pair in each iteration\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.split()  # Assuming simple word splitting\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None \n",
    "        self.failure_pops = None\n",
    "\n",
    "    def train(self, corpus):\n",
    "        vocabulary = self._build_vocabulary(corpus)  # Build the word list\n",
    "        self.trie, self.failure_links, self.failure_pops = self._precompute(vocabulary)\n",
    "\n",
    "    def _encode(self, word):\n",
    "        subword_ids = []\n",
    "        current_node = self.trie.root\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "            if char in current_node.children:\n",
    "                current_node = current_node.children[char]\n",
    "                if current_node.token_id:\n",
    "                    subword_ids.append(current_node.token_id)\n",
    "            else:  # No direct match. Implement failure logic\n",
    "                while True:\n",
    "                    if current_node.failure_link is not None:\n",
    "                        current_node = current_node.failure_link\n",
    "\n",
    "                        # Check for pops to skip further backtracking\n",
    "                        for _ in range(current_node.failure_pop):\n",
    "                            current_node = current_node.failure_link\n",
    "\n",
    "                        if char in current_node.children:\n",
    "                            current_node = current_node.children[char]\n",
    "                            if current_node.token_id:\n",
    "                                subword_ids.append(current_node.token_id)\n",
    "                            break  # Successful subword transition \n",
    "\n",
    "                    else:  # No failure link, we're back at root level\n",
    "                        # Handle situation based on your application\n",
    "                        # e.g., append an unknown token \n",
    "                        subword_ids.append(self.unk_token_id)  \n",
    "                        break  \n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        if model_type == \"unigram\":\n",
    "            tokens = self._unigram_tokenize(corpus)  #  Hypothetical tokenize function\n",
    "            vocabulary = self._build_unigram_vocab(tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        # Count token frequencies\n",
    "        token_freqs = collections.Counter(tokens)\n",
    "        # Select the most frequent tokens up to vocab_size\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(token_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction\n",
    "        self.trie = Trie()  # Assuming you have a Trie class as well\n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token)  \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "    def _unigram_tokenize(self, corpus):\n",
    "        \"\"\"\n",
    "        Tokenizes the given corpus into unigram tokens, checking against the built vocabulary.\n",
    "        Tokens not found in the vocabulary are treated as unknowns.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): The text corpus to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list of str: A list of tokens extracted from the corpus, adjusted to fit the vocabulary.\n",
    "        \"\"\"\n",
    "        # Initial tokenization based on word boundaries and punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', corpus)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, actual_vocab_size):\n",
    "        self.vocab = vocab\n",
    "        self.actual_vocab_size = actual_vocab_size\n",
    "        self.unk_token_id = self.vocab.get(\"[UNK]\", 1)  # Get ID of [UNK] \n",
    "        self.max_subword_length = max(len(token) for token in vocab.keys())\n",
    "        self.pattern = re.compile(r'\\b\\w+\\b|[\\s\\.,!?;]')\n",
    "\n",
    "        # Build the Trie\n",
    "        self.trie = Trie()\n",
    "        for token, token_id in vocab.items():\n",
    "            self.trie.insert(token, token_id)\n",
    "\n",
    "    def _find_subwords(self, word):\n",
    "        # 1. Trie Lookup \n",
    "        subword_ids = self.trie.find_subwords(word)\n",
    "\n",
    "        # 2. Fallback to Original Logic \n",
    "        if len(subword_ids) == 1 and subword_ids[0] == self.unk_token_id:  \n",
    "            subwords = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                for j in range(self.max_subword_length, 0, -1):\n",
    "                    subword = word[i:i+j]\n",
    "                    if subword in self.vocab:\n",
    "                        subwords.append(self.vocab[subword])\n",
    "                        i += j\n",
    "                        break \n",
    "                else: \n",
    "                    subwords.append(self.vocab[\"[UNK]\"])\n",
    "                    i += 1\n",
    "            subword_ids = subwords  # Replace with token IDs\n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        token_ids = [self.vocab.get(\"[CLS]\", 1)]  # Use [CLS] token or [UNK] if not found\n",
    "        tokens = self.pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            token_id = self.vocab.get(token, self.vocab.get(\"[UNK]\", 1))  # Fallback to [UNK] if token is not found\n",
    "            token_ids.append(token_id)\n",
    "        token_ids.append(self.vocab.get(\"[SEP]\", 1))  # Use [SEP] token or [UNK] if not found\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "# Training Code\n",
    "\n",
    "# Load corpus and build vocab\n",
    "corpus = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "tokens = tokenize(corpus)\n",
    "vocab = build_vocab(tokens)\n",
    "actual_vocab_size = len(vocab)  # This includes [PAD], [UNK], [CLS], [SEP], [MASK]\n",
    "\n",
    "print(f\"Actual vocabulary size (including special tokens): {actual_vocab_size}\")\n",
    "\n",
    "tokenizer = Tokenizer(vocab=vocab, actual_vocab_size=actual_vocab_size)\n",
    "\n",
    "# Data Loading (Illustrative)\n",
    "train_texts = [corpus]  # Treat your whole sample as one \"document\"\n",
    "train_dataset = TextDataset(train_texts, tokenizer, max_seq_len=512)\n",
    "\n",
    "print(\"Tokenizer unk_token_id:\", tokenizer.unk_token_id) \n",
    "print(\"Tokenizer Vocabulary:\", tokenizer.vocab)\n",
    "\n",
    "\n",
    "\n",
    "device='cpu'\n",
    "freq_threshold_values = [10, 50, 100, 200, 500]  \n",
    "best_validation_accuracy = 0.0 \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for freq_threshold in freq_threshold_values:\n",
    "\n",
    "    # Model instantiation and training setup\n",
    "    model = TransformerModel(\n",
    "        vocab = vocab,\n",
    "        vocab_size=actual_vocab_size,     \n",
    "        embedding_dim=128,\n",
    "        max_seq_len=512,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        freq_threshold=freq_threshold,  # frequency threshold for splitting vocab\n",
    "        smaller_embed_dim=64\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4) \n",
    "    meta_optimizer = AdaptiveWeightDecayOptimizer(model.parameters(), lr=1e-5) \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    meta_update_freq = 5\n",
    "\n",
    "    # Training loop adjusted for the updated model architecture\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)  # Assuming labels are of shape [batch_size, sequence_length]\n",
    "\n",
    "            # Forward pass, model now returns logits and pooled_output\n",
    "            logits, pooled_output = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Correctly reshape logits to match the labels' shape\n",
    "            # Change from [1, 512, vocab_size] to [512, vocab_size] to align with labels\n",
    "            logits = logits.view(-1, logits.size(-1))  # Reshape logits for loss calculation\n",
    "            \n",
    "            labels = labels.view(-1)  # Ensure labels are a flat vector\n",
    "\n",
    "            # Calculate loss using logits for token-level predictions\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Meta-update occasionally\n",
    "            if (i + 1) % meta_update_freq == 0:\n",
    "                meta_optimizer.zero_grad()\n",
    "                # Recalculate or reuse the loss for the meta-update\n",
    "                meta_loss = combined_loss(logits.detach(), labels.detach(), model)\n",
    "                meta_loss.backward()\n",
    "                meta_optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "# Save model state\n",
    "model_path = \"D:\\\\EXPERT_WEIGHTS\\\\encoding_transformer.bin\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save tokenizer using pickle for simplicity\n",
    "import pickle\n",
    "tokenizer_path = \"D:\\\\EXPERT_WEIGHTS\\\\tokenizer.pkl\"\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "\n",
    "# Assuming TransformerModel and Tokenizer classes are defined in the scope\n",
    "\n",
    "# Load the model\n",
    "model_loaded = TransformerModel(\n",
    "    vocab=vocab,  # Ensure `vocab` is loaded or defined in the scope\n",
    "    vocab_size=actual_vocab_size,\n",
    "    embedding_dim=128,\n",
    "    max_seq_len=512,\n",
    "    nhead=8,\n",
    "    dim_feedforward=2048,\n",
    "    freq_threshold=freq_threshold,  # Define or load `freq_threshold` as appropriate\n",
    "    smaller_embed_dim=64\n",
    ")\n",
    "model_loaded.load_state_dict(torch.load(\"D:\\\\EXPERT_WEIGHTS\\\\encoding_transformer.bin\"))\n",
    "model_loaded.eval()  # Set to evaluation mode\n",
    "\n",
    "# Load the tokenizer\n",
    "with open(\"D:\\\\EXPERT_WEIGHTS\\\\tokenizer.pkl\", 'rb') as f:\n",
    "    tokenizer_loaded = pickle.load(f)\n",
    "\n",
    "\n",
    "# Example text\n",
    "text = \"Here is some text to encode\"\n",
    "\n",
    "# Tokenize the input\n",
    "encoded_input = tokenizer_loaded.encode(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "# Predict with your model\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "    output = model_loaded(encoded_input)\n",
    "\n",
    "# Process the output as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SP and BPE Hybrid Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual vocabulary size (including special tokens): 1743\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 681\u001b[0m\n\u001b[0;32m    679\u001b[0m bpe_model \u001b[38;5;241m=\u001b[39m BPE()\n\u001b[0;32m    680\u001b[0m bpe_model\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m vocab\n\u001b[1;32m--> 681\u001b[0m \u001b[43mbpe_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train BPE with your corpus\u001b[39;00m\n\u001b[0;32m    682\u001b[0m sentence_piece_model \u001b[38;5;241m=\u001b[39m SentencePiece()\n\u001b[0;32m    683\u001b[0m hybrid_tokenizer \u001b[38;5;241m=\u001b[39m HybridTokenizer(bpe_model, sentence_piece_model, vocab)\n",
      "Cell \u001b[1;32mIn[10], line 423\u001b[0m, in \u001b[0;36mBPE.train\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_merges):\n\u001b[0;32m    422\u001b[0m     pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_stats(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m--> 423\u001b[0m     best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_vocab(best, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28mprint\u001b[39m(best)\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # Ensure special tokens are included\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def combined_loss(output, target, model, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)  \n",
    "    regularization_loss = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if isinstance(param, nn.Parameter):  \n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1\n",
    "\n",
    "    return task_loss + regularization_loss\n",
    "\n",
    "# Encoding Transformer Code\n",
    "\n",
    "class AdaptiveDropoutLayer(nn.Module):\n",
    "    def __init__(self, init_dropout_rate=0.1):\n",
    "        super(AdaptiveDropoutLayer, self).__init__()\n",
    "        # Use logit transformation for stability\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.log_alpha)\n",
    "        # Convert p from a tensor to a float\n",
    "        p_value = p.item()  # This extracts the scalar value as a Python float\n",
    "        return nn.functional.dropout(x, p=p_value, training=self.training)\n",
    "\n",
    "\n",
    "class AdaptiveWeightDecayOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, init_l2_strength=0.01):\n",
    "        super().__init__(params, {'lr': lr})\n",
    "        self.log_l2_strength = nn.Parameter(torch.tensor(math.log(init_l2_strength)).float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = torch.exp(self.log_l2_strength)  \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay) \n",
    "                p.update(d_p, group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Create positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension (B x T x C)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Tensor of shape [Batch Size, Sequence Length, Embedding Dimension]\n",
    "        # Adjust positional encoding to match the input size and device\n",
    "        pe = self.pe[:, :x.size(1)]\n",
    "        # Assuming x is on the correct device, pe will be automatically aligned to the same device\n",
    "        return pe\n",
    "\n",
    "\n",
    "class MultiHeadLinformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, k=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k if k is not None else embed_dim // num_heads  # Projection dimension per head\n",
    "\n",
    "        self.key_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.value_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        # Project keys and values\n",
    "        keys = self.key_projections(query)\n",
    "        values = self.value_projections(query)\n",
    "        \n",
    "        # Reshape into [batch_size, num_heads, seq_len, k]\n",
    "        keys = keys.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        values = values.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention (scaled dot-product attention)\n",
    "        # Scaling by the square root of the depth of the key vectors to prevent large values in the dot product\n",
    "        # which could push the softmax function into regions where it has extremely small gradients\n",
    "        keys = keys / (self.k ** 0.5)\n",
    "        attention_scores = torch.softmax(torch.matmul(keys, values.transpose(-2, -1)), dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention_scores, values)\n",
    "        \n",
    "        # Concatenate heads and project back to original embedding dimension\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, self.num_heads * self.k)\n",
    "        out = self.out_projection(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AdaptiveEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab,  vocab_size, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "        super(AdaptiveEmbeddingLayer, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab_size\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.large_embed_dim = large_embed_dim\n",
    "        self.small_embed_dim = small_embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.split_vocab(vocab, freq_threshold)  \n",
    "\n",
    "        self.frequent_embeddings = nn.Embedding(num_embeddings=len(self.frequent_vocab), embedding_dim=large_embed_dim)\n",
    "        self.infrequent_embeddings = nn.Embedding(num_embeddings=len(self.infrequent_vocab), embedding_dim=small_embed_dim)\n",
    "        self.infrequent_projection = nn.Linear(small_embed_dim, large_embed_dim)\n",
    "        self.positional_embeddings = PositionalEncoding(large_embed_dim, max_seq_len)\n",
    "\n",
    "\n",
    "    def split_vocab(self, vocab, freq_threshold):\n",
    "        token_counts = [(token, count) for token, count in vocab.items()]\n",
    "        token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "        \n",
    "        self.frequent_vocab = dict(token_counts[:split_point])\n",
    "        self.infrequent_vocab = dict(token_counts[split_point:])\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        device = token_ids.device\n",
    "        seq_len = token_ids.size(1)\n",
    "        batch_size = token_ids.size(0)  # Obtain batch size from token_ids tensor\n",
    "\n",
    "        # Initialize embeddings tensor\n",
    "        embeddings = torch.zeros(token_ids.shape[0], seq_len, self.large_embed_dim, device=device)\n",
    "\n",
    "        # Map token_ids to indices for frequent and infrequent vocab\n",
    "        frequent_indices = torch.zeros_like(token_ids)\n",
    "        infrequent_indices = torch.zeros_like(token_ids)\n",
    "        \n",
    "        for token_id, index in self.vocab.items():\n",
    "            mask = token_ids == token_id\n",
    "            if token_id in self.frequent_vocab:\n",
    "                # Map to index in frequent_vocab\n",
    "                frequent_indices[mask] = self.frequent_vocab[token_id]\n",
    "            elif token_id in self.infrequent_vocab:\n",
    "                # Map to index in infrequent_vocab\n",
    "                infrequent_indices[mask] = self.infrequent_vocab[token_id]\n",
    "\n",
    "        # Create masks for frequent and infrequent tokens\n",
    "        frequent_mask = frequent_indices > 0\n",
    "        infrequent_mask = infrequent_indices > 0\n",
    "\n",
    "        # Embed frequent tokens\n",
    "        if frequent_mask.any():\n",
    "            frequent_embeddings = self.frequent_embeddings(frequent_indices[frequent_mask])\n",
    "            embeddings[frequent_mask] = frequent_embeddings\n",
    "\n",
    "        # Embed and project infrequent tokens\n",
    "        if infrequent_mask.any():\n",
    "            infrequent_embeddings = self.infrequent_embeddings(infrequent_indices[infrequent_mask])\n",
    "            infrequent_embeddings_projected = self.infrequent_projection(infrequent_embeddings)\n",
    "            embeddings[infrequent_mask] = infrequent_embeddings_projected\n",
    "\n",
    "        # Apply positional embeddings\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        position_embeddings = self.positional_embeddings(position_ids)  # Generate for seq_len\n",
    "\n",
    "        # Ensure positional embeddings are broadcastable to the embeddings tensor\n",
    "        # This step may not be necessary if your positional embeddings are already correctly shaped\n",
    "        if position_embeddings.size(0) != batch_size:\n",
    "            position_embeddings = position_embeddings.expand(batch_size, -1, -1)\n",
    "\n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"Positional embeddings shape: {position_embeddings.shape}\")\n",
    "        embeddings += position_embeddings\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadLinformerAttention(embed_dim=d_model, num_heads=nhead)\n",
    "        self.dropout1 = AdaptiveDropoutLayer()  # Use AdaptiveDropoutLayer\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            AdaptiveDropoutLayer(),  # Use AdaptiveDropoutLayer here as well\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.dropout2 = AdaptiveDropoutLayer()  # And here\n",
    " \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2 = self.norm1(src)\n",
    "        attn_output = self.attn(src2)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout2(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab, vocab_size, embedding_dim, max_seq_len, nhead, dim_feedforward, \n",
    "                 freq_threshold, smaller_embed_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab,\n",
    "            vocab_size=vocab_size, \n",
    "            freq_threshold=freq_threshold,  \n",
    "            large_embed_dim=embedding_dim,       \n",
    "            small_embed_dim=smaller_embed_dim,   \n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.encoder = TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward)\n",
    "        self.pooler = Pooler(embedding_dim)  # Retain Pooler for sentence-level representation\n",
    "        # Add an output projection layer for token-level predictions\n",
    "        self.output_projection = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        encoded = self.encoder(embedded, src_mask=attention_mask)\n",
    "        # Get pooled output for sentence-level tasks\n",
    "        pooled_output = self.pooler(encoded)\n",
    "        # Project encoded output to vocabulary size for token-level predictions\n",
    "        logits = self.output_projection(encoded)\n",
    "        return logits, pooled_output\n",
    "\n",
    "# Tokenizing Code\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len= 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)[:self.max_seq_len]         \n",
    "        labels = encoded_text.copy()  # Copy encoded text for labels\n",
    "\n",
    "        # Apply dynamic masking\n",
    "        masked_indices = np.random.rand(len(encoded_text)) < 0.15\n",
    "        for i in range(len(encoded_text)):\n",
    "            if masked_indices[i]:\n",
    "                encoded_text[i] = self.tokenizer.vocab[\"[MASK]\"]\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "        labels += [-100] * padding_length  # Use -100 for padding positions\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "        self.frequency = 0  # Track frequency of subwords for simplified ranking\n",
    "\n",
    "from heapq import nlargest\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id, frequency):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "        node.frequency = frequency  # Assign frequency at the leaf node\n",
    "\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        \"\"\"\n",
    "        Finds the most probable subwords for a given token based on frequency.\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        best_subwords = []\n",
    "\n",
    "        def dfs(current_node, subword='', collected_subwords=[]):\n",
    "            if current_node.token_id is not None:\n",
    "                # Convert frequency to a simple probability for demonstration\n",
    "                probability = current_node.frequency / sum(node.frequency for node in collected_subwords)\n",
    "                collected_subwords.append((subword, probability, current_node.token_id))\n",
    "\n",
    "            for char, next_node in current_node.children.items():\n",
    "                dfs(next_node, subword + char, collected_subwords)\n",
    "\n",
    "        dfs(node)\n",
    "        # Instead of selecting based purely on frequency, consider the 'probability'\n",
    "        best_subwords = sorted(best_subwords, key=lambda x: x[1], reverse=True)\n",
    "        return [subword[2] for subword in best_subwords][:5] if best_subwords else [self.unk_token_id]\n",
    "\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(best)  # Track most frequent pair in each iteration\n",
    "\n",
    "    def preprocess_to_characters(self, word):\n",
    "        return list(word)\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.split()  # Assuming simple word splitting\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None \n",
    "        self.failure_pops = None\n",
    "\n",
    "    def train(self, corpus):\n",
    "        vocabulary = self._build_vocabulary(corpus)  # Build the word list\n",
    "        self.trie, self.failure_links, self.failure_pops = self._precompute(vocabulary)\n",
    "\n",
    "    def _encode(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes input text into subwords, using a trie to find the most probable subword sequences.\n",
    "        \"\"\"\n",
    "        tokens = text.split()  # Simplified tokenization\n",
    "        encoded_tokens = []\n",
    "        for token in tokens:\n",
    "            subword_ids = self.trie.find_subwords(token)\n",
    "            encoded_tokens.extend(subword_ids)\n",
    "        return encoded_tokens\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        if model_type == \"unigram\":\n",
    "            tokens = self._unigram_tokenize(corpus)  #  Hypothetical tokenize function\n",
    "            vocabulary = self._build_unigram_vocab(tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        # Count token frequencies\n",
    "        token_freqs = collections.Counter(tokens)\n",
    "        # Select the most frequent tokens up to vocab_size\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(token_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction\n",
    "        self.trie = Trie()  # Assuming you have a Trie class as well\n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token)  \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "    def _unigram_tokenize(self, corpus):\n",
    "        \"\"\"\n",
    "        Tokenizes the given corpus into unigram tokens, checking against the built vocabulary.\n",
    "        Tokens not found in the vocabulary are treated as unknowns.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): The text corpus to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list of str: A list of tokens extracted from the corpus, adjusted to fit the vocabulary.\n",
    "        \"\"\"\n",
    "        # Initial tokenization based on word boundaries and punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', corpus)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "\n",
    "class HybridTokenizer:\n",
    "    def __init__(self, bpe_model, sentence_piece_model, vocab):\n",
    "        self.bpe_model = bpe_model\n",
    "        self.sentence_piece_model = sentence_piece_model\n",
    "        self.vocab = vocab  # Your existing vocabulary\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Apply BPE tokenization\n",
    "        bpe_tokens = self.bpe_model.encode(text)\n",
    "        # Further tokenize each BPE token with SentencePiece\n",
    "        sp_tokens = [self.sentence_piece_model._encode(token) for token in bpe_tokens]\n",
    "        # Flatten the list of lists\n",
    "        flat_tokens = [item for sublist in sp_tokens for item in sublist]\n",
    "        # Convert flat tokens to token IDs\n",
    "        token_ids = [self.vocab.get(token, self.vocab.get(\"[UNK]\", 1)) for token in flat_tokens]\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, actual_vocab_size):\n",
    "        self.vocab = vocab\n",
    "        self.actual_vocab_size = actual_vocab_size\n",
    "        self.unk_token_id = self.vocab.get(\"[UNK]\", 1)  # Get ID of [UNK] \n",
    "        self.max_subword_length = max(len(token) for token in vocab.keys())\n",
    "        self.pattern = re.compile(r'\\b\\w+\\b|[\\s\\.,!?;]')\n",
    "\n",
    "        # Build the Trie\n",
    "        self.trie = Trie()\n",
    "        for token, token_id in vocab.items():\n",
    "            self.trie.insert(token, token_id)\n",
    "\n",
    "    def _find_subwords(self, word):\n",
    "        # 1. Trie Lookup \n",
    "        subword_ids = self.trie.find_subwords(word)\n",
    "\n",
    "        # 2. Fallback to Original Logic \n",
    "        if len(subword_ids) == 1 and subword_ids[0] == self.unk_token_id:  \n",
    "            subwords = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                for j in range(self.max_subword_length, 0, -1):\n",
    "                    subword = word[i:i+j]\n",
    "                    if subword in self.vocab:\n",
    "                        subwords.append(self.vocab[subword])\n",
    "                        i += j\n",
    "                        break \n",
    "                else: \n",
    "                    subwords.append(self.vocab[\"[UNK]\"])\n",
    "                    i += 1\n",
    "            subword_ids = subwords  # Replace with token IDs\n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        token_ids = [self.vocab.get(\"[CLS]\", 1)]  # Use [CLS] token or [UNK] if not found\n",
    "        tokens = self.pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            token_id = self.vocab.get(token, self.vocab.get(\"[UNK]\", 1))  # Fallback to [UNK] if token is not found\n",
    "            token_ids.append(token_id)\n",
    "        token_ids.append(self.vocab.get(\"[SEP]\", 1))  # Use [SEP] token or [UNK] if not found\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "\n",
    "class HybridTokenizer_v2:\n",
    "    def __init__(self, sentence_piece_model, bpe_model):\n",
    "        # Load or initialize the SentencePiece and BPE models\n",
    "        self.sentence_piece_model = sentence_piece_model  # Assume this is a loaded or initialized SentencePiece model\n",
    "        self.bpe_model = bpe_model  # Assume this is a loaded or initialized BPE model\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Encode text with SentencePiece\n",
    "        sp_tokens = self.sentence_piece_model.encode(text, out_type=str)\n",
    "\n",
    "        # Further encode each SentencePiece token with BPE\n",
    "        tokens = []\n",
    "        for token in sp_tokens:\n",
    "            tokens.extend(self.bpe_model.encode(token))\n",
    "\n",
    "        # Convert tokens to token IDs based on BPE's vocabulary\n",
    "        token_ids = [self.bpe_model.token_to_id(token) for token in tokens]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        # Convert token IDs back to tokens based on BPE's vocabulary\n",
    "        tokens = [self.bpe_model.id_to_token(token_id) for token_id in token_ids]\n",
    "\n",
    "        # Decode tokens back to text\n",
    "        text = self.sentence_piece_model.decode(tokens)\n",
    "        return text\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        # Method to convert a token to its ID using BPE vocabulary\n",
    "        return self.bpe_model.token_to_id(token)\n",
    "\n",
    "    def id_to_token(self, token_id):\n",
    "        # Method to convert a token ID back to its token using BPE vocabulary\n",
    "        return self.bpe_model.id_to_token(token_id)\n",
    "\n",
    "\n",
    "# Training Code\n",
    "\n",
    "# Load corpus and build vocab\n",
    "corpus = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "tokens = tokenize(corpus)\n",
    "vocab = build_vocab(tokens)\n",
    "actual_vocab_size = len(vocab)  # This includes [PAD], [UNK], [CLS], [SEP], [MASK]\n",
    "train_texts = [corpus]  # Treat your whole sample as one \"document\"\n",
    "print(f\"Actual vocabulary size (including special tokens): {actual_vocab_size}\")\n",
    "\n",
    "# Regular Tokenizer\n",
    "#tokenizer = Tokenizer(vocab=vocab, actual_vocab_size=actual_vocab_size)\n",
    "#train_dataset = TextDataset(train_texts, tokenizer, max_seq_len=512)\n",
    "#train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Hybrid option\n",
    "bpe_model = BPE()\n",
    "bpe_model.vocab = vocab\n",
    "bpe_model.train(corpus)  # Train BPE with your corpus\n",
    "sentence_piece_model = SentencePiece()\n",
    "hybrid_tokenizer = HybridTokenizer(bpe_model, sentence_piece_model, vocab)\n",
    "hybrid_train_dataset = TextDataset(train_texts, hybrid_tokenizer, max_seq_len=512)\n",
    "hybrid_train_dataloader = DataLoader(hybrid_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "device='cpu'\n",
    "freq_threshold_values = [10, 50, 100, 200, 500]  \n",
    "best_validation_accuracy = 0.0 \n",
    "\n",
    "for freq_threshold in freq_threshold_values:\n",
    "\n",
    "    # Model instantiation and training setup\n",
    "    model = TransformerModel(\n",
    "        vocab = vocab,\n",
    "        vocab_size=actual_vocab_size,     \n",
    "        embedding_dim=128,\n",
    "        max_seq_len=512,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        freq_threshold=freq_threshold,  # frequency threshold for splitting vocab\n",
    "        smaller_embed_dim=64\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4) \n",
    "    meta_optimizer = AdaptiveWeightDecayOptimizer(model.parameters(), lr=1e-5) \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    meta_update_freq = 5\n",
    "\n",
    "    # Training loop adjusted for the updated model architecture\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(hybrid_train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)  # Assuming labels are of shape [batch_size, sequence_length]\n",
    "\n",
    "            # Forward pass, model now returns logits and pooled_output\n",
    "            logits, pooled_output = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Correctly reshape logits to match the labels' shape\n",
    "            # Change from [1, 512, vocab_size] to [512, vocab_size] to align with labels\n",
    "            logits = logits.view(-1, logits.size(-1))  # Reshape logits for loss calculation\n",
    "            \n",
    "            labels = labels.view(-1)  # Ensure labels are a flat vector\n",
    "\n",
    "            # Calculate loss using logits for token-level predictions\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Meta-update occasionally\n",
    "            if (i + 1) % meta_update_freq == 0:\n",
    "                meta_optimizer.zero_grad()\n",
    "                # Recalculate or reuse the loss for the meta-update\n",
    "                meta_loss = combined_loss(logits.detach(), labels.detach(), model)\n",
    "                meta_loss.backward()\n",
    "                meta_optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss / len(hybrid_train_dataloader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model state\n",
    "model_path = \"D:\\\\EXPERT_WEIGHTS\\\\encoding_transformer.bin\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save tokenizer using pickle for simplicity\n",
    "import pickle\n",
    "tokenizer_path = \"D:\\\\EXPERT_WEIGHTS\\\\tokenizer.pkl\"\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model_loaded = TransformerModel(\n",
    "    vocab=vocab,  # Ensure `vocab` is loaded or defined in the scope\n",
    "    vocab_size=actual_vocab_size,\n",
    "    embedding_dim=128,\n",
    "    max_seq_len=512,\n",
    "    nhead=8,\n",
    "    dim_feedforward=2048,\n",
    "    freq_threshold=freq_threshold,  # Define or load `freq_threshold` as appropriate\n",
    "    smaller_embed_dim=64\n",
    ")\n",
    "model_loaded.load_state_dict(torch.load(\"D:\\\\EXPERT_WEIGHTS\\\\encoding_transformer.bin\"))\n",
    "model_loaded.eval()  # Set to evaluation mode\n",
    "\n",
    "# Load the tokenizer\n",
    "with open(\"D:\\\\EXPERT_WEIGHTS\\\\tokenizer.pkl\", 'rb') as f:\n",
    "    tokenizer_loaded = pickle.load(f)\n",
    "\n",
    "\n",
    "# Example text\n",
    "text = \"Here is some text to encode\"\n",
    "\n",
    "# Tokenize the input\n",
    "encoded_input = tokenizer_loaded.encode(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "# Predict with your model\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "    output = model_loaded(encoded_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE...\n",
      "Merging: ('the', '</w>')\n",
      "Merging: ('of', '</w>')\n",
      "Merging: ('to', '</w>')\n",
      "Merging: ('and', '</w>')\n",
      "Merging: ('a', '</w>')\n",
      "Merging: ('is', '</w>')\n",
      "Merging: ('in', '</w>')\n",
      "Merging: ('that', '</w>')\n",
      "Merging: ('on', '</w>')\n",
      "Merging: ('for', '</w>')\n",
      "Training SentencePiece...\n",
      "Sample from merged vocabulary: ['How </w>', 'transferable </w>', 'are </w>', 'features </w>', 'in</w>', 'deep </w>', 'neural </w>', 'networks? </w>', 'Jason </w>', 'Yosinski,1 </w>', 'Jeff </w>', 'Clune,2 </w>', 'Yoshua </w>', 'Bengio,3 </w>', 'and</w>', 'Hod </w>', 'Lipson4 </w>', '1 </w>', 'Dept. </w>', 'Computer </w>', 'Science, </w>', 'Cornell </w>', 'University </w>', '2 </w>', 'of</w>', 'Wyoming </w>', '3 </w>', 'Science </w>', '& </w>', 'Operations </w>', 'Research, </w>', 'Montreal </w>', '4 </w>', 'Mechanical </w>', 'Aerospace </w>', 'Engineering, </w>', 'Abstract </w>', 'Many </w>', 'networks </w>', 'trained </w>', 'on</w>', 'natural </w>', 'images </w>', 'exhibit </w>', 'a</w>', 'curious </w>', 'phenomenon </w>', 'common: </w>', 'the</w>', 'first </w>', 'layer </w>', 'they </w>', 'learn </w>', 'similar </w>', 'to</w>', 'Gabor </w>', 'filters </w>', 'color </w>', 'blobs. </w>', 'Such </w>', 'first-layer </w>', 'appear </w>', 'not </w>', 'be </w>', 'specific </w>', 'particular </w>', 'dataset </w>', 'or </w>', 'task, </w>', 'but </w>', 'general </w>', 'that</w>', 'applicable </w>', 'many </w>', 'datasets </w>', 'tasks. </w>', 'Features </w>', 'must </w>', 'eventually </w>', 'transition </w>', 'from </w>', 'by </w>', 'last </w>', 'network, </w>', 'this </w>', 'has </w>', 'been </w>', 'studied </w>', 'extensively. </w>', 'In </w>', 'paper </w>', 'we </w>', 'experimentally </w>', 'quantify </w>', 'generality </w>', 'versus </w>', 'specificity </w>', 'neurons </w>', 'each </w>', 'convolutional </w>']\n",
      "Merged vocab size: 2342\n",
      "Initializing WordPiece...\n",
      "[['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']]\n"
     ]
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # Ensure special tokens are included\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Tokenizing Code\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len= 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)[:self.max_seq_len]         \n",
    "        labels = encoded_text.copy()  # Copy encoded text for labels\n",
    "\n",
    "        # Apply dynamic masking\n",
    "        masked_indices = np.random.rand(len(encoded_text)) < 0.15\n",
    "        for i in range(len(encoded_text)):\n",
    "            if masked_indices[i]:\n",
    "                encoded_text[i] = self.tokenizer.vocab[\"[MASK]\"]\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "        labels += [-100] * padding_length  # Use -100 for padding positions\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "        self.frequency = 0  # Track frequency of subwords for simplified ranking\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id, frequency):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "        node.frequency = frequency  # Assign frequency at the leaf node\n",
    "\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        \"\"\"\n",
    "        Finds the most probable subwords for a given token based on frequency.\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        best_subwords = []\n",
    "\n",
    "        def dfs(current_node, subword='', collected_subwords=[]):\n",
    "            if current_node.token_id is not None:\n",
    "                # Convert frequency to a simple probability for demonstration\n",
    "                probability = current_node.frequency / sum(node.frequency for node in collected_subwords)\n",
    "                collected_subwords.append((subword, probability, current_node.token_id))\n",
    "\n",
    "            for char, next_node in current_node.children.items():\n",
    "                dfs(next_node, subword + char, collected_subwords)\n",
    "\n",
    "        dfs(node)\n",
    "        # Instead of selecting based purely on frequency, consider the 'probability'\n",
    "        best_subwords = sorted(best_subwords, key=lambda x: x[1], reverse=True)\n",
    "        return [subword[2] for subword in best_subwords][:5] if best_subwords else [self.unk_token_id]\n",
    "\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "    def compute_failure_links(self):\n",
    "        root = self.root\n",
    "        root.failure_link = root  # The root's failure link points to itself\n",
    "        queue = [root]\n",
    "\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)\n",
    "\n",
    "                # Set the failure link for the child_node\n",
    "                if current_node == root:\n",
    "                    child_node.failure_link = root\n",
    "                else:\n",
    "                    # Follow the current node's failure link to find the longest suffix for the child_node\n",
    "                    failure_candidate = current_node.failure_link\n",
    "                    while failure_candidate != root and char not in failure_candidate.children:\n",
    "                        failure_candidate = failure_candidate.failure_link\n",
    "                    child_node.failure_link = failure_candidate.children.get(char, root)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            if not pairs:  # Check if there are no more pairs to merge\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(\"Merging: {}\".format(best))  # Track most frequent pair in each iteration\n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies, tokenized at the character level.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.strip().split()\n",
    "            for word in words:\n",
    "                # No longer splitting the word into characters with spaces\n",
    "                vocab[word + \" </w>\"] += 1  # Marking end of word with </w>\n",
    "        return vocab\n",
    "\n",
    "    def preprocess_to_characters(self, word):\n",
    "        return list(word)\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None\n",
    "        self.failure_pops = None\n",
    "        self.vocab = {}  # Initialize vocab as an empty dictionary\n",
    "\n",
    "    def train(self, corpus):\n",
    "        self.vocab = self._build_vocabulary(corpus)\n",
    "        self._precompute(self.vocab) \n",
    "\n",
    "    def _encode(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes input text into subwords, using a trie to find the most probable subword sequences.\n",
    "        \"\"\"\n",
    "        tokens = text.split()  # Simplified tokenization\n",
    "        encoded_tokens = []\n",
    "        for token in tokens:\n",
    "            subword_ids = self.trie.find_subwords(token)\n",
    "            encoded_tokens.extend(subword_ids)\n",
    "        return encoded_tokens\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        # Adjusted to handle the corpus as a list\n",
    "        if model_type == \"unigram\":\n",
    "            # Aggregate tokens from all texts in the corpus\n",
    "            all_tokens = []\n",
    "            for text in corpus:\n",
    "                tokens = self._unigram_tokenize(text)\n",
    "                all_tokens.extend(tokens)\n",
    "            vocabulary = self._build_unigram_vocab(all_tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            # Assuming a similar adjustment for BPE if necessary\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _unigram_tokenize(self, text):\n",
    "        # Now properly handles a single text/string\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        # Count token frequencies\n",
    "        token_freqs = collections.Counter(tokens)\n",
    "        # Select the most frequent tokens up to vocab_size\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(token_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Initialize the trie\n",
    "        self.trie = Trie()\n",
    "        for token_id, (token, frequency) in enumerate(vocabulary.items()):\n",
    "            # Insert tokens into the trie\n",
    "            self.trie.insert(token, token_id, frequency)\n",
    "\n",
    "        # Compute failure links after the trie has been fully constructed\n",
    "        self.trie.compute_failure_links() \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Lowercase the text to standardize it\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Replace or remove special characters as needed\n",
    "        # For demonstration, replacing common quotation marks and removing non-ASCII characters\n",
    "        text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "        text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "        \n",
    "        # Whitespace normalization\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        \n",
    "        # Optional: Handling numbers (here, we just replace them with a placeholder)\n",
    "        text = re.sub(r\"\\d+\", \"<num>\", text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        output_tokens = []\n",
    "        text = self.preprocess(text)  # Apply preprocessing\n",
    "        for word in text.strip().split():\n",
    "            if len(word) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            if word in self.vocab:\n",
    "                output_tokens.append(word)\n",
    "            else:\n",
    "                is_bad, sub_tokens = False, []\n",
    "                start = 0\n",
    "                while start < len(word):\n",
    "                    end = len(word)\n",
    "                    cur_substr = None\n",
    "                    while start < end:\n",
    "                        substr = \"##\" + word[start:end] if start > 0 else word[start:end]\n",
    "                        if substr in self.vocab:\n",
    "                            cur_substr = substr\n",
    "                            break\n",
    "                        end -= 1\n",
    "\n",
    "                    if cur_substr is None:\n",
    "                        is_bad = True\n",
    "                        break\n",
    "\n",
    "                    sub_tokens.append(cur_substr)\n",
    "                    start = end\n",
    "\n",
    "                if is_bad:\n",
    "                    output_tokens.append(self.unk_token)\n",
    "                else:\n",
    "                    output_tokens.extend(sub_tokens)\n",
    "\n",
    "        return output_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ContextualTokenizer:\n",
    "    def __init__(self, corpus, vocab_size=10000, max_seq_len=512):\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.bpe = BPE()\n",
    "        self.sentence_piece = SentencePiece()\n",
    "        self.word_piece = None  # Will be initialized post-BPE and SentencePiece training\n",
    "        self.tokenizer_vocab = None  # Final vocabulary used by WordPieceTokenizer\n",
    "\n",
    "    def train_tokenizers(self):\n",
    "        # Train BPE and build its vocabulary\n",
    "        print(\"Training BPE...\")\n",
    "        self.bpe.train(self.corpus)\n",
    "        bpe_vocab = self.bpe.vocab\n",
    "\n",
    "        # Train SentencePiece and build its vocabulary\n",
    "        print(\"Training SentencePiece...\")\n",
    "        self.sentence_piece.train(self.corpus)\n",
    "        sentence_piece_vocab = self.sentence_piece._build_vocabulary(self.corpus, self.vocab_size)\n",
    "\n",
    "        # Merge BPE and SentencePiece vocabularies for WordPiece\n",
    "        merged_vocab = {**bpe_vocab, **sentence_piece_vocab}\n",
    "        self.tokenizer_vocab = {token: idx for idx, token in enumerate(merged_vocab.keys())}\n",
    "        # After merging the vocabularies\n",
    "        print(\"Sample from merged vocabulary:\", list(self.tokenizer_vocab.keys())[:100])\n",
    "        # Right after merging vocabularies\n",
    "        print(\"Merged vocab size:\", len(self.tokenizer_vocab))\n",
    "\n",
    "        # Initialize WordPiece with the merged vocabulary\n",
    "        print(\"Initializing WordPiece...\")\n",
    "        self.word_piece = WordPieceTokenizer(self.tokenizer_vocab)\n",
    "\n",
    "    def tokenize_corpus(self):\n",
    "        # Tokenize the corpus using WordPiece\n",
    "        tokenized_corpus = [self.word_piece.tokenize(text) for text in self.corpus]\n",
    "        return tokenized_corpus\n",
    "\n",
    "    def create_dataset(self, texts):\n",
    "        # Utilize the WordPieceTokenizer's encoded tokens to create a dataset\n",
    "        dataset = TextDataset(texts, self.word_piece, self.max_seq_len)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "\n",
    "# Test on my own txt file\n",
    "# Load the corpus\n",
    "text = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "\n",
    "# Initialize your tokenizer with the corpus (adjust as needed)\n",
    "contextual_tokenizer = ContextualTokenizer(corpus=[text], vocab_size=10000, max_seq_len=512)\n",
    "contextual_tokenizer.train_tokenizers()\n",
    "\n",
    "# Tokenize the corpus and create a dataset\n",
    "tokenized_corpus = contextual_tokenizer.tokenize_corpus()\n",
    "dataset = contextual_tokenizer.create_dataset(tokenized_corpus)\n",
    "print(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def combined_loss(output, target, model, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)  \n",
    "    regularization_loss = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if isinstance(param, nn.Parameter):  \n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1\n",
    "\n",
    "    return task_loss + regularization_loss\n",
    "\n",
    "\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "        self.frequency = 0  # Track frequency of subwords for simplified ranking\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id, frequency):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "        node.frequency = frequency  # Assign frequency at the leaf node\n",
    "\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        \"\"\"\n",
    "        Finds the most probable subwords for a given token based on frequency.\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        best_subwords = []\n",
    "\n",
    "        def dfs(current_node, subword='', collected_subwords=[]):\n",
    "            if current_node.token_id is not None:\n",
    "                # Convert frequency to a simple probability for demonstration\n",
    "                probability = current_node.frequency / sum(node.frequency for node in collected_subwords)\n",
    "                collected_subwords.append((subword, probability, current_node.token_id))\n",
    "\n",
    "            for char, next_node in current_node.children.items():\n",
    "                dfs(next_node, subword + char, collected_subwords)\n",
    "\n",
    "        dfs(node)\n",
    "        # Instead of selecting based purely on frequency, consider the 'probability'\n",
    "        best_subwords = sorted(best_subwords, key=lambda x: x[1], reverse=True)\n",
    "        return [subword[2] for subword in best_subwords][:5] if best_subwords else [self.unk_token_id]\n",
    "\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "    def compute_failure_links(self):\n",
    "        root = self.root\n",
    "        root.failure_link = root  # The root's failure link points to itself\n",
    "        queue = [root]\n",
    "\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)\n",
    "\n",
    "                # Set the failure link for the child_node\n",
    "                if current_node == root:\n",
    "                    child_node.failure_link = root\n",
    "                else:\n",
    "                    # Follow the current node's failure link to find the longest suffix for the child_node\n",
    "                    failure_candidate = current_node.failure_link\n",
    "                    while failure_candidate != root and char not in failure_candidate.children:\n",
    "                        failure_candidate = failure_candidate.failure_link\n",
    "                    child_node.failure_link = failure_candidate.children.get(char, root)\n",
    "\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            if not pairs:  # Check if there are no more pairs to merge\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(\"Merging: {}\".format(best))  # Track most frequent pair in each iteration\n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies, tokenized at the character level.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.strip().split()\n",
    "            for word in words:\n",
    "                # No longer splitting the word into characters with spaces\n",
    "                vocab[word + \" </w>\"] += 1  # Marking end of word with </w>\n",
    "        return vocab\n",
    "\n",
    "    def preprocess_to_characters(self, word):\n",
    "        return list(word)\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None\n",
    "        self.failure_pops = None\n",
    "        self.vocab = {}  # Initialize vocab as an empty dictionary\n",
    "\n",
    "    def train(self, corpus):\n",
    "        self.vocab = self._build_vocabulary(corpus)\n",
    "        self._precompute(self.vocab) \n",
    "\n",
    "    def _encode(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes input text into subwords, using a trie to find the most probable subword sequences.\n",
    "        \"\"\"\n",
    "        tokens = text.split()  # Simplified tokenization\n",
    "        encoded_tokens = []\n",
    "        for token in tokens:\n",
    "            subword_ids = self.trie.find_subwords(token)\n",
    "            encoded_tokens.extend(subword_ids)\n",
    "        return encoded_tokens\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        # Adjusted to handle the corpus as a list\n",
    "        if model_type == \"unigram\":\n",
    "            # Aggregate tokens from all texts in the corpus\n",
    "            all_tokens = []\n",
    "            for text in corpus:\n",
    "                tokens = self._unigram_tokenize(text)\n",
    "                all_tokens.extend(tokens)\n",
    "            vocabulary = self._build_unigram_vocab(all_tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            # Assuming a similar adjustment for BPE if necessary\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _unigram_tokenize(self, text):\n",
    "        # Now properly handles a single text/string\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        # Count token frequencies\n",
    "        token_freqs = collections.Counter(tokens)\n",
    "        # Select the most frequent tokens up to vocab_size\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(token_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Initialize the trie\n",
    "        self.trie = Trie()\n",
    "        for token_id, (token, frequency) in enumerate(vocabulary.items()):\n",
    "            # Insert tokens into the trie\n",
    "            self.trie.insert(token, token_id, frequency)\n",
    "\n",
    "        # Compute failure links after the trie has been fully constructed\n",
    "        self.trie.compute_failure_links() \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Lowercase the text to standardize it\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Replace or remove special characters as needed\n",
    "        # For demonstration, replacing common quotation marks and removing non-ASCII characters\n",
    "        text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "        text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "        \n",
    "        # Whitespace normalization\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        \n",
    "        # Optional: Handling numbers (here, we just replace them with a placeholder)\n",
    "        text = re.sub(r\"\\d+\", \"<num>\", text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        output_tokens = []\n",
    "        text = self.preprocess(text)  # Apply preprocessing\n",
    "        for word in text.strip().split():\n",
    "            if len(word) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            if word in self.vocab:\n",
    "                output_tokens.append(word)\n",
    "            else:\n",
    "                is_bad, sub_tokens = False, []\n",
    "                start = 0\n",
    "                while start < len(word):\n",
    "                    end = len(word)\n",
    "                    cur_substr = None\n",
    "                    while start < end:\n",
    "                        substr = \"##\" + word[start:end] if start > 0 else word[start:end]\n",
    "                        if substr in self.vocab:\n",
    "                            cur_substr = substr\n",
    "                            break\n",
    "                        end -= 1\n",
    "\n",
    "                    if cur_substr is None:\n",
    "                        is_bad = True\n",
    "                        break\n",
    "\n",
    "                    sub_tokens.append(cur_substr)\n",
    "                    start = end\n",
    "\n",
    "                if is_bad:\n",
    "                    output_tokens.append(self.unk_token)\n",
    "                else:\n",
    "                    output_tokens.extend(sub_tokens)\n",
    "\n",
    "        return output_tokens\n",
    "\n",
    "\n",
    "\n",
    "class TransformerMergingNetwork(nn.Module):\n",
    "    def __init__(self, combined_vocab, vocab_size_bpe, vocab_size_sp, vocab_size_wp, \n",
    "                 embedding_dim, nhead, dim_feedforward, freq_threshold, smaller_embed_dim, \n",
    "                 max_seq_len=512, initial_max_frequency=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_bpe = nn.Embedding(vocab_size_bpe, embedding_dim)\n",
    "        self.token_embedding_sp = nn.Embedding(vocab_size_sp, embedding_dim)\n",
    "        self.token_embedding_wp = nn.Embedding(vocab_size_wp, embedding_dim)\n",
    "        self.max_frequency = initial_max_frequency\n",
    "        self.freq_embedding = nn.Embedding(self.max_frequency, embedding_dim)  \n",
    "\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(3 * max_seq_len, embedding_dim)) \n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(embedding_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, bpe_tokens, bpe_freqs, sp_tokens, sp_freqs, wp_tokens, wp_freqs):\n",
    "        embedded_bpe = self.token_embedding_bpe(bpe_tokens) + self.positional_encoding[:len(bpe_tokens)] \n",
    "        embedded_sp = self.token_embedding_sp(sp_tokens) + self.positional_encoding[len(bpe_tokens):(len(bpe_tokens) + len(sp_tokens))]\n",
    "        embedded_wp = self.token_embedding_wp(wp_tokens) + self.positional_encoding[(len(bpe_tokens) + len(sp_tokens)):] \n",
    "        if torch.any(torch.cat((bpe_freqs, sp_freqs, wp_freqs)) >= self.max_frequency):   # Combined check\n",
    "            self._increase_max_frequency() \n",
    "        embedded_freqs = self.freq_embedding(torch.cat((bpe_freqs, sp_freqs, wp_freqs)))\n",
    "\n",
    "        # Concatenate all embeddings across tokenizers in sequence with frequencies\n",
    "        combined_input = torch.cat((embedded_bpe, embedded_sp, embedded_wp, embedded_freqs), dim=1)\n",
    "\n",
    "        encoded_output = self.transformer_encoder(combined_input)\n",
    "        merge_weights = self.sigmoid(self.output_layer(encoded_output))\n",
    "        return merge_weights\n",
    "\n",
    "    def _increase_max_frequency(self):\n",
    "        new_max_frequency = int(self.max_frequency * 1.5)  \n",
    "        self.freq_embedding = nn.Embedding(new_max_frequency, self.freq_embedding.embedding_dim)  # Resize \n",
    "        self.max_frequency = new_max_frequency \n",
    "\n",
    "    def merging_loss(self, bpe_embeddings, sp_embeddings, wp_embeddings, merge_weights, margin=1.0):\n",
    "        \"\"\"\n",
    "        Calculates the merging loss.\n",
    "\n",
    "        Args:\n",
    "            bpe_embeddings:  Embeddings for BPE tokens from the combined output of Transformers network.\n",
    "            sp_embeddings: Embeddings for SentencePiece tokens.\n",
    "            wp_embeddings: Embeddings for WordPiece tokens.\n",
    "            merge_weights: Predicted merge weights (output of the Transformer model).\n",
    "            margin: Margin for contrastive loss component.\n",
    "\n",
    "        Returns:\n",
    "            loss: The calculated merging loss.\n",
    "        \"\"\"\n",
    "\n",
    "        # Cross-Tokenizer Presence Loss\n",
    "        presence_loss = torch.mean(merge_weights**2)  # Squared weights favor high confidence merges\n",
    "\n",
    "        # Semantic Similarity Loss (using Contrastive Loss as an example)\n",
    "        similarity_loss = 0.0\n",
    "\n",
    "        for bpe_emb, sp_emb, wp_emb in zip(bpe_embeddings, sp_embeddings, wp_embeddings):\n",
    "            # Calculate pairwise distances between embeddings (cosine distance example)\n",
    "            dist_bpe_sp = 1 - torch.cosine_similarity(bpe_emb, sp_emb) \n",
    "            dist_bpe_wp = 1 - torch.cosine_similarity(bpe_emb, wp_emb)  \n",
    "            dist_sp_wp = 1 - torch.cosine_similarity(sp_emb, wp_emb) \n",
    "\n",
    "            # Contrastive terms: Encourage matching pairs, discourage non-matching\n",
    "            similarity_loss += torch.maximum(0.0, dist_bpe_sp - margin)  \n",
    "            similarity_loss += torch.maximum(0.0, dist_bpe_wp - margin)\n",
    "            similarity_loss += torch.maximum(0.0, dist_sp_wp - margin)\n",
    "\n",
    "        # Combine with appropriate weights\n",
    "        alpha = 0.6  # Example - Adjust as needed\n",
    "        total_loss = alpha * presence_loss + (1 - alpha) * similarity_loss  \n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE...\n",
      "Merging: ('the', '</w>')\n",
      "Merging: ('of', '</w>')\n",
      "Merging: ('to', '</w>')\n",
      "Merging: ('and', '</w>')\n",
      "Merging: ('a', '</w>')\n",
      "Merging: ('is', '</w>')\n",
      "Merging: ('in', '</w>')\n",
      "Merging: ('that', '</w>')\n",
      "Merging: ('on', '</w>')\n",
      "Merging: ('for', '</w>')\n",
      "Training SentencePiece...\n",
      "Sample from merged vocabulary: ['How </w>', 'transferable </w>', 'are </w>', 'features </w>', 'in</w>', 'deep </w>', 'neural </w>', 'networks? </w>', 'Jason </w>', 'Yosinski,1 </w>', 'Jeff </w>', 'Clune,2 </w>', 'Yoshua </w>', 'Bengio,3 </w>', 'and</w>', 'Hod </w>', 'Lipson4 </w>', '1 </w>', 'Dept. </w>', 'Computer </w>', 'Science, </w>', 'Cornell </w>', 'University </w>', '2 </w>', 'of</w>', 'Wyoming </w>', '3 </w>', 'Science </w>', '& </w>', 'Operations </w>', 'Research, </w>', 'Montreal </w>', '4 </w>', 'Mechanical </w>', 'Aerospace </w>', 'Engineering, </w>', 'Abstract </w>', 'Many </w>', 'networks </w>', 'trained </w>', 'on</w>', 'natural </w>', 'images </w>', 'exhibit </w>', 'a</w>', 'curious </w>', 'phenomenon </w>', 'common: </w>', 'the</w>', 'first </w>', 'layer </w>', 'they </w>', 'learn </w>', 'similar </w>', 'to</w>', 'Gabor </w>', 'filters </w>', 'color </w>', 'blobs. </w>', 'Such </w>', 'first-layer </w>', 'appear </w>', 'not </w>', 'be </w>', 'specific </w>', 'particular </w>', 'dataset </w>', 'or </w>', 'task, </w>', 'but </w>', 'general </w>', 'that</w>', 'applicable </w>', 'many </w>', 'datasets </w>', 'tasks. </w>', 'Features </w>', 'must </w>', 'eventually </w>', 'transition </w>', 'from </w>', 'by </w>', 'last </w>', 'network, </w>', 'this </w>', 'has </w>', 'been </w>', 'studied </w>', 'extensively. </w>', 'In </w>', 'paper </w>', 'we </w>', 'experimentally </w>', 'quantify </w>', 'generality </w>', 'versus </w>', 'specificity </w>', 'neurons </w>', 'each </w>', 'convolutional </w>']\n",
      "Merged vocab size: 2342\n",
      "Initializing WordPiece...\n",
      "Contextual Tokenizer tokenize_corpus: [['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], [], [], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], [], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], [], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], [], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], [], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextDataset' object has no attribute 'tokenized_texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 807\u001b[0m\n\u001b[0;32m    805\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    806\u001b[0m total_loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 807\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[49], line 40\u001b[0m, in \u001b[0;36mTextDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 40\u001b[0m     encoded_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenized_texts\u001b[49m[idx][:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len]\n\u001b[0;32m     41\u001b[0m     labels \u001b[38;5;241m=\u001b[39m encoded_text\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# Copy encoded text for labels\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Apply dynamic masking\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextDataset' object has no attribute 'tokenized_texts'"
     ]
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = [line.strip() for line in file.readlines()]\n",
    "    return texts\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # Ensure special tokens are included\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Tokenizing Code\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len= 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded_text = self.tokenized_texts[idx][:self.max_seq_len]\n",
    "        labels = encoded_text.copy()  # Copy encoded text for labels\n",
    "\n",
    "        # Apply dynamic masking\n",
    "        masked_indices = np.random.rand(len(encoded_text)) < 0.15\n",
    "        for i in range(len(encoded_text)):\n",
    "            if masked_indices[i]:\n",
    "                encoded_text[i] = self.tokenizer.vocab[\"[MASK]\"]\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "        labels += [-100] * padding_length  # Use -100 for padding positions\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "        self.frequency = 0  # Track frequency of subwords for simplified ranking\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id, frequency):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "        node.frequency = frequency  # Assign frequency at the leaf node\n",
    "\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        \"\"\"\n",
    "        Finds the most probable subwords for a given token based on frequency.\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        best_subwords = []\n",
    "\n",
    "        def dfs(current_node, subword='', collected_subwords=[]):\n",
    "            if current_node.token_id is not None:\n",
    "                # Convert frequency to a simple probability for demonstration\n",
    "                probability = current_node.frequency / sum(node.frequency for node in collected_subwords)\n",
    "                collected_subwords.append((subword, probability, current_node.token_id))\n",
    "\n",
    "            for char, next_node in current_node.children.items():\n",
    "                dfs(next_node, subword + char, collected_subwords)\n",
    "\n",
    "        dfs(node)\n",
    "        # Instead of selecting based purely on frequency, consider the 'probability'\n",
    "        best_subwords = sorted(best_subwords, key=lambda x: x[1], reverse=True)\n",
    "        return [subword[2] for subword in best_subwords][:5] if best_subwords else [self.unk_token_id]\n",
    "\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "    def compute_failure_links(self):\n",
    "        root = self.root\n",
    "        root.failure_link = root  # The root's failure link points to itself\n",
    "        queue = [root]\n",
    "\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)\n",
    "\n",
    "                # Set the failure link for the child_node\n",
    "                if current_node == root:\n",
    "                    child_node.failure_link = root\n",
    "                else:\n",
    "                    # Follow the current node's failure link to find the longest suffix for the child_node\n",
    "                    failure_candidate = current_node.failure_link\n",
    "                    while failure_candidate != root and char not in failure_candidate.children:\n",
    "                        failure_candidate = failure_candidate.failure_link\n",
    "                    child_node.failure_link = failure_candidate.children.get(char, root)\n",
    "\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            if not pairs:  # Check if there are no more pairs to merge\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(\"Merging: {}\".format(best))  # Track most frequent pair in each iteration\n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies, tokenized at the character level.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.strip().split()\n",
    "            for word in words:\n",
    "                # No longer splitting the word into characters with spaces\n",
    "                vocab[word + \" </w>\"] += 1  # Marking end of word with </w>\n",
    "        return vocab\n",
    "\n",
    "    def preprocess_to_characters(self, word):\n",
    "        return list(word)\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None\n",
    "        self.failure_pops = None\n",
    "        self.vocab = {}  # Initialize vocab as an empty dictionary\n",
    "\n",
    "    def train(self, corpus):\n",
    "        self.vocab = self._build_vocabulary(corpus)\n",
    "        self._precompute(self.vocab) \n",
    "\n",
    "    def _encode(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes input text into subwords, using a trie to find the most probable subword sequences.\n",
    "        \"\"\"\n",
    "        tokens = text.split()  # Simplified tokenization\n",
    "        encoded_tokens = []\n",
    "        for token in tokens:\n",
    "            subword_ids = self.trie.find_subwords(token)\n",
    "            encoded_tokens.extend(subword_ids)\n",
    "        return encoded_tokens\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        # Adjusted to handle the corpus as a list\n",
    "        if model_type == \"unigram\":\n",
    "            # Aggregate tokens from all texts in the corpus\n",
    "            all_tokens = []\n",
    "            for text in corpus:\n",
    "                tokens = self._unigram_tokenize(text)\n",
    "                all_tokens.extend(tokens)\n",
    "            vocabulary = self._build_unigram_vocab(all_tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            # Assuming a similar adjustment for BPE if necessary\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _unigram_tokenize(self, text):\n",
    "        # Now properly handles a single text/string\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        # Count token frequencies\n",
    "        token_freqs = collections.Counter(tokens)\n",
    "        # Select the most frequent tokens up to vocab_size\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(token_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Initialize the trie\n",
    "        self.trie = Trie()\n",
    "        for token_id, (token, frequency) in enumerate(vocabulary.items()):\n",
    "            # Insert tokens into the trie\n",
    "            self.trie.insert(token, token_id, frequency)\n",
    "\n",
    "        # Compute failure links after the trie has been fully constructed\n",
    "        self.trie.compute_failure_links() \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Lowercase the text to standardize it\n",
    "        text = text.lower()\n",
    "        \n",
    "        \n",
    "        # Replace or remove special characters as needed\n",
    "        # For demonstration, replacing common quotation marks and removing non-ASCII characters\n",
    "        text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "        text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "        \n",
    "        # Whitespace normalization\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        \n",
    "        # Optional: Handling numbers (here, we just replace them with a placeholder)\n",
    "        text = re.sub(r\"\\d+\", \"<num>\", text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        output_tokens = []\n",
    "        text = self.preprocess(text)  # Apply preprocessing\n",
    "        for word in text.strip().split():\n",
    "            if len(word) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            if word in self.vocab:\n",
    "                output_tokens.append(word)\n",
    "            else:\n",
    "                is_bad, sub_tokens = False, []\n",
    "                start = 0\n",
    "                while start < len(word):\n",
    "                    end = len(word)\n",
    "                    cur_substr = None\n",
    "                    while start < end:\n",
    "                        substr = \"##\" + word[start:end] if start > 0 else word[start:end]\n",
    "                        if substr in self.vocab:\n",
    "                            cur_substr = substr\n",
    "                            break\n",
    "                        end -= 1\n",
    "\n",
    "                    if cur_substr is None:\n",
    "                        is_bad = True\n",
    "                        break\n",
    "\n",
    "                    sub_tokens.append(cur_substr)\n",
    "                    start = end\n",
    "\n",
    "                if is_bad:\n",
    "                    output_tokens.append(self.unk_token)\n",
    "                else:\n",
    "                    output_tokens.extend(sub_tokens)\n",
    "\n",
    "        return output_tokens\n",
    "\n",
    "\n",
    "\n",
    "class ContextualTokenizer:\n",
    "    def __init__(self, corpus, vocab_size=10000, max_seq_len=512):\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.bpe = BPE()\n",
    "        self.sentence_piece = SentencePiece()\n",
    "        self.word_piece = None  # Will be initialized post-BPE and SentencePiece training\n",
    "        self.tokenizer_vocab = None  # Final vocabulary used by WordPieceTokenizer\n",
    "\n",
    "    def train_tokenizers(self):\n",
    "        # Train BPE and build its vocabulary\n",
    "        print(\"Training BPE...\")\n",
    "        self.bpe.train(self.corpus)\n",
    "        bpe_vocab = self.bpe.vocab\n",
    "\n",
    "        # Train SentencePiece and build its vocabulary\n",
    "        print(\"Training SentencePiece...\")\n",
    "        self.sentence_piece.train(self.corpus)\n",
    "        sentence_piece_vocab = self.sentence_piece._build_vocabulary(self.corpus, self.vocab_size)\n",
    "\n",
    "        # Merge BPE and SentencePiece vocabularies for WordPiece\n",
    "        merged_vocab = {**bpe_vocab, **sentence_piece_vocab}\n",
    "        self.tokenizer_vocab = {token: idx for idx, token in enumerate(merged_vocab.keys())}\n",
    "        # After merging the vocabularies\n",
    "        print(\"Sample from merged vocabulary:\", list(self.tokenizer_vocab.keys())[:100])\n",
    "        # Right after merging vocabularies\n",
    "        print(\"Merged vocab size:\", len(self.tokenizer_vocab))\n",
    "\n",
    "        # Initialize WordPiece with the merged vocabulary\n",
    "        print(\"Initializing WordPiece...\")\n",
    "        self.word_piece = WordPieceTokenizer(self.tokenizer_vocab)\n",
    "\n",
    "    def tokenize_corpus(self):\n",
    "        # Tokenize the corpus using WordPiece\n",
    "        tokenized_corpus = [self.word_piece.tokenize(text) for text in self.corpus]\n",
    "        return tokenized_corpus\n",
    "\n",
    "    def create_dataset(self, texts):\n",
    "        # Utilize the WordPieceTokenizer's encoded tokens to create a dataset\n",
    "        dataset = TextDataset(texts, self.word_piece, self.max_seq_len)\n",
    "        return dataset\n",
    "\n",
    "# Encoding Transformer Code\n",
    "\n",
    "class AdaptiveDropoutLayer(nn.Module):\n",
    "    def __init__(self, init_dropout_rate=0.1):\n",
    "        super(AdaptiveDropoutLayer, self).__init__()\n",
    "        # Use logit transformation for stability\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.log_alpha)\n",
    "        # Convert p from a tensor to a float\n",
    "        p_value = p.item()  # This extracts the scalar value as a Python float\n",
    "        return nn.functional.dropout(x, p=p_value, training=self.training)\n",
    "\n",
    "\n",
    "class AdaptiveWeightDecayOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, init_l2_strength=0.01):\n",
    "        super().__init__(params, {'lr': lr})\n",
    "        self.log_l2_strength = nn.Parameter(torch.tensor(math.log(init_l2_strength)).float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = torch.exp(self.log_l2_strength)  \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay) \n",
    "                p.update(d_p, group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Create positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension (B x T x C)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Tensor of shape [Batch Size, Sequence Length, Embedding Dimension]\n",
    "        # Adjust positional encoding to match the input size and device\n",
    "        pe = self.pe[:, :x.size(1)]\n",
    "        # Assuming x is on the correct device, pe will be automatically aligned to the same device\n",
    "        return pe\n",
    "\n",
    "\n",
    "class MultiHeadLinformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, k=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k if k is not None else embed_dim // num_heads  # Projection dimension per head\n",
    "\n",
    "        self.key_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.value_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        # Project keys and values\n",
    "        keys = self.key_projections(query)\n",
    "        values = self.value_projections(query)\n",
    "        \n",
    "        # Reshape into [batch_size, num_heads, seq_len, k]\n",
    "        keys = keys.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        values = values.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention (scaled dot-product attention)\n",
    "        # Scaling by the square root of the depth of the key vectors to prevent large values in the dot product\n",
    "        # which could push the softmax function into regions where it has extremely small gradients\n",
    "        keys = keys / (self.k ** 0.5)\n",
    "        attention_scores = torch.softmax(torch.matmul(keys, values.transpose(-2, -1)), dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention_scores, values)\n",
    "        \n",
    "        # Concatenate heads and project back to original embedding dimension\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, self.num_heads * self.k)\n",
    "        out = self.out_projection(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class AdaptiveEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab,  vocab_size, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "        super(AdaptiveEmbeddingLayer, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab_size\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.large_embed_dim = large_embed_dim\n",
    "        self.small_embed_dim = small_embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.split_vocab(vocab, freq_threshold)  \n",
    "\n",
    "        self.frequent_embeddings = nn.Embedding(num_embeddings=len(self.frequent_vocab), embedding_dim=large_embed_dim)\n",
    "        self.infrequent_embeddings = nn.Embedding(num_embeddings=len(self.infrequent_vocab), embedding_dim=small_embed_dim)\n",
    "        self.infrequent_projection = nn.Linear(small_embed_dim, large_embed_dim)\n",
    "        self.positional_embeddings = PositionalEncoding(large_embed_dim, max_seq_len)\n",
    "\n",
    "\n",
    "    def split_vocab(self, vocab, freq_threshold):\n",
    "        token_counts = [(token, count) for token, count in vocab.items()]\n",
    "        token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "        \n",
    "        self.frequent_vocab = dict(token_counts[:split_point])\n",
    "        self.infrequent_vocab = dict(token_counts[split_point:])\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        device = token_ids.device\n",
    "        seq_len = token_ids.size(1)\n",
    "        batch_size = token_ids.size(0)  # Obtain batch size from token_ids tensor\n",
    "\n",
    "        # Initialize embeddings tensor\n",
    "        embeddings = torch.zeros(token_ids.shape[0], seq_len, self.large_embed_dim, device=device)\n",
    "\n",
    "        # Map token_ids to indices for frequent and infrequent vocab\n",
    "        frequent_indices = torch.zeros_like(token_ids)\n",
    "        infrequent_indices = torch.zeros_like(token_ids)\n",
    "        \n",
    "        for token_id, index in self.vocab.items():\n",
    "            mask = token_ids == token_id\n",
    "            if token_id in self.frequent_vocab:\n",
    "                # Map to index in frequent_vocab\n",
    "                frequent_indices[mask] = self.frequent_vocab[token_id]\n",
    "            elif token_id in self.infrequent_vocab:\n",
    "                # Map to index in infrequent_vocab\n",
    "                infrequent_indices[mask] = self.infrequent_vocab[token_id]\n",
    "\n",
    "        # Create masks for frequent and infrequent tokens\n",
    "        frequent_mask = frequent_indices > 0\n",
    "        infrequent_mask = infrequent_indices > 0\n",
    "\n",
    "        # Embed frequent tokens\n",
    "        if frequent_mask.any():\n",
    "            frequent_embeddings = self.frequent_embeddings(frequent_indices[frequent_mask])\n",
    "            embeddings[frequent_mask] = frequent_embeddings\n",
    "\n",
    "        # Embed and project infrequent tokens\n",
    "        if infrequent_mask.any():\n",
    "            infrequent_embeddings = self.infrequent_embeddings(infrequent_indices[infrequent_mask])\n",
    "            infrequent_embeddings_projected = self.infrequent_projection(infrequent_embeddings)\n",
    "            embeddings[infrequent_mask] = infrequent_embeddings_projected\n",
    "\n",
    "        # Apply positional embeddings\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        position_embeddings = self.positional_embeddings(position_ids)  # Generate for seq_len\n",
    "\n",
    "        # Ensure positional embeddings are broadcastable to the embeddings tensor\n",
    "        # This step may not be necessary if your positional embeddings are already correctly shaped\n",
    "        if position_embeddings.size(0) != batch_size:\n",
    "            position_embeddings = position_embeddings.expand(batch_size, -1, -1)\n",
    "\n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"Positional embeddings shape: {position_embeddings.shape}\")\n",
    "        embeddings += position_embeddings\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadLinformerAttention(embed_dim=d_model, num_heads=nhead)\n",
    "        self.dropout1 = AdaptiveDropoutLayer()  # Use AdaptiveDropoutLayer\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            AdaptiveDropoutLayer(),  # Use AdaptiveDropoutLayer here as well\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.dropout2 = AdaptiveDropoutLayer()  # And here\n",
    " \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2 = self.norm1(src)\n",
    "        attn_output = self.attn(src2)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout2(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class TransformerMergingNetwork(nn.Module):\n",
    "    def __init__(self, combined_vocab, vocab_size_bpe, vocab_size_sp, vocab_size_wp, \n",
    "                 embedding_dim, nhead, dim_feedforward, freq_threshold, smaller_embed_dim, \n",
    "                 max_seq_len=512, initial_max_frequency=1000):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # Adaptive Embedding Layers\n",
    "        self.token_embedding_bpe = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab_size_bpe, \n",
    "            vocab_size=len(vocab_size_bpe),  # Size based on the combined vocabulary\n",
    "            freq_threshold=freq_threshold,\n",
    "            large_embed_dim=embedding_dim,\n",
    "            small_embed_dim=smaller_embed_dim,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.token_embedding_sp = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab_size_sp, \n",
    "            vocab_size=len(vocab_size_sp),  # Size based on the combined vocabulary\n",
    "            freq_threshold=freq_threshold,\n",
    "            large_embed_dim=embedding_dim,\n",
    "            small_embed_dim=smaller_embed_dim,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.token_embedding_wp = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab_size_wp, \n",
    "            vocab_size=len(vocab_size_wp),  # Size based on the combined vocabulary\n",
    "            freq_threshold=freq_threshold,\n",
    "            large_embed_dim=embedding_dim,\n",
    "            small_embed_dim=smaller_embed_dim,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "\n",
    "        # Positional Encoding (Adapt if your implementation differs)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(3 * max_seq_len, embedding_dim)) \n",
    "\n",
    "        # Transformer Encoder with Linformer Attention\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            TransformerEncoderLayer(\n",
    "                d_model=embedding_dim, \n",
    "                nhead=nhead, \n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=0.1  # Example dropout - may need adjustment\n",
    "            ),\n",
    "            num_layers=1  # Replace with the desired number of layers\n",
    "        )\n",
    "\n",
    "        # Output and Loss Calculation Components\n",
    "        self.output_layer = nn.Linear(embedding_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.max_frequency = initial_max_frequency  \n",
    "\n",
    "        # Frequency embedding - keep to embed frequencies\n",
    "        self.freq_embedding = nn.Embedding(self.max_frequency, embedding_dim) \n",
    "\n",
    "    def forward(self, bpe_tokens, bpe_freqs, sp_tokens, sp_freqs, wp_tokens, wp_freqs):\n",
    "        # Compute embeddings for each tokenizer's tokens\n",
    "        embedded_bpe = self.token_embedding_bpe(bpe_tokens) + self.positional_encoding[:len(bpe_tokens)]\n",
    "        embedded_sp = self.token_embedding_sp(sp_tokens) + self.positional_encoding[len(bpe_tokens):(len(bpe_tokens) + len(sp_tokens))]\n",
    "        embedded_wp = self.token_embedding_wp(wp_tokens) + self.positional_encoding[(len(bpe_tokens) + len(sp_tokens)):]\n",
    "        \n",
    "        # Embedding frequencies\n",
    "        embedded_freqs = self.freq_embedding(torch.cat((bpe_freqs, sp_freqs, wp_freqs)))\n",
    "        \n",
    "        # Combine embeddings for input to the transformer encoder\n",
    "        combined_input = torch.cat((embedded_bpe, embedded_sp, embedded_wp, embedded_freqs), dim=0)  # Adjust dim as needed\n",
    "        \n",
    "        # Pass combined embeddings through the transformer encoder\n",
    "        encoded_output = self.transformer_encoder(combined_input.unsqueeze(0))  # Adjust shape as needed\n",
    "        \n",
    "        # Compute merge weights from encoded output\n",
    "        merge_weights = self.sigmoid(self.output_layer(encoded_output.squeeze(0)))  # Adjust squeezing as needed\n",
    "        \n",
    "        # Return merge weights and the original token embeddings\n",
    "        return merge_weights, embedded_bpe, embedded_sp, embedded_wp\n",
    "\n",
    "\n",
    "    def merging_loss(self, bpe_embeddings, sp_embeddings, wp_embeddings, merge_weights, margin=1.0):\n",
    "        \"\"\"\n",
    "        Calculates the merging loss.\n",
    "\n",
    "        Args:\n",
    "            bpe_embeddings:  Embeddings for BPE tokens from the combined output of Transformers network.\n",
    "            sp_embeddings: Embeddings for SentencePiece tokens.\n",
    "            wp_embeddings: Embeddings for WordPiece tokens.\n",
    "            merge_weights: Predicted merge weights (output of the Transformer model).\n",
    "            margin: Margin for contrastive loss component.\n",
    "\n",
    "        Returns:\n",
    "            loss: The calculated merging loss.\n",
    "        \"\"\"\n",
    "\n",
    "        # Cross-Tokenizer Presence Loss\n",
    "        presence_loss = torch.mean(merge_weights**2)  # Squared weights favor high confidence merges\n",
    "\n",
    "        # Semantic Similarity Loss (using Contrastive Loss as an example)\n",
    "        similarity_loss = 0.0\n",
    "\n",
    "        for bpe_emb, sp_emb, wp_emb in zip(bpe_embeddings, sp_embeddings, wp_embeddings):\n",
    "            # Calculate pairwise distances between embeddings (cosine distance example)\n",
    "            dist_bpe_sp = 1 - torch.cosine_similarity(bpe_emb, sp_emb) \n",
    "            dist_bpe_wp = 1 - torch.cosine_similarity(bpe_emb, wp_emb)  \n",
    "            dist_sp_wp = 1 - torch.cosine_similarity(sp_emb, wp_emb) \n",
    "\n",
    "            # Contrastive terms: Encourage matching pairs, discourage non-matching\n",
    "            similarity_loss += torch.maximum(0.0, dist_bpe_sp - margin)  \n",
    "            similarity_loss += torch.maximum(0.0, dist_bpe_wp - margin)\n",
    "            similarity_loss += torch.maximum(0.0, dist_sp_wp - margin)\n",
    "\n",
    "        # Combine with appropriate weights\n",
    "        alpha = 0.6  # Example - Adjust as needed\n",
    "        total_loss = alpha * presence_loss + (1 - alpha) * similarity_loss  \n",
    "\n",
    "        return total_loss\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def combined_loss(output, target, model, merge_loss, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    # Task-specific loss\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)\n",
    "    \n",
    "    # Regularization loss\n",
    "    regularization_loss = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2 regularization\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1 regularization\n",
    "\n",
    "    # Total loss includes task loss, merge loss, and regularization loss\n",
    "    total_loss = task_loss + merge_loss + regularization_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Assuming load_corpus now returns a list of texts\n",
    "texts = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "\n",
    "# Create an instance of ContextualTokenizer and train tokenizers\n",
    "mixed_tokenizer = ContextualTokenizer(corpus=texts, vocab_size=10000, max_seq_len=512)\n",
    "mixed_tokenizer.train_tokenizers()\n",
    "\n",
    "# Tokenize the corpus and create a dataset\n",
    "tokenized_corpus = mixed_tokenizer.tokenize_corpus()\n",
    "print(f\"Contextual Tokenizer tokenize_corpus: {tokenized_corpus}\")\n",
    "tokenized_dataset = mixed_tokenizer.create_dataset(tokenized_corpus)\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size= 32)\n",
    "log_interval = 2\n",
    "num_epochs = 5\n",
    "initial_lr = 1e-4\n",
    "num_warmup_steps = 1000\n",
    "# Assuming dataloader is your DataLoader object and you're planning for 5 epochs\n",
    "num_training_steps = len(dataloader) * num_epochs\n",
    "\n",
    "optimizer = AdaptiveWeightDecayOptimizer(model.parameters(), lr=initial_lr)\n",
    "\n",
    "scheduler = cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps)\n",
    "\n",
    "# Example training loop adjustment\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss_epoch = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Assume model returns logits, merge weights, and embeddings\n",
    "        logits, merge_weights, bpe_embeddings, sp_embeddings, wp_embeddings = model(inputs)\n",
    "        \n",
    "        # Compute merge loss\n",
    "        merge_loss_value = model.merging_loss(bpe_embeddings, sp_embeddings, wp_embeddings, merge_weights)\n",
    "        \n",
    "        # Compute combined loss\n",
    "        loss = combined_loss(logits, labels, model, merge_loss_value, l2_reg_strength=0.01, l1_reg_strength=0.005)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update the learning rate\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {total_loss_epoch / (batch_idx + 1):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging: ('the', '</w>')\n",
      "Merging: ('of', '</w>')\n",
      "Merging: ('to', '</w>')\n",
      "Merging: ('and', '</w>')\n",
      "Merging: ('a', '</w>')\n",
      "Merging: ('is', '</w>')\n",
      "Merging: ('in', '</w>')\n",
      "Merging: ('that', '</w>')\n",
      "Merging: ('on', '</w>')\n",
      "Merging: ('for', '</w>')\n",
      "BPE Vocab (top 20):  [('How </w>', 1), ('transferable </w>', 1), ('are </w>', 73), ('features </w>', 59), ('in</w>', 113), ('deep </w>', 6), ('neural </w>', 21), ('networks? </w>', 1), ('Jason </w>', 1), ('Yosinski,1 </w>', 1), ('Jeff </w>', 1), ('Clune,2 </w>', 1), ('Yoshua </w>', 1), ('Bengio,3 </w>', 1), ('and</w>', 211), ('Hod </w>', 1), ('Lipson4 </w>', 1), ('1 </w>', 17), ('Dept. </w>', 4), ('Computer </w>', 3)]\n",
      "SP Vocab (top 20):  [('[', 0), ('U', 1), ('N', 2), ('K', 3), (']', 4)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 319\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBPE Vocab (top 20): \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(bpe_vocab\u001b[38;5;241m.\u001b[39mitems())[:\u001b[38;5;241m20\u001b[39m])\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSP Vocab (top 20): \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(sp_vocab\u001b[38;5;241m.\u001b[39mitems())[:\u001b[38;5;241m20\u001b[39m])\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBPE Encoding: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mbpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSP Encoding: \u001b[39m\u001b[38;5;124m\"\u001b[39m, sp\u001b[38;5;241m.\u001b[39m_encode(texts))\n\u001b[0;32m    323\u001b[0m merged_vocab \u001b[38;5;241m=\u001b[39m merge_vocabularies(bpe_vocab, sp_vocab, \n\u001b[0;32m    324\u001b[0m                                   strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintersection\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# Or \"union\" \u001b[39;00m\n\u001b[0;32m    325\u001b[0m                                   resolve_conflict\u001b[38;5;241m=\u001b[39mprioritize_bpe) \n",
      "Cell \u001b[1;32mIn[58], line 152\u001b[0m, in \u001b[0;36mBPE.encode\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word_chars), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    151\u001b[0m     subword \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(word_chars[:i]) \n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subword \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab:\n\u001b[0;32m    153\u001b[0m         subwords\u001b[38;5;241m.\u001b[39mappend(subword)\n\u001b[0;32m    154\u001b[0m         word_chars \u001b[38;5;241m=\u001b[39m word_chars[i:]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = [line.strip() for line in file.readlines()]\n",
    "    return texts\n",
    "\n",
    "texts = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "        self.frequency = 0  # Track frequency of subwords for simplified ranking\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id, frequency):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "        node.frequency = frequency  # Assign frequency at the leaf node\n",
    "\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        \"\"\"\n",
    "        Finds the most probable subwords for a given token based on frequency.\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        best_subwords = []\n",
    "\n",
    "        def dfs(current_node, subword='', collected_subwords=[]):\n",
    "            if current_node.token_id is not None:\n",
    "                # Prevent zero division\n",
    "                total_frequency = sum(node.frequency for node in collected_subwords)\n",
    "                if total_frequency > 0:  \n",
    "                    probability = current_node.frequency / total_frequency\n",
    "                else:\n",
    "                    probability = 0  # Default probability or handle differently\n",
    "\n",
    "                collected_subwords.append(current_node) \n",
    "            for char, next_node in current_node.children.items():\n",
    "                dfs(next_node, subword + char, collected_subwords)\n",
    "\n",
    "        dfs(node)\n",
    "        # Instead of selecting based purely on frequency, consider the 'probability'\n",
    "        best_subwords = sorted(best_subwords, key=lambda x: x[1], reverse=True)\n",
    "        return [subword[2] for subword in best_subwords][:5] if best_subwords else [self.unk_token_id]\n",
    "\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "    def compute_failure_links(self):\n",
    "        root = self.root\n",
    "        root.failure_link = root  # The root's failure link points to itself\n",
    "        queue = [root]\n",
    "\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)\n",
    "\n",
    "                # Set the failure link for the child_node\n",
    "                if current_node == root:\n",
    "                    child_node.failure_link = root\n",
    "                else:\n",
    "                    # Follow the current node's failure link to find the longest suffix for the child_node\n",
    "                    failure_candidate = current_node.failure_link\n",
    "                    while failure_candidate != root and char not in failure_candidate.children:\n",
    "                        failure_candidate = failure_candidate.failure_link\n",
    "                    child_node.failure_link = failure_candidate.children.get(char, root)\n",
    "\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None\n",
    "        self.failure_pops = None\n",
    "        self.vocab = {}  # Initialize vocab as an empty dictionary\n",
    "\n",
    "    def train(self, corpus):\n",
    "        self.vocab = self._build_vocabulary(corpus)\n",
    "        self._precompute(self.vocab) \n",
    "        return self.vocab\n",
    "\n",
    "    def _encode(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes input text into subwords, using a trie to find the most probable subword sequences.\n",
    "        \"\"\"\n",
    "        tokens = text.split()  # Simplified tokenization\n",
    "        encoded_tokens = []\n",
    "        for token in tokens:\n",
    "            subword_ids = self.trie.find_subwords(token)\n",
    "            encoded_tokens.extend(subword_ids)\n",
    "        return encoded_tokens\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        # Adjusted to handle the corpus as a list\n",
    "        if model_type == \"unigram\":\n",
    "            # Aggregate tokens from all texts in the corpus\n",
    "            all_tokens = []\n",
    "            for text in corpus:\n",
    "                tokens = self._unigram_tokenize(text)\n",
    "                all_tokens.extend(tokens)\n",
    "            vocabulary = self._build_unigram_vocab(all_tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            # Assuming a similar adjustment for BPE if necessary\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _unigram_tokenize(self, text):\n",
    "        # Now properly handles a single text/string\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        char_freqs = collections.Counter()\n",
    "        for token in tokens:\n",
    "            for char in token:  \n",
    "                char_freqs[char] += 1\n",
    "\n",
    "        vocab = {char: idx for idx, (char, _) in enumerate(char_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Initialize the trie\n",
    "        self.trie = Trie()\n",
    "        for token_id, (token, frequency) in enumerate(vocabulary.items()):\n",
    "            # Insert tokens into the trie\n",
    "            self.trie.insert(token, token_id, frequency)\n",
    "\n",
    "        # Compute failure links after the trie has been fully constructed\n",
    "        self.trie.compute_failure_links() \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            if not pairs:  # Check if there are no more pairs to merge\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(\"Merging: {}\".format(best))  # Track most frequent pair in each iteration\n",
    "\n",
    "        return self.vocab\n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies, tokenized at the character level.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.strip().split()\n",
    "            for word in words:\n",
    "                # No longer splitting the word into characters with spaces\n",
    "                vocab[word + \" </w>\"] += 1  # Marking end of word with </w>\n",
    "        return vocab\n",
    "\n",
    "    def preprocess_to_characters(self, word):\n",
    "        return list(word)\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "\n",
    "def merge_vocabularies(bpe_vocab, sp_vocab, strategy=\"intersection\", resolve_conflict=None):\n",
    "    \"\"\"\n",
    "    Merges BPE and SentencePiece vocabularies.\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: BPE vocabulary (dict: subword -> index)\n",
    "        sp_vocab: SentencePiece vocabulary (dict: subword -> index)\n",
    "        strategy: \"intersection\" or \"union\"\n",
    "        resolve_conflict: Function to determine priority when the same subword exists in both.\n",
    "\n",
    "    Returns:\n",
    "        dict: The merged vocabulary \n",
    "    \"\"\"\n",
    "\n",
    "    if strategy == \"intersection\":\n",
    "        merged_vocab = set(bpe_vocab.keys()) & set(sp_vocab.keys())\n",
    "    elif strategy == \"union\":\n",
    "        merged_vocab = set(bpe_vocab.keys()) | set(sp_vocab.keys())\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Choose 'intersection' or 'union'\")\n",
    "\n",
    "    # Handle potential conflicts with a resolution function \n",
    "    if resolve_conflict:\n",
    "        final_vocab = {}\n",
    "        for subword in merged_vocab:\n",
    "            if subword in bpe_vocab and subword in sp_vocab:\n",
    "                final_vocab[subword] = resolve_conflict(subword, bpe_vocab[subword], sp_vocab[subword])\n",
    "            else:\n",
    "                final_vocab[subword] = bpe_vocab.get(subword, sp_vocab[subword])\n",
    "        return final_vocab\n",
    "\n",
    "    return {subword: idx for idx, subword in enumerate(merged_vocab)}\n",
    "\n",
    "# Example for BPE-first conflict resolution   \n",
    "def prioritize_bpe(subword, bpe_index, sp_index):\n",
    "    return bpe_index \n",
    "\n",
    "\n",
    "bpe = BPE()  \n",
    "bpe_vocab = bpe.train(texts)  # Now actually performs BPE training\n",
    "\n",
    "sp = SentencePiece()\n",
    "sp_vocab = sp.train(texts)   # SentencePiece training is executed\n",
    "\n",
    "print(\"BPE Vocab (top 20): \", list(bpe_vocab.items())[:20])\n",
    "print(\"SP Vocab (top 20): \", list(sp_vocab.items())[:20])\n",
    "\n",
    "print(\"BPE Encoding: \", bpe.encode(texts))\n",
    "print(\"SP Encoding: \", sp._encode(texts))\n",
    "\n",
    "\n",
    "merged_vocab = merge_vocabularies(bpe_vocab, sp_vocab, \n",
    "                                  strategy=\"intersection\", # Or \"union\" \n",
    "                                  resolve_conflict=prioritize_bpe) \n",
    "\n",
    "\n",
    "\n",
    "print(merged_vocab)\n",
    "\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        output_tokens = []\n",
    "\n",
    "        for word in text.strip().split():\n",
    "            if len(word) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            if word in self.vocab:\n",
    "                output_tokens.append(word)\n",
    "            else:\n",
    "                is_bad = False  # Flag if no suitable subwords are found\n",
    "                start = 0\n",
    "                sub_tokens = []\n",
    "\n",
    "                while start < len(word):  \n",
    "                    end = len(word) \n",
    "                    cur_substr = None\n",
    "                    while start < end: \n",
    "                        substr = \"##\" + word[start:end]  # \"##\" prefix convention\n",
    "                        if substr in self.vocab:\n",
    "                            cur_substr = substr\n",
    "                            break\n",
    "                        end -= 1 \n",
    "                    \n",
    "                    if cur_substr is None: \n",
    "                        is_bad = True\n",
    "                        break\n",
    "                                        \n",
    "                    sub_tokens.append(cur_substr)\n",
    "                    start = end \n",
    "\n",
    "                if is_bad:\n",
    "                    output_tokens.append(self.unk_token)\n",
    "                else:\n",
    "                    output_tokens.extend(sub_tokens) \n",
    "\n",
    "        return output_tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[': 0, 'U': 1, 'N': 2, 'K': 3, ']': 4}\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None\n",
    "        self.frequency = 0\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self, unk_token_id=0):\n",
    "        self.root = TrieNode()\n",
    "        self.unk_token_id = unk_token_id\n",
    "\n",
    "    def insert(self, token, token_id, frequency):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "        node.frequency = frequency\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        \"\"\"Finds the most probable subwords based on frequency.\"\"\"\n",
    "        node = self.root\n",
    "        best_subwords = []\n",
    "\n",
    "        def dfs(current_node, subword='', collected_subwords=[]):\n",
    "            if current_node.token_id is not None:\n",
    "                # Update to correctly calculate total_frequency based on the structure of collected_subwords\n",
    "                total_frequency = sum(n.frequency for _, _, n in collected_subwords) + current_node.frequency\n",
    "                probability = current_node.frequency / total_frequency if total_frequency else 0\n",
    "                collected_subwords.append((subword, probability, current_node))\n",
    "\n",
    "            for char, next_node in current_node.children.items():\n",
    "                dfs(next_node, subword + char, list(collected_subwords))  # Create a copy of the list to avoid shared state\n",
    "\n",
    "        dfs(node)\n",
    "        best_subwords = sorted(best_subwords, key=lambda x: x[1], reverse=True)\n",
    "        return [subword for subword, _, _ in best_subwords][:5] or [self.unk_token_id]\n",
    "\n",
    "\n",
    "    def compute_failure_links(self):\n",
    "        root = self.root\n",
    "        root.failure_link = root  # Root's failure link points to itself\n",
    "        queue = [root]\n",
    "\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)\n",
    "\n",
    "                # Follow failure link to find the longest suffix for the child_node\n",
    "                failure_candidate = current_node.failure_link\n",
    "                while failure_candidate != root and char not in failure_candidate.children:\n",
    "                    failure_candidate = failure_candidate.failure_link\n",
    "                child_node.failure_link = failure_candidate.children.get(char, root)\n",
    "\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self, unk_token=\"[UNK]\"):\n",
    "        self.trie = None\n",
    "        self.vocab = {}\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_token_id = 0  # Assign an ID for the unknown token\n",
    "\n",
    "    def train(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        self.vocab = self._build_vocabulary(corpus, vocab_size, model_type)        \n",
    "        self._precompute(self.vocab)\n",
    "        return self.vocab\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encodes input text into subwords based on the trained model.\"\"\"\n",
    "        tokens = text.split() \n",
    "        encoded_tokens = []\n",
    "        for token in tokens:\n",
    "            subword_ids = self.trie.find_subwords(token)\n",
    "            encoded_tokens.extend(subword_ids)\n",
    "        return encoded_tokens\n",
    "\n",
    "    def decode(self, subword_ids):\n",
    "        \"\"\"Reconstructs text from a sequence of subword IDs.\"\"\"\n",
    "        text = \"\"\n",
    "        for subword_id in subword_ids:\n",
    "            if subword_id != self.unk_token_id:\n",
    "                text += self.vocab.get(subword_id, self.unk_token) \n",
    "            else:\n",
    "                text += self.unk_token  \n",
    "        return text\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size, model_type):\n",
    "        \"\"\"Constructs the SentencePiece vocabulary.\"\"\"\n",
    "        vocabulary = {}  # Use a dictionary for clarity\n",
    "        if model_type == \"unigram\":\n",
    "            all_tokens = []\n",
    "            for text in corpus:\n",
    "                tokens = self._unigram_tokenize(text)\n",
    "                all_tokens.extend(tokens)\n",
    "            vocabulary = self._build_unigram_vocab(all_tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            # Implement BPE vocabulary building if needed\n",
    "            pass \n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        char_freqs = collections.Counter()\n",
    "        for token in tokens:\n",
    "            for char in token:\n",
    "                char_freqs[char] += 1\n",
    "\n",
    "        vocabulary = {char: idx for idx, (char, _) in enumerate(char_freqs.most_common(vocab_size))}\n",
    "        return vocabulary\n",
    "\n",
    "    def _unigram_tokenize(self, text):\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                adjusted_tokens.append(self.unk_token)\n",
    "        return adjusted_tokens\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        \"\"\"Precomputes data structures for efficient subword segmentation.\"\"\"\n",
    "        self.trie = Trie(self.unk_token_id)\n",
    "        for token_id, (token, frequency) in enumerate(vocabulary.items()):\n",
    "            self.trie.insert(token, token_id, frequency)\n",
    "        self.trie.compute_failure_links() \n",
    "\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            if not pairs:  # Check if there are no more pairs to merge\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(\"Merging: {}\".format(best))  # Track most frequent pair in each iteration\n",
    "\n",
    "        return self.vocab\n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies, tokenized at the character level.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.strip().split()\n",
    "            for word in words:\n",
    "                # No longer splitting the word into characters with spaces\n",
    "                vocab[word + \" </w>\"] += 1  # Marking end of word with </w>\n",
    "        return vocab\n",
    "\n",
    "    def preprocess_to_characters(self, word):\n",
    "        return list(word)\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "\n",
    "my_sp = SentencePiece()\n",
    "\n",
    "# Sample corpus (pre-tokenization may need adjustment)\n",
    "text_data = [\n",
    "    \"How transferable are features in deep neural networks?\",\n",
    "    \"This is another sample.\"\n",
    "]\n",
    "\n",
    "model = my_sp.train(text_data) \n",
    "print(my_sp.vocab)\n",
    "# Example encoding\n",
    "result = my_sp.encode(\"transfer\") \n",
    "print(result)  # This might output something like ['t', 'r', 'a', 'n', 'sf', 'er']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SP & BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence: [0, 0, 28, 50, 26]\n",
      "Decoded Sentence: SentencePiece  SentencePiece  subword  tokenization  .\n",
      "Re-Encoded Sentence: [0, 0, 28, 50, 26]\n",
      "Re-Decoded Sentence: SentencePiece  SentencePiece  subword  tokenization  .\n",
      "Sample Vocabulary Check:\n",
      "SentencePiece: 0\n",
      "is: 1\n",
      "an: 2\n",
      "unsupervised: 3\n",
      "text: 4\n",
      "tokenizer: 5\n",
      "and: 6\n",
      "detokenizer: 7\n",
      "mainly: 8\n",
      "for: 9\n",
      "Found 0 subtokens in vocabulary.\n",
      "Trie Construction Completed Successfully\n",
      "Trie built successfully.\n",
      "Token found: 'SentencePiece'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'provide'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'subword'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'to'\n",
      "Character 'k' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Character 'z' not found, adding [UNK] and resetting\n",
      "Token found: 'a'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Character 'n' not found, adding [UNK] and resetting\n",
      "Token found: '.'\n",
      "Tokenized Sentence: ['SentencePiece', 'provide', 'subword', 'to', '[UNK]', 'e', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import json\n",
    "\n",
    "class SimpleSentencePiece:\n",
    "    def __init__(self, model_type=\"bpe\", vocab_size=8000):\n",
    "        self.vocab = {}\n",
    "        self.id_to_subword = {}\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.unk_token_id = 0\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model = None if model_type == \"bpe\" else None  # Placeholder for future model type implementations\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def train(self, text):\n",
    "        if self.model_type == \"bpe\":\n",
    "            self.model = BPE(num_merges=self.vocab_size, unk_token_id=self.unk_token_id) # Assuming you want to use vocab_size as num_merges\n",
    "            self.model.train(text)\n",
    "            self.vocab = self.model.vocab\n",
    "            self.id_to_subword = {i: word for word, i in self.vocab.items()}\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported yet.\")\n",
    "\n",
    "        \n",
    "    def encode(self, text):\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        return self.model.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if not self.id_to_subword:\n",
    "            raise ValueError(\"Vocabulary is empty. Ensure the model is trained first.\")\n",
    "        # Reconstruct the text and remove the end-of-word marker '</w>'\n",
    "        text = \" \".join([self.id_to_subword.get(id_, self.unk_token) for id_ in ids])\n",
    "        text = text.replace(\" </w>\", \"\").replace(\"</w>\", \" \").strip()\n",
    "        return text\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        if self.model_type == \"bpe\":\n",
    "            model_data = {\n",
    "                'vocab': self.vocab,\n",
    "                'merges': self.model.merges,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'model_type': self.model_type\n",
    "            }\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(model_data, f)\n",
    "                \n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "        self.vocab = model_data['vocab']\n",
    "        self.id_to_subword = {i: word for word, i in self.vocab.items()}\n",
    "        self.vocab_size = model_data.get('vocab_size', 8000)\n",
    "        if model_data['model_type'] == \"bpe\":\n",
    "            # Correctly initialize the BPE model with num_merges instead of vocab_size\n",
    "            self.model = BPE(num_merges=self.vocab_size, unk_token_id=self.unk_token_id)\n",
    "            self.model.merges = model_data['merges']\n",
    "            # Since vocab is built based on merges, we need to rebuild it or ensure it's correctly loaded\n",
    "            self.model.vocab = self.vocab\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {model_data['model_type']} not supported yet.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, num_merges=100, unk_token_id=0):  # Accept unk_token_id parameter\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        self.num_merges = num_merges\n",
    "        self.unk_token_id = unk_token_id  # Store the unknown token ID\n",
    "\n",
    "    def train(self, text):\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "        vocab = collections.Counter(words)\n",
    "        vocab = {word + '</w>': count for word, count in vocab.items()}\n",
    "        \n",
    "        for _ in range(self.num_merges):  # Use the num_merges from the instance variable\n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best, vocab)\n",
    "            self.merges.append(best)\n",
    "\n",
    "        self.vocab = {word: i for i, word in enumerate(vocab.keys())}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stats(vocab):\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[symbols[i], symbols[i+1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_vocab(pair, vocab):\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in vocab:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = vocab[word]\n",
    "        return v_out\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text into subwords using learned BPE merges.\"\"\"\n",
    "        encoded_tokens = []\n",
    "        for word in re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE):\n",
    "            word += '</w>'\n",
    "            subwords = [word]  # Start with the entire word as one subword\n",
    "            for merge in self.merges:\n",
    "                new_subwords = []\n",
    "                for subword in subwords:\n",
    "                    # If the merge is in subword, split it; otherwise, keep it as is\n",
    "                    if ' '.join(merge) in subword:\n",
    "                        new_subwords.extend(subword.replace(' '.join(merge), ''.join(merge)).split(' '))\n",
    "                    else:\n",
    "                        new_subwords.append(subword)\n",
    "                subwords = new_subwords\n",
    "            encoded_tokens.extend(subwords)\n",
    "        return [self.vocab.get(token, self.unk_token_id) for token in encoded_tokens]\n",
    "    \n",
    "        # New method to save trained model\n",
    "    def save_model(self, filepath):\n",
    "        with open(filepath, 'w') as file:\n",
    "            for merge in self.merges:\n",
    "                file.write(' '.join(merge) + '\\n')\n",
    "    \n",
    "    # New method to load trained model\n",
    "    def load_model(self, filepath):\n",
    "        self.merges = []\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                pair = tuple(line.strip().split(' '))\n",
    "                self.merges.append(pair)\n",
    "\n",
    "\n",
    "\n",
    "class WordPiece:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.root = self.build_trie(vocab)\n",
    "        self.compute_failure_links(self.root)\n",
    "        print(\"Trie built successfully.\")\n",
    "\n",
    "    # Add debug prints to build_trie to confirm structure\n",
    "    def build_trie(self, vocab):\n",
    "        root = TrieNode()\n",
    "        for token in vocab:\n",
    "            node = root\n",
    "            for char in token:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.is_end = True\n",
    "            node.token = token\n",
    "        print(\"Trie Construction Completed Successfully\")\n",
    "        return root\n",
    "\n",
    "\n",
    "    def compute_failure_links(self, root):\n",
    "        queue = [root]\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "            for char, child_node in current_node.children.items():\n",
    "                failure_node = current_node.failure_link\n",
    "                while failure_node and char not in failure_node.children:\n",
    "                    failure_node = failure_node.failure_link\n",
    "                child_node.failure_link = failure_node.children[char] if failure_node else root\n",
    "                queue.append(child_node)\n",
    "\n",
    "    # Improved debug prints in tokenize method\n",
    "    def tokenize(self, text):\n",
    "        node = self.root\n",
    "        tokens = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(text):\n",
    "            char = text[i]\n",
    "            if char == ' ':\n",
    "                # Handle space as a delimiter\n",
    "                print(\"Space encountered, resetting to root\")\n",
    "                node = self.root  # Reset to root\n",
    "                i += 1  # Move to the next character\n",
    "                continue\n",
    "\n",
    "            if char not in node.children:\n",
    "                print(f\"Character '{char}' not found, adding [UNK] and resetting\")\n",
    "                tokens.append(self.unk_token)  # Add [UNK] for unmatched character\n",
    "                node = self.root  # Reset to root\n",
    "                i += 1  # Move to the next character\n",
    "                continue  # Start fresh from the next character\n",
    "\n",
    "            node = node.children[char]\n",
    "\n",
    "            if node.is_end:\n",
    "                print(f\"Token found: '{node.token}'\")\n",
    "                tokens.append(node.token)  # Add the token found\n",
    "                node = self.root  # Reset for the next token\n",
    "\n",
    "            i += 1  # Move to the next character\n",
    "\n",
    "        return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 'known_words': SentencePiece is an unsupervised tokenizer.\n",
      "Token found: 'SentencePiece'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'is'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'a'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'unsupervised'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'to'\n",
      "Character 'k' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Character 'z' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Character '.' not found, adding [UNK] and resetting\n",
      "Tokenized: ['SentencePiece', 'is', 'a', 'unsupervised', 'to', '[UNK]', 'e', '[UNK]', '[UNK]', 'e', '[UNK]']\n",
      "\n",
      "Testing 'subwords': Subword tokenization improves neural model performance.\n",
      "Character 'u' not found, adding [UNK] and resetting\n",
      "Character 'w' not found, adding [UNK] and resetting\n",
      "Character 'r' not found, adding [UNK] and resetting\n",
      "Space encountered, resetting to root\n",
      "Token found: 'to'\n",
      "Character 'k' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Character 'z' not found, adding [UNK] and resetting\n",
      "Token found: 'a'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Character 'n' not found, adding [UNK] and resetting\n",
      "Space encountered, resetting to root\n",
      "Character 'r' not found, adding [UNK] and resetting\n",
      "Character 'v' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'neural'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'model'\n",
      "Space encountered, resetting to root\n",
      "Character 'e' not found, adding [UNK] and resetting\n",
      "Character 'f' not found, adding [UNK] and resetting\n",
      "Character 'r' not found, adding [UNK] and resetting\n",
      "Character 'n' not found, adding [UNK] and resetting\n",
      "Character 'e' not found, adding [UNK] and resetting\n",
      "Token found: '.'\n",
      "Tokenized: ['[UNK]', '[UNK]', '[UNK]', 'to', '[UNK]', 'e', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'e', 'neural', 'model', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '.']\n",
      "\n",
      "Testing 'mixed_sentence': Tokenizing with SentencePiece and BPE provides flexibility.\n",
      "Character 'T' not found, adding [UNK] and resetting\n",
      "Character 'k' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Character 'z' not found, adding [UNK] and resetting\n",
      "Token found: 'in'\n",
      "Token found: 'g'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'with'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'SentencePiece'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'a'\n",
      "Character 'd' not found, adding [UNK] and resetting\n",
      "Space encountered, resetting to root\n",
      "Token found: 'BPE'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'provide'\n",
      "Space encountered, resetting to root\n",
      "Character 'l' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Character 'x' not found, adding [UNK] and resetting\n",
      "Character 'b' not found, adding [UNK] and resetting\n",
      "Character 'l' not found, adding [UNK] and resetting\n",
      "Character 't' not found, adding [UNK] and resetting\n",
      "Character 'y' not found, adding [UNK] and resetting\n",
      "Token found: '.'\n",
      "Tokenized: ['[UNK]', '[UNK]', 'e', '[UNK]', '[UNK]', 'in', 'g', 'with', 'SentencePiece', 'a', '[UNK]', 'BPE', 'provide', '[UNK]', 'e', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '.']\n",
      "\n",
      "Testing 'case_sensitivity': sentencepiece provides SubWord Tokenization.\n",
      "Character 'p' not found, adding [UNK] and resetting\n",
      "Character 'e' not found, adding [UNK] and resetting\n",
      "Character 'e' not found, adding [UNK] and resetting\n",
      "Space encountered, resetting to root\n",
      "Token found: 'provide'\n",
      "Space encountered, resetting to root\n",
      "Character 'u' not found, adding [UNK] and resetting\n",
      "Character 'W' not found, adding [UNK] and resetting\n",
      "Character 'r' not found, adding [UNK] and resetting\n",
      "Space encountered, resetting to root\n",
      "Character 'T' not found, adding [UNK] and resetting\n",
      "Character 'k' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Character 'z' not found, adding [UNK] and resetting\n",
      "Token found: 'a'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Character 'n' not found, adding [UNK] and resetting\n",
      "Token found: '.'\n",
      "Tokenized: ['[UNK]', '[UNK]', '[UNK]', 'provide', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'e', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '.']\n",
      "\n",
      "Testing 'punctuation_handling': Tokenization, especially for NLP, is crucial.\n",
      "Character 'T' not found, adding [UNK] and resetting\n",
      "Character 'k' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Character 'z' not found, adding [UNK] and resetting\n",
      "Token found: 'a'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Character 'n' not found, adding [UNK] and resetting\n",
      "Token found: ','\n",
      "Space encountered, resetting to root\n",
      "Token found: 'e'\n",
      "Character 'p' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Token found: 'a'\n",
      "Character 'l' not found, adding [UNK] and resetting\n",
      "Character 'y' not found, adding [UNK] and resetting\n",
      "Space encountered, resetting to root\n",
      "Token found: 'for'\n",
      "Space encountered, resetting to root\n",
      "Character 'L' not found, adding [UNK] and resetting\n",
      "Character 'P' not found, adding [UNK] and resetting\n",
      "Token found: ','\n",
      "Space encountered, resetting to root\n",
      "Token found: 'is'\n",
      "Space encountered, resetting to root\n",
      "Character 'r' not found, adding [UNK] and resetting\n",
      "Character 'c' not found, adding [UNK] and resetting\n",
      "Character 'a' not found, adding [UNK] and resetting\n",
      "Character '.' not found, adding [UNK] and resetting\n",
      "Tokenized: ['[UNK]', '[UNK]', 'e', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', ',', 'e', '[UNK]', 'e', '[UNK]', 'a', '[UNK]', '[UNK]', 'for', '[UNK]', '[UNK]', ',', 'is', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n",
      "\n",
      "Testing 'complex_subword': Neural networks benefit from subword-level processing.\n",
      "Token found: 'Neural'\n",
      "Space encountered, resetting to root\n",
      "Character 't' not found, adding [UNK] and resetting\n",
      "Character 'o' not found, adding [UNK] and resetting\n",
      "Character 'k' not found, adding [UNK] and resetting\n",
      "Space encountered, resetting to root\n",
      "Character 'n' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Character 'i' not found, adding [UNK] and resetting\n",
      "Space encountered, resetting to root\n",
      "Token found: 'from'\n",
      "Space encountered, resetting to root\n",
      "Token found: 'subword'\n",
      "Token found: '-'\n",
      "Character 'e' not found, adding [UNK] and resetting\n",
      "Character 'e' not found, adding [UNK] and resetting\n",
      "Space encountered, resetting to root\n",
      "Character 'c' not found, adding [UNK] and resetting\n",
      "Token found: 'e'\n",
      "Character 's' not found, adding [UNK] and resetting\n",
      "Token found: 'in'\n",
      "Token found: 'g'\n",
      "Token found: '.'\n",
      "Tokenized: ['Neural', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'e', '[UNK]', 'from', 'subword', '-', '[UNK]', '[UNK]', '[UNK]', 'e', '[UNK]', 'in', 'g', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample text for training the model\n",
    "sample_text = \"\"\"\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer \n",
    "mainly for Neural Network-based text generation systems where the \n",
    "vocabulary size is predetermined prior to the neural model training. \n",
    "SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) \n",
    "and unigram language model) with the extension of direct training from \n",
    "raw sentences. Compared to other subword tokenization methods, \n",
    "SentencePiece can provide a better tokenization in some cases.\n",
    "\"\"\"\n",
    "\n",
    "# Number of merges for BPE; adjust based on your needs\n",
    "num_merges = 100\n",
    "\n",
    "# Initialize and train the SimpleSentencePiece model with BPE\n",
    "ssp = SimpleSentencePiece(model_type=\"bpe\", vocab_size=1000)\n",
    "ssp.train(sample_text)\n",
    "\n",
    "# Encode a sample sentence\n",
    "encoded_sentence = ssp.encode(\"SentencePiece provides subword tokenization.\")\n",
    "print(\"Encoded Sentence:\", encoded_sentence)\n",
    "\n",
    "# Decode the encoded sentence\n",
    "decoded_sentence = ssp.decode(encoded_sentence)\n",
    "print(\"Decoded Sentence:\", decoded_sentence)\n",
    "\n",
    "# Save the trained model\n",
    "ssp.save_model(\"ssp_model.json\")\n",
    "\n",
    "# Create a new instance and load the model\n",
    "new_ssp = SimpleSentencePiece(model_type=\"bpe\")\n",
    "new_ssp.load_model(\"ssp_model.json\")\n",
    "\n",
    "# Re-encode and decode using the loaded model\n",
    "re_encoded_sentence = new_ssp.encode(\"SentencePiece provides subword tokenization.\")\n",
    "re_decoded_sentence = new_ssp.decode(re_encoded_sentence)\n",
    "print(\"Re-Encoded Sentence:\", re_encoded_sentence)\n",
    "print(\"Re-Decoded Sentence:\", re_decoded_sentence)\n",
    "\n",
    "\n",
    "# Assuming ssp is your SimpleSentencePiece instance\n",
    "vocab = ssp.vocab  # This gets the vocabulary after BPE training\n",
    "\n",
    "def adapt_vocab_for_wordpiece(ssp_vocab):\n",
    "    adapted_vocab = {}\n",
    "    for token, id_or_freq in ssp_vocab.items():\n",
    "        # Check if a token is a continuation subword and not a standalone word\n",
    "        # Since BPE might not mark subwords in a way WordPiece expects, we adapt based on our best approximation\n",
    "        if not token.startswith(\" \") and not token.endswith(\"</w>\"):\n",
    "            adapted_token = \"##\" + token.replace(\"</w>\", \"\")  # Removing BPE's end-of-word marker and prepending \"##\"\n",
    "        else:\n",
    "            adapted_token = token.replace(\"</w>\", \"\")  # Just remove the BPE's end-of-word marker for standalone words\n",
    "\n",
    "        adapted_vocab[adapted_token] = id_or_freq\n",
    "    return adapted_vocab\n",
    "\n",
    "\n",
    "# Assuming ssp is your SimpleSentencePiece instance after BPE training\n",
    "wordpiece_vocab = adapt_vocab_for_wordpiece(ssp.vocab)\n",
    "\n",
    "# Debugging step to ensure vocabulary completeness\n",
    "def debug_vocab(adapted_vocab):\n",
    "    print(\"Sample Vocabulary Check:\")\n",
    "    # Iterate over the first 10 key-value pairs in the adapted vocabulary\n",
    "    for i, (token, id_or_freq) in enumerate(adapted_vocab.items()):\n",
    "        print(f\"{token}: {id_or_freq}\")\n",
    "        if i >= 9:  # Stop after printing 10 entries\n",
    "            break\n",
    "    # Specifically check for subtokens if your tokenizer expects them\n",
    "    subtokens = [token for token in adapted_vocab.keys() if token.startswith(\"##\")]\n",
    "    print(f\"Found {len(subtokens)} subtokens in vocabulary.\")\n",
    "\n",
    "\n",
    "# Ensure wordpiece_vocab is a list of vocabulary tokens\n",
    "debug_vocab(wordpiece_vocab)  # Call this after initializing wordpiece_vocab\n",
    "\n",
    "\n",
    "\n",
    "# Initialize WordPiece with the adapted vocabulary\n",
    "wordpiece = WordPiece(wordpiece_vocab)\n",
    "\n",
    "# Tokenize a sample sentence using WordPiece\n",
    "sample_sentence = \"SentencePiece provides subword tokenization.\"\n",
    "tokenized_sentence = wordpiece.tokenize(sample_sentence)\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "\n",
    "\n",
    "# Creating a dictionary of test sentences as outlined in the instructions\n",
    "\n",
    "test_sentences = {\n",
    "    \"known_words\": \"SentencePiece is an unsupervised tokenizer.\",\n",
    "    \"subwords\": \"Subword tokenization improves neural model performance.\",\n",
    "    \"mixed_sentence\": \"Tokenizing with SentencePiece and BPE provides flexibility.\",\n",
    "    \"case_sensitivity\": \"sentencepiece provides SubWord Tokenization.\",\n",
    "    \"punctuation_handling\": \"Tokenization, especially for NLP, is crucial.\",\n",
    "    \"complex_subword\": \"Neural networks benefit from subword-level processing.\"\n",
    "}\n",
    "\n",
    "# Display the dictionary for confirmation\n",
    "test_sentences\n",
    "\n",
    "for label, sentence in test_sentences.items():\n",
    "    print(f\"Testing '{label}': {sentence}\")\n",
    "    tokenized = wordpiece.tokenize(sentence)\n",
    "    print(f\"Tokenized: {tokenized}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# probsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence: [0, 0, 28, 50, 26]\n",
      "Decoded Sentence: SentencePiece  SentencePiece  subword  tokenization  .\n",
      "Re-Encoded Sentence: [0, 0, 28, 50, 26]\n",
      "Re-Decoded Sentence: [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "Sample Vocabulary Check:\n",
      "SentencePiece: 0\n",
      "is: 1\n",
      "an: 2\n",
      "unsupervised: 3\n",
      "text: 4\n",
      "tokenizer: 5\n",
      "and: 6\n",
      "detokenizer: 7\n",
      "mainly: 8\n",
      "for: 9\n",
      "Found 0 subtokens in vocabulary.\n",
      "Trie Construction Completed Successfully\n",
      "Trie built successfully.\n",
      "Tokenized Sentence: [None, None, None, 'e', None, 'e', 'provide', 'subword', 'tokenization', '.']\n",
      "Testing 'known_words': SentencePiece is an unsupervised tokenizer.\n",
      "Tokenized: [None, None, None, 'e', None, 'e', 'is', 'an', 'unsupervised', 'tokenizer', '.']\n",
      "\n",
      "Testing 'subwords': Subword tokenization improves neural model performance.\n",
      "Tokenized: ['subword', 'tokenization', None, None, None, None, 'e', 'neural', 'model', None, 'e', None, 'for', None, None, None, 'e', '.']\n",
      "\n",
      "Testing 'mixed_sentence': Tokenizing with SentencePiece and BPE provides flexibility.\n",
      "Tokenized: [None, 'in', 'g', 'with', None, None, None, 'e', None, 'e', 'and', None, None, 'e', 'provide', None, None, None, None, None, None, None, None, None, '[UNK]', '.']\n",
      "\n",
      "Testing 'case_sensitivity': sentencepiece provides SubWord Tokenization.\n",
      "Tokenized: [None, None, None, 'e', None, 'e', 'provide', 'subword', 'tokenization', '.']\n",
      "\n",
      "Testing 'punctuation_handling': Tokenization, especially for NLP, is crucial.\n",
      "Tokenized: ['tokenization', ',', 'e', None, None, 'e', None, None, 'a', None, None, '[UNK]', 'for', None, None, ',', 'is', None, None, None, None, None, 'a', '.']\n",
      "\n",
      "Testing 'complex_subword': Neural networks benefit from subword-level processing.\n",
      "Tokenized: ['neural', None, None, None, None, None, '[UNK]', None, None, None, None, 'from', 'subword', '-', None, 'e', None, 'e', None, None, 'e', None, None, None, 'g', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import json\n",
    "class SimpleSentencePiece:\n",
    "    def __init__(self, model_type=\"bpe\", vocab_size=8000):\n",
    "        self.vocab = {}\n",
    "        self.id_to_subword = {}\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.unk_token_id = 0\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model = None if model_type == \"bpe\" else None\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def train(self, text):\n",
    "        if self.model_type == \"bpe\":\n",
    "            self.model = BPE(num_merges=self.vocab_size, unk_token_id=self.unk_token_id)\n",
    "            self.model.train(text)\n",
    "            self.vocab = self.model.vocab\n",
    "            self.id_to_subword = {i: word for word, i in self.vocab.items()}\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported yet.\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.preprocess_text(text)  # Preprocess text before encoding\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        return self.model.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if not self.id_to_subword:\n",
    "            raise ValueError(\"Vocabulary is empty. Ensure the model is trained first.\")\n",
    "        text = \" \".join([self.id_to_subword.get(id_, self.unk_token) for id_ in ids])\n",
    "        text = text.replace(\" </w>\", \"\").replace(\"</w>\", \" \").strip()\n",
    "        return text\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Convert text to lowercase to ensure case insensitivity\n",
    "        text = text.lower()\n",
    "        # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "        text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Trim leading and trailing spaces\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        model_data = {\n",
    "            'vocab': self.vocab,\n",
    "            'id_to_subword': self.id_to_subword,\n",
    "            'model_type': self.model_type,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            # Potentially include other relevant attributes\n",
    "        }\n",
    "        # Save the high-level tokenizer settings\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(model_data, f)\n",
    "        \n",
    "        # Now save the BPE model specifically\n",
    "        if self.model_type == \"bpe\" and self.model:\n",
    "            self.model.save_model(filepath + \"_bpe\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        self.vocab = model_data['vocab']\n",
    "        self.id_to_subword = model_data['id_to_subword']\n",
    "        self.model_type = model_data['model_type']\n",
    "        self.vocab_size = model_data['vocab_size']\n",
    "        \n",
    "        # Assuming model_type is still \"bpe\", we now load the BPE model\n",
    "        if self.model_type == \"bpe\":\n",
    "            self.model = BPE(self.vocab_size, self.unk_token_id)\n",
    "            self.model.load_model(filepath + \"_bpe\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, num_merges=100, unk_token_id=0):  # Accept unk_token_id parameter\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        self.num_merges = num_merges\n",
    "        self.unk_token_id = unk_token_id  # Store the unknown token ID\n",
    "\n",
    "    def train(self, text):\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "        vocab = collections.Counter(words)\n",
    "        vocab = {word + '</w>': count for word, count in vocab.items()}\n",
    "        \n",
    "        for _ in range(self.num_merges):  # Use the num_merges from the instance variable\n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best, vocab)\n",
    "            self.merges.append(best)\n",
    "\n",
    "        self.vocab = {word: i for i, word in enumerate(vocab.keys())}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stats(vocab):\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[symbols[i], symbols[i+1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_vocab(pair, vocab):\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in vocab:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = vocab[word]\n",
    "        return v_out\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text into subwords using learned BPE merges.\"\"\"\n",
    "        encoded_tokens = []\n",
    "        for word in re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE):\n",
    "            word += '</w>'\n",
    "            subwords = [word]  # Start with the entire word as one subword\n",
    "            for merge in self.merges:\n",
    "                new_subwords = []\n",
    "                for subword in subwords:\n",
    "                    # If the merge is in subword, split it; otherwise, keep it as is\n",
    "                    if ' '.join(merge) in subword:\n",
    "                        new_subwords.extend(subword.replace(' '.join(merge), ''.join(merge)).split(' '))\n",
    "                    else:\n",
    "                        new_subwords.append(subword)\n",
    "                subwords = new_subwords\n",
    "            encoded_tokens.extend(subwords)\n",
    "        return [self.vocab.get(token, self.unk_token_id) for token in encoded_tokens]\n",
    "    \n",
    "        # New method to save trained model\n",
    "    def save_model(self, filepath):\n",
    "        bpe_data = {\n",
    "            'merges': self.merges,\n",
    "            'vocab': self.vocab,\n",
    "            'num_merges': self.num_merges,\n",
    "            # Include other attributes as needed\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(bpe_data, f)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            bpe_data = json.load(f)\n",
    "        \n",
    "        self.merges = bpe_data['merges']\n",
    "        self.vocab = bpe_data['vocab']\n",
    "        self.num_merges = bpe_data['num_merges']\n",
    "\n",
    "\n",
    "\n",
    "class WordPiece:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.root = self.build_trie(vocab)\n",
    "        self.compute_failure_links(self.root)\n",
    "        print(\"Trie built successfully.\")\n",
    "\n",
    "    # Add debug prints to build_trie to confirm structure\n",
    "    def build_trie(self, vocab):\n",
    "        root = TrieNode()\n",
    "        for token in vocab:\n",
    "            node = root\n",
    "            for char in token:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.is_end = True\n",
    "            node.token = token\n",
    "        print(\"Trie Construction Completed Successfully\")\n",
    "        return root\n",
    "\n",
    "\n",
    "    def compute_failure_links(self, root):\n",
    "        queue = [root]\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "            for char, child_node in current_node.children.items():\n",
    "                failure_node = current_node.failure_link\n",
    "                while failure_node and char not in failure_node.children:\n",
    "                    failure_node = failure_node.failure_link\n",
    "                child_node.failure_link = failure_node.children[char] if failure_node else root\n",
    "                queue.append(child_node)\n",
    "\n",
    "    # Improved debug prints in tokenize method\n",
    "    def tokenize(self, text):\n",
    "        # Preprocess input text\n",
    "        text = self.preprocess_text(text)\n",
    "        node = self.root\n",
    "        tokens = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(text):\n",
    "            char = text[i]\n",
    "            if char == ' ':\n",
    "                node = self.root\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if char not in node.children:\n",
    "                if node != self.root:\n",
    "                    tokens.append(node.token)  # Add the longest token found\n",
    "                    node = self.root  # Reset to root\n",
    "                    continue\n",
    "                else:\n",
    "                    tokens.append(self.unk_token)\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "            node = node.children[char]\n",
    "            if node.is_end:\n",
    "                if i + 1 == len(text) or text[i + 1] == ' ':\n",
    "                    tokens.append(node.token)\n",
    "                    node = self.root\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        # Convert text to lowercase to ensure case insensitivity\n",
    "        text = text.lower()\n",
    "\n",
    "        # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "        # This depends on how your vocabulary handles punctuation\n",
    "        text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Trim leading and trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "# Sample text for training the model\n",
    "sample_text = \"\"\"\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer \n",
    "mainly for Neural Network-based text generation systems where the \n",
    "vocabulary size is predetermined prior to the neural model training. \n",
    "SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) \n",
    "and unigram language model) with the extension of direct training from \n",
    "raw sentences. Compared to other subword tokenization methods, \n",
    "SentencePiece can provide a better tokenization in some cases.\n",
    "\"\"\"\n",
    "\n",
    "# Number of merges for BPE; adjust based on your needs\n",
    "num_merges = 100\n",
    "\n",
    "# Initialize and train the SimpleSentencePiece model with BPE\n",
    "ssp = SimpleSentencePiece(model_type=\"bpe\", vocab_size=1000)\n",
    "ssp.train(sample_text)\n",
    "\n",
    "# Encode and decode a sample sentence\n",
    "encoded_sentence = ssp.encode(\"SentencePiece provides subword tokenization.\")\n",
    "decoded_sentence = ssp.decode(encoded_sentence)\n",
    "print(\"Encoded Sentence:\", encoded_sentence)\n",
    "print(\"Decoded Sentence:\", decoded_sentence)\n",
    "\n",
    "# Save the trained model\n",
    "ssp.save_model(\"ssp_model.json\")\n",
    "\n",
    "# Create a new instance and load the model\n",
    "new_ssp = SimpleSentencePiece(model_type=\"bpe\")\n",
    "new_ssp.load_model(\"ssp_model.json\")\n",
    "\n",
    "# Re-encode and decode using the loaded model\n",
    "re_encoded_sentence = new_ssp.encode(\"SentencePiece provides subword tokenization.\")\n",
    "re_decoded_sentence = new_ssp.decode(re_encoded_sentence)\n",
    "print(\"Re-Encoded Sentence:\", re_encoded_sentence)\n",
    "print(\"Re-Decoded Sentence:\", re_decoded_sentence)\n",
    "\n",
    "\n",
    "# Assuming ssp is your SimpleSentencePiece instance\n",
    "vocab = ssp.vocab  # This gets the vocabulary after BPE training\n",
    "\n",
    "def adapt_vocab_for_wordpiece(ssp_vocab):\n",
    "    adapted_vocab = {}\n",
    "    for token, id_or_freq in ssp_vocab.items():\n",
    "        # Check if a token is a continuation subword and not a standalone word\n",
    "        # Since BPE might not mark subwords in a way WordPiece expects, we adapt based on our best approximation\n",
    "        if not token.startswith(\" \") and not token.endswith(\"</w>\"):\n",
    "            adapted_token = \"##\" + token.replace(\"</w>\", \"\")  # Removing BPE's end-of-word marker and prepending \"##\"\n",
    "        else:\n",
    "            adapted_token = token.replace(\"</w>\", \"\")  # Just remove the BPE's end-of-word marker for standalone words\n",
    "\n",
    "        adapted_vocab[adapted_token] = id_or_freq\n",
    "    return adapted_vocab\n",
    "\n",
    "\n",
    "# Assuming ssp is your SimpleSentencePiece instance after BPE training\n",
    "wordpiece_vocab = adapt_vocab_for_wordpiece(ssp.vocab)\n",
    "\n",
    "# Debugging step to ensure vocabulary completeness\n",
    "def debug_vocab(adapted_vocab):\n",
    "    print(\"Sample Vocabulary Check:\")\n",
    "    # Iterate over the first 10 key-value pairs in the adapted vocabulary\n",
    "    for i, (token, id_or_freq) in enumerate(adapted_vocab.items()):\n",
    "        print(f\"{token}: {id_or_freq}\")\n",
    "        if i >= 9:  # Stop after printing 10 entries\n",
    "            break\n",
    "    # Specifically check for subtokens if your tokenizer expects them\n",
    "    subtokens = [token for token in adapted_vocab.keys() if token.startswith(\"##\")]\n",
    "    print(f\"Found {len(subtokens)} subtokens in vocabulary.\")\n",
    "\n",
    "\n",
    "# Ensure wordpiece_vocab is a list of vocabulary tokens\n",
    "debug_vocab(wordpiece_vocab)  # Call this after initializing wordpiece_vocab\n",
    "\n",
    "\n",
    "\n",
    "# Initialize WordPiece with the adapted vocabulary\n",
    "wordpiece = WordPiece(wordpiece_vocab)\n",
    "\n",
    "# Tokenize a sample sentence using WordPiece\n",
    "sample_sentence = \"SentencePiece provides subword tokenization.\"\n",
    "tokenized_sentence = wordpiece.tokenize(sample_sentence)\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "\n",
    "\n",
    "# Creating a dictionary of test sentences as outlined in the instructions\n",
    "\n",
    "test_sentences = {\n",
    "    \"known_words\": \"SentencePiece is an unsupervised tokenizer.\",\n",
    "    \"subwords\": \"Subword tokenization improves neural model performance.\",\n",
    "    \"mixed_sentence\": \"Tokenizing with SentencePiece and BPE provides flexibility.\",\n",
    "    \"case_sensitivity\": \"sentencepiece provides SubWord Tokenization.\",\n",
    "    \"punctuation_handling\": \"Tokenization, especially for NLP, is crucial.\",\n",
    "    \"complex_subword\": \"Neural networks benefit from subword-level processing.\"\n",
    "}\n",
    "\n",
    "# Display the dictionary for confirmation\n",
    "test_sentences\n",
    "\n",
    "for label, sentence in test_sentences.items():\n",
    "    print(f\"Testing '{label}': {sentence}\")\n",
    "    tokenized = wordpiece.tokenize(sentence)\n",
    "    print(f\"Tokenized: {tokenized}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Fast WordPiece Tokenization\n",
      "Decoded: Fast  Fast  tokenization\n",
      "\n",
      "Original: Xinying Song† Alex Salcianu† Yang Song‡∗ Dave Dopson† Denny Zhou†\n",
      "Decoded: Fast  Fast  †  Fast  salcianu  †  Fast  Fast  ‡  ∗  Fast  Fast  †  Fast  Fast  †\n",
      "\n",
      "Original: †Google Research, Mountain View, CA\n",
      "Decoded: †  google  Fast  ,  mountain  Fast  ,  Fast\n",
      "\n",
      "Original: †{xysong,salcianu,ddopson,dennyzhou}@google.com\n",
      "Decoded: †  {  xysong  ,  salcianu  ,  ddopson  ,  dennyzhou  }  @  google  .  com\n",
      "\n",
      "Original: ‡Kuaishou Technology, Beijing, China\n",
      "Decoded: ‡  kuaishou  Fast  ,  Fast  ,  Fast\n",
      "\n",
      "Original: ‡yangsong@kuaishou.com\n",
      "Decoded: ‡  yangsong  @  kuaishou  .  com\n",
      "\n",
      "Original: Abstract\n",
      "Decoded: Fast\n",
      "\n",
      "Original: Tokenization is a fundamental preprocessing\n",
      "Decoded: tokenization  is  a  fundamental  preprocessing\n",
      "\n",
      "Original: step for almost all NLP tasks. In this paper,\n",
      "Decoded: step  for  almost  all  Fast  tasks  .  in  this  paper  ,\n",
      "\n",
      "Original: we propose efficient algorithms for the WordPiece tokenization used in BERT, from singleword tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a longest-matchfirst strategy, known as maximum matching.\n",
      "Decoded: we  propose  efficient  algorithms  for  the  Fast  tokenization  used  in  bert  ,  from  singleword  tokenization  to  general  text  (  e  .  g  .  ,  sentence  )  tokenization  .  when  tokenizing  a  single  word  ,  Fast  uses  a  longest  -  matchfirst  strategy  ,  known  as  maximum  matching  .\n",
      "\n",
      "Original: The best known algorithms so far are 푂(푛\n",
      "Decoded: the  best  known  algorithms  so  far  are  푂  (  푛\n",
      "\n",
      "Original: 2\n",
      "Decoded: 2\n",
      "\n",
      "Original: )\n",
      "Decoded: )\n",
      "\n",
      "Original: (where 푛 is the input length) or 푂(푛푚) (where\n",
      "Decoded: (  where  푛  is  the  input  length  )  or  푂  (  푛푚  )  (  where\n",
      "\n",
      "Original: 푚 is the maximum vocabulary token length).\n",
      "Decoded: 푚  is  the  maximum  vocabulary  token  length  )  .\n",
      "\n",
      "Original: We propose a novel algorithm whose tokenization complexity is strictly 푂(푛). Our method is\n",
      "Decoded: we  propose  a  novel  algorithm  whose  tokenization  complexity  is  strictly  푂  (  푛  )  .  our  method  is\n",
      "\n",
      "Original: inspired by the Aho-Corasick algorithm. We\n",
      "Decoded: inspired  by  the  Fast  -  Fast  algorithm  .  we\n",
      "\n",
      "Original: introduce additional linkages on top of the trie\n",
      "Decoded: introduce  additional  linkages  on  top  of  the  trie\n",
      "\n",
      "Original: built from the vocabulary, allowing smart transitions when the trie matching cannot continue.\n",
      "Decoded: built  from  the  vocabulary  ,  allowing  smart  transitions  when  the  trie  matching  cannot  continue  .\n",
      "\n",
      "Original: For general text, we further propose an algorithm that combines pre-tokenization (splitting\n",
      "Decoded: for  general  text  ,  we  further  propose  an  algorithm  that  combines  pre  -  tokenization  (  splitting\n",
      "\n",
      "Original: the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our method is 8.2x faster\n",
      "Decoded: the  text  into  words  )  and  our  linear  -  time  Fast  method  into  a  single  pass  .  experimental  results  show  that  our  method  is  8  .  2x  faster\n",
      "\n",
      "Original: than HuggingFace Tokenizers and 5.1x faster\n",
      "Decoded: than  Fast  Fast  and  5  .  1x  faster\n",
      "\n",
      "Original: than TensorFlow Text on average for general\n",
      "Decoded: than  tensorflow  text  on  average  for  general\n",
      "\n",
      "Original: text tokenization.\n",
      "Decoded: text  tokenization  .\n",
      "\n",
      "Original: 1 Introduction\n",
      "Decoded: 1  Fast\n",
      "\n",
      "Original: Tokenization is the process of splitting text into\n",
      "Decoded: tokenization  is  the  process  of  splitting  text  into\n",
      "\n",
      "Original: smaller units called tokens (e.g., words). It is a\n",
      "Decoded: smaller  units  called  tokens  (  e  .  g  .  ,  words  )  .  it  is  a\n",
      "\n",
      "Original: fundamental preprocessing step for almost all NLP\n",
      "Decoded: fundamental  preprocessing  step  for  almost  all  Fast\n",
      "\n",
      "Original: applications: sentiment analysis, question answering, machine translation, information retrieval, etc.\n",
      "Decoded: applications  :  sentiment  analysis  ,  question  answering  ,  machine  translation  ,  information  retrieval  ,  etc  .\n",
      "\n",
      "Original: Modern NLP models like BERT (Devlin et al.,\n",
      "Decoded: modern  Fast  models  like  bert  (  Fast  et  al  .  ,\n",
      "\n",
      "Original: 2019), GPT-3 (Brown et al., 2020), and XLNet (Yang et al., 2019) tokenize text into subword units (Schuster and Nakajima, 2012; Sennrich\n",
      "Decoded: 2019  )  ,  Fast  -  3  (  Fast  et  al  .  ,  2020  )  ,  and  Fast  (  Fast  et  al  .  ,  2019  )  tokenize  text  into  subword  units  (  Fast  and  Fast  ,  2012  ;  Fast\n",
      "\n",
      "Original: et al., 2016; Kudo, 2018). As a midpoint between\n",
      "Decoded: et  al  .  ,  2016  ;  Fast  ,  2018  )  .  as  a  midpoint  between\n",
      "\n",
      "Original: words and characters, subword units retain linguistic meaning (like morphemes), while alleviating\n",
      "Decoded: words  and  characters  ,  subword  units  retain  linguistic  meaning  (  like  morphemes  )  ,  while  alleviating\n",
      "\n",
      "Original: out-of-vocabulary situations even with a relatively\n",
      "Decoded: out  -  of  -  vocabulary  situations  even  with  a  relatively\n",
      "\n",
      "Original: small-size vocabulary.\n",
      "Decoded: small  -  size  vocabulary  .\n",
      "\n",
      "Original: ∗ Research conducted while working at Google.\n",
      "Decoded: ∗  Fast  conducted  while  working  at  google  .\n",
      "\n",
      "Original: In this paper, we propose efficient algorithms\n",
      "Decoded: in  this  paper  ,  we  propose  efficient  algorithms\n",
      "\n",
      "Original: for WordPiece, the subword tokenization used in\n",
      "Decoded: for  Fast  ,  the  subword  tokenization  used  in\n",
      "\n",
      "Original: BERT (Devlin et al., 2019). Given Unicode text\n",
      "Decoded: bert  (  Fast  et  al  .  ,  2019  )  .  given  Fast  text\n",
      "\n",
      "Original: that has already been cleaned up and normalized,\n",
      "Decoded: that  has  already  been  cleaned  up  and  normalized  ,\n",
      "\n",
      "Original: WordPiece has two steps: (1) pre-tokenize the text\n",
      "Decoded: Fast  has  two  steps  :  (  1  )  pre  -  tokenize  the  text\n",
      "\n",
      "Original: into words (by splitting on punctuation and whitespaces), and (2) tokenize each word into wordpieces.\n",
      "Decoded: into  words  (  by  splitting  on  punctuation  and  whitespaces  )  ,  and  (  2  )  tokenize  each  word  into  wordpieces  .\n",
      "\n",
      "Original: For single-word tokenization, WordPiece uses\n",
      "Decoded: for  single  -  word  tokenization  ,  Fast  uses\n",
      "\n",
      "Original: a greedy longest-match-first strategy: iteratively\n",
      "Decoded: a  greedy  longest  -  match  -  first  strategy  :  iteratively\n",
      "\n",
      "Original: pick the longest prefix of the remaining text that\n",
      "Decoded: pick  the  longest  prefix  of  the  remaining  text  that\n",
      "\n",
      "Original: matches a vocabulary token. This is well-known as\n",
      "Decoded: matches  a  vocabulary  token  .  this  is  well  -  known  as\n",
      "\n",
      "Original: Maximum Matching or MaxMatch (Palmer, 2000),\n",
      "Decoded: maximum  matching  or  Fast  (  Fast  ,  2000  )  ,\n",
      "\n",
      "Original: which has also been used for Chinese word segmentation since 1980s (Liu and Liang, 1986).\n",
      "Decoded: which  has  also  been  used  for  Fast  word  segmentation  since  1980s  (  Fast  and  Fast  ,  1986  )  .\n",
      "\n",
      "Original: Despite its wide use in NLP for decades, to the\n",
      "Decoded: despite  its  wide  use  in  Fast  for  decades  ,  to  the\n",
      "\n",
      "Original: best of our knowledge, the most efficient MaxMatch algorithms so far are 푂(푛\n",
      "Decoded: best  of  our  knowledge  ,  the  most  efficient  Fast  algorithms  so  far  are  푂  (  푛\n",
      "\n",
      "Original: 2\n",
      "Decoded: 2\n",
      "\n",
      "Original: ) (where 푛 is the\n",
      "Decoded: )  (  where  푛  is  the\n",
      "\n",
      "Original: input word length) or 푂(푛푚) (where 푚 is the maximum vocabulary token length) (see Section 2).\n",
      "Decoded: input  word  length  )  or  푂  (  푛푚  )  (  where  푚  is  the  maximum  vocabulary  token  length  )  (  see  section  2  )  .\n",
      "\n",
      "Original: It’s worth noting that the latter has a vocabularyspecific multiplicative factor 푚, which can be large\n",
      "Decoded: it  ’  s  worth  noting  that  the  latter  has  a  vocabularyspecific  multiplicative  factor  푚  ,  which  can  be  large\n",
      "\n",
      "Original: when the vocabulary contains long words.\n",
      "Decoded: when  the  vocabulary  contains  long  words  .\n",
      "\n",
      "Original: We propose LinMaxMatch, a novel MaxMatch\n",
      "Decoded: we  propose  Fast  ,  a  novel  Fast\n",
      "\n",
      "Original: algorithm for WordPiece tokenization, whose\n",
      "Decoded: algorithm  for  Fast  tokenization  ,  whose\n",
      "\n",
      "Original: tokenization time is strictly 푂(푛) without any\n",
      "Decoded: tokenization  time  is  strictly  푂  (  푛  )  without  any\n",
      "\n",
      "Original: vocabulary-specific multiplicative factors. Inspired\n",
      "Decoded: vocabulary  -  specific  multiplicative  factors  .  inspired\n",
      "\n",
      "Original: by the Aho-Corasick algorithm (Aho and Corasick, 1975), we organize vocabulary tokens in a\n",
      "Decoded: by  the  Fast  -  Fast  algorithm  (  Fast  and  Fast  ,  1975  )  ,  we  organize  vocabulary  tokens  in  a\n",
      "\n",
      "Original: trie (Fredkin, 1960) and introduce precomputed\n",
      "Decoded: trie  (  Fast  ,  1960  )  and  introduce  precomputed\n",
      "\n",
      "Original: failure links and failure pops. During tokenization, if an input character does not match any trie\n",
      "Decoded: failure  links  and  failure  pops  .  during  tokenization  ,  if  an  input  character  does  not  match  any  trie\n",
      "\n",
      "Original: edge, we perform smart transitions to avoid backtracking to earlier input characters. This involves\n",
      "Decoded: edge  ,  we  perform  smart  transitions  to  avoid  backtracking  to  earlier  input  characters  .  this  involves\n",
      "\n",
      "Original: collecting the recognized tokens (i.e., failure pops)\n",
      "Decoded: collecting  the  recognized  tokens  (  i  .  e  .  ,  failure  pops  )\n",
      "\n",
      "Original: and moving to a trie node (via the failure link),\n",
      "Decoded: and  moving  to  a  trie  node  (  via  the  failure  link  )  ,\n",
      "\n",
      "Original: from where we continue to match the same character (Section 3).\n",
      "Decoded: from  where  we  continue  to  match  the  same  character  (  section  3  )  .\n",
      "\n",
      "Original: For general text tokenization, referred to as\n",
      "Decoded: for  general  text  tokenization  ,  referred  to  as\n",
      "\n",
      "Original: end-to-end tokenization in this paper, we propose\n",
      "Decoded: end  -  to  -  end  tokenization  in  this  paper  ,  we  propose\n",
      "\n",
      "Original: E2E WordPiece, an end-to-end algorithm that combines pre-tokenization and WordPiece tokenization\n",
      "Decoded: Fast  Fast  ,  an  end  -  to  -  end  algorithm  that  combines  pre  -  tokenization  and  Fast  tokenization\n",
      "\n",
      "Original: arXiv:2012.15524v3 [cs.CL] 5 Oct 2021\n",
      "Decoded: Fast  :  2012  .  15524v3  [  cs  .  Fast  ]  5  Fast  2021\n",
      "\n",
      "Original: into a single, linear-time pass (Section 4).\n",
      "Decoded: into  a  single  ,  linear  -  time  pass  (  section  4  )  .\n",
      "\n",
      "Original: Experimental results show that our method is\n",
      "Decoded: experimental  results  show  that  our  method  is\n",
      "\n",
      "Original: 8.2x faster than HuggingFace Tokenizers (HuggingFace, 2020) and 5.1x faster than TensorFlow Text (Google, 2020) on average for general\n",
      "Decoded: 8  .  2x  faster  than  Fast  Fast  (  Fast  ,  2020  )  and  5  .  1x  faster  than  tensorflow  text  (  google  ,  2020  )  on  average  for  general\n",
      "\n",
      "Original: text tokenization (Section 5).\n",
      "Decoded: text  tokenization  (  section  5  )  .\n",
      "\n",
      "Original: Although tokenization is relatively faster than\n",
      "Decoded: although  tokenization  is  relatively  faster  than\n",
      "\n",
      "Original: other steps, it’s still worth improving the performance: Tokenization is a prerequisite step for\n",
      "Decoded: other  steps  ,  it  ’  s  still  worth  improving  the  performance  :  tokenization  is  a  prerequisite  step  for\n",
      "\n",
      "Original: almost all NLP tasks, and any improvement on\n",
      "Decoded: almost  all  Fast  tasks  ,  and  any  improvement  on\n",
      "\n",
      "Original: its efficiency helps reduce the latency of the entire inference. One potential impact of the work,\n",
      "Decoded: its  efficiency  helps  reduce  the  latency  of  the  entire  inference  .  one  potential  impact  of  the  work  ,\n",
      "\n",
      "Original: for example, is on mobile NLP applications. Ondevice models are generally highly optimized for\n",
      "Decoded: for  example  ,  is  on  mobile  Fast  applications  .  Fast  models  are  generally  highly  optimized  for\n",
      "\n",
      "Original: reducing latency, e.g., by distilling or compressing\n",
      "Decoded: reducing  latency  ,  e  .  g  .  ,  by  distilling  or  compressing\n",
      "\n",
      "Original: larger models. Thus, the impact of tokenization\n",
      "Decoded: larger  models  .  thus  ,  the  impact  of  tokenization\n",
      "\n",
      "Original: can be significant here. Another impact is on aggregate computational savings for Web services\n",
      "Decoded: can  be  significant  here  .  another  impact  is  on  aggregate  computational  savings  for  Fast  services\n",
      "\n",
      "Original: like Google, Facebook, Twitter, etc. For example,\n",
      "Decoded: like  google  ,  Fast  ,  Fast  ,  etc  .  for  example  ,\n",
      "\n",
      "Original: Google uses BERT to power its Web search nowadays.1 Google serves billions of search queries per\n",
      "Decoded: google  uses  bert  to  power  its  Fast  search  nowadays  .  1  google  serves  billions  of  search  queries  per\n",
      "\n",
      "Original: day, and it processes hundreds of trillions of Web\n",
      "Decoded: day  ,  and  it  processes  hundreds  of  trillions  of  Fast\n",
      "\n",
      "Original: pages in index building. By employing a faster\n",
      "Decoded: pages  in  index  building  .  by  employing  a  faster\n",
      "\n",
      "Original: tokenization system, the aggregate computational\n",
      "Decoded: tokenization  system  ,  the  aggregate  computational\n",
      "\n",
      "Original: savings would be material, which also benefits the\n",
      "Decoded: savings  would  be  material  ,  which  also  benefits  the\n",
      "\n",
      "Original: environment (for less power consumption).\n",
      "Decoded: environment  (  for  less  power  consumption  )  .\n",
      "\n",
      "Original: This paper also makes a theoretical contribution. The proposed LinMaxMatch algorithm solves\n",
      "Decoded: this  paper  also  makes  a  theoretical  contribution  .  the  proposed  Fast  algorithm  solves\n",
      "\n",
      "Original: the decades-old MaxMatch problem in the optimal\n",
      "Decoded: the  decades  -  old  Fast  problem  in  the  optimal\n",
      "\n",
      "Original: 푂(푛) time, and the idea is applicable to other string\n",
      "Decoded: 푂  (  푛  )  time  ,  and  the  idea  is  applicable  to  other  string\n",
      "\n",
      "Original: matching or rewriting problems (Section 3.6).\n",
      "Decoded: matching  or  rewriting  problems  (  section  3  .  6  )  .\n",
      "\n",
      "Original: The code will be available at https://www.\n",
      "Decoded: the  code  will  be  available  at  https  :  /  /  www  .\n",
      "\n",
      "Original: tensorflow.org/text.\n",
      "Decoded: tensorflow  .  org  /  text  .\n",
      "\n",
      "Original: 2 Related Work\n",
      "Decoded: 2  related  work\n",
      "\n",
      "Original: Maximum Matching (or MaxMatch) has been used\n",
      "Decoded: maximum  matching  (  or  Fast  )  has  been  used\n",
      "\n",
      "Original: for Chinese word segmentation (CWS) since the\n",
      "Decoded: for  Fast  word  segmentation  (  Fast  )  since  the\n",
      "\n",
      "Original: 1980s (Liu and Liang, 1986; Palmer, 2000). Recent\n",
      "Decoded: 1980s  (  Fast  and  Fast  ,  1986  ;  Fast  ,  2000  )  .  recent\n",
      "\n",
      "Original: CWS work focuses on machine learning-based segmentation approaches, but MaxMatch remains a\n",
      "Decoded: Fast  work  focuses  on  machine  learning  -  based  segmentation  approaches  ,  but  Fast  remains  a\n",
      "\n",
      "Original: Fast WordPiece Tokenization\n",
      "Decoded: Fast  Fast  tokenization\n",
      "\n",
      "Original: Xinying Song† Alex Salcianu† Yang Song‡∗ Dave Dopson† Denny Zhou†\n",
      "Decoded: Fast  Fast  †  Fast  salcianu  †  Fast  Fast  ‡  ∗  Fast  Fast  †  Fast  Fast  †\n",
      "\n",
      "Original: †Google Research, Mountain View, CA\n",
      "Decoded: †  google  Fast  ,  mountain  Fast  ,  Fast\n",
      "\n",
      "Original: †{xysong,salcianu,ddopson,dennyzhou}@google.com\n",
      "Decoded: †  {  xysong  ,  salcianu  ,  ddopson  ,  dennyzhou  }  @  google  .  com\n",
      "\n",
      "Original: ‡Kuaishou Technology, Beijing, China\n",
      "Decoded: ‡  kuaishou  Fast  ,  Fast  ,  Fast\n",
      "\n",
      "Original: ‡yangsong@kuaishou.com\n",
      "Decoded: ‡  yangsong  @  kuaishou  .  com\n",
      "\n",
      "Original: Abstract\n",
      "Decoded: Fast\n",
      "\n",
      "Original: Tokenization is a fundamental preprocessing\n",
      "Decoded: tokenization  is  a  fundamental  preprocessing\n",
      "\n",
      "Original: step for almost all NLP tasks. In this paper,\n",
      "Decoded: step  for  almost  all  Fast  tasks  .  in  this  paper  ,\n",
      "\n",
      "Original: we propose efficient algorithms for the WordPiece tokenization used in BERT, from singleword tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a longest-matchfirst strategy, known as maximum matching.\n",
      "Decoded: we  propose  efficient  algorithms  for  the  Fast  tokenization  used  in  bert  ,  from  singleword  tokenization  to  general  text  (  e  .  g  .  ,  sentence  )  tokenization  .  when  tokenizing  a  single  word  ,  Fast  uses  a  longest  -  matchfirst  strategy  ,  known  as  maximum  matching  .\n",
      "\n",
      "Original: The best known algorithms so far are 푂(푛\n",
      "Decoded: the  best  known  algorithms  so  far  are  푂  (  푛\n",
      "\n",
      "Original: 2\n",
      "Decoded: 2\n",
      "\n",
      "Original: )\n",
      "Decoded: )\n",
      "\n",
      "Original: (where 푛 is the input length) or 푂(푛푚) (where\n",
      "Decoded: (  where  푛  is  the  input  length  )  or  푂  (  푛푚  )  (  where\n",
      "\n",
      "Original: 푚 is the maximum vocabulary token length).\n",
      "Decoded: 푚  is  the  maximum  vocabulary  token  length  )  .\n",
      "\n",
      "Original: We propose a novel algorithm whose tokenization complexity is strictly 푂(푛). Our method is\n",
      "Decoded: we  propose  a  novel  algorithm  whose  tokenization  complexity  is  strictly  푂  (  푛  )  .  our  method  is\n",
      "\n",
      "Original: inspired by the Aho-Corasick algorithm. We\n",
      "Decoded: inspired  by  the  Fast  -  Fast  algorithm  .  we\n",
      "\n",
      "Original: introduce additional linkages on top of the trie\n",
      "Decoded: introduce  additional  linkages  on  top  of  the  trie\n",
      "\n",
      "Original: built from the vocabulary, allowing smart transitions when the trie matching cannot continue.\n",
      "Decoded: built  from  the  vocabulary  ,  allowing  smart  transitions  when  the  trie  matching  cannot  continue  .\n",
      "\n",
      "Original: For general text, we further propose an algorithm that combines pre-tokenization (splitting\n",
      "Decoded: for  general  text  ,  we  further  propose  an  algorithm  that  combines  pre  -  tokenization  (  splitting\n",
      "\n",
      "Original: the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our method is 8.2x faster\n",
      "Decoded: the  text  into  words  )  and  our  linear  -  time  Fast  method  into  a  single  pass  .  experimental  results  show  that  our  method  is  8  .  2x  faster\n",
      "\n",
      "Original: than HuggingFace Tokenizers and 5.1x faster\n",
      "Decoded: than  Fast  Fast  and  5  .  1x  faster\n",
      "\n",
      "Original: than TensorFlow Text on average for general\n",
      "Decoded: than  tensorflow  text  on  average  for  general\n",
      "\n",
      "Original: text tokenization.\n",
      "Decoded: text  tokenization  .\n",
      "\n",
      "Original: 1 Introduction\n",
      "Decoded: 1  Fast\n",
      "\n",
      "Original: Tokenization is the process of splitting text into\n",
      "Decoded: tokenization  is  the  process  of  splitting  text  into\n",
      "\n",
      "Original: smaller units called tokens (e.g., words). It is a\n",
      "Decoded: smaller  units  called  tokens  (  e  .  g  .  ,  words  )  .  it  is  a\n",
      "\n",
      "Original: fundamental preprocessing step for almost all NLP\n",
      "Decoded: fundamental  preprocessing  step  for  almost  all  Fast\n",
      "\n",
      "Original: applications: sentiment analysis, question answering, machine translation, information retrieval, etc.\n",
      "Decoded: applications  :  sentiment  analysis  ,  question  answering  ,  machine  translation  ,  information  retrieval  ,  etc  .\n",
      "\n",
      "Original: Modern NLP models like BERT (Devlin et al.,\n",
      "Decoded: modern  Fast  models  like  bert  (  Fast  et  al  .  ,\n",
      "\n",
      "Original: 2019), GPT-3 (Brown et al., 2020), and XLNet (Yang et al., 2019) tokenize text into subword units (Schuster and Nakajima, 2012; Sennrich\n",
      "Decoded: 2019  )  ,  Fast  -  3  (  Fast  et  al  .  ,  2020  )  ,  and  Fast  (  Fast  et  al  .  ,  2019  )  tokenize  text  into  subword  units  (  Fast  and  Fast  ,  2012  ;  Fast\n",
      "\n",
      "Original: et al., 2016; Kudo, 2018). As a midpoint between\n",
      "Decoded: et  al  .  ,  2016  ;  Fast  ,  2018  )  .  as  a  midpoint  between\n",
      "\n",
      "Original: words and characters, subword units retain linguistic meaning (like morphemes), while alleviating\n",
      "Decoded: words  and  characters  ,  subword  units  retain  linguistic  meaning  (  like  morphemes  )  ,  while  alleviating\n",
      "\n",
      "Original: out-of-vocabulary situations even with a relatively\n",
      "Decoded: out  -  of  -  vocabulary  situations  even  with  a  relatively\n",
      "\n",
      "Original: small-size vocabulary.\n",
      "Decoded: small  -  size  vocabulary  .\n",
      "\n",
      "Original: ∗ Research conducted while working at Google.\n",
      "Decoded: ∗  Fast  conducted  while  working  at  google  .\n",
      "\n",
      "Original: In this paper, we propose efficient algorithms\n",
      "Decoded: in  this  paper  ,  we  propose  efficient  algorithms\n",
      "\n",
      "Original: for WordPiece, the subword tokenization used in\n",
      "Decoded: for  Fast  ,  the  subword  tokenization  used  in\n",
      "\n",
      "Original: BERT (Devlin et al., 2019). Given Unicode text\n",
      "Decoded: bert  (  Fast  et  al  .  ,  2019  )  .  given  Fast  text\n",
      "\n",
      "Original: that has already been cleaned up and normalized,\n",
      "Decoded: that  has  already  been  cleaned  up  and  normalized  ,\n",
      "\n",
      "Original: WordPiece has two steps: (1) pre-tokenize the text\n",
      "Decoded: Fast  has  two  steps  :  (  1  )  pre  -  tokenize  the  text\n",
      "\n",
      "Original: into words (by splitting on punctuation and whitespaces), and (2) tokenize each word into wordpieces.\n",
      "Decoded: into  words  (  by  splitting  on  punctuation  and  whitespaces  )  ,  and  (  2  )  tokenize  each  word  into  wordpieces  .\n",
      "\n",
      "Original: For single-word tokenization, WordPiece uses\n",
      "Decoded: for  single  -  word  tokenization  ,  Fast  uses\n",
      "\n",
      "Original: a greedy longest-match-first strategy: iteratively\n",
      "Decoded: a  greedy  longest  -  match  -  first  strategy  :  iteratively\n",
      "\n",
      "Original: pick the longest prefix of the remaining text that\n",
      "Decoded: pick  the  longest  prefix  of  the  remaining  text  that\n",
      "\n",
      "Original: matches a vocabulary token. This is well-known as\n",
      "Decoded: matches  a  vocabulary  token  .  this  is  well  -  known  as\n",
      "\n",
      "Original: Maximum Matching or MaxMatch (Palmer, 2000),\n",
      "Decoded: maximum  matching  or  Fast  (  Fast  ,  2000  )  ,\n",
      "\n",
      "Original: which has also been used for Chinese word segmentation since 1980s (Liu and Liang, 1986).\n",
      "Decoded: which  has  also  been  used  for  Fast  word  segmentation  since  1980s  (  Fast  and  Fast  ,  1986  )  .\n",
      "\n",
      "Original: Despite its wide use in NLP for decades, to the\n",
      "Decoded: despite  its  wide  use  in  Fast  for  decades  ,  to  the\n",
      "\n",
      "Original: best of our knowledge, the most efficient MaxMatch algorithms so far are 푂(푛\n",
      "Decoded: best  of  our  knowledge  ,  the  most  efficient  Fast  algorithms  so  far  are  푂  (  푛\n",
      "\n",
      "Original: 2\n",
      "Decoded: 2\n",
      "\n",
      "Original: ) (where 푛 is the\n",
      "Decoded: )  (  where  푛  is  the\n",
      "\n",
      "Original: input word length) or 푂(푛푚) (where 푚 is the maximum vocabulary token length) (see Section 2).\n",
      "Decoded: input  word  length  )  or  푂  (  푛푚  )  (  where  푚  is  the  maximum  vocabulary  token  length  )  (  see  section  2  )  .\n",
      "\n",
      "Original: It’s worth noting that the latter has a vocabularyspecific multiplicative factor 푚, which can be large\n",
      "Decoded: it  ’  s  worth  noting  that  the  latter  has  a  vocabularyspecific  multiplicative  factor  푚  ,  which  can  be  large\n",
      "\n",
      "Original: when the vocabulary contains long words.\n",
      "Decoded: when  the  vocabulary  contains  long  words  .\n",
      "\n",
      "Original: We propose LinMaxMatch, a novel MaxMatch\n",
      "Decoded: we  propose  Fast  ,  a  novel  Fast\n",
      "\n",
      "Original: algorithm for WordPiece tokenization, whose\n",
      "Decoded: algorithm  for  Fast  tokenization  ,  whose\n",
      "\n",
      "Original: tokenization time is strictly 푂(푛) without any\n",
      "Decoded: tokenization  time  is  strictly  푂  (  푛  )  without  any\n",
      "\n",
      "Original: vocabulary-specific multiplicative factors. Inspired\n",
      "Decoded: vocabulary  -  specific  multiplicative  factors  .  inspired\n",
      "\n",
      "Original: by the Aho-Corasick algorithm (Aho and Corasick, 1975), we organize vocabulary tokens in a\n",
      "Decoded: by  the  Fast  -  Fast  algorithm  (  Fast  and  Fast  ,  1975  )  ,  we  organize  vocabulary  tokens  in  a\n",
      "\n",
      "Original: trie (Fredkin, 1960) and introduce precomputed\n",
      "Decoded: trie  (  Fast  ,  1960  )  and  introduce  precomputed\n",
      "\n",
      "Original: failure links and failure pops. During tokenization, if an input character does not match any trie\n",
      "Decoded: failure  links  and  failure  pops  .  during  tokenization  ,  if  an  input  character  does  not  match  any  trie\n",
      "\n",
      "Original: edge, we perform smart transitions to avoid backtracking to earlier input characters. This involves\n",
      "Decoded: edge  ,  we  perform  smart  transitions  to  avoid  backtracking  to  earlier  input  characters  .  this  involves\n",
      "\n",
      "Original: collecting the recognized tokens (i.e., failure pops)\n",
      "Decoded: collecting  the  recognized  tokens  (  i  .  e  .  ,  failure  pops  )\n",
      "\n",
      "Original: and moving to a trie node (via the failure link),\n",
      "Decoded: and  moving  to  a  trie  node  (  via  the  failure  link  )  ,\n",
      "\n",
      "Original: from where we continue to match the same character (Section 3).\n",
      "Decoded: from  where  we  continue  to  match  the  same  character  (  section  3  )  .\n",
      "\n",
      "Original: For general text tokenization, referred to as\n",
      "Decoded: for  general  text  tokenization  ,  referred  to  as\n",
      "\n",
      "Original: end-to-end tokenization in this paper, we propose\n",
      "Decoded: end  -  to  -  end  tokenization  in  this  paper  ,  we  propose\n",
      "\n",
      "Original: E2E WordPiece, an end-to-end algorithm that combines pre-tokenization and WordPiece tokenization\n",
      "Decoded: Fast  Fast  ,  an  end  -  to  -  end  algorithm  that  combines  pre  -  tokenization  and  Fast  tokenization\n",
      "\n",
      "Original: arXiv:2012.15524v3 [cs.CL] 5 Oct 2021\n",
      "Decoded: Fast  :  2012  .  15524v3  [  cs  .  Fast  ]  5  Fast  2021\n",
      "\n",
      "Original: into a single, linear-time pass (Section 4).\n",
      "Decoded: into  a  single  ,  linear  -  time  pass  (  section  4  )  .\n",
      "\n",
      "Original: Experimental results show that our method is\n",
      "Decoded: experimental  results  show  that  our  method  is\n",
      "\n",
      "Original: 8.2x faster than HuggingFace Tokenizers (HuggingFace, 2020) and 5.1x faster than TensorFlow Text (Google, 2020) on average for general\n",
      "Decoded: 8  .  2x  faster  than  Fast  Fast  (  Fast  ,  2020  )  and  5  .  1x  faster  than  tensorflow  text  (  google  ,  2020  )  on  average  for  general\n",
      "\n",
      "Original: text tokenization (Section 5).\n",
      "Decoded: text  tokenization  (  section  5  )  .\n",
      "\n",
      "Original: Although tokenization is relatively faster than\n",
      "Decoded: although  tokenization  is  relatively  faster  than\n",
      "\n",
      "Original: other steps, it’s still worth improving the performance: Tokenization is a prerequisite step for\n",
      "Decoded: other  steps  ,  it  ’  s  still  worth  improving  the  performance  :  tokenization  is  a  prerequisite  step  for\n",
      "\n",
      "Original: almost all NLP tasks, and any improvement on\n",
      "Decoded: almost  all  Fast  tasks  ,  and  any  improvement  on\n",
      "\n",
      "Original: its efficiency helps reduce the latency of the entire inference. One potential impact of the work,\n",
      "Decoded: its  efficiency  helps  reduce  the  latency  of  the  entire  inference  .  one  potential  impact  of  the  work  ,\n",
      "\n",
      "Original: for example, is on mobile NLP applications. Ondevice models are generally highly optimized for\n",
      "Decoded: for  example  ,  is  on  mobile  Fast  applications  .  Fast  models  are  generally  highly  optimized  for\n",
      "\n",
      "Original: reducing latency, e.g., by distilling or compressing\n",
      "Decoded: reducing  latency  ,  e  .  g  .  ,  by  distilling  or  compressing\n",
      "\n",
      "Original: larger models. Thus, the impact of tokenization\n",
      "Decoded: larger  models  .  thus  ,  the  impact  of  tokenization\n",
      "\n",
      "Original: can be significant here. Another impact is on aggregate computational savings for Web services\n",
      "Decoded: can  be  significant  here  .  another  impact  is  on  aggregate  computational  savings  for  Fast  services\n",
      "\n",
      "Original: like Google, Facebook, Twitter, etc. For example,\n",
      "Decoded: like  google  ,  Fast  ,  Fast  ,  etc  .  for  example  ,\n",
      "\n",
      "Original: Google uses BERT to power its Web search nowadays.1 Google serves billions of search queries per\n",
      "Decoded: google  uses  bert  to  power  its  Fast  search  nowadays  .  1  google  serves  billions  of  search  queries  per\n",
      "\n",
      "Original: day, and it processes hundreds of trillions of Web\n",
      "Decoded: day  ,  and  it  processes  hundreds  of  trillions  of  Fast\n",
      "\n",
      "Original: pages in index building. By employing a faster\n",
      "Decoded: pages  in  index  building  .  by  employing  a  faster\n",
      "\n",
      "Original: tokenization system, the aggregate computational\n",
      "Decoded: tokenization  system  ,  the  aggregate  computational\n",
      "\n",
      "Original: savings would be material, which also benefits the\n",
      "Decoded: savings  would  be  material  ,  which  also  benefits  the\n",
      "\n",
      "Original: environment (for less power consumption).\n",
      "Decoded: environment  (  for  less  power  consumption  )  .\n",
      "\n",
      "Original: This paper also makes a theoretical contribution. The proposed LinMaxMatch algorithm solves\n",
      "Decoded: this  paper  also  makes  a  theoretical  contribution  .  the  proposed  Fast  algorithm  solves\n",
      "\n",
      "Original: the decades-old MaxMatch problem in the optimal\n",
      "Decoded: the  decades  -  old  Fast  problem  in  the  optimal\n",
      "\n",
      "Original: 푂(푛) time, and the idea is applicable to other string\n",
      "Decoded: 푂  (  푛  )  time  ,  and  the  idea  is  applicable  to  other  string\n",
      "\n",
      "Original: matching or rewriting problems (Section 3.6).\n",
      "Decoded: matching  or  rewriting  problems  (  section  3  .  6  )  .\n",
      "\n",
      "Original: The code will be available at https://www.\n",
      "Decoded: the  code  will  be  available  at  https  :  /  /  www  .\n",
      "\n",
      "Original: tensorflow.org/text.\n",
      "Decoded: tensorflow  .  org  /  text  .\n",
      "\n",
      "Original: 2 Related Work\n",
      "Decoded: 2  related  work\n",
      "\n",
      "Original: Maximum Matching (or MaxMatch) has been used\n",
      "Decoded: maximum  matching  (  or  Fast  )  has  been  used\n",
      "\n",
      "Original: for Chinese word segmentation (CWS) since the\n",
      "Decoded: for  Fast  word  segmentation  (  Fast  )  since  the\n",
      "\n",
      "Original: 1980s (Liu and Liang, 1986; Palmer, 2000). Recent\n",
      "Decoded: 1980s  (  Fast  and  Fast  ,  1986  ;  Fast  ,  2000  )  .  recent\n",
      "\n",
      "Original: CWS work focuses on machine learning-based segmentation approaches, but MaxMatch remains a\n",
      "Decoded: Fast  work  focuses  on  machine  learning  -  based  segmentation  approaches  ,  but  Fast  remains  a\n",
      "\n",
      "vocab ; 2566 , new_vocab: 2566\n",
      "Sample Vocabulary Check:\n",
      "Fast: 0\n",
      "WordPiece: 1\n",
      "Tokenization: 2\n",
      "Xinying: 3\n",
      "Song: 4\n",
      "†: 5\n",
      "Alex: 6\n",
      "Salcianu: 7\n",
      "Yang: 8\n",
      "‡: 9\n",
      "Found 0 subtokens in vocabulary.\n",
      "Trie Construction Completed Successfully\n",
      "Trie built successfully.\n",
      "Testing ': Fast WordPiece Tokenization\n",
      "Tokenized: ['tokenization']\n",
      "\n",
      "Testing ': Xinying Song† Alex Salcianu† Yang Song‡∗ Dave Dopson† Denny Zhou†\n",
      "Tokenized: ['xi', 'n', 'yi', 'n', 'g', 'son', 'g', '†', 'al', 'salcianu', '†', 'son', 'g', '‡', '∗', None, 'do', None, 'son', '†', 'z', None, None, '†']\n",
      "\n",
      "Testing ': †Google Research, Mountain View, CA\n",
      "Tokenized: ['†', 'google', ',', 'mountain', ',']\n",
      "\n",
      "Testing ': †{xysong,salcianu,ddopson,dennyzhou}@google.com\n",
      "Tokenized: ['†', '{', 'xysong', ',', 'salcianu', ',', 'ddopson', ',', 'dennyzhou', '}', '@', 'google', '.', 'com']\n",
      "\n",
      "Testing ': ‡Kuaishou Technology, Beijing, China\n",
      "Tokenized: ['‡', 'kuaishou', None, None, 'o', 'g', 'y', ',', None, None, 'in', 'g', ',', None, 'in', 'a']\n",
      "\n",
      "Testing ': ‡yangsong@kuaishou.com\n",
      "Tokenized: ['‡', 'yangsong', '@', 'kuaishou', '.', 'com']\n",
      "\n",
      "Testing ': Abstract\n",
      "Tokenized: ['abs', None, 'c', 't']\n",
      "\n",
      "Testing ': Tokenization is a fundamental preprocessing\n",
      "Tokenized: ['tokenization', 'is', 'a', 'fundamental', 'preprocessing']\n",
      "\n",
      "Testing ': step for almost all NLP tasks. In this paper,\n",
      "Tokenized: ['step', 'for', 'almost', 'all', 'n', None, 'tasks', '.', 'in', 'this', 'paper', ',']\n",
      "\n",
      "Testing ': we propose efficient algorithms for the WordPiece tokenization used in BERT, from singleword tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a longest-matchfirst strategy, known as maximum matching.\n",
      "Tokenized: ['we', 'propose', 'efficient', 'algorithms', 'for', 'the', 'tokenization', 'used', 'in', 'bert', ',', 'from', 'singleword', 'tokenization', 'to', 'general', 'text', '(', 'e', '.', 'g', '.', ',', 'sentence', ')', 'tokenization', '.', 'when', 'tokenizing', 'a', 'single', 'word', ',', 'uses', 'a', 'longest', '-', 'matchfirst', 'strategy', ',', 'known', 'as', 'maximum', 'matching', '.']\n",
      "\n",
      "Testing ': The best known algorithms so far are 푂(푛\n",
      "Tokenized: ['the', 'best', 'known', 'algorithms', 'so', 'far', 'are', '푂', '(', '푛']\n",
      "\n",
      "Testing ': 2\n",
      "Tokenized: ['2']\n",
      "\n",
      "Testing ': )\n",
      "Tokenized: [')']\n",
      "\n",
      "Testing ': (where 푛 is the input length) or 푂(푛푚) (where\n",
      "Tokenized: ['(', 'where', '푛', 'is', 'the', 'input', 'length', ')', 'or', '푂', '(', '푛푚', ')', '(', 'where']\n",
      "\n",
      "Testing ': 푚 is the maximum vocabulary token length).\n",
      "Tokenized: ['푚', 'is', 'the', 'maximum', 'vocabulary', 'token', 'length', ')', '.']\n",
      "\n",
      "Testing ': We propose a novel algorithm whose tokenization complexity is strictly 푂(푛). Our method is\n",
      "Tokenized: ['we', 'propose', 'a', 'novel', 'algorithm', 'whose', 'tokenization', 'complexity', 'is', 'strictly', '푂', '(', '푛', ')', '.', 'our', 'method', 'is']\n",
      "\n",
      "Testing ': inspired by the Aho-Corasick algorithm. We\n",
      "Tokenized: ['inspired', 'by', 'the', 'a', None, '-', None, 'as', 'i', 'ck', 'algorithm', '.', 'we']\n",
      "\n",
      "Testing ': introduce additional linkages on top of the trie\n",
      "Tokenized: ['introduce', 'additional', 'linkages', 'on', 'top', 'of', 'the', 'trie']\n",
      "\n",
      "Testing ': built from the vocabulary, allowing smart transitions when the trie matching cannot continue.\n",
      "Tokenized: ['built', 'from', 'the', 'vocabulary', ',', 'allowing', 'smart', 'transitions', 'when', 'the', 'trie', 'matching', 'cannot', 'continue', '.']\n",
      "\n",
      "Testing ': For general text, we further propose an algorithm that combines pre-tokenization (splitting\n",
      "Tokenized: ['for', 'general', 'text', ',', 'we', 'further', 'propose', 'an', 'algorithm', 'that', 'combines', 'pre', '-', 'tokenization', '(', 'splitting']\n",
      "\n",
      "Testing ': the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our method is 8.2x faster\n",
      "Tokenized: ['the', 'text', 'into', 'words', ')', 'and', 'our', 'linear', '-', 'time', 'method', 'into', 'a', 'single', 'pass', '.', 'experimental', 'results', 'show', 'that', 'our', 'method', 'is', '8', '.', '2x', 'faster']\n",
      "\n",
      "Testing ': than HuggingFace Tokenizers and 5.1x faster\n",
      "Tokenized: ['than', None, 'g', None, 'n', 'g', None, 'e', 'tokenizer', 's', 'and', '5', '.', '1x', 'faster']\n",
      "\n",
      "Testing ': than TensorFlow Text on average for general\n",
      "Tokenized: ['than', 'tensorflow', 'text', 'on', 'average', 'for', 'general']\n",
      "\n",
      "Testing ': text tokenization.\n",
      "Tokenized: ['text', 'tokenization', '.']\n",
      "\n",
      "Testing ': 1 Introduction\n",
      "Tokenized: ['1', None, None, 'on']\n",
      "\n",
      "Testing ': Tokenization is the process of splitting text into\n",
      "Tokenized: ['tokenization', 'is', 'the', 'process', 'of', 'splitting', 'text', 'into']\n",
      "\n",
      "Testing ': smaller units called tokens (e.g., words). It is a\n",
      "Tokenized: ['smaller', 'units', 'called', 'tokens', '(', 'e', '.', 'g', '.', ',', 'words', ')', '.', 'it', 'is', 'a']\n",
      "\n",
      "Testing ': fundamental preprocessing step for almost all NLP\n",
      "Tokenized: ['fundamental', 'preprocessing', 'step', 'for', 'almost', 'all', 'n', None]\n",
      "\n",
      "Testing ': applications: sentiment analysis, question answering, machine translation, information retrieval, etc.\n",
      "Tokenized: ['applications', ':', 'sentiment', 'analysis', ',', 'question', 'answering', ',', 'machine', 'translation', ',', 'information', 'retrieval', ',', 'etc', '.']\n",
      "\n",
      "Testing ': Modern NLP models like BERT (Devlin et al.,\n",
      "Tokenized: ['modern', 'n', None, 'models', 'like', 'bert', '(', None, 'et', 'al', '.', ',']\n",
      "\n",
      "Testing ': 2019), GPT-3 (Brown et al., 2020), and XLNet (Yang et al., 2019) tokenize text into subword units (Schuster and Nakajima, 2012; Sennrich\n",
      "Tokenized: ['2019', ')', ',', 'g', None, 't', '-', '3', '(', None, 'own', 'et', 'al', '.', ',', '2020', ')', ',', 'and', 'x', None, 'net', '(', 'et', 'al', '.', ',', '2019', ')', 'tokenize', 'text', 'into', 'subword', 'units', '(', None, 'us', 'and', None, 'k', 'a', None, ',', '2012', ';', None, 'n', None]\n",
      "\n",
      "Testing ': et al., 2016; Kudo, 2018). As a midpoint between\n",
      "Tokenized: ['et', 'al', '.', ',', '2016', ';', None, 'do', ',', '2018', ')', '.', 'as', 'a', 'midpoint', 'between']\n",
      "\n",
      "Testing ': words and characters, subword units retain linguistic meaning (like morphemes), while alleviating\n",
      "Tokenized: ['words', 'and', 'characters', ',', 'subword', 'units', 'retain', 'linguistic', 'meaning', '(', 'like', 'morphemes', ')', ',', 'while', 'alleviating']\n",
      "\n",
      "Testing ': out-of-vocabulary situations even with a relatively\n",
      "Tokenized: ['out', '-', 'of', '-', 'vocabulary', 'situations', 'even', 'with', 'a', 'relatively']\n",
      "\n",
      "Testing ': small-size vocabulary.\n",
      "Tokenized: ['small', '-', 'size', 'vocabulary', '.']\n",
      "\n",
      "Testing ': ∗ Research conducted while working at Google.\n",
      "Tokenized: ['∗', 'conducted', 'while', 'working', 'at', 'google', '.']\n",
      "\n",
      "Testing ': In this paper, we propose efficient algorithms\n",
      "Tokenized: ['in', 'this', 'paper', ',', 'we', 'propose', 'efficient', 'algorithms']\n",
      "\n",
      "Testing ': for WordPiece, the subword tokenization used in\n",
      "Tokenized: ['for', ',', 'the', 'subword', 'tokenization', 'used', 'in']\n",
      "\n",
      "Testing ': BERT (Devlin et al., 2019). Given Unicode text\n",
      "Tokenized: ['bert', '(', None, 'et', 'al', '.', ',', '2019', ')', '.', 'given', None, 'code', 'text']\n",
      "\n",
      "Testing ': that has already been cleaned up and normalized,\n",
      "Tokenized: ['that', 'has', 'already', 'been', 'cleaned', 'up', 'and', 'normalized', ',']\n",
      "\n",
      "Testing ': WordPiece has two steps: (1) pre-tokenize the text\n",
      "Tokenized: ['has', 'two', 'steps', ':', '(', '1', ')', 'pre', '-', 'tokenize', 'the', 'text']\n",
      "\n",
      "Testing ': into words (by splitting on punctuation and whitespaces), and (2) tokenize each word into wordpieces.\n",
      "Tokenized: ['into', 'words', '(', 'by', 'splitting', 'on', 'punctuation', 'and', 'whitespaces', ')', ',', 'and', '(', '2', ')', 'tokenize', 'each', 'word', 'into', 'wordpieces', '.']\n",
      "\n",
      "Testing ': For single-word tokenization, WordPiece uses\n",
      "Tokenized: ['for', 'single', '-', 'word', 'tokenization', ',', 'uses']\n",
      "\n",
      "Testing ': a greedy longest-match-first strategy: iteratively\n",
      "Tokenized: ['a', 'greedy', 'longest', '-', 'match', '-', 'first', 'strategy', ':', 'iteratively']\n",
      "\n",
      "Testing ': pick the longest prefix of the remaining text that\n",
      "Tokenized: ['pick', 'the', 'longest', 'prefix', 'of', 'the', 'remaining', 'text', 'that']\n",
      "\n",
      "Testing ': matches a vocabulary token. This is well-known as\n",
      "Tokenized: ['matches', 'a', 'vocabulary', 'token', '.', 'this', 'is', 'well', '-', 'known', 'as']\n",
      "\n",
      "Testing ': Maximum Matching or MaxMatch (Palmer, 2000),\n",
      "Tokenized: ['maximum', 'matching', 'or', 'max', 'match', '(', None, None, ',', '2000', ')', ',']\n",
      "\n",
      "Testing ': which has also been used for Chinese word segmentation since 1980s (Liu and Liang, 1986).\n",
      "Tokenized: ['which', 'has', 'also', 'been', 'used', 'for', None, 'in', None, 'e', 'word', 'segmentation', 'since', '1980s', '(', None, 'and', None, 'an', 'g', ',', '1986', ')', '.']\n",
      "\n",
      "Testing ': Despite its wide use in NLP for decades, to the\n",
      "Tokenized: ['despite', 'its', 'wide', 'use', 'in', 'n', None, 'for', 'decades', ',', 'to', 'the']\n",
      "\n",
      "Testing ': best of our knowledge, the most efficient MaxMatch algorithms so far are 푂(푛\n",
      "Tokenized: ['best', 'of', 'our', 'knowledge', ',', 'the', 'most', 'efficient', 'max', 'match', 'algorithms', 'so', 'far', 'are', '푂', '(', '푛']\n",
      "\n",
      "Testing ': 2\n",
      "Tokenized: ['2']\n",
      "\n",
      "Testing ': ) (where 푛 is the\n",
      "Tokenized: [')', '(', 'where', '푛', 'is', 'the']\n",
      "\n",
      "Testing ': input word length) or 푂(푛푚) (where 푚 is the maximum vocabulary token length) (see Section 2).\n",
      "Tokenized: ['input', 'word', 'length', ')', 'or', '푂', '(', '푛푚', ')', '(', 'where', '푚', 'is', 'the', 'maximum', 'vocabulary', 'token', 'length', ')', '(', 'see', 'section', '2', ')', '.']\n",
      "\n",
      "Testing ': It’s worth noting that the latter has a vocabularyspecific multiplicative factor 푚, which can be large\n",
      "Tokenized: ['it', '’', 's', 'worth', 'noting', 'that', 'the', 'latter', 'has', 'a', 'vocabularyspecific', 'multiplicative', 'factor', '푚', ',', 'which', 'can', 'be', 'large']\n",
      "\n",
      "Testing ': when the vocabulary contains long words.\n",
      "Tokenized: ['when', 'the', 'vocabulary', 'contains', 'long', 'words', '.']\n",
      "\n",
      "Testing ': We propose LinMaxMatch, a novel MaxMatch\n",
      "Tokenized: ['we', 'propose', None, 'max', 'match', ',', 'a', 'novel', 'max', 'match']\n",
      "\n",
      "Testing ': algorithm for WordPiece tokenization, whose\n",
      "Tokenized: ['algorithm', 'for', 'tokenization', ',', 'whose']\n",
      "\n",
      "Testing ': tokenization time is strictly 푂(푛) without any\n",
      "Tokenized: ['tokenization', 'time', 'is', 'strictly', '푂', '(', '푛', ')', 'without', 'any']\n",
      "\n",
      "Testing ': vocabulary-specific multiplicative factors. Inspired\n",
      "Tokenized: ['vocabulary', '-', 'specific', 'multiplicative', 'factors', '.', 'inspired']\n",
      "\n",
      "Testing ': by the Aho-Corasick algorithm (Aho and Corasick, 1975), we organize vocabulary tokens in a\n",
      "Tokenized: ['by', 'the', 'a', None, '-', None, 'as', 'i', 'ck', 'algorithm', '(', 'a', 'and', None, 'as', 'i', 'ck', ',', '1975', ')', ',', 'we', 'organize', 'vocabulary', 'tokens', 'in', 'a']\n",
      "\n",
      "Testing ': trie (Fredkin, 1960) and introduce precomputed\n",
      "Tokenized: ['trie', '(', None, 'd', ',', '1960', ')', 'and', 'introduce', 'precomputed']\n",
      "\n",
      "Testing ': failure links and failure pops. During tokenization, if an input character does not match any trie\n",
      "Tokenized: ['failure', 'links', 'and', 'failure', 'pops', '.', 'during', 'tokenization', ',', 'if', 'an', 'input', 'character', 'does', 'not', 'match', 'any', 'trie']\n",
      "\n",
      "Testing ': edge, we perform smart transitions to avoid backtracking to earlier input characters. This involves\n",
      "Tokenized: ['edge', ',', 'we', 'perform', 'smart', 'transitions', 'to', 'avoid', 'backtracking', 'to', 'earlier', 'input', 'characters', '.', 'this', 'involves']\n",
      "\n",
      "Testing ': collecting the recognized tokens (i.e., failure pops)\n",
      "Tokenized: ['collecting', 'the', 'recognized', 'tokens', '(', 'i', '.', 'e', '.', ',', 'failure', 'pops', ')']\n",
      "\n",
      "Testing ': and moving to a trie node (via the failure link),\n",
      "Tokenized: ['and', 'moving', 'to', 'a', 'trie', 'node', '(', 'via', 'the', 'failure', 'link', ')', ',']\n",
      "\n",
      "Testing ': from where we continue to match the same character (Section 3).\n",
      "Tokenized: ['from', 'where', 'we', 'continue', 'to', 'match', 'the', 'same', 'character', '(', 'section', '3', ')', '.']\n",
      "\n",
      "Testing ': For general text tokenization, referred to as\n",
      "Tokenized: ['for', 'general', 'text', 'tokenization', ',', 'referred', 'to', 'as']\n",
      "\n",
      "Testing ': end-to-end tokenization in this paper, we propose\n",
      "Tokenized: ['end', '-', 'to', '-', 'end', 'tokenization', 'in', 'this', 'paper', ',', 'we', 'propose']\n",
      "\n",
      "Testing ': E2E WordPiece, an end-to-end algorithm that combines pre-tokenization and WordPiece tokenization\n",
      "Tokenized: ['e', '2', 'e', ',', 'an', 'end', '-', 'to', '-', 'end', 'algorithm', 'that', 'combines', 'pre', '-', 'tokenization', 'and', 'tokenization']\n",
      "\n",
      "Testing ': arXiv:2012.15524v3 [cs.CL] 5 Oct 2021\n",
      "Tokenized: [None, 'xi', None, ':', '2012', '.', '15524v3', '[', 'cs', '.', None, ']', '5', None, 't', '2021']\n",
      "\n",
      "Testing ': into a single, linear-time pass (Section 4).\n",
      "Tokenized: ['into', 'a', 'single', ',', 'linear', '-', 'time', 'pass', '(', 'section', '4', ')', '.']\n",
      "\n",
      "Testing ': Experimental results show that our method is\n",
      "Tokenized: ['experimental', 'results', 'show', 'that', 'our', 'method', 'is']\n",
      "\n",
      "Testing ': 8.2x faster than HuggingFace Tokenizers (HuggingFace, 2020) and 5.1x faster than TensorFlow Text (Google, 2020) on average for general\n",
      "Tokenized: ['8', '.', '2x', 'faster', 'than', None, 'g', None, 'n', 'g', None, 'e', 'tokenizer', 's', '(', None, 'g', None, 'n', 'g', None, 'e', ',', '2020', ')', 'and', '5', '.', '1x', 'faster', 'than', 'tensorflow', 'text', '(', 'google', ',', '2020', ')', 'on', 'average', 'for', 'general']\n",
      "\n",
      "Testing ': text tokenization (Section 5).\n",
      "Tokenized: ['text', 'tokenization', '(', 'section', '5', ')', '.']\n",
      "\n",
      "Testing ': Although tokenization is relatively faster than\n",
      "Tokenized: ['although', 'tokenization', 'is', 'relatively', 'faster', 'than']\n",
      "\n",
      "Testing ': other steps, it’s still worth improving the performance: Tokenization is a prerequisite step for\n",
      "Tokenized: ['other', 'steps', ',', 'it', '’', 's', 'still', 'worth', 'improving', 'the', 'performance', ':', 'tokenization', 'is', 'a', 'prerequisite', 'step', 'for']\n",
      "\n",
      "Testing ': almost all NLP tasks, and any improvement on\n",
      "Tokenized: ['almost', 'all', 'n', None, 'tasks', ',', 'and', 'any', 'improvement', 'on']\n",
      "\n",
      "Testing ': its efficiency helps reduce the latency of the entire inference. One potential impact of the work,\n",
      "Tokenized: ['its', 'efficiency', 'helps', 'reduce', 'the', 'latency', 'of', 'the', 'entire', 'inference', '.', 'one', 'potential', 'impact', 'of', 'the', 'work', ',']\n",
      "\n",
      "Testing ': for example, is on mobile NLP applications. Ondevice models are generally highly optimized for\n",
      "Tokenized: ['for', 'example', ',', 'is', 'on', 'mobile', 'n', None, 'applications', '.', 'on', None, 'i', 'c', 'e', 'models', 'are', 'generally', 'highly', 'optimized', 'for']\n",
      "\n",
      "Testing ': reducing latency, e.g., by distilling or compressing\n",
      "Tokenized: ['reducing', 'latency', ',', 'e', '.', 'g', '.', ',', 'by', 'distilling', 'or', 'compressing']\n",
      "\n",
      "Testing ': larger models. Thus, the impact of tokenization\n",
      "Tokenized: ['larger', 'models', '.', 'thus', ',', 'the', 'impact', 'of', 'tokenization']\n",
      "\n",
      "Testing ': can be significant here. Another impact is on aggregate computational savings for Web services\n",
      "Tokenized: ['can', 'be', 'significant', 'here', '.', 'another', 'impact', 'is', 'on', 'aggregate', 'computational', 'savings', 'for', 'we', 'b', 'services']\n",
      "\n",
      "Testing ': like Google, Facebook, Twitter, etc. For example,\n",
      "Tokenized: ['like', 'google', ',', None, 'e', None, 'k', ',', None, 'it', ',', 'etc', '.', 'for', 'example', ',']\n",
      "\n",
      "Testing ': Google uses BERT to power its Web search nowadays.1 Google serves billions of search queries per\n",
      "Tokenized: ['google', 'uses', 'bert', 'to', 'power', 'its', 'we', 'b', 'search', 'nowadays', '.', '1', 'google', 'serves', 'billions', 'of', 'search', 'queries', 'per']\n",
      "\n",
      "Testing ': day, and it processes hundreds of trillions of Web\n",
      "Tokenized: ['day', ',', 'and', 'it', 'processes', 'hundreds', 'of', 'trillions', 'of', 'we', 'b']\n",
      "\n",
      "Testing ': pages in index building. By employing a faster\n",
      "Tokenized: ['pages', 'in', 'index', 'building', '.', 'by', 'employing', 'a', 'faster']\n",
      "\n",
      "Testing ': tokenization system, the aggregate computational\n",
      "Tokenized: ['tokenization', 'system', ',', 'the', 'aggregate', 'computational']\n",
      "\n",
      "Testing ': savings would be material, which also benefits the\n",
      "Tokenized: ['savings', 'would', 'be', 'material', ',', 'which', 'also', 'benefits', 'the']\n",
      "\n",
      "Testing ': environment (for less power consumption).\n",
      "Tokenized: ['environment', '(', 'for', 'less', 'power', 'consumption', ')', '.']\n",
      "\n",
      "Testing ': This paper also makes a theoretical contribution. The proposed LinMaxMatch algorithm solves\n",
      "Tokenized: ['this', 'paper', 'also', 'makes', 'a', 'theoretical', 'contribution', '.', 'the', 'proposed', None, 'max', 'match', 'algorithm', 'solves']\n",
      "\n",
      "Testing ': the decades-old MaxMatch problem in the optimal\n",
      "Tokenized: ['the', 'decades', '-', 'old', 'max', 'match', 'problem', 'in', 'the', 'optimal']\n",
      "\n",
      "Testing ': 푂(푛) time, and the idea is applicable to other string\n",
      "Tokenized: ['푂', '(', '푛', ')', 'time', ',', 'and', 'the', 'idea', 'is', 'applicable', 'to', 'other', 'string']\n",
      "\n",
      "Testing ': matching or rewriting problems (Section 3.6).\n",
      "Tokenized: ['matching', 'or', 'rewriting', 'problems', '(', 'section', '3', '.', '6', ')', '.']\n",
      "\n",
      "Testing ': The code will be available at https://www.\n",
      "Tokenized: ['the', 'code', 'will', 'be', 'available', 'at', 'https', ':', '/', '/', 'www', '.']\n",
      "\n",
      "Testing ': tensorflow.org/text.\n",
      "Tokenized: ['tensorflow', '.', 'org', '/', 'text', '.']\n",
      "\n",
      "Testing ': 2 Related Work\n",
      "Tokenized: ['2', 'related', 'work']\n",
      "\n",
      "Testing ': Maximum Matching (or MaxMatch) has been used\n",
      "Tokenized: ['maximum', 'matching', '(', 'or', 'max', 'match', ')', 'has', 'been', 'used']\n",
      "\n",
      "Testing ': for Chinese word segmentation (CWS) since the\n",
      "Tokenized: ['for', None, 'in', None, 'e', 'word', 'segmentation', '(', 'c', None, 's', ')', 'since', 'the']\n",
      "\n",
      "Testing ': 1980s (Liu and Liang, 1986; Palmer, 2000). Recent\n",
      "Tokenized: ['1980s', '(', None, 'and', None, 'an', 'g', ',', '1986', ';', None, None, ',', '2000', ')', '.', 'recent']\n",
      "\n",
      "Testing ': CWS work focuses on machine learning-based segmentation approaches, but MaxMatch remains a\n",
      "Tokenized: ['c', None, 's', 'work', 'focuses', 'on', 'machine', 'learning', '-', 'based', 'segmentation', 'approaches', ',', 'but', 'max', 'match', 'remains', 'a']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on larger text\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import json\n",
    "class SimpleSentencePiece:\n",
    "    def __init__(self, model_type=\"bpe\", vocab_size=8000):\n",
    "        self.vocab = {}\n",
    "        self.id_to_subword = {}\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.unk_token_id = 0\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model = None if model_type == \"bpe\" else None\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def train(self, text):\n",
    "        if self.model_type == \"bpe\":\n",
    "            self.model = BPE(num_merges=self.vocab_size, unk_token_id=self.unk_token_id)\n",
    "            self.model.train(text)\n",
    "            self.vocab = self.model.vocab\n",
    "            self.id_to_subword = {i: word for word, i in self.vocab.items()}\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported yet.\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.preprocess_text(text)  # Preprocess text before encoding\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        return self.model.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if not self.id_to_subword:\n",
    "            raise ValueError(\"Vocabulary is empty. Ensure the model is trained first.\")\n",
    "        text = \" \".join([self.id_to_subword.get(id_, self.unk_token) for id_ in ids])\n",
    "        text = text.replace(\" </w>\", \"\").replace(\"</w>\", \" \").strip()\n",
    "        return text\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Convert text to lowercase to ensure case insensitivity\n",
    "        text = text.lower()\n",
    "        # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "        text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Trim leading and trailing spaces\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        model_data = {\n",
    "            'vocab': self.vocab,\n",
    "            'id_to_subword': self.id_to_subword,\n",
    "            'model_type': self.model_type,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            # Potentially include other relevant attributes\n",
    "        }\n",
    "        # Save the high-level tokenizer settings\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(model_data, f)\n",
    "        \n",
    "        # Now save the BPE model specifically\n",
    "        if self.model_type == \"bpe\" and self.model:\n",
    "            self.model.save_model(filepath + \"_bpe\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        self.vocab = model_data['vocab']\n",
    "        self.id_to_subword = model_data['id_to_subword']\n",
    "        self.model_type = model_data['model_type']\n",
    "        self.vocab_size = model_data['vocab_size']\n",
    "        \n",
    "        # Assuming model_type is still \"bpe\", we now load the BPE model\n",
    "        if self.model_type == \"bpe\":\n",
    "            self.model = BPE(self.vocab_size, self.unk_token_id)\n",
    "            self.model.load_model(filepath + \"_bpe\")\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, num_merges=100, unk_token_id=0):  # Accept unk_token_id parameter\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        self.num_merges = num_merges\n",
    "        self.unk_token_id = unk_token_id  # Store the unknown token ID\n",
    "\n",
    "    def train(self, text):\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "        vocab = collections.Counter(words)\n",
    "        vocab = {word + '</w>': count for word, count in vocab.items()}\n",
    "        \n",
    "        for _ in range(self.num_merges):  # Use the num_merges from the instance variable\n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best, vocab)\n",
    "            self.merges.append(best)\n",
    "\n",
    "        self.vocab = {word: i for i, word in enumerate(vocab.keys())}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stats(vocab):\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[symbols[i], symbols[i+1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_vocab(pair, vocab):\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in vocab:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = vocab[word]\n",
    "        return v_out\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text into subwords using learned BPE merges.\"\"\"\n",
    "        encoded_tokens = []\n",
    "        for word in re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE):\n",
    "            word += '</w>'\n",
    "            subwords = [word]  # Start with the entire word as one subword\n",
    "            for merge in self.merges:\n",
    "                new_subwords = []\n",
    "                for subword in subwords:\n",
    "                    # If the merge is in subword, split it; otherwise, keep it as is\n",
    "                    if ' '.join(merge) in subword:\n",
    "                        new_subwords.extend(subword.replace(' '.join(merge), ''.join(merge)).split(' '))\n",
    "                    else:\n",
    "                        new_subwords.append(subword)\n",
    "                subwords = new_subwords\n",
    "            encoded_tokens.extend(subwords)\n",
    "        return [self.vocab.get(token, self.unk_token_id) for token in encoded_tokens]\n",
    "    \n",
    "        # New method to save trained model\n",
    "    def save_model(self, filepath):\n",
    "        bpe_data = {\n",
    "            'merges': self.merges,\n",
    "            'vocab': self.vocab,\n",
    "            'num_merges': self.num_merges,\n",
    "            # Include other attributes as needed\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(bpe_data, f)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            bpe_data = json.load(f)\n",
    "        \n",
    "        self.merges = bpe_data['merges']\n",
    "        self.vocab = bpe_data['vocab']\n",
    "        self.num_merges = bpe_data['num_merges']\n",
    "\n",
    "class WordPiece:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.root = self.build_trie(vocab)\n",
    "        self.compute_failure_links(self.root)\n",
    "        print(\"Trie built successfully.\")\n",
    "\n",
    "    # Add debug prints to build_trie to confirm structure\n",
    "    def build_trie(self, vocab):\n",
    "        root = TrieNode()\n",
    "        for token in vocab:\n",
    "            node = root\n",
    "            for char in token:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.is_end = True\n",
    "            node.token = token\n",
    "        print(\"Trie Construction Completed Successfully\")\n",
    "        return root\n",
    "\n",
    "\n",
    "    def compute_failure_links(self, root):\n",
    "        queue = [root]\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "            for char, child_node in current_node.children.items():\n",
    "                failure_node = current_node.failure_link\n",
    "                while failure_node and char not in failure_node.children:\n",
    "                    failure_node = failure_node.failure_link\n",
    "                child_node.failure_link = failure_node.children[char] if failure_node else root\n",
    "                queue.append(child_node)\n",
    "\n",
    "    # Improved debug prints in tokenize method\n",
    "    def tokenize(self, text):\n",
    "        # Preprocess input text\n",
    "        text = self.preprocess_text(text)\n",
    "        node = self.root\n",
    "        tokens = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(text):\n",
    "            char = text[i]\n",
    "            if char == ' ':\n",
    "                node = self.root\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if char not in node.children:\n",
    "                if node != self.root:\n",
    "                    tokens.append(node.token)  # Add the longest token found\n",
    "                    node = self.root  # Reset to root\n",
    "                    continue\n",
    "                else:\n",
    "                    tokens.append(self.unk_token)\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "            node = node.children[char]\n",
    "            if node.is_end:\n",
    "                if i + 1 == len(text) or text[i + 1] == ' ':\n",
    "                    tokens.append(node.token)\n",
    "                    node = self.root\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        # Convert text to lowercase to ensure case insensitivity\n",
    "        text = text.lower()\n",
    "\n",
    "        # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "        # This depends on how your vocabulary handles punctuation\n",
    "        text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Trim leading and trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = [line.strip() for line in file.readlines()]\n",
    "    return texts\n",
    "\n",
    "texts = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "\n",
    "num_merges = 100\n",
    "\n",
    "# Initialize and train the SimpleSentencePiece model with BPE\n",
    "ssp = SimpleSentencePiece(model_type=\"bpe\", vocab_size=1000)\n",
    "ssp.train('\\n'.join(texts))  # Train the model on the entire dataset\n",
    "\n",
    "# Test the model on a subset or the entire dataset\n",
    "for i, text in enumerate(texts[:100]):  # Example: test on the first 100 texts\n",
    "    encoded = ssp.encode(text)\n",
    "    decoded = ssp.decode(encoded)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Decoded: {decoded}\\n\")\n",
    "\n",
    "# Save the trained model\n",
    "ssp.save_model(\"ssp_model.json\")\n",
    "\n",
    "# Create a new instance and load the model\n",
    "new_ssp = SimpleSentencePiece(model_type=\"bpe\")\n",
    "new_ssp.load_model(\"ssp_model.json\")\n",
    "\n",
    "# New Model\n",
    "for i, text in enumerate(texts[:100]):  # Example: test on the first 100 texts\n",
    "    re_encoded = ssp.encode(text)\n",
    "    re_decoded = ssp.decode(re_encoded)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Decoded: {re_decoded}\\n\")\n",
    "\n",
    "\n",
    "# Assuming ssp is your SimpleSentencePiece instance\n",
    "vocab = ssp.vocab  # This gets the vocabulary after BPE training\n",
    "new_vocab = new_ssp.vocab  # This gets the vocabulary after BPE training\n",
    "\n",
    "print(f\"vocab ; {len(vocab)} , new_vocab: {len(new_vocab)}\")\n",
    "\n",
    "def adapt_vocab_for_wordpiece(ssp_vocab):\n",
    "    adapted_vocab = {}\n",
    "    for token, id_or_freq in ssp_vocab.items():\n",
    "        # Check if a token is a continuation subword and not a standalone word\n",
    "        # Since BPE might not mark subwords in a way WordPiece expects, we adapt based on our best approximation\n",
    "        if not token.startswith(\" \") and not token.endswith(\"</w>\"):\n",
    "            adapted_token = \"##\" + token.replace(\"</w>\", \"\")  # Removing BPE's end-of-word marker and prepending \"##\"\n",
    "        else:\n",
    "            adapted_token = token.replace(\"</w>\", \"\")  # Just remove the BPE's end-of-word marker for standalone words\n",
    "\n",
    "        adapted_vocab[adapted_token] = id_or_freq\n",
    "    return adapted_vocab\n",
    "\n",
    "\n",
    "# Assuming ssp is your SimpleSentencePiece instance after BPE training\n",
    "wordpiece_vocab = adapt_vocab_for_wordpiece(ssp.vocab)\n",
    "\n",
    "# Debugging step to ensure vocabulary completeness\n",
    "def debug_vocab(adapted_vocab):\n",
    "    print(\"Sample Vocabulary Check:\")\n",
    "    # Iterate over the first 10 key-value pairs in the adapted vocabulary\n",
    "    for i, (token, id_or_freq) in enumerate(adapted_vocab.items()):\n",
    "        print(f\"{token}: {id_or_freq}\")\n",
    "        if i >= 9:  # Stop after printing 10 entries\n",
    "            break\n",
    "    # Specifically check for subtokens if your tokenizer expects them\n",
    "    subtokens = [token for token in adapted_vocab.keys() if token.startswith(\"##\")]\n",
    "    print(f\"Found {len(subtokens)} subtokens in vocabulary.\")\n",
    "\n",
    "\n",
    "# Ensure wordpiece_vocab is a list of vocabulary tokens\n",
    "debug_vocab(wordpiece_vocab)  # Call this after initializing wordpiece_vocab\n",
    "\n",
    "# Initialize WordPiece with the adapted vocabulary\n",
    "wordpiece = WordPiece(wordpiece_vocab)\n",
    "\n",
    "for i, text in enumerate(texts[:100]):\n",
    "    print(f\"Testing ': {text}\")\n",
    "    tokenized = wordpiece.tokenize(text)\n",
    "    print(f\"Tokenized: {tokenized}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
