{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "\n",
    "# Assuming you have a way to load your vocab mapping (word to index)\n",
    "# For simplicity, let's pretend we have a vocab dictionary and a reverse_vocab for encoding and decoding\n",
    "vocab = {\"[PAD]\": 0, \"[UNK]\": 1}  # Add the rest of your vocabulary here\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "pad_token_id = vocab[\"[PAD]\"]\n",
    "unk_token_id = vocab[\"[UNK]\"]\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.pattern = re.compile(r'[\\w]+|[^\\w\\s]')  # Regex to split words and punctuation\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.pattern.findall(text)  # Improved tokenization\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            subwords = self.find_subwords(token)\n",
    "            token_ids.extend(subwords)\n",
    "        return token_ids\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        subwords = []\n",
    "        i = 0\n",
    "        while i < len(token):\n",
    "            found_subword = False\n",
    "            for j in range(len(token), i, -1):\n",
    "                subword = token[i:j]\n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(self.vocab[subword])\n",
    "                    i = j\n",
    "                    found_subword = True\n",
    "                    break\n",
    "            if not found_subword:\n",
    "                subwords.append(unk_token_id)  # Fallback to UNK\n",
    "                i += 1  # Move to the next character\n",
    "        return subwords\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        if padding_length > 0:\n",
    "            encoded_text += [pad_token_id] * padding_length\n",
    "        else:\n",
    "            encoded_text = encoded_text[:self.max_seq_len]\n",
    "\n",
    "        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Adjusting the EmbeddingLayer to not use the Tokenizer's non-existent vocab attribute\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        return self.embedding(token_ids)\n",
    "\n",
    "# Correcting TransformerEncoderLayer's forward method to properly use MultiHeadAttention\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.norm1(src)\n",
    "        q, _ = self.self_attn(src2, src2, src2)\n",
    "        src = src + self.dropout(q)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "# Correction: Pooler squeezes the wrong dimension; it should squeeze dimension 0 (batch dimension is assumed to be 1 here)\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "# Load pre-trained tokenizer and adjust vocab_size accordingly\n",
    "tokenizer = Tokenizer(vocab=vocab)\n",
    "\n",
    "# Assuming vocab_size is the length of your vocab dictionary\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    EmbeddingLayer(vocab_size=vocab_size, embedding_dim=embedding_dim),\n",
    "    TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dim_feedforward=2048, dropout=0.1),\n",
    "    Pooler(d_model=embedding_dim)\n",
    ")\n",
    "\n",
    "# Correcting the training and evaluation loop\n",
    "# Load and preprocess data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_texts = train_data[\"text\"].tolist()\n",
    "train_labels = train_data[\"label\"].tolist()\n",
    "\n",
    "# Convert texts and labels into a Dataset and DataLoader\n",
    "max_seq_len = 512\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop corrected for proper input handling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for token_ids, labels in train_dataloader:\n",
    "        token_ids, labels = token_ids.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.positional_embeddings = PositionalEncoding(max_seq_len, embedding_dim)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        token_embeds = self.token_embeddings(token_ids)  # [batch_size, seq_len, embedding_dim]\n",
    "        position_embeds = self.positional_embeddings(token_embeds)  # [seq_len, embedding_dim]\n",
    "        return token_embeds + position_embeds\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: D:\\EXPERT_WEIGHTS\n",
      "Working Directory After change WD: D:\\EXPERT_WEIGHTS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "os.chdir('D:\\EXPERT_WEIGHTS')\n",
    "\n",
    "print(\"Working Directory After change WD:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer unk_token_id: 1\n",
      "Tokenizer Vocabulary: {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, ' ': 4, '\\n': 5, '.': 6, 'the': 7, ',': 8, 'of': 9, 'to': 10, 'and': 11, 'a': 12, 'is': 13, 'in': 14, 'that': 15, 'on': 16, '1': 17, 'networks': 18, 'shot': 19, 'for': 20, 'are': 21, 'we': 22, 'network': 23, 'We': 24, 'features': 25, 'dataset': 26, 'layer': 27, 'layers': 28, 'as': 29, '2': 30, 'from': 31, 'class': 32, 'with': 33, '0': 34, 'by': 35, 'each': 36, '5': 37, 'The': 38, 'classes': 39, 'training': 40, '3': 41, '4': 42, 'be': 43, 'performance': 44, 'or': 45, 'this': 46, 'learning': 47, 'trained': 48, 'A': 49, 'first': 50, 'points': 51, 'way': 52, 'distance': 53, 'task': 54, 'which': 55, 'embedding': 56, 'random': 57, 'set': 58, 'target': 59, 'an': 60, 'B': 61, 'al': 62, 'et': 63, 'prototypical': 64, '7': 65, 'can': 66, 'than': 67, 'few': 68, 'not': 69, 'at': 70, 'base': 71, 'two': 72, 'our': 73, 'results': 74, '6': 75, 'approach': 76, 'classification': 77, 'fine': 78, 'number': 79, 'one': 80, 'support': 81, 'transfer': 82, 'In': 83, 'data': 84, 'n': 85, 'use': 86, 'it': 87, 'over': 88, 'these': 89, 'using': 90, 'examples': 91, 'neural': 92, 'per': 93, 'test': 94, 'when': 95, 'This': 96, 'have': 97, 'only': 98, 'then': 99, 'also': 100, 'general': 101, 'new': 102, 'query': 103, 'accuracy': 104, 'because': 105, 'better': 106, 'different': 107, 'episodes': 108, 'meta': 109, 'more': 110, '20': 111, 'AnB': 112, 'Figure': 113, 'episode': 114, 'learn': 115, 'learned': 116, 'other': 117, 'same': 118, 'similar': 119, 'space': 120, 'tasks': 121, 'x': 122, 'both': 123, 'images': 124, 'natural': 125, 'specific': 126, 'train': 127, 'well': 128, '29': 129, 'Euclidean': 130, 'ImageNet': 131, 'but': 132, 'even': 133, 'given': 134, 'higher': 135, 'linear': 136, 'matching': 137, 'model': 138, 'prototype': 139, 'they': 140, 'weights': 141, 'BnB': 142, 'N': 143, 'Section': 144, 'baseB': 145, 'been': 146, 'into': 147, 'mean': 148, 'perform': 149, 'their': 150, 'top': 151, 'transferred': 152, 'used': 153, 'ck': 154, 'has': 155, 'non': 156, 'show': 157, 'simple': 158, 'tuning': 159, 'where': 160, 'will': 161, 'work': 162, 'zero': 163, '50': 164, '8': 165, 'Bregman': 166, 'between': 167, 'case': 168, 'classifier': 169, 'datasets': 170, 'effect': 171, 'experiments': 172, 'filters': 173, 'loss': 174, 'made': 175, 'randomly': 176, 'so': 177, 'there': 178, 'was': 179, ';': 180, 'For': 181, 'Prototypical': 182, 'These': 183, 'all': 184, 'co': 185, 'compare': 186, 'dimensional': 187, 'e': 188, 'frozen': 189, 'half': 190, 'k': 191, 'large': 192, 'may': 193, 'point': 194, 'prototypes': 195, 'split': 196, 'splits': 197, 'such': 198, 'time': 199, 'tuned': 200, '22': 201, '98': 202, 'Cosine': 203, 'SJE': 204, 'cluster': 205, 'do': 206, 'drop': 207, 'example': 208, 'fφ': 209, 'generalization': 210, 'level': 211, 'lower': 212, 'produce': 213, 'result': 214, 'selffer': 215, 'state': 216, 'upper': 217, 'validation': 218, 'vector': 219, '60': 220, 'Few': 221, 'Omniglot': 222, 'Table': 223, 'Vinyals': 224, 'art': 225, 'boost': 226, 'color': 227, 'could': 228, 'embedded': 229, 'found': 230, 'how': 231, 'its': 232, 'just': 233, 'man': 234, 'method': 235, 'nearest': 236, 'small': 237, 'toward': 238, 'transferring': 239, 'without': 240, '2009': 241, '28': 242, '64': 243, '9': 244, 'B3B': 245, 'CUB': 246, 'LSTM': 247, 'Larochelle': 248, 'Performance': 249, 'Ravi': 250, 'Top': 251, 'after': 252, 'based': 253, 'computed': 254, 'convolutional': 255, 'distances': 256, 'divergence': 257, 'either': 258, 'find': 259, 'function': 260, 'g': 261, 'here': 262, 'however': 263, 'if': 264, 'labeled': 265, 'left': 266, 'less': 267, 'middle': 268, 'much': 269, 'overfitting': 270, 'parameters': 271, 'particular': 272, 'possible': 273, 'proposed': 274, 'representation': 275, 'separate': 276, 'softmax': 277, 'them': 278, 'treatment': 279, 'vs': 280, 'within': 281, 'would': 282, '10': 283, '16': 284, '2011': 285, '23': 286, 'A3B': 287, 'Euclid': 288, 'However': 289, 'J': 290, 'K': 291, 'Matching': 292, 'NC': 293, 'NETWORKS': 294, 'On': 295, 'Sk': 296, 'University': 297, 'Zero': 298, 'achieve': 299, 'adapted': 300, 'chosen': 301, 'containing': 302, 'copied': 303, 'cosine': 304, 'd': 305, 'deep': 306, 'diamonds': 307, 'dissimilar': 308, 'does': 309, 'due': 310, 'experiment': 311, 'extent': 312, 'further': 313, 'image': 314, 'instead': 315, 'make': 316, 'metric': 317, 'order': 318, 'output': 319, 'particularly': 320, 'procedure': 321, 'rather': 322, 'recent': 323, 'reported': 324, 'represent': 325, 'row': 326, 'seen': 327, 'smaller': 328, 'squared': 329, 'transferability': 330, 'transition': 331, 'were': 332, 'whether': 333, 'xi': 334, 'yi': 335, '1000': 336, '200': 337, '2012': 338, '30': 339, '500': 340, 'Classification': 341, 'D': 342, 'FCE': 343, 'Gabor': 344, 'GoogLeNet': 345, 'If': 346, 'It': 347, 'Jarrett': 348, 'Model': 349, 'NS': 350, 'To': 351, 'above': 352, 'almost': 353, 'another': 354, 'any': 355, 'approaches': 356, 'architecture': 357, 'cat': 358, 'contains': 359, 'corresponding': 360, 'create': 361, 'degree': 362, 'density': 363, 'divergences': 364, 'end': 365, 'episodic': 366, 'equivalent': 367, 'error': 368, 'expected': 369, 'feature': 370, 'following': 371, 'generalize': 372, 'i': 373, 'including': 374, 'input': 375, 'issues': 376, 'last': 377, 'margin': 378, 'miniImageNet': 379, 'must': 380, 'neighbor': 381, 'neurons': 382, 'original': 383, 'performed': 384, 'problem': 385, 'red': 386, 'right': 387, 'second': 388, 'several': 389, 'shown': 390, 'shows': 391, 'specificity': 392, 'statistic': 393, 'subset': 394, 'surprising': 395, 'vectors': 396, 'weight': 397, 'y': 398, '15': 399, '19': 400, '40': 401, '450k': 402, '54': 403, '?': 404, 'Acc': 405, 'As': 406, 'Caltech': 407, 'Dept': 408, 'Each': 409, 'MATCHING': 410, 'NCA': 411, 'Nc': 412, 'Networks': 413, 'Our': 414, 'RANDOMSAMPLE': 415, 'RM': 416, 'S': 417, 'Thus': 418, 'Training': 419, 'When': 420, 'adaptation': 421, 'applied': 422, 'attribute': 423, 'average': 424, 'baseA': 425, 'blobs': 426, 'blue': 427, 'bottom': 428, 'call': 429, 'choice': 430, 'choices': 431, 'circles': 432, 'curious': 433, 'define': 434, 'descent': 435, 'difficulties': 436, 'distribution': 437, 'during': 438, 'effects': 439, 'evidence': 440, 'exhibit': 441, 'exp': 442, 'family': 443, 'follow': 444, 'four': 445, 'fragile': 446, 'functions': 447, 'generality': 448, 'greatly': 449, 'improves': 450, 'initialized': 451, 'line': 452, 'literature': 453, 'makes': 454, 'match': 455, 'methods': 456, 'mixture': 457, 'models': 458, 'multiple': 459, 'normalization': 460, 'optimization': 461, 'out': 462, 'paper': 463, 'provide': 464, 'provided': 465, 'quantify': 466, 'rate': 467, 'regular': 468, 's': 469, 'scenario': 470, 'single': 471, 'size': 472, 'statistician': 473, 'straightforward': 474, 'those': 475, 'three': 476, 'versions': 477, 'via': 478, 'white': 479, '000': 480, '100': 481, '2014': 482, '25': 483, '26': 484, '31': 485, '43': 486, '56': 487, '600': 488, '84': 489, '93': 490, 'AlexNet': 491, 'Bengio': 492, 'BnA': 493, 'Computer': 494, 'DA': 495, 'DS': 496, 'Datasets': 497, 'Equation': 498, 'Features': 499, 'Fine': 500, 'KNN': 501, 'Layer': 502, 'Learning': 503, 'NQ': 504, 'Nets': 505, 'OURS': 506, 'One': 507, 'Random': 508, 'SGD': 509, 'Science': 510, 'Select': 511, 'Since': 512, 'WA1': 513, 'accuracies': 514, 'across': 515, 'analysis': 516, 'attributes': 517, 'averaged': 518, 'baselines': 519, 'being': 520, 'bias': 521, 'bird': 522, 'character': 523, 'choose': 524, 'classify': 525, 'clustering': 526, 'come': 527, 'common': 528, 'comparison': 529, 'computation': 530, 'compute': 531, 'dark': 532, 'demonstrate': 533, 'denotes': 534, 'depends': 535, 'details': 536, 'determined': 537, 'directly': 538, 'discussed': 539, 'distant': 540, 'dominate': 541, 'dominates': 542, 'drops': 543, 'eight': 544, 'enable': 545, 'enough': 546, 'entire': 547, 'equivalence': 548, 'estimation': 549, 'expect': 550, 'exponential': 551, 'extend': 552, 'felid': 553, 'final': 554, 'finding': 555, 'five': 556, 'fixed': 557, 'gradient': 558, 'hand': 559, 'having': 560, 'high': 561, 'hyperparameters': 562, 'idea': 563, 'identical': 564, 'improve': 565, 'increases': 566, 'individual': 567, 'inductive': 568, 'initializing': 569, 'iterations': 570, 'keep': 571, 'key': 572, 'knowledge': 573, 'label': 574, 'labels': 575, 'larger': 576, 'learns': 577, 'least': 578, 'length': 579, 'light': 580, 'like': 581, 'limited': 582, 'local': 583, 'making': 584, 'many': 585, 'material': 586, 'meaning': 587, 'means': 588, 'near': 589, 'need': 590, 'observed': 591, 'obtain': 592, 'optimal': 593, 'ours': 594, 'pairs': 595, 'phenomenon': 596, 'pooling': 597, 'posterior': 598, 'previously': 599, 'processing': 600, 'propose': 601, 'pφ': 602, 'quite': 603, 'related': 604, 'respect': 605, 'retraining': 606, 'rotations': 607, 'scenarios': 608, 'seems': 609, 'sense': 610, 'setting': 611, 'showing': 612, 'simpler': 613, 'simply': 614, 'since': 615, 'some': 616, 'splitting': 617, 'study': 618, 'subplot': 619, 'suggests': 620, 'supervised': 621, 'supplementary': 622, 'take': 623, 'thus': 624, 'tune': 625, 'unit': 626, 'untrained': 627, 'useful': 628, 'utilizes': 629, 'various': 630, 'very': 631, 'vk': 632, 'weighted': 633, 'while': 634, 'z': 635, 'φ': 636, '024': 637, '101': 638, '11': 639, '12': 640, '13': 641, '14': 642, '17': 643, '2013': 644, '27': 645, '312': 646, '42': 647, '44': 648, '449': 649, '49': 650, '551': 651, '58': 652, '62': 653, '66': 654, '70': 655, '71': 656, '78': 657, '79': 658, '80': 659, '95': 660, '99': 661, 'ALE': 662, 'Abstract': 663, 'Accuracy': 664, 'Adam': 665, 'Algorithm': 666, 'All': 667, 'An': 668, 'Another': 669, 'Birds': 670, 'Bottom': 671, 'Comparison': 672, 'Cornell': 673, 'DVk': 674, 'Dist': 675, 'Donahue': 676, 'Fei': 677, 'Finally': 678, 'First': 679, 'Further': 680, 'ILSVRC': 681, 'Introduction': 682, 'Krizhevsky': 683, 'LG': 684, 'LMNN': 685, 'Large': 686, 'Lines': 687, 'Mahalanobis': 688, 'Man': 689, 'Mensink': 690, 'Natural': 691, 'Note': 692, 'Of': 693, 'PROTOTYPICAL': 694, 'Proto': 695, 'Qk': 696, 'R': 697, 'Results': 698, 'Such': 699, 'That': 700, 'Their': 701, 'There': 702, 'They': 703, 'Toronto': 704, 'Transfer': 705, 'Tune': 706, 'Twitter': 707, 'UCSD': 708, 'WA2': 709, 'WA3': 710, 'WB1': 711, 'WB2': 712, 'WB3': 713, 'While': 714, 'X': 715, 'able': 716, 'about': 717, 'achieves': 718, 'affected': 719, 'aggregated': 720, 'algorithms': 721, 'allowed': 722, 'allows': 723, 'applying': 724, 'approximate': 725, 'approximately': 726, 'arXiv': 727, 'architectural': 728, 'assigned': 729, 'assigning': 730, 'axis': 731, 'bars': 732, 'batch': 733, 'become': 734, 'beneficial': 735, 'block': 736, 'c1': 737, 'c2': 738, 'c3': 739, 'called': 740, 'carefully': 741, 'cases': 742, 'categories': 743, 'chance': 744, 'characters': 745, 'chopped': 746, 'closer': 747, 'collectively': 748, 'com': 749, 'combination': 750, 'comparable': 751, 'compared': 752, 'computing': 753, 'conditional': 754, 'conducted': 755, 'confidence': 756, 'conjecture': 757, 'connects': 758, 'considering': 759, 'consist': 760, 'contain': 761, 'control': 762, 'controls': 763, 'copy': 764, 'created': 765, 'cs': 766, 'custom': 767, 'decisions': 768, 'decoupled': 769, 'decreases': 770, 'degradation': 771, 'demonstrated': 772, 'depend': 773, 'depending': 774, 'design': 775, 'designed': 776, 'detectors': 777, 'determine': 778, 'developed': 779, 'did': 780, 'differ': 781, 'discriminative': 782, 'distinct': 783, 'divided': 784, 'domain': 785, 'draws': 786, 'dϕ': 787, 'effectively': 788, 'effectiveness': 789, 'efficient': 790, 'elements': 791, 'embeddings': 792, 'empirical': 793, 'empirically': 794, 'entities': 795, 'exact': 796, 'expense': 797, 'experimental': 798, 'experimentally': 799, 'explanation': 800, 'extension': 801, 'extensions': 802, 'fact': 803, 'fully': 804, 'gap': 805, 'generalizing': 806, 'generated': 807, 'generative': 808, 'giving': 809, 'good': 810, 'group': 811, 'grows': 812, 'handle': 813, 'help': 814, 'helpful': 815, 'hexagons': 816, 'implementation': 817, 'improved': 818, 'improvement': 819, 'improving': 820, 'include': 821, 'increased': 822, 'indicate': 823, 'indicates': 824, 'indicating': 825, 'initialization': 826, 'insight': 827, 'intervals': 828, 'involves': 829, 'k0': 830, 'known': 831, 'lead': 832, 'learnable': 833, 'leopard': 834, 'levels': 835, 'lingers': 836, 'lion': 837, 'log': 838, 'longer': 839, 'main': 840, 'mapping': 841, 'max': 842, 'maximize': 843, 'might': 844, 'minimal': 845, 'modeling': 846, 'most': 847, 'naturally': 848, 'nature': 849, 'nearly': 850, 'negatively': 851, 'neighboring': 852, 'nets': 853, 'next': 854, 'nonlinearity': 855, 'normalized': 856, 'note': 857, 'noticing': 858, 'object': 859, 'obtained': 860, 'occur': 861, 'optimize': 862, 'ordinary': 863, 'originally': 864, 'overfit': 865, 'own': 866, 'partitioning': 867, 'performing': 868, 'place': 869, 'plus': 870, 'poorly': 871, 'pre': 872, 'predict': 873, 'predictions': 874, 'previous': 875, 'primarily': 876, 'process': 877, 'produces': 878, 'producing': 879, 'quantified': 880, 'questions': 881, 'quickly': 882, 'reason': 883, 'reference': 884, 'regime': 885, 'relatively': 886, 'relevant': 887, 'remaining': 888, 'report': 889, 'representations': 890, 'represents': 891, 'require': 892, 'retrained': 893, 'reveals': 894, 'rows': 895, 'scratch': 896, 'sections': 897, 'serve': 898, 'severely': 899, 'should': 900, 'showed': 901, 'significant': 902, 'similarity': 903, 'sizes': 904, 'slightly': 905, 'solution': 906, 'specialization': 907, 'species': 908, 'spherical': 909, 'standard': 910, 'statistically': 911, 'still': 912, 'strictly': 913, 'subsets': 914, 'substantial': 915, 'subtracting': 916, 'successful': 917, 'suggesting': 918, 'tend': 919, 'terms': 920, 'therefore': 921, 'third': 922, 'through': 923, 'tiger': 924, 'total': 925, 'transformation': 926, 'transformed': 927, 'true': 928, 'unsupervised': 929, 'up': 930, 'updates': 931, 'us': 932, 'uses': 933, 'utilizing': 934, 'version': 935, 'worse': 936, 'written': 937, 'x1': 938, 'xN': 939, 'y1': 940, 'yN': 941, 'yield': 942, 'yields': 943, 'θ': 944, '00': 945, '05': 946, '05175v2': 947, '09': 948, '0k': 949, '1024': 950, '1200': 951, '1411': 952, '1600': 953, '1623': 954, '167': 955, '1703': 956, '1792v1': 957, '1995': 958, '1The': 959, '2000': 960, '2004': 961, '2013a': 962, '2013b': 963, '2017': 964, '21': 965, '24': 966, '281': 967, '2Note': 968, '37': 969, '3AnA': 970, '4We': 971, '51': 972, '52': 973, '55': 974, '625': 975, '645': 976, '68': 977, '73': 978, '77': 979, '788': 980, '800': 981, '86': 982, '90': 983, '96': 984, '97': 985, 'A8B': 986, 'Additionally': 987, 'Aerospace': 988, 'Also': 989, 'Alternately': 990, 'Although': 991, 'Analysis': 992, 'At': 993, 'B8B': 994, 'BASELINE': 995, 'Because': 996, 'By': 997, 'CLUSTERING': 998, 'CU': 999, 'Caffe': 1000, 'Can': 1001, 'Caruana': 1002, 'Challenge': 1003, 'Choices': 1004, 'Classes': 1005, 'Clune': 1006, 'Compared': 1007, 'Components': 1008, 'Compute': 1009, 'Conclusion': 1010, 'Conclusions': 1011, 'DNet': 1012, 'Dark': 1013, 'Degradation': 1014, 'Deng': 1015, 'Density': 1016, 'Design': 1017, 'Discussion': 1018, 'Dissimilar': 1019, 'Distance': 1020, 'Dk': 1021, 'Does': 1022, 'Early': 1023, 'Edwards': 1024, 'Egyptian': 1025, 'Engineering': 1026, 'Episode': 1027, 'Error': 1028, 'Estimation': 1029, 'Examples': 1030, 'Experimental': 1031, 'Experiments': 1032, 'Felidae': 1033, 'Fergus': 1034, 'Fisher': 1035, 'Fortunately': 1036, 'Fourth': 1037, 'GPU': 1038, 'Gaussian': 1039, 'Gaussians': 1040, 'Generality': 1041, 'Girshick': 1042, 'Given': 1043, 'Gradient': 1044, 'Here': 1045, 'Hinton': 1046, 'Hod': 1047, 'How': 1048, 'ILSVRC2012': 1049, 'Image': 1050, 'Indeed': 1051, 'Initial': 1052, 'Initialize': 1053, 'Input': 1054, 'Instead': 1055, 'Institute': 1056, 'Into': 1057, 'Intuitively': 1058, 'Jake': 1059, 'Jason': 1060, 'Jeff': 1061, 'Jia': 1062, 'Jun': 1063, 'KL': 1064, 'Kevin': 1065, 'LEARNER': 1066, 'Layers': 1067, 'Le': 1068, 'Learner': 1069, 'Lee': 1070, 'Left': 1071, 'Legendre': 1072, 'Light': 1073, 'Like': 1074, 'Linear': 1075, 'Lipson4': 1076, 'M': 1077, 'META': 1078, 'Many': 1079, 'Measured': 1080, 'Mechanical': 1081, 'Meta': 1082, 'Mixture': 1083, 'Modern': 1084, 'Modifying': 1085, 'Montreal': 1086, 'Moreover': 1087, 'Much': 1088, 'NEAREST': 1089, 'NEIGHBORS': 1090, 'NETS': 1091, 'NEURAL': 1092, 'Neighborhood': 1093, 'Neither': 1094, 'No': 1095, 'Notably': 1096, 'Notation': 1097, 'Nov': 1098, 'Numbered': 1099, 'Operations': 1100, 'Output': 1101, 'Overall': 1102, 'Overview': 1103, 'P': 1104, 'PROTO': 1105, 'Persian': 1106, 'Points': 1107, 'Previously': 1108, 'Prototype': 1109, 'Pseudocode': 1110, 'ReLU': 1111, 'Recent': 1112, 'Recognition': 1113, 'Reed': 1114, 'Reinterpretation': 1115, 'Related': 1116, 'Relative': 1117, 'Research': 1118, 'Richard': 1119, 'Right': 1120, 'Rippel': 1121, 'SAMPLE': 1122, 'STATISTICIAN': 1123, 'SVM': 1124, 'Salakhutdinov': 1125, 'Scale': 1126, 'Second': 1127, 'Separate': 1128, 'Sermanet': 1129, 'Setup': 1130, 'Shot': 1131, 'Siamese': 1132, 'Similar': 1133, 'Similarly': 1134, 'Snell': 1135, 'Specificity': 1136, 'Splitting': 1137, 'Storkey': 1138, 'Swersky': 1139, 'Thanks': 1140, 'Third': 1141, 'Transferability': 1142, 'Two': 1143, 'Unlike': 1144, 'Update': 1145, 'V': 1146, 'Vector': 1147, 'Visual': 1148, 'WA4': 1149, 'WA5': 1150, 'WA6': 1151, 'WA7': 1152, 'WA8': 1153, 'WB4': 1154, 'WB5': 1155, 'WB6': 1156, 'WB7': 1157, 'WB8': 1158, 'Weights': 1159, 'Where': 1160, 'Whereas': 1161, 'With': 1162, 'Work': 1163, 'Wyoming': 1164, 'Y': 1165, 'Yoshua': 1166, 'Yosinski': 1167, 'Zeiler': 1168, 'Zemel': 1169, 'ability': 1170, 'abs': 1171, 'absolute': 1172, 'accommodate': 1173, 'account': 1174, 'achieving': 1175, 'act': 1176, 'activations': 1177, 'adapt': 1178, 'addition': 1179, 'additional': 1180, 'addressing': 1181, 'advance': 1182, 'advantage': 1183, 'advantageous': 1184, 'affect': 1185, 'against': 1186, 'aggregate': 1187, 'aggregation': 1188, 'aim': 1189, 'algorithm': 1190, 'allowing': 1191, 'alone': 1192, 'alphabets': 1193, 'alternative': 1194, 'although': 1195, 'amount': 1196, 'analyze': 1197, 'answers': 1198, 'anything': 1199, 'anywhere': 1200, 'apparent': 1201, 'apparently': 1202, 'appealing': 1203, 'appear': 1204, 'appearance': 1205, 'applicable': 1206, 'applies': 1207, 'apply': 1208, 'arbitrary': 1209, 'around': 1210, 'ask': 1211, 'assess': 1212, 'assign': 1213, 'assignment': 1214, 'associated': 1215, 'assumption': 1216, 'assumptions': 1217, 'attack': 1218, 'attained': 1219, 'attains': 1220, 'attempt': 1221, 'attempts': 1222, 'attention': 1223, 'attributed': 1224, 'augmenting': 1225, 'author': 1226, 'autoencoder': 1227, 'available': 1228, 'b': 1229, 'back': 1230, 'backprop': 1231, 'backpropagate': 1232, 'batches': 1233, 'befits': 1234, 'beginning': 1235, 'behavior': 1236, 'believe': 1237, 'belonging': 1238, 'below': 1239, 'benchmark': 1240, 'benefits': 1241, 'best': 1242, 'beyond': 1243, 'bi': 1244, 'biological': 1245, 'blob': 1246, 'blocks': 1247, 'boosting': 1248, 'bug': 1249, 'cannot': 1250, 'carries': 1251, 'cats': 1252, 'cause': 1253, 'causes': 1254, 'change': 1255, 'characteristics': 1256, 'characterize': 1257, 'cheetah': 1258, 'choosing': 1259, 'ck0': 1260, 'clarity': 1261, 'classconditional': 1262, 'classified': 1263, 'classifiers': 1264, 'closely': 1265, 'clusters': 1266, 'coadaptation': 1267, 'code': 1268, 'collected': 1269, 'comes': 1270, 'commonly': 1271, 'comparing': 1272, 'competition': 1273, 'completely': 1274, 'complex': 1275, 'complicated': 1276, 'component': 1277, 'composed': 1278, 'composition': 1279, 'comprised': 1280, 'comprises': 1281, 'computationally': 1282, 'concise': 1283, 'conclusions': 1284, 'configuration': 1285, 'connecting': 1286, 'connections': 1287, 'consideration': 1288, 'considered': 1289, 'constant': 1290, 'constrain': 1291, 'construct': 1292, 'constructed': 1293, 'constructing': 1294, 'contained': 1295, 'continuous': 1296, 'contrast': 1297, 'contributions': 1298, 'converge': 1299, 'convergence': 1300, 'convex': 1301, 'convolution': 1302, 'core': 1303, 'cost': 1304, 'couple': 1305, 'course': 1306, 'creates': 1307, 'crop': 1308, 'crops': 1309, 'cumulant': 1310, 'currently': 1311, 'cut': 1312, 'cvpr2016': 1313, 'datapoint': 1314, 'days': 1315, 'deal': 1316, 'decay': 1317, 'decline': 1318, 'decoupling': 1319, 'deeper': 1320, 'defined': 1321, 'definition': 1322, 'definitions': 1323, 'degrees': 1324, 'densities': 1325, 'derived': 1326, 'describes': 1327, 'description': 1328, 'descriptions': 1329, 'despite': 1330, 'differences': 1331, 'differentiable': 1332, 'differs': 1333, 'difficult': 1334, 'difficulty': 1335, 'dimension': 1336, 'diminishes': 1337, 'direction': 1338, 'directional': 1339, 'directions': 1340, 'distinction': 1341, 'distributions': 1342, 'divide': 1343, 'document': 1344, 'doesn': 1345, 'dogs': 1346, 'domains': 1347, 'done': 1348, 'dot': 1349, 'dotted': 1350, 'downloaded': 1351, 'draw': 1352, 'drawn': 1353, 'dynamically': 1354, 'dynamics': 1355, 'easier': 1356, 'effective': 1357, 'ellipsoidal': 1358, 'else': 1359, 'elsewhere': 1360, 'embed': 1361, 'emphasize': 1362, 'encode': 1363, 'encoder': 1364, 'encoding': 1365, 'encourages': 1366, 'environment': 1367, 'epochs': 1368, 'equally': 1369, 'errors': 1370, 'etc': 1371, 'eventually': 1372, 'every': 1373, 'excellent': 1374, 'except': 1375, 'exists': 1376, 'expensive': 1377, 'explorations': 1378, 'extends': 1379, 'extensible': 1380, 'extensive': 1381, 'extensively': 1382, 'extra': 1383, 'extracted': 1384, 'extremely': 1385, 'f': 1386, 'failed': 1387, 'fairly': 1388, 'faithful': 1389, 'falls': 1390, 'far': 1391, 'feather': 1392, 'felids': 1393, 'fewer': 1394, 'figure': 1395, 'files': 1396, 'filter': 1397, 'finetuned': 1398, 'fitted': 1399, 'fix': 1400, 'flexibility': 1401, 'flipped': 1402, 'focus': 1403, 'forces': 1404, 'form': 1405, 'formed': 1406, 'forms': 1407, 'formulate': 1408, 'fragilely': 1409, 'future': 1410, 'gaining': 1411, 'gains': 1412, 'generalizes': 1413, 'generally': 1414, 'get': 1415, 'getting': 1416, 'github': 1417, 'goal': 1418, 'grained': 1419, 'grayscale': 1420, 'greater': 1421, 'groups': 1422, 'gϑ': 1423, 'halves': 1424, 'handwritten': 1425, 'hard': 1426, 'held': 1427, 'helps': 1428, 'hierarchy': 1429, 'hinge': 1430, 'hold': 1431, 'holds': 1432, 'horizontally': 1433, 'http': 1434, 'https': 1435, 'human': 1436, 'humans': 1437, 'hyperparameter': 1438, 'hypothesis': 1439, 'hypothesize': 1440, 'ii': 1441, 'illustration': 1442, 'image2': 1443, 'importance': 1444, 'important': 1445, 'imposes': 1446, 'improvements': 1447, 'incorporate': 1448, 'incorporated': 1449, 'increase': 1450, 'indeed': 1451, 'independent': 1452, 'indices': 1453, 'inference': 1454, 'inferred': 1455, 'information': 1456, 'informative': 1457, 'initial': 1458, 'initialize': 1459, 'interact': 1460, 'interactions': 1461, 'interest': 1462, 'interested': 1463, 'interpretation': 1464, 'interpretations': 1465, 'interpreted': 1466, 'introduced': 1467, 'intuitive': 1468, 'involving': 1469, 'issue': 1470, 'itself': 1471, 'jaguar': 1472, 'join': 1473, 'jointly': 1474, 'justify': 1475, 'keeping': 1476, 'know': 1477, 'kz': 1478, 'latter': 1479, 'learner': 1480, 'leave': 1481, 'likewise': 1482, 'linearity': 1483, 'linearly': 1484, 'lines': 1485, 'linger': 1486, 'little': 1487, 'locked': 1488, 'logX': 1489, 'lost': 1490, 'low': 1491, 'lynx': 1492, 'm': 1493, 'manner': 1494, 'marker': 1495, 'measure': 1496, 'mechanism': 1497, 'mentioned': 1498, 'mimic': 1499, 'mini': 1500, 'minimizing': 1501, 'mirrors': 1502, 'mistakes': 1503, 'modal': 1504, 'modern': 1505, 'modifying': 1506, 'monitoring': 1507, 'mountain': 1508, 'multi': 1509, 'multimodal': 1510, 'multiples': 1511, 'naive': 1512, 'namely': 1513, 'negative': 1514, 'neighborhood': 1515, 'net': 1516, 'never': 1517, 'no': 1518, 'nor': 1519, 'normalize': 1520, 'notation': 1521, 'noted': 1522, 'notions': 1523, 'novel': 1524, 'objective': 1525, 'objectives': 1526, 'observation': 1527, 'obtaining': 1528, 'obviates': 1529, 'occurrence': 1530, 'occurs': 1531, 'off': 1532, 'often': 1533, 'old': 1534, 'opposed': 1535, 'optimized': 1536, 'orange': 1537, 'ordering': 1538, 'outline': 1539, 'outperforms': 1540, 'overlapping': 1541, 'parameter': 1542, 'parameterization': 1543, 'parent': 1544, 'part': 1545, 'patterns': 1546, 'perfectly': 1547, 'performances': 1548, 'performs': 1549, 'permissible': 1550, 'persists': 1551, 'phase': 1552, 'plausible': 1553, 'plots': 1554, 'plotted': 1555, 'post': 1556, 'powerful': 1557, 'prediction': 1558, 'preliminary': 1559, 'preparing': 1560, 'presents': 1561, 'prevents': 1562, 'probabilities': 1563, 'probability': 1564, 'proceeds': 1565, 'produced': 1566, 'progress': 1567, 'promising': 1568, 'pronounced': 1569, 'publication': 1570, 'pψ': 1571, 'quantifying': 1572, 'question': 1573, 'raises': 1574, 'ranges': 1575, 'rapidly': 1576, 'rates': 1577, 'raw': 1578, 're': 1579, 'recovers': 1580, 'rectangles': 1581, 'rectification': 1582, 'rediscovered': 1583, 'reedscot': 1584, 'refer': 1585, 'reflect': 1586, 'regardless': 1587, 'regularization': 1588, 'regularize': 1589, 'reinitialize': 1590, 'relate': 1591, 'relates': 1592, 'relative': 1593, 'relearn': 1594, 'relearned': 1595, 'relearning': 1596, 'released': 1597, 'relies': 1598, 'relu': 1599, 'rely': 1600, 'remainder': 1601, 'repeated': 1602, 'replacement': 1603, 'representative': 1604, 'representatives': 1605, 'represented': 1606, 'reproduce': 1607, 'repurpose': 1608, 'required': 1609, 'requires': 1610, 'requiring': 1611, 'researchers': 1612, 'resemble': 1613, 'resembles': 1614, 'resizing': 1615, 'retrain': 1616, 'rigorous': 1617, 'risk': 1618, 'run': 1619, 'runs': 1620, 'sample': 1621, 'sampled': 1622, 'save': 1623, 'say': 1624, 'schedule': 1625, 'scheme': 1626, 'secondary': 1627, 'selecting': 1628, 'semantically': 1629, 'sets': 1630, 'settings': 1631, 'setup': 1632, 'seven': 1633, 'shape': 1634, 'shared': 1635, 'shifted': 1636, 'shown3': 1637, 'significantly': 1638, 'simplicity': 1639, 'simplify': 1640, 'situation': 1641, 'slight': 1642, 'snow': 1643, 'software': 1644, 'somewhere': 1645, 'sophisticated': 1646, 'sparse': 1647, 'special': 1648, 'specifically': 1649, 'specifies': 1650, 'speed': 1651, 'spread': 1652, 'step': 1653, 'stopping': 1654, 'stops': 1655, 'store': 1656, 'strikingly': 1657, 'studied': 1658, 'studies': 1659, 'studying': 1660, 'subject': 1661, 'subsampling': 1662, 'substantially': 1663, 'successfully': 1664, 'successive': 1665, 'suddenly': 1666, 'suffers': 1667, 'suitability': 1668, 'suitable': 1669, 'suited': 1670, 'summarize': 1671, 'summarizes': 1672, 'summary': 1673, 'suspicion': 1674, 'systems': 1675, 't': 1676, 'tabby': 1677, 'tackle': 1678, 'taken': 1679, 'takes': 1680, 'taking': 1681, 'tanh': 1682, 'technique': 1683, 'techniques': 1684, 'tell': 1685, 'tendency': 1686, 'term': 1687, 'tested': 1688, 'tests': 1689, 'text': 1690, 'theart': 1691, 'themselves': 1692, 'thereby': 1693, 'things': 1694, 'though': 1695, 'tool': 1696, 'trains': 1697, 'transferable': 1698, 'treatments': 1699, 'tweak': 1700, 'tweaking': 1701, 'type': 1702, 'types': 1703, 'under': 1704, 'underlying': 1705, 'uniformly': 1706, 'uniquely': 1707, 'unlabeled': 1708, 'until': 1709, 'upon': 1710, 'usual': 1711, 'usually': 1712, 'utilize': 1713, 'v1': 1714, 'v2': 1715, 'v3': 1716, 'variance': 1717, 'variant': 1718, 'variants': 1719, 'variational': 1720, 'vast': 1721, 'versa': 1722, 'versus': 1723, 'vertical': 1724, 'vice': 1725, 'viewed': 1726, 'visual': 1727, 'vital': 1728, 'want': 1729, 'ways': 1730, 'whereas': 1731, 'whole': 1732, 'whose': 1733, 'wide': 1734, 'won': 1735, 'xk': 1736, 'yosinski': 1737, 'µ': 1738, 'θk': 1739, 'ψ': 1740, 'ϕ': 1741}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 584\u001b[0m\n\u001b[0;32m    571\u001b[0m best_validation_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m freq_threshold \u001b[38;5;129;01min\u001b[39;00m freq_threshold_values:\n\u001b[0;32m    574\u001b[0m \n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# Model instantiation and training setup\u001b[39;00m\n\u001b[0;32m    576\u001b[0m     model \u001b[38;5;241m=\u001b[39m TransformerModel(\n\u001b[0;32m    577\u001b[0m         vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocab), \n\u001b[0;32m    578\u001b[0m         embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, \n\u001b[0;32m    579\u001b[0m         max_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, \n\u001b[0;32m    580\u001b[0m         nhead\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \n\u001b[0;32m    581\u001b[0m         dim_feedforward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m    582\u001b[0m         freq_threshold\u001b[38;5;241m=\u001b[39mfreq_threshold, \n\u001b[0;32m    583\u001b[0m         smaller_embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m\n\u001b[1;32m--> 584\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)  \u001b[38;5;66;03m# Ensure you have previously defined 'device'\u001b[39;00m\n\u001b[0;32m    586\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m) \n\u001b[0;32m    587\u001b[0m     meta_optimizer \u001b[38;5;241m=\u001b[39m AdaptiveWeightDecayOptimizer(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def combined_loss(output, target, model, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)  \n",
    "    regularization_loss = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if isinstance(param, nn.Parameter):  \n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1\n",
    "\n",
    "    return task_loss + regularization_loss\n",
    "\n",
    "# Encoding Transformer Code\n",
    "\n",
    "class AdaptiveDropoutLayer(nn.Module):\n",
    "    def __init__(self, init_dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())  # Use logit transformation for stability\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.log_alpha) \n",
    "        return nn.functional.dropout(x, p=p, training=self.training) \n",
    "\n",
    "class AdaptiveWeightDecayOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, init_l2_strength=0.01):\n",
    "        super().__init__(params, {'lr': lr})\n",
    "        self.log_l2_strength = nn.Parameter(torch.tensor(math.log(init_l2_strength)).float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = torch.exp(self.log_l2_strength)  \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay) \n",
    "                p.update(d_p, group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.positional_embeddings = nn.Parameter(torch.zeros(max_seq_len, embedding_dim), requires_grad=False)\n",
    "        \n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(math.log(10000.0) / embedding_dim))\n",
    "        \n",
    "        self.positional_embeddings[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_embeddings[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.positional_embeddings[:x.size(1), :]\n",
    "\n",
    "class MultiHeadLinformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, k=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k if k is not None else embed_dim // num_heads  # Projection dimension\n",
    "\n",
    "        # Separate projections for each head\n",
    "        self.key_projections = nn.ModuleList([nn.Linear(embed_dim, self.k) for _ in range(num_heads)])\n",
    "        self.value_projections = nn.ModuleList([nn.Linear(embed_dim, self.k) for _ in range(num_heads)]) \n",
    "\n",
    "        self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query):\n",
    "        seq_len, batch_size, _ = query.size()\n",
    "        heads = []  # Store output from each head\n",
    "\n",
    "        for head_idx in range(self.num_heads):\n",
    "            projected_keys = self.key_projections[head_idx](query)\n",
    "            projected_values = self.value_projections[head_idx](query)\n",
    "\n",
    "            # Calculate attention using projected keys and values\n",
    "            attention = torch.softmax(projected_keys.transpose(2, 3) @ projected_values, dim=-1) \n",
    "\n",
    "            out = attention @ projected_values.view(batch_size, seq_len, self.num_heads, self.k)\n",
    "            out = out.transpose(1, 2).contiguous().view(seq_len, batch_size, self.embed_dim)\n",
    "\n",
    "            heads.append(out)\n",
    "\n",
    "        # Concatenate outputs from all heads\n",
    "        out = torch.cat(heads, dim=-1) \n",
    "        out = self.out_projection(out) \n",
    "        return out\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len):  # Removed labels \n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "        encoded_text = self.dynamic_masking(encoded_text)\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def dynamic_masking(self, encoded_text):\n",
    "        for i in range(len(encoded_text)):\n",
    "            if np.random.rand() < 0.15:  # 15% chance like BERT\n",
    "                encoded_text[i] = tokenizer.vocab[\"[MASK]\"] \n",
    "        return encoded_text\n",
    "\n",
    "class AdaptiveEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.split_vocab(vocab, freq_threshold)\n",
    "\n",
    "        self.frequent_embeddings = nn.Embedding(len(self.frequent_vocab), large_embed_dim)\n",
    "        self.infrequent_embeddings = nn.Embedding(len(self.infrequent_vocab), small_embed_dim)\n",
    "\n",
    "        self.positional_embeddings = PositionalEncoding(max_seq_len, large_embed_dim)  \n",
    "\n",
    "\n",
    "    def split_vocab(self, vocab, freq_threshold):\n",
    "        token_counts = [(token, count) for token, count in vocab.items()] \n",
    "        token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "\n",
    "        self.frequent_vocab = {token: i for i, (token, _) in enumerate(token_counts[:split_point])}\n",
    "        self.infrequent_vocab = {token: i for i, (token, _) in enumerate(token_counts[split_point:])}\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        token_embeds = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.frequent_vocab:\n",
    "                embed = self.frequent_embeddings(torch.tensor(token_id).long())\n",
    "            else:\n",
    "                embed = self.infrequent_embeddings(torch.tensor(token_id).long())\n",
    "            token_embeds.append(embed)\n",
    "\n",
    "        token_embeds = torch.stack(token_embeds)\n",
    "        position_embeds = self.positional_embeddings(token_embeds)\n",
    "        return token_embeds + position_embeds\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadLinformerAttention(embed_dim=d_model, num_heads=nhead)\n",
    "        self.dropout1 = AdaptiveDropoutLayer()  # Use AdaptiveDropoutLayer\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            AdaptiveDropoutLayer(),  # Use AdaptiveDropoutLayer here as well\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.dropout2 = AdaptiveDropoutLayer()  # And here\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2 = self.norm1(src)\n",
    "        attn_output = self.attn(src2)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout2(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, nhead, dim_feedforward, \n",
    "                 freq_threshold, smaller_embed_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab, \n",
    "            freq_threshold=freq_threshold,  \n",
    "            large_embed_dim=embedding_dim,       \n",
    "            small_embed_dim=smaller_embed_dim,   \n",
    "            max_seq_len=max_seq_len\n",
    "        )        \n",
    "        self.encoder = TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward)\n",
    "        self.pooler = Pooler(embedding_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        encoded = self.encoder(embedded, src_mask=attention_mask)\n",
    "        pooled = self.pooler(encoded)\n",
    "        return pooled\n",
    "\n",
    "# Tokenizing Code\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        node = self.root\n",
    "        subword_ids = []\n",
    "        for char in token:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "                if node.token_id is not None:\n",
    "                    subword_ids.append(node.token_id)\n",
    "                    break  # Assuming one token maps to one subword for simplicity\n",
    "            else:\n",
    "                break  # No further subword match found\n",
    "        if not subword_ids:  # If no subword was found\n",
    "            subword_ids.append(self.unk_token_id if hasattr(self, 'unk_token_id') else 1) \n",
    "        return subword_ids\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(best)  # Track most frequent pair in each iteration\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.split()  # Assuming simple word splitting\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None \n",
    "        self.failure_pops = None\n",
    "\n",
    "    def train(self, corpus):\n",
    "        vocabulary = self._build_vocabulary(corpus)  # Build the word list\n",
    "        self.trie, self.failure_links, self.failure_pops = self._precompute(vocabulary)\n",
    "\n",
    "    def _encode(self, word):\n",
    "        subword_ids = []\n",
    "        current_node = self.trie.root\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "            if char in current_node.children:\n",
    "                current_node = current_node.children[char]\n",
    "                if current_node.token_id:\n",
    "                    subword_ids.append(current_node.token_id)\n",
    "            else:  # No direct match. Implement failure logic\n",
    "                while True:\n",
    "                    if current_node.failure_link is not None:\n",
    "                        current_node = current_node.failure_link\n",
    "\n",
    "                        # Check for pops to skip further backtracking\n",
    "                        for _ in range(current_node.failure_pop):\n",
    "                            current_node = current_node.failure_link\n",
    "\n",
    "                        if char in current_node.children:\n",
    "                            current_node = current_node.children[char]\n",
    "                            if current_node.token_id:\n",
    "                                subword_ids.append(current_node.token_id)\n",
    "                            break  # Successful subword transition \n",
    "\n",
    "                    else:  # No failure link, we're back at root level\n",
    "                        # Handle situation based on your application\n",
    "                        # e.g., append an unknown token \n",
    "                        subword_ids.append(self.unk_token_id)  \n",
    "                        break  \n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        if model_type == \"unigram\":\n",
    "            tokens = self._unigram_tokenize(corpus)  #  Hypothetical tokenize function\n",
    "            vocabulary = self._build_unigram_vocab(tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        # Count token frequencies\n",
    "        token_freqs = collections.Counter(tokens)\n",
    "        # Select the most frequent tokens up to vocab_size\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(token_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction\n",
    "        self.trie = Trie()  # Assuming you have a Trie class as well\n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token)  \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "    def _unigram_tokenize(self, corpus):\n",
    "        \"\"\"\n",
    "        Tokenizes the given corpus into unigram tokens, checking against the built vocabulary.\n",
    "        Tokens not found in the vocabulary are treated as unknowns.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): The text corpus to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list of str: A list of tokens extracted from the corpus, adjusted to fit the vocabulary.\n",
    "        \"\"\"\n",
    "        # Initial tokenization based on word boundaries and punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', corpus)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token_id = self.vocab.get(\"[UNK]\", 1)  # Get ID of [UNK] \n",
    "        self.max_subword_length = max(len(token) for token in vocab.keys())\n",
    "        self.pattern = re.compile(r'\\b\\w+\\b|[\\s\\.,!?;]')\n",
    "\n",
    "        # Build the Trie\n",
    "        self.trie = Trie()\n",
    "        for token, token_id in vocab.items():\n",
    "            self.trie.insert(token, token_id)\n",
    "\n",
    "    def _find_subwords(self, word):\n",
    "        # 1. Trie Lookup \n",
    "        subword_ids = self.trie.find_subwords(word)\n",
    "\n",
    "        # 2. Fallback to Original Logic \n",
    "        if len(subword_ids) == 1 and subword_ids[0] == self.unk_token_id:  \n",
    "            subwords = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                for j in range(self.max_subword_length, 0, -1):\n",
    "                    subword = word[i:i+j]\n",
    "                    if subword in self.vocab:\n",
    "                        subwords.append(self.vocab[subword])\n",
    "                        i += j\n",
    "                        break \n",
    "                else: \n",
    "                    subwords.append(self.vocab[\"[UNK]\"])\n",
    "                    i += 1\n",
    "            subword_ids = subwords  # Replace with token IDs\n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Add [CLS] token at the beginning\n",
    "        token_ids = [self.vocab[\"[CLS]\"]]\n",
    "        tokens = self.pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            subword_ids = self._find_subwords(token)\n",
    "            token_ids.extend(subword_ids)\n",
    "        # Add [SEP] token at the end\n",
    "        token_ids.append(self.vocab[\"[SEP]\"])\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "# Training Code\n",
    "\n",
    "# Load corpus and build vocab\n",
    "corpus = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "tokens = tokenize(corpus)\n",
    "vocab = build_vocab(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(vocab=vocab)\n",
    "\n",
    "# Data Loading (Illustrative)\n",
    "train_texts = [corpus]  # Treat your whole sample as one \"document\"\n",
    "train_dataset = TextDataset(train_texts, tokenizer, max_seq_len=512)\n",
    "\n",
    "print(\"Tokenizer unk_token_id:\", tokenizer.unk_token_id) \n",
    "print(\"Tokenizer Vocabulary:\", tokenizer.vocab)\n",
    "\n",
    "'''\n",
    "# Sample text for testing your tokenizer\n",
    "test_sentences = [\n",
    "    \"This is a sample sentence.\",\n",
    "    \"Let's tokenize some unusual words with punctuation, shall we?\",\n",
    "    \"1234 numbers or combinations? How does the tokenizer handle this?\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    tokenized_output = tokenizer.encode(sentence)\n",
    "    print(f\"Original Sentence: {sentence}\")\n",
    "    print(f\"Tokenized Output: {tokenized_output}\")\n",
    "    print(\"-\" * 50)  # A separator for visual clarity\n",
    "\n",
    "'''\n",
    "\n",
    "device='cpu'\n",
    "freq_threshold_values = [10, 50, 100, 200, 500]  \n",
    "best_validation_accuracy = 0.0 \n",
    "\n",
    "for freq_threshold in freq_threshold_values:\n",
    "\n",
    "    # Model instantiation and training setup\n",
    "    model = TransformerModel(\n",
    "        vocab_size=len(vocab), \n",
    "        embedding_dim=128, \n",
    "        max_seq_len=512, \n",
    "        nhead=8, \n",
    "        dim_feedforward=2048,\n",
    "        freq_threshold=freq_threshold, \n",
    "        smaller_embed_dim=64\n",
    "    ).to(device)  # Ensure you have previously defined 'device'\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4) \n",
    "    meta_optimizer = AdaptiveWeightDecayOptimizer(model.parameters(), lr=1e-5) \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    meta_update_freq = 5\n",
    "\n",
    "    # Training loop corrected for model architecture\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad() \n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\n",
    "            # Meta-update occasionally \n",
    "            if (i + 1) % meta_update_freq == 0:\n",
    "                meta_optimizer.zero_grad() \n",
    "                loss = combined_loss(output, labels, model) \n",
    "                loss.backward()\n",
    "                meta_optimizer.step()  \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
