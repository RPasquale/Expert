{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "\n",
    "# Assuming you have a way to load your vocab mapping (word to index)\n",
    "# For simplicity, let's pretend we have a vocab dictionary and a reverse_vocab for encoding and decoding\n",
    "vocab = {\"[PAD]\": 0, \"[UNK]\": 1}  # Add the rest of your vocabulary here\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "pad_token_id = vocab[\"[PAD]\"]\n",
    "unk_token_id = vocab[\"[UNK]\"]\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.pattern = re.compile(r'[\\w]+|[^\\w\\s]')  # Regex to split words and punctuation\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.pattern.findall(text)  # Improved tokenization\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            subwords = self.find_subwords(token)\n",
    "            token_ids.extend(subwords)\n",
    "        return token_ids\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        subwords = []\n",
    "        i = 0\n",
    "        while i < len(token):\n",
    "            found_subword = False\n",
    "            for j in range(len(token), i, -1):\n",
    "                subword = token[i:j]\n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(self.vocab[subword])\n",
    "                    i = j\n",
    "                    found_subword = True\n",
    "                    break\n",
    "            if not found_subword:\n",
    "                subwords.append(unk_token_id)  # Fallback to UNK\n",
    "                i += 1  # Move to the next character\n",
    "        return subwords\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        if padding_length > 0:\n",
    "            encoded_text += [pad_token_id] * padding_length\n",
    "        else:\n",
    "            encoded_text = encoded_text[:self.max_seq_len]\n",
    "\n",
    "        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Adjusting the EmbeddingLayer to not use the Tokenizer's non-existent vocab attribute\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        return self.embedding(token_ids)\n",
    "\n",
    "# Correcting TransformerEncoderLayer's forward method to properly use MultiHeadAttention\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.norm1(src)\n",
    "        q, _ = self.self_attn(src2, src2, src2)\n",
    "        src = src + self.dropout(q)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "# Correction: Pooler squeezes the wrong dimension; it should squeeze dimension 0 (batch dimension is assumed to be 1 here)\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "# Load pre-trained tokenizer and adjust vocab_size accordingly\n",
    "tokenizer = Tokenizer(vocab=vocab)\n",
    "\n",
    "# Assuming vocab_size is the length of your vocab dictionary\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    EmbeddingLayer(vocab_size=vocab_size, embedding_dim=embedding_dim),\n",
    "    TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dim_feedforward=2048, dropout=0.1),\n",
    "    Pooler(d_model=embedding_dim)\n",
    ")\n",
    "\n",
    "# Correcting the training and evaluation loop\n",
    "# Load and preprocess data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_texts = train_data[\"text\"].tolist()\n",
    "train_labels = train_data[\"label\"].tolist()\n",
    "\n",
    "# Convert texts and labels into a Dataset and DataLoader\n",
    "max_seq_len = 512\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop corrected for proper input handling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for token_ids, labels in train_dataloader:\n",
    "        token_ids, labels = token_ids.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.positional_embeddings = PositionalEncoding(max_seq_len, embedding_dim)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        token_embeds = self.token_embeddings(token_ids)  # [batch_size, seq_len, embedding_dim]\n",
    "        position_embeds = self.positional_embeddings(token_embeds)  # [seq_len, embedding_dim]\n",
    "        return token_embeds + position_embeds\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: D:\\EXPERT_WEIGHTS\n",
      "Working Directory After change WD: D:\\EXPERT_WEIGHTS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "os.chdir('D:\\EXPERT_WEIGHTS')\n",
    "\n",
    "print(\"Working Directory After change WD:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual vocabulary size (including special tokens): 1743\n",
      "Tokenizer unk_token_id: 1\n",
      "Tokenizer Vocabulary: {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, ' ': 5, '\\n': 6, '.': 7, 'the': 8, ',': 9, 'of': 10, 'to': 11, 'and': 12, 'a': 13, 'is': 14, 'in': 15, 'that': 16, 'on': 17, '1': 18, 'networks': 19, 'shot': 20, 'for': 21, 'are': 22, 'we': 23, 'network': 24, 'We': 25, 'features': 26, 'dataset': 27, 'layer': 28, 'layers': 29, 'as': 30, '2': 31, 'from': 32, 'class': 33, 'with': 34, '0': 35, 'by': 36, 'each': 37, '5': 38, 'The': 39, 'classes': 40, 'training': 41, '3': 42, '4': 43, 'be': 44, 'performance': 45, 'or': 46, 'this': 47, 'learning': 48, 'trained': 49, 'A': 50, 'first': 51, 'points': 52, 'way': 53, 'distance': 54, 'task': 55, 'which': 56, 'embedding': 57, 'random': 58, 'set': 59, 'target': 60, 'an': 61, 'B': 62, 'al': 63, 'et': 64, 'prototypical': 65, '7': 66, 'can': 67, 'than': 68, 'few': 69, 'not': 70, 'at': 71, 'base': 72, 'two': 73, 'our': 74, 'results': 75, '6': 76, 'approach': 77, 'classification': 78, 'fine': 79, 'number': 80, 'one': 81, 'support': 82, 'transfer': 83, 'In': 84, 'data': 85, 'n': 86, 'use': 87, 'it': 88, 'over': 89, 'these': 90, 'using': 91, 'examples': 92, 'neural': 93, 'per': 94, 'test': 95, 'when': 96, 'This': 97, 'have': 98, 'only': 99, 'then': 100, 'also': 101, 'general': 102, 'new': 103, 'query': 104, 'accuracy': 105, 'because': 106, 'better': 107, 'different': 108, 'episodes': 109, 'meta': 110, 'more': 111, '20': 112, 'AnB': 113, 'Figure': 114, 'episode': 115, 'learn': 116, 'learned': 117, 'other': 118, 'same': 119, 'similar': 120, 'space': 121, 'tasks': 122, 'x': 123, 'both': 124, 'images': 125, 'natural': 126, 'specific': 127, 'train': 128, 'well': 129, '29': 130, 'Euclidean': 131, 'ImageNet': 132, 'but': 133, 'even': 134, 'given': 135, 'higher': 136, 'linear': 137, 'matching': 138, 'model': 139, 'prototype': 140, 'they': 141, 'weights': 142, 'BnB': 143, 'N': 144, 'Section': 145, 'baseB': 146, 'been': 147, 'into': 148, 'mean': 149, 'perform': 150, 'their': 151, 'top': 152, 'transferred': 153, 'used': 154, 'ck': 155, 'has': 156, 'non': 157, 'show': 158, 'simple': 159, 'tuning': 160, 'where': 161, 'will': 162, 'work': 163, 'zero': 164, '50': 165, '8': 166, 'Bregman': 167, 'between': 168, 'case': 169, 'classifier': 170, 'datasets': 171, 'effect': 172, 'experiments': 173, 'filters': 174, 'loss': 175, 'made': 176, 'randomly': 177, 'so': 178, 'there': 179, 'was': 180, ';': 181, 'For': 182, 'Prototypical': 183, 'These': 184, 'all': 185, 'co': 186, 'compare': 187, 'dimensional': 188, 'e': 189, 'frozen': 190, 'half': 191, 'k': 192, 'large': 193, 'may': 194, 'point': 195, 'prototypes': 196, 'split': 197, 'splits': 198, 'such': 199, 'time': 200, 'tuned': 201, '22': 202, '98': 203, 'Cosine': 204, 'SJE': 205, 'cluster': 206, 'do': 207, 'drop': 208, 'example': 209, 'fφ': 210, 'generalization': 211, 'level': 212, 'lower': 213, 'produce': 214, 'result': 215, 'selffer': 216, 'state': 217, 'upper': 218, 'validation': 219, 'vector': 220, '60': 221, 'Few': 222, 'Omniglot': 223, 'Table': 224, 'Vinyals': 225, 'art': 226, 'boost': 227, 'color': 228, 'could': 229, 'embedded': 230, 'found': 231, 'how': 232, 'its': 233, 'just': 234, 'man': 235, 'method': 236, 'nearest': 237, 'small': 238, 'toward': 239, 'transferring': 240, 'without': 241, '2009': 242, '28': 243, '64': 244, '9': 245, 'B3B': 246, 'CUB': 247, 'LSTM': 248, 'Larochelle': 249, 'Performance': 250, 'Ravi': 251, 'Top': 252, 'after': 253, 'based': 254, 'computed': 255, 'convolutional': 256, 'distances': 257, 'divergence': 258, 'either': 259, 'find': 260, 'function': 261, 'g': 262, 'here': 263, 'however': 264, 'if': 265, 'labeled': 266, 'left': 267, 'less': 268, 'middle': 269, 'much': 270, 'overfitting': 271, 'parameters': 272, 'particular': 273, 'possible': 274, 'proposed': 275, 'representation': 276, 'separate': 277, 'softmax': 278, 'them': 279, 'treatment': 280, 'vs': 281, 'within': 282, 'would': 283, '10': 284, '16': 285, '2011': 286, '23': 287, 'A3B': 288, 'Euclid': 289, 'However': 290, 'J': 291, 'K': 292, 'Matching': 293, 'NC': 294, 'NETWORKS': 295, 'On': 296, 'Sk': 297, 'University': 298, 'Zero': 299, 'achieve': 300, 'adapted': 301, 'chosen': 302, 'containing': 303, 'copied': 304, 'cosine': 305, 'd': 306, 'deep': 307, 'diamonds': 308, 'dissimilar': 309, 'does': 310, 'due': 311, 'experiment': 312, 'extent': 313, 'further': 314, 'image': 315, 'instead': 316, 'make': 317, 'metric': 318, 'order': 319, 'output': 320, 'particularly': 321, 'procedure': 322, 'rather': 323, 'recent': 324, 'reported': 325, 'represent': 326, 'row': 327, 'seen': 328, 'smaller': 329, 'squared': 330, 'transferability': 331, 'transition': 332, 'were': 333, 'whether': 334, 'xi': 335, 'yi': 336, '1000': 337, '200': 338, '2012': 339, '30': 340, '500': 341, 'Classification': 342, 'D': 343, 'FCE': 344, 'Gabor': 345, 'GoogLeNet': 346, 'If': 347, 'It': 348, 'Jarrett': 349, 'Model': 350, 'NS': 351, 'To': 352, 'above': 353, 'almost': 354, 'another': 355, 'any': 356, 'approaches': 357, 'architecture': 358, 'cat': 359, 'contains': 360, 'corresponding': 361, 'create': 362, 'degree': 363, 'density': 364, 'divergences': 365, 'end': 366, 'episodic': 367, 'equivalent': 368, 'error': 369, 'expected': 370, 'feature': 371, 'following': 372, 'generalize': 373, 'i': 374, 'including': 375, 'input': 376, 'issues': 377, 'last': 378, 'margin': 379, 'miniImageNet': 380, 'must': 381, 'neighbor': 382, 'neurons': 383, 'original': 384, 'performed': 385, 'problem': 386, 'red': 387, 'right': 388, 'second': 389, 'several': 390, 'shown': 391, 'shows': 392, 'specificity': 393, 'statistic': 394, 'subset': 395, 'surprising': 396, 'vectors': 397, 'weight': 398, 'y': 399, '15': 400, '19': 401, '40': 402, '450k': 403, '54': 404, '?': 405, 'Acc': 406, 'As': 407, 'Caltech': 408, 'Dept': 409, 'Each': 410, 'MATCHING': 411, 'NCA': 412, 'Nc': 413, 'Networks': 414, 'Our': 415, 'RANDOMSAMPLE': 416, 'RM': 417, 'S': 418, 'Thus': 419, 'Training': 420, 'When': 421, 'adaptation': 422, 'applied': 423, 'attribute': 424, 'average': 425, 'baseA': 426, 'blobs': 427, 'blue': 428, 'bottom': 429, 'call': 430, 'choice': 431, 'choices': 432, 'circles': 433, 'curious': 434, 'define': 435, 'descent': 436, 'difficulties': 437, 'distribution': 438, 'during': 439, 'effects': 440, 'evidence': 441, 'exhibit': 442, 'exp': 443, 'family': 444, 'follow': 445, 'four': 446, 'fragile': 447, 'functions': 448, 'generality': 449, 'greatly': 450, 'improves': 451, 'initialized': 452, 'line': 453, 'literature': 454, 'makes': 455, 'match': 456, 'methods': 457, 'mixture': 458, 'models': 459, 'multiple': 460, 'normalization': 461, 'optimization': 462, 'out': 463, 'paper': 464, 'provide': 465, 'provided': 466, 'quantify': 467, 'rate': 468, 'regular': 469, 's': 470, 'scenario': 471, 'single': 472, 'size': 473, 'statistician': 474, 'straightforward': 475, 'those': 476, 'three': 477, 'versions': 478, 'via': 479, 'white': 480, '000': 481, '100': 482, '2014': 483, '25': 484, '26': 485, '31': 486, '43': 487, '56': 488, '600': 489, '84': 490, '93': 491, 'AlexNet': 492, 'Bengio': 493, 'BnA': 494, 'Computer': 495, 'DA': 496, 'DS': 497, 'Datasets': 498, 'Equation': 499, 'Features': 500, 'Fine': 501, 'KNN': 502, 'Layer': 503, 'Learning': 504, 'NQ': 505, 'Nets': 506, 'OURS': 507, 'One': 508, 'Random': 509, 'SGD': 510, 'Science': 511, 'Select': 512, 'Since': 513, 'WA1': 514, 'accuracies': 515, 'across': 516, 'analysis': 517, 'attributes': 518, 'averaged': 519, 'baselines': 520, 'being': 521, 'bias': 522, 'bird': 523, 'character': 524, 'choose': 525, 'classify': 526, 'clustering': 527, 'come': 528, 'common': 529, 'comparison': 530, 'computation': 531, 'compute': 532, 'dark': 533, 'demonstrate': 534, 'denotes': 535, 'depends': 536, 'details': 537, 'determined': 538, 'directly': 539, 'discussed': 540, 'distant': 541, 'dominate': 542, 'dominates': 543, 'drops': 544, 'eight': 545, 'enable': 546, 'enough': 547, 'entire': 548, 'equivalence': 549, 'estimation': 550, 'expect': 551, 'exponential': 552, 'extend': 553, 'felid': 554, 'final': 555, 'finding': 556, 'five': 557, 'fixed': 558, 'gradient': 559, 'hand': 560, 'having': 561, 'high': 562, 'hyperparameters': 563, 'idea': 564, 'identical': 565, 'improve': 566, 'increases': 567, 'individual': 568, 'inductive': 569, 'initializing': 570, 'iterations': 571, 'keep': 572, 'key': 573, 'knowledge': 574, 'label': 575, 'labels': 576, 'larger': 577, 'learns': 578, 'least': 579, 'length': 580, 'light': 581, 'like': 582, 'limited': 583, 'local': 584, 'making': 585, 'many': 586, 'material': 587, 'meaning': 588, 'means': 589, 'near': 590, 'need': 591, 'observed': 592, 'obtain': 593, 'optimal': 594, 'ours': 595, 'pairs': 596, 'phenomenon': 597, 'pooling': 598, 'posterior': 599, 'previously': 600, 'processing': 601, 'propose': 602, 'pφ': 603, 'quite': 604, 'related': 605, 'respect': 606, 'retraining': 607, 'rotations': 608, 'scenarios': 609, 'seems': 610, 'sense': 611, 'setting': 612, 'showing': 613, 'simpler': 614, 'simply': 615, 'since': 616, 'some': 617, 'splitting': 618, 'study': 619, 'subplot': 620, 'suggests': 621, 'supervised': 622, 'supplementary': 623, 'take': 624, 'thus': 625, 'tune': 626, 'unit': 627, 'untrained': 628, 'useful': 629, 'utilizes': 630, 'various': 631, 'very': 632, 'vk': 633, 'weighted': 634, 'while': 635, 'z': 636, 'φ': 637, '024': 638, '101': 639, '11': 640, '12': 641, '13': 642, '14': 643, '17': 644, '2013': 645, '27': 646, '312': 647, '42': 648, '44': 649, '449': 650, '49': 651, '551': 652, '58': 653, '62': 654, '66': 655, '70': 656, '71': 657, '78': 658, '79': 659, '80': 660, '95': 661, '99': 662, 'ALE': 663, 'Abstract': 664, 'Accuracy': 665, 'Adam': 666, 'Algorithm': 667, 'All': 668, 'An': 669, 'Another': 670, 'Birds': 671, 'Bottom': 672, 'Comparison': 673, 'Cornell': 674, 'DVk': 675, 'Dist': 676, 'Donahue': 677, 'Fei': 678, 'Finally': 679, 'First': 680, 'Further': 681, 'ILSVRC': 682, 'Introduction': 683, 'Krizhevsky': 684, 'LG': 685, 'LMNN': 686, 'Large': 687, 'Lines': 688, 'Mahalanobis': 689, 'Man': 690, 'Mensink': 691, 'Natural': 692, 'Note': 693, 'Of': 694, 'PROTOTYPICAL': 695, 'Proto': 696, 'Qk': 697, 'R': 698, 'Results': 699, 'Such': 700, 'That': 701, 'Their': 702, 'There': 703, 'They': 704, 'Toronto': 705, 'Transfer': 706, 'Tune': 707, 'Twitter': 708, 'UCSD': 709, 'WA2': 710, 'WA3': 711, 'WB1': 712, 'WB2': 713, 'WB3': 714, 'While': 715, 'X': 716, 'able': 717, 'about': 718, 'achieves': 719, 'affected': 720, 'aggregated': 721, 'algorithms': 722, 'allowed': 723, 'allows': 724, 'applying': 725, 'approximate': 726, 'approximately': 727, 'arXiv': 728, 'architectural': 729, 'assigned': 730, 'assigning': 731, 'axis': 732, 'bars': 733, 'batch': 734, 'become': 735, 'beneficial': 736, 'block': 737, 'c1': 738, 'c2': 739, 'c3': 740, 'called': 741, 'carefully': 742, 'cases': 743, 'categories': 744, 'chance': 745, 'characters': 746, 'chopped': 747, 'closer': 748, 'collectively': 749, 'com': 750, 'combination': 751, 'comparable': 752, 'compared': 753, 'computing': 754, 'conditional': 755, 'conducted': 756, 'confidence': 757, 'conjecture': 758, 'connects': 759, 'considering': 760, 'consist': 761, 'contain': 762, 'control': 763, 'controls': 764, 'copy': 765, 'created': 766, 'cs': 767, 'custom': 768, 'decisions': 769, 'decoupled': 770, 'decreases': 771, 'degradation': 772, 'demonstrated': 773, 'depend': 774, 'depending': 775, 'design': 776, 'designed': 777, 'detectors': 778, 'determine': 779, 'developed': 780, 'did': 781, 'differ': 782, 'discriminative': 783, 'distinct': 784, 'divided': 785, 'domain': 786, 'draws': 787, 'dϕ': 788, 'effectively': 789, 'effectiveness': 790, 'efficient': 791, 'elements': 792, 'embeddings': 793, 'empirical': 794, 'empirically': 795, 'entities': 796, 'exact': 797, 'expense': 798, 'experimental': 799, 'experimentally': 800, 'explanation': 801, 'extension': 802, 'extensions': 803, 'fact': 804, 'fully': 805, 'gap': 806, 'generalizing': 807, 'generated': 808, 'generative': 809, 'giving': 810, 'good': 811, 'group': 812, 'grows': 813, 'handle': 814, 'help': 815, 'helpful': 816, 'hexagons': 817, 'implementation': 818, 'improved': 819, 'improvement': 820, 'improving': 821, 'include': 822, 'increased': 823, 'indicate': 824, 'indicates': 825, 'indicating': 826, 'initialization': 827, 'insight': 828, 'intervals': 829, 'involves': 830, 'k0': 831, 'known': 832, 'lead': 833, 'learnable': 834, 'leopard': 835, 'levels': 836, 'lingers': 837, 'lion': 838, 'log': 839, 'longer': 840, 'main': 841, 'mapping': 842, 'max': 843, 'maximize': 844, 'might': 845, 'minimal': 846, 'modeling': 847, 'most': 848, 'naturally': 849, 'nature': 850, 'nearly': 851, 'negatively': 852, 'neighboring': 853, 'nets': 854, 'next': 855, 'nonlinearity': 856, 'normalized': 857, 'note': 858, 'noticing': 859, 'object': 860, 'obtained': 861, 'occur': 862, 'optimize': 863, 'ordinary': 864, 'originally': 865, 'overfit': 866, 'own': 867, 'partitioning': 868, 'performing': 869, 'place': 870, 'plus': 871, 'poorly': 872, 'pre': 873, 'predict': 874, 'predictions': 875, 'previous': 876, 'primarily': 877, 'process': 878, 'produces': 879, 'producing': 880, 'quantified': 881, 'questions': 882, 'quickly': 883, 'reason': 884, 'reference': 885, 'regime': 886, 'relatively': 887, 'relevant': 888, 'remaining': 889, 'report': 890, 'representations': 891, 'represents': 892, 'require': 893, 'retrained': 894, 'reveals': 895, 'rows': 896, 'scratch': 897, 'sections': 898, 'serve': 899, 'severely': 900, 'should': 901, 'showed': 902, 'significant': 903, 'similarity': 904, 'sizes': 905, 'slightly': 906, 'solution': 907, 'specialization': 908, 'species': 909, 'spherical': 910, 'standard': 911, 'statistically': 912, 'still': 913, 'strictly': 914, 'subsets': 915, 'substantial': 916, 'subtracting': 917, 'successful': 918, 'suggesting': 919, 'tend': 920, 'terms': 921, 'therefore': 922, 'third': 923, 'through': 924, 'tiger': 925, 'total': 926, 'transformation': 927, 'transformed': 928, 'true': 929, 'unsupervised': 930, 'up': 931, 'updates': 932, 'us': 933, 'uses': 934, 'utilizing': 935, 'version': 936, 'worse': 937, 'written': 938, 'x1': 939, 'xN': 940, 'y1': 941, 'yN': 942, 'yield': 943, 'yields': 944, 'θ': 945, '00': 946, '05': 947, '05175v2': 948, '09': 949, '0k': 950, '1024': 951, '1200': 952, '1411': 953, '1600': 954, '1623': 955, '167': 956, '1703': 957, '1792v1': 958, '1995': 959, '1The': 960, '2000': 961, '2004': 962, '2013a': 963, '2013b': 964, '2017': 965, '21': 966, '24': 967, '281': 968, '2Note': 969, '37': 970, '3AnA': 971, '4We': 972, '51': 973, '52': 974, '55': 975, '625': 976, '645': 977, '68': 978, '73': 979, '77': 980, '788': 981, '800': 982, '86': 983, '90': 984, '96': 985, '97': 986, 'A8B': 987, 'Additionally': 988, 'Aerospace': 989, 'Also': 990, 'Alternately': 991, 'Although': 992, 'Analysis': 993, 'At': 994, 'B8B': 995, 'BASELINE': 996, 'Because': 997, 'By': 998, 'CLUSTERING': 999, 'CU': 1000, 'Caffe': 1001, 'Can': 1002, 'Caruana': 1003, 'Challenge': 1004, 'Choices': 1005, 'Classes': 1006, 'Clune': 1007, 'Compared': 1008, 'Components': 1009, 'Compute': 1010, 'Conclusion': 1011, 'Conclusions': 1012, 'DNet': 1013, 'Dark': 1014, 'Degradation': 1015, 'Deng': 1016, 'Density': 1017, 'Design': 1018, 'Discussion': 1019, 'Dissimilar': 1020, 'Distance': 1021, 'Dk': 1022, 'Does': 1023, 'Early': 1024, 'Edwards': 1025, 'Egyptian': 1026, 'Engineering': 1027, 'Episode': 1028, 'Error': 1029, 'Estimation': 1030, 'Examples': 1031, 'Experimental': 1032, 'Experiments': 1033, 'Felidae': 1034, 'Fergus': 1035, 'Fisher': 1036, 'Fortunately': 1037, 'Fourth': 1038, 'GPU': 1039, 'Gaussian': 1040, 'Gaussians': 1041, 'Generality': 1042, 'Girshick': 1043, 'Given': 1044, 'Gradient': 1045, 'Here': 1046, 'Hinton': 1047, 'Hod': 1048, 'How': 1049, 'ILSVRC2012': 1050, 'Image': 1051, 'Indeed': 1052, 'Initial': 1053, 'Initialize': 1054, 'Input': 1055, 'Instead': 1056, 'Institute': 1057, 'Into': 1058, 'Intuitively': 1059, 'Jake': 1060, 'Jason': 1061, 'Jeff': 1062, 'Jia': 1063, 'Jun': 1064, 'KL': 1065, 'Kevin': 1066, 'LEARNER': 1067, 'Layers': 1068, 'Le': 1069, 'Learner': 1070, 'Lee': 1071, 'Left': 1072, 'Legendre': 1073, 'Light': 1074, 'Like': 1075, 'Linear': 1076, 'Lipson4': 1077, 'M': 1078, 'META': 1079, 'Many': 1080, 'Measured': 1081, 'Mechanical': 1082, 'Meta': 1083, 'Mixture': 1084, 'Modern': 1085, 'Modifying': 1086, 'Montreal': 1087, 'Moreover': 1088, 'Much': 1089, 'NEAREST': 1090, 'NEIGHBORS': 1091, 'NETS': 1092, 'NEURAL': 1093, 'Neighborhood': 1094, 'Neither': 1095, 'No': 1096, 'Notably': 1097, 'Notation': 1098, 'Nov': 1099, 'Numbered': 1100, 'Operations': 1101, 'Output': 1102, 'Overall': 1103, 'Overview': 1104, 'P': 1105, 'PROTO': 1106, 'Persian': 1107, 'Points': 1108, 'Previously': 1109, 'Prototype': 1110, 'Pseudocode': 1111, 'ReLU': 1112, 'Recent': 1113, 'Recognition': 1114, 'Reed': 1115, 'Reinterpretation': 1116, 'Related': 1117, 'Relative': 1118, 'Research': 1119, 'Richard': 1120, 'Right': 1121, 'Rippel': 1122, 'SAMPLE': 1123, 'STATISTICIAN': 1124, 'SVM': 1125, 'Salakhutdinov': 1126, 'Scale': 1127, 'Second': 1128, 'Separate': 1129, 'Sermanet': 1130, 'Setup': 1131, 'Shot': 1132, 'Siamese': 1133, 'Similar': 1134, 'Similarly': 1135, 'Snell': 1136, 'Specificity': 1137, 'Splitting': 1138, 'Storkey': 1139, 'Swersky': 1140, 'Thanks': 1141, 'Third': 1142, 'Transferability': 1143, 'Two': 1144, 'Unlike': 1145, 'Update': 1146, 'V': 1147, 'Vector': 1148, 'Visual': 1149, 'WA4': 1150, 'WA5': 1151, 'WA6': 1152, 'WA7': 1153, 'WA8': 1154, 'WB4': 1155, 'WB5': 1156, 'WB6': 1157, 'WB7': 1158, 'WB8': 1159, 'Weights': 1160, 'Where': 1161, 'Whereas': 1162, 'With': 1163, 'Work': 1164, 'Wyoming': 1165, 'Y': 1166, 'Yoshua': 1167, 'Yosinski': 1168, 'Zeiler': 1169, 'Zemel': 1170, 'ability': 1171, 'abs': 1172, 'absolute': 1173, 'accommodate': 1174, 'account': 1175, 'achieving': 1176, 'act': 1177, 'activations': 1178, 'adapt': 1179, 'addition': 1180, 'additional': 1181, 'addressing': 1182, 'advance': 1183, 'advantage': 1184, 'advantageous': 1185, 'affect': 1186, 'against': 1187, 'aggregate': 1188, 'aggregation': 1189, 'aim': 1190, 'algorithm': 1191, 'allowing': 1192, 'alone': 1193, 'alphabets': 1194, 'alternative': 1195, 'although': 1196, 'amount': 1197, 'analyze': 1198, 'answers': 1199, 'anything': 1200, 'anywhere': 1201, 'apparent': 1202, 'apparently': 1203, 'appealing': 1204, 'appear': 1205, 'appearance': 1206, 'applicable': 1207, 'applies': 1208, 'apply': 1209, 'arbitrary': 1210, 'around': 1211, 'ask': 1212, 'assess': 1213, 'assign': 1214, 'assignment': 1215, 'associated': 1216, 'assumption': 1217, 'assumptions': 1218, 'attack': 1219, 'attained': 1220, 'attains': 1221, 'attempt': 1222, 'attempts': 1223, 'attention': 1224, 'attributed': 1225, 'augmenting': 1226, 'author': 1227, 'autoencoder': 1228, 'available': 1229, 'b': 1230, 'back': 1231, 'backprop': 1232, 'backpropagate': 1233, 'batches': 1234, 'befits': 1235, 'beginning': 1236, 'behavior': 1237, 'believe': 1238, 'belonging': 1239, 'below': 1240, 'benchmark': 1241, 'benefits': 1242, 'best': 1243, 'beyond': 1244, 'bi': 1245, 'biological': 1246, 'blob': 1247, 'blocks': 1248, 'boosting': 1249, 'bug': 1250, 'cannot': 1251, 'carries': 1252, 'cats': 1253, 'cause': 1254, 'causes': 1255, 'change': 1256, 'characteristics': 1257, 'characterize': 1258, 'cheetah': 1259, 'choosing': 1260, 'ck0': 1261, 'clarity': 1262, 'classconditional': 1263, 'classified': 1264, 'classifiers': 1265, 'closely': 1266, 'clusters': 1267, 'coadaptation': 1268, 'code': 1269, 'collected': 1270, 'comes': 1271, 'commonly': 1272, 'comparing': 1273, 'competition': 1274, 'completely': 1275, 'complex': 1276, 'complicated': 1277, 'component': 1278, 'composed': 1279, 'composition': 1280, 'comprised': 1281, 'comprises': 1282, 'computationally': 1283, 'concise': 1284, 'conclusions': 1285, 'configuration': 1286, 'connecting': 1287, 'connections': 1288, 'consideration': 1289, 'considered': 1290, 'constant': 1291, 'constrain': 1292, 'construct': 1293, 'constructed': 1294, 'constructing': 1295, 'contained': 1296, 'continuous': 1297, 'contrast': 1298, 'contributions': 1299, 'converge': 1300, 'convergence': 1301, 'convex': 1302, 'convolution': 1303, 'core': 1304, 'cost': 1305, 'couple': 1306, 'course': 1307, 'creates': 1308, 'crop': 1309, 'crops': 1310, 'cumulant': 1311, 'currently': 1312, 'cut': 1313, 'cvpr2016': 1314, 'datapoint': 1315, 'days': 1316, 'deal': 1317, 'decay': 1318, 'decline': 1319, 'decoupling': 1320, 'deeper': 1321, 'defined': 1322, 'definition': 1323, 'definitions': 1324, 'degrees': 1325, 'densities': 1326, 'derived': 1327, 'describes': 1328, 'description': 1329, 'descriptions': 1330, 'despite': 1331, 'differences': 1332, 'differentiable': 1333, 'differs': 1334, 'difficult': 1335, 'difficulty': 1336, 'dimension': 1337, 'diminishes': 1338, 'direction': 1339, 'directional': 1340, 'directions': 1341, 'distinction': 1342, 'distributions': 1343, 'divide': 1344, 'document': 1345, 'doesn': 1346, 'dogs': 1347, 'domains': 1348, 'done': 1349, 'dot': 1350, 'dotted': 1351, 'downloaded': 1352, 'draw': 1353, 'drawn': 1354, 'dynamically': 1355, 'dynamics': 1356, 'easier': 1357, 'effective': 1358, 'ellipsoidal': 1359, 'else': 1360, 'elsewhere': 1361, 'embed': 1362, 'emphasize': 1363, 'encode': 1364, 'encoder': 1365, 'encoding': 1366, 'encourages': 1367, 'environment': 1368, 'epochs': 1369, 'equally': 1370, 'errors': 1371, 'etc': 1372, 'eventually': 1373, 'every': 1374, 'excellent': 1375, 'except': 1376, 'exists': 1377, 'expensive': 1378, 'explorations': 1379, 'extends': 1380, 'extensible': 1381, 'extensive': 1382, 'extensively': 1383, 'extra': 1384, 'extracted': 1385, 'extremely': 1386, 'f': 1387, 'failed': 1388, 'fairly': 1389, 'faithful': 1390, 'falls': 1391, 'far': 1392, 'feather': 1393, 'felids': 1394, 'fewer': 1395, 'figure': 1396, 'files': 1397, 'filter': 1398, 'finetuned': 1399, 'fitted': 1400, 'fix': 1401, 'flexibility': 1402, 'flipped': 1403, 'focus': 1404, 'forces': 1405, 'form': 1406, 'formed': 1407, 'forms': 1408, 'formulate': 1409, 'fragilely': 1410, 'future': 1411, 'gaining': 1412, 'gains': 1413, 'generalizes': 1414, 'generally': 1415, 'get': 1416, 'getting': 1417, 'github': 1418, 'goal': 1419, 'grained': 1420, 'grayscale': 1421, 'greater': 1422, 'groups': 1423, 'gϑ': 1424, 'halves': 1425, 'handwritten': 1426, 'hard': 1427, 'held': 1428, 'helps': 1429, 'hierarchy': 1430, 'hinge': 1431, 'hold': 1432, 'holds': 1433, 'horizontally': 1434, 'http': 1435, 'https': 1436, 'human': 1437, 'humans': 1438, 'hyperparameter': 1439, 'hypothesis': 1440, 'hypothesize': 1441, 'ii': 1442, 'illustration': 1443, 'image2': 1444, 'importance': 1445, 'important': 1446, 'imposes': 1447, 'improvements': 1448, 'incorporate': 1449, 'incorporated': 1450, 'increase': 1451, 'indeed': 1452, 'independent': 1453, 'indices': 1454, 'inference': 1455, 'inferred': 1456, 'information': 1457, 'informative': 1458, 'initial': 1459, 'initialize': 1460, 'interact': 1461, 'interactions': 1462, 'interest': 1463, 'interested': 1464, 'interpretation': 1465, 'interpretations': 1466, 'interpreted': 1467, 'introduced': 1468, 'intuitive': 1469, 'involving': 1470, 'issue': 1471, 'itself': 1472, 'jaguar': 1473, 'join': 1474, 'jointly': 1475, 'justify': 1476, 'keeping': 1477, 'know': 1478, 'kz': 1479, 'latter': 1480, 'learner': 1481, 'leave': 1482, 'likewise': 1483, 'linearity': 1484, 'linearly': 1485, 'lines': 1486, 'linger': 1487, 'little': 1488, 'locked': 1489, 'logX': 1490, 'lost': 1491, 'low': 1492, 'lynx': 1493, 'm': 1494, 'manner': 1495, 'marker': 1496, 'measure': 1497, 'mechanism': 1498, 'mentioned': 1499, 'mimic': 1500, 'mini': 1501, 'minimizing': 1502, 'mirrors': 1503, 'mistakes': 1504, 'modal': 1505, 'modern': 1506, 'modifying': 1507, 'monitoring': 1508, 'mountain': 1509, 'multi': 1510, 'multimodal': 1511, 'multiples': 1512, 'naive': 1513, 'namely': 1514, 'negative': 1515, 'neighborhood': 1516, 'net': 1517, 'never': 1518, 'no': 1519, 'nor': 1520, 'normalize': 1521, 'notation': 1522, 'noted': 1523, 'notions': 1524, 'novel': 1525, 'objective': 1526, 'objectives': 1527, 'observation': 1528, 'obtaining': 1529, 'obviates': 1530, 'occurrence': 1531, 'occurs': 1532, 'off': 1533, 'often': 1534, 'old': 1535, 'opposed': 1536, 'optimized': 1537, 'orange': 1538, 'ordering': 1539, 'outline': 1540, 'outperforms': 1541, 'overlapping': 1542, 'parameter': 1543, 'parameterization': 1544, 'parent': 1545, 'part': 1546, 'patterns': 1547, 'perfectly': 1548, 'performances': 1549, 'performs': 1550, 'permissible': 1551, 'persists': 1552, 'phase': 1553, 'plausible': 1554, 'plots': 1555, 'plotted': 1556, 'post': 1557, 'powerful': 1558, 'prediction': 1559, 'preliminary': 1560, 'preparing': 1561, 'presents': 1562, 'prevents': 1563, 'probabilities': 1564, 'probability': 1565, 'proceeds': 1566, 'produced': 1567, 'progress': 1568, 'promising': 1569, 'pronounced': 1570, 'publication': 1571, 'pψ': 1572, 'quantifying': 1573, 'question': 1574, 'raises': 1575, 'ranges': 1576, 'rapidly': 1577, 'rates': 1578, 'raw': 1579, 're': 1580, 'recovers': 1581, 'rectangles': 1582, 'rectification': 1583, 'rediscovered': 1584, 'reedscot': 1585, 'refer': 1586, 'reflect': 1587, 'regardless': 1588, 'regularization': 1589, 'regularize': 1590, 'reinitialize': 1591, 'relate': 1592, 'relates': 1593, 'relative': 1594, 'relearn': 1595, 'relearned': 1596, 'relearning': 1597, 'released': 1598, 'relies': 1599, 'relu': 1600, 'rely': 1601, 'remainder': 1602, 'repeated': 1603, 'replacement': 1604, 'representative': 1605, 'representatives': 1606, 'represented': 1607, 'reproduce': 1608, 'repurpose': 1609, 'required': 1610, 'requires': 1611, 'requiring': 1612, 'researchers': 1613, 'resemble': 1614, 'resembles': 1615, 'resizing': 1616, 'retrain': 1617, 'rigorous': 1618, 'risk': 1619, 'run': 1620, 'runs': 1621, 'sample': 1622, 'sampled': 1623, 'save': 1624, 'say': 1625, 'schedule': 1626, 'scheme': 1627, 'secondary': 1628, 'selecting': 1629, 'semantically': 1630, 'sets': 1631, 'settings': 1632, 'setup': 1633, 'seven': 1634, 'shape': 1635, 'shared': 1636, 'shifted': 1637, 'shown3': 1638, 'significantly': 1639, 'simplicity': 1640, 'simplify': 1641, 'situation': 1642, 'slight': 1643, 'snow': 1644, 'software': 1645, 'somewhere': 1646, 'sophisticated': 1647, 'sparse': 1648, 'special': 1649, 'specifically': 1650, 'specifies': 1651, 'speed': 1652, 'spread': 1653, 'step': 1654, 'stopping': 1655, 'stops': 1656, 'store': 1657, 'strikingly': 1658, 'studied': 1659, 'studies': 1660, 'studying': 1661, 'subject': 1662, 'subsampling': 1663, 'substantially': 1664, 'successfully': 1665, 'successive': 1666, 'suddenly': 1667, 'suffers': 1668, 'suitability': 1669, 'suitable': 1670, 'suited': 1671, 'summarize': 1672, 'summarizes': 1673, 'summary': 1674, 'suspicion': 1675, 'systems': 1676, 't': 1677, 'tabby': 1678, 'tackle': 1679, 'taken': 1680, 'takes': 1681, 'taking': 1682, 'tanh': 1683, 'technique': 1684, 'techniques': 1685, 'tell': 1686, 'tendency': 1687, 'term': 1688, 'tested': 1689, 'tests': 1690, 'text': 1691, 'theart': 1692, 'themselves': 1693, 'thereby': 1694, 'things': 1695, 'though': 1696, 'tool': 1697, 'trains': 1698, 'transferable': 1699, 'treatments': 1700, 'tweak': 1701, 'tweaking': 1702, 'type': 1703, 'types': 1704, 'under': 1705, 'underlying': 1706, 'uniformly': 1707, 'uniquely': 1708, 'unlabeled': 1709, 'until': 1710, 'upon': 1711, 'usual': 1712, 'usually': 1713, 'utilize': 1714, 'v1': 1715, 'v2': 1716, 'v3': 1717, 'variance': 1718, 'variant': 1719, 'variants': 1720, 'variational': 1721, 'vast': 1722, 'versa': 1723, 'versus': 1724, 'vertical': 1725, 'vice': 1726, 'viewed': 1727, 'visual': 1728, 'vital': 1729, 'want': 1730, 'ways': 1731, 'whereas': 1732, 'whole': 1733, 'whose': 1734, 'wide': 1735, 'won': 1736, 'xk': 1737, 'yosinski': 1738, 'µ': 1739, 'θk': 1740, 'ψ': 1741, 'ϕ': 1742}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robbi\\AppData\\Local\\Temp\\ipykernel_35548\\377544582.py:192: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embed = self.infrequent_embeddings(torch.tensor(token_id).long())\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 612\u001b[0m\n\u001b[0;32m    610\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m    611\u001b[0m input_ids, attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 612\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, labels)\n\u001b[0;32m    614\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 252\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 252\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(embedded, src_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m    254\u001b[0m     pooled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoded)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 192\u001b[0m, in \u001b[0;36mAdaptiveEmbeddingLayer.forward\u001b[1;34m(self, token_ids)\u001b[0m\n\u001b[0;32m    190\u001b[0m         embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrequent_embeddings(torch\u001b[38;5;241m.\u001b[39mtensor(token_id)\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m         embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfrequent_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     token_embeds\u001b[38;5;241m.\u001b[39mappend(embed)\n\u001b[0;32m    195\u001b[0m token_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(token_embeds)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # Ensure special tokens are included\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def combined_loss(output, target, model, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)  \n",
    "    regularization_loss = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if isinstance(param, nn.Parameter):  \n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1\n",
    "\n",
    "    return task_loss + regularization_loss\n",
    "\n",
    "# Encoding Transformer Code\n",
    "\n",
    "class AdaptiveDropoutLayer(nn.Module):\n",
    "    def __init__(self, init_dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())  # Use logit transformation for stability\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.log_alpha) \n",
    "        return nn.functional.dropout(x, p=p, training=self.training) \n",
    "\n",
    "class AdaptiveWeightDecayOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, init_l2_strength=0.01):\n",
    "        super().__init__(params, {'lr': lr})\n",
    "        self.log_l2_strength = nn.Parameter(torch.tensor(math.log(init_l2_strength)).float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = torch.exp(self.log_l2_strength)  \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay) \n",
    "                p.update(d_p, group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.positional_embeddings = nn.Parameter(torch.zeros(max_seq_len, embedding_dim), requires_grad=False)\n",
    "        \n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(math.log(10000.0) / embedding_dim))\n",
    "        \n",
    "        self.positional_embeddings[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_embeddings[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.positional_embeddings[:x.size(1), :]\n",
    "\n",
    "class MultiHeadLinformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, k=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k if k is not None else embed_dim // num_heads  # Projection dimension\n",
    "\n",
    "        # Separate projections for each head\n",
    "        self.key_projections = nn.ModuleList([nn.Linear(embed_dim, self.k) for _ in range(num_heads)])\n",
    "        self.value_projections = nn.ModuleList([nn.Linear(embed_dim, self.k) for _ in range(num_heads)]) \n",
    "\n",
    "        self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query):\n",
    "        seq_len, batch_size, _ = query.size()\n",
    "        heads = []  # Store output from each head\n",
    "\n",
    "        for head_idx in range(self.num_heads):\n",
    "            projected_keys = self.key_projections[head_idx](query)\n",
    "            projected_values = self.value_projections[head_idx](query)\n",
    "\n",
    "            # Calculate attention using projected keys and values\n",
    "            attention = torch.softmax(projected_keys.transpose(2, 3) @ projected_values, dim=-1) \n",
    "\n",
    "            out = attention @ projected_values.view(batch_size, seq_len, self.num_heads, self.k)\n",
    "            out = out.transpose(1, 2).contiguous().view(seq_len, batch_size, self.embed_dim)\n",
    "\n",
    "            heads.append(out)\n",
    "\n",
    "        # Concatenate outputs from all heads\n",
    "        out = torch.cat(heads, dim=-1) \n",
    "        out = self.out_projection(out) \n",
    "        return out\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "        labels = encoded_text.copy()  # Copy encoded text for labels\n",
    "\n",
    "        # Apply dynamic masking\n",
    "        masked_indices = np.random.rand(len(encoded_text)) < 0.15\n",
    "        for i in range(len(encoded_text)):\n",
    "            if masked_indices[i]:\n",
    "                encoded_text[i] = self.tokenizer.vocab[\"[MASK]\"]\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "        labels += [-100] * padding_length  # Use -100 for padding positions, ignored by nn.CrossEntropyLoss\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class AdaptiveEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.split_vocab(vocab, freq_threshold)\n",
    "\n",
    "        # Initialize embeddings with the correct size\n",
    "        self.frequent_embeddings = nn.Embedding(num_embeddings=len(self.frequent_vocab), embedding_dim=large_embed_dim)\n",
    "        self.infrequent_embeddings = nn.Embedding(num_embeddings=len(self.infrequent_vocab), embedding_dim=small_embed_dim)\n",
    "\n",
    "\n",
    "        self.positional_embeddings = PositionalEncoding(max_seq_len, large_embed_dim)  \n",
    "\n",
    "\n",
    "    def split_vocab(self, vocab, freq_threshold):\n",
    "        token_counts = [(token, count) for token, count in vocab.items()] \n",
    "        token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "\n",
    "        self.frequent_vocab = {token: i for i, (token, _) in enumerate(token_counts[:split_point])}\n",
    "        self.infrequent_vocab = {token: i for i, (token, _) in enumerate(token_counts[split_point:])}\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        token_embeds = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.frequent_vocab:\n",
    "                embed = self.frequent_embeddings(torch.tensor(token_id).long())\n",
    "            else:\n",
    "                embed = self.infrequent_embeddings(torch.tensor(token_id).long())\n",
    "            token_embeds.append(embed)\n",
    "\n",
    "        token_embeds = torch.stack(token_embeds)\n",
    "        position_embeds = self.positional_embeddings(token_embeds)\n",
    "        return token_embeds + position_embeds\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadLinformerAttention(embed_dim=d_model, num_heads=nhead)\n",
    "        self.dropout1 = AdaptiveDropoutLayer()  # Use AdaptiveDropoutLayer\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            AdaptiveDropoutLayer(),  # Use AdaptiveDropoutLayer here as well\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.dropout2 = AdaptiveDropoutLayer()  # And here\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2 = self.norm1(src)\n",
    "        attn_output = self.attn(src2)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout2(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, nhead, dim_feedforward, \n",
    "                 freq_threshold, smaller_embed_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab, \n",
    "            freq_threshold=freq_threshold,  \n",
    "            large_embed_dim=embedding_dim,       \n",
    "            small_embed_dim=smaller_embed_dim,   \n",
    "            max_seq_len=max_seq_len\n",
    "        )        \n",
    "        self.encoder = TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward)\n",
    "        self.pooler = Pooler(embedding_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        encoded = self.encoder(embedded, src_mask=attention_mask)\n",
    "        pooled = self.pooler(encoded)\n",
    "        return pooled\n",
    "\n",
    "# Tokenizing Code\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        node = self.root\n",
    "        subword_ids = []\n",
    "        for char in token:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "                if node.token_id is not None:\n",
    "                    subword_ids.append(node.token_id)\n",
    "                    break  # Assuming one token maps to one subword for simplicity\n",
    "            else:\n",
    "                break  # No further subword match found\n",
    "        if not subword_ids:  # If no subword was found\n",
    "            subword_ids.append(self.unk_token_id if hasattr(self, 'unk_token_id') else 1) \n",
    "        return subword_ids\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(best)  # Track most frequent pair in each iteration\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.split()  # Assuming simple word splitting\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None \n",
    "        self.failure_pops = None\n",
    "\n",
    "    def train(self, corpus):\n",
    "        vocabulary = self._build_vocabulary(corpus)  # Build the word list\n",
    "        self.trie, self.failure_links, self.failure_pops = self._precompute(vocabulary)\n",
    "\n",
    "    def _encode(self, word):\n",
    "        subword_ids = []\n",
    "        current_node = self.trie.root\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "            if char in current_node.children:\n",
    "                current_node = current_node.children[char]\n",
    "                if current_node.token_id:\n",
    "                    subword_ids.append(current_node.token_id)\n",
    "            else:  # No direct match. Implement failure logic\n",
    "                while True:\n",
    "                    if current_node.failure_link is not None:\n",
    "                        current_node = current_node.failure_link\n",
    "\n",
    "                        # Check for pops to skip further backtracking\n",
    "                        for _ in range(current_node.failure_pop):\n",
    "                            current_node = current_node.failure_link\n",
    "\n",
    "                        if char in current_node.children:\n",
    "                            current_node = current_node.children[char]\n",
    "                            if current_node.token_id:\n",
    "                                subword_ids.append(current_node.token_id)\n",
    "                            break  # Successful subword transition \n",
    "\n",
    "                    else:  # No failure link, we're back at root level\n",
    "                        # Handle situation based on your application\n",
    "                        # e.g., append an unknown token \n",
    "                        subword_ids.append(self.unk_token_id)  \n",
    "                        break  \n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        if model_type == \"unigram\":\n",
    "            tokens = self._unigram_tokenize(corpus)  #  Hypothetical tokenize function\n",
    "            vocabulary = self._build_unigram_vocab(tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        # Count token frequencies\n",
    "        token_freqs = collections.Counter(tokens)\n",
    "        # Select the most frequent tokens up to vocab_size\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(token_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction\n",
    "        self.trie = Trie()  # Assuming you have a Trie class as well\n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token)  \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "    def _unigram_tokenize(self, corpus):\n",
    "        \"\"\"\n",
    "        Tokenizes the given corpus into unigram tokens, checking against the built vocabulary.\n",
    "        Tokens not found in the vocabulary are treated as unknowns.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): The text corpus to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list of str: A list of tokens extracted from the corpus, adjusted to fit the vocabulary.\n",
    "        \"\"\"\n",
    "        # Initial tokenization based on word boundaries and punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', corpus)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, actual_vocab_size):\n",
    "        self.vocab = vocab\n",
    "        self.actual_vocab_size = actual_vocab_size\n",
    "        self.unk_token_id = self.vocab.get(\"[UNK]\", 1)  # Get ID of [UNK] \n",
    "        self.max_subword_length = max(len(token) for token in vocab.keys())\n",
    "        self.pattern = re.compile(r'\\b\\w+\\b|[\\s\\.,!?;]')\n",
    "\n",
    "        # Build the Trie\n",
    "        self.trie = Trie()\n",
    "        for token, token_id in vocab.items():\n",
    "            self.trie.insert(token, token_id)\n",
    "\n",
    "    def _find_subwords(self, word):\n",
    "        # 1. Trie Lookup \n",
    "        subword_ids = self.trie.find_subwords(word)\n",
    "\n",
    "        # 2. Fallback to Original Logic \n",
    "        if len(subword_ids) == 1 and subword_ids[0] == self.unk_token_id:  \n",
    "            subwords = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                for j in range(self.max_subword_length, 0, -1):\n",
    "                    subword = word[i:i+j]\n",
    "                    if subword in self.vocab:\n",
    "                        subwords.append(self.vocab[subword])\n",
    "                        i += j\n",
    "                        break \n",
    "                else: \n",
    "                    subwords.append(self.vocab[\"[UNK]\"])\n",
    "                    i += 1\n",
    "            subword_ids = subwords  # Replace with token IDs\n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Add [CLS] token at the beginning\n",
    "        token_ids = [self.vocab[\"[CLS]\"]]\n",
    "        tokens = self.pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            subword_ids = self._find_subwords(token)\n",
    "            # Ensure all token IDs are within the actual vocabulary size\n",
    "            subword_ids = [id if id < self.actual_vocab_size else self.vocab[\"[UNK]\"] for id in subword_ids]\n",
    "            token_ids.extend(subword_ids)\n",
    "        # Add [SEP] token at the end\n",
    "        token_ids.append(self.vocab[\"[SEP]\"])\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "# Training Code\n",
    "\n",
    "# Load corpus and build vocab\n",
    "corpus = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "tokens = tokenize(corpus)\n",
    "vocab = build_vocab(tokens)\n",
    "actual_vocab_size = len(vocab)  # This includes [PAD], [UNK], [CLS], [SEP], [MASK]\n",
    "\n",
    "print(f\"Actual vocabulary size (including special tokens): {actual_vocab_size}\")\n",
    "\n",
    "tokenizer = Tokenizer(vocab=vocab, actual_vocab_size=actual_vocab_size)\n",
    "\n",
    "# Data Loading (Illustrative)\n",
    "train_texts = [corpus]  # Treat your whole sample as one \"document\"\n",
    "train_dataset = TextDataset(train_texts, tokenizer, max_seq_len=512)\n",
    "\n",
    "print(\"Tokenizer unk_token_id:\", tokenizer.unk_token_id) \n",
    "print(\"Tokenizer Vocabulary:\", tokenizer.vocab)\n",
    "\n",
    "'''\n",
    "# Sample text for testing your tokenizer\n",
    "test_sentences = [\n",
    "    \"This is a sample sentence.\",\n",
    "    \"Let's tokenize some unusual words with punctuation, shall we?\",\n",
    "    \"1234 numbers or combinations? How does the tokenizer handle this?\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    tokenized_output = tokenizer.encode(sentence)\n",
    "    print(f\"Original Sentence: {sentence}\")\n",
    "    print(f\"Tokenized Output: {tokenized_output}\")\n",
    "    print(\"-\" * 50)  # A separator for visual clarity\n",
    "\n",
    "'''\n",
    "\n",
    "device='cpu'\n",
    "freq_threshold_values = [10, 50, 100, 200, 500]  \n",
    "best_validation_accuracy = 0.0 \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for freq_threshold in freq_threshold_values:\n",
    "\n",
    "    # Model instantiation and training setup\n",
    "    model = TransformerModel(\n",
    "        vocab_size=actual_vocab_size,\n",
    "        embedding_dim=128,\n",
    "        max_seq_len=512,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        freq_threshold=freq_threshold,  # Define your frequency threshold for splitting vocab\n",
    "        smaller_embed_dim=64\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4) \n",
    "    meta_optimizer = AdaptiveWeightDecayOptimizer(model.parameters(), lr=1e-5) \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    meta_update_freq = 5\n",
    "\n",
    "    # Training loop corrected for model architecture\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad() \n",
    "            input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\n",
    "            # Meta-update occasionally \n",
    "            if (i + 1) % meta_update_freq == 0:\n",
    "                meta_optimizer.zero_grad() \n",
    "                loss = combined_loss(output, labels, model) \n",
    "                loss.backward()\n",
    "                meta_optimizer.step()  \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual vocabulary size (including special tokens): 1743\n",
      "Tokenizer unk_token_id: 1\n",
      "Tokenizer Vocabulary: {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, ' ': 5, '\\n': 6, '.': 7, 'the': 8, ',': 9, 'of': 10, 'to': 11, 'and': 12, 'a': 13, 'is': 14, 'in': 15, 'that': 16, 'on': 17, '1': 18, 'networks': 19, 'shot': 20, 'for': 21, 'are': 22, 'we': 23, 'network': 24, 'We': 25, 'features': 26, 'dataset': 27, 'layer': 28, 'layers': 29, 'as': 30, '2': 31, 'from': 32, 'class': 33, 'with': 34, '0': 35, 'by': 36, 'each': 37, '5': 38, 'The': 39, 'classes': 40, 'training': 41, '3': 42, '4': 43, 'be': 44, 'performance': 45, 'or': 46, 'this': 47, 'learning': 48, 'trained': 49, 'A': 50, 'first': 51, 'points': 52, 'way': 53, 'distance': 54, 'task': 55, 'which': 56, 'embedding': 57, 'random': 58, 'set': 59, 'target': 60, 'an': 61, 'B': 62, 'al': 63, 'et': 64, 'prototypical': 65, '7': 66, 'can': 67, 'than': 68, 'few': 69, 'not': 70, 'at': 71, 'base': 72, 'two': 73, 'our': 74, 'results': 75, '6': 76, 'approach': 77, 'classification': 78, 'fine': 79, 'number': 80, 'one': 81, 'support': 82, 'transfer': 83, 'In': 84, 'data': 85, 'n': 86, 'use': 87, 'it': 88, 'over': 89, 'these': 90, 'using': 91, 'examples': 92, 'neural': 93, 'per': 94, 'test': 95, 'when': 96, 'This': 97, 'have': 98, 'only': 99, 'then': 100, 'also': 101, 'general': 102, 'new': 103, 'query': 104, 'accuracy': 105, 'because': 106, 'better': 107, 'different': 108, 'episodes': 109, 'meta': 110, 'more': 111, '20': 112, 'AnB': 113, 'Figure': 114, 'episode': 115, 'learn': 116, 'learned': 117, 'other': 118, 'same': 119, 'similar': 120, 'space': 121, 'tasks': 122, 'x': 123, 'both': 124, 'images': 125, 'natural': 126, 'specific': 127, 'train': 128, 'well': 129, '29': 130, 'Euclidean': 131, 'ImageNet': 132, 'but': 133, 'even': 134, 'given': 135, 'higher': 136, 'linear': 137, 'matching': 138, 'model': 139, 'prototype': 140, 'they': 141, 'weights': 142, 'BnB': 143, 'N': 144, 'Section': 145, 'baseB': 146, 'been': 147, 'into': 148, 'mean': 149, 'perform': 150, 'their': 151, 'top': 152, 'transferred': 153, 'used': 154, 'ck': 155, 'has': 156, 'non': 157, 'show': 158, 'simple': 159, 'tuning': 160, 'where': 161, 'will': 162, 'work': 163, 'zero': 164, '50': 165, '8': 166, 'Bregman': 167, 'between': 168, 'case': 169, 'classifier': 170, 'datasets': 171, 'effect': 172, 'experiments': 173, 'filters': 174, 'loss': 175, 'made': 176, 'randomly': 177, 'so': 178, 'there': 179, 'was': 180, ';': 181, 'For': 182, 'Prototypical': 183, 'These': 184, 'all': 185, 'co': 186, 'compare': 187, 'dimensional': 188, 'e': 189, 'frozen': 190, 'half': 191, 'k': 192, 'large': 193, 'may': 194, 'point': 195, 'prototypes': 196, 'split': 197, 'splits': 198, 'such': 199, 'time': 200, 'tuned': 201, '22': 202, '98': 203, 'Cosine': 204, 'SJE': 205, 'cluster': 206, 'do': 207, 'drop': 208, 'example': 209, 'fφ': 210, 'generalization': 211, 'level': 212, 'lower': 213, 'produce': 214, 'result': 215, 'selffer': 216, 'state': 217, 'upper': 218, 'validation': 219, 'vector': 220, '60': 221, 'Few': 222, 'Omniglot': 223, 'Table': 224, 'Vinyals': 225, 'art': 226, 'boost': 227, 'color': 228, 'could': 229, 'embedded': 230, 'found': 231, 'how': 232, 'its': 233, 'just': 234, 'man': 235, 'method': 236, 'nearest': 237, 'small': 238, 'toward': 239, 'transferring': 240, 'without': 241, '2009': 242, '28': 243, '64': 244, '9': 245, 'B3B': 246, 'CUB': 247, 'LSTM': 248, 'Larochelle': 249, 'Performance': 250, 'Ravi': 251, 'Top': 252, 'after': 253, 'based': 254, 'computed': 255, 'convolutional': 256, 'distances': 257, 'divergence': 258, 'either': 259, 'find': 260, 'function': 261, 'g': 262, 'here': 263, 'however': 264, 'if': 265, 'labeled': 266, 'left': 267, 'less': 268, 'middle': 269, 'much': 270, 'overfitting': 271, 'parameters': 272, 'particular': 273, 'possible': 274, 'proposed': 275, 'representation': 276, 'separate': 277, 'softmax': 278, 'them': 279, 'treatment': 280, 'vs': 281, 'within': 282, 'would': 283, '10': 284, '16': 285, '2011': 286, '23': 287, 'A3B': 288, 'Euclid': 289, 'However': 290, 'J': 291, 'K': 292, 'Matching': 293, 'NC': 294, 'NETWORKS': 295, 'On': 296, 'Sk': 297, 'University': 298, 'Zero': 299, 'achieve': 300, 'adapted': 301, 'chosen': 302, 'containing': 303, 'copied': 304, 'cosine': 305, 'd': 306, 'deep': 307, 'diamonds': 308, 'dissimilar': 309, 'does': 310, 'due': 311, 'experiment': 312, 'extent': 313, 'further': 314, 'image': 315, 'instead': 316, 'make': 317, 'metric': 318, 'order': 319, 'output': 320, 'particularly': 321, 'procedure': 322, 'rather': 323, 'recent': 324, 'reported': 325, 'represent': 326, 'row': 327, 'seen': 328, 'smaller': 329, 'squared': 330, 'transferability': 331, 'transition': 332, 'were': 333, 'whether': 334, 'xi': 335, 'yi': 336, '1000': 337, '200': 338, '2012': 339, '30': 340, '500': 341, 'Classification': 342, 'D': 343, 'FCE': 344, 'Gabor': 345, 'GoogLeNet': 346, 'If': 347, 'It': 348, 'Jarrett': 349, 'Model': 350, 'NS': 351, 'To': 352, 'above': 353, 'almost': 354, 'another': 355, 'any': 356, 'approaches': 357, 'architecture': 358, 'cat': 359, 'contains': 360, 'corresponding': 361, 'create': 362, 'degree': 363, 'density': 364, 'divergences': 365, 'end': 366, 'episodic': 367, 'equivalent': 368, 'error': 369, 'expected': 370, 'feature': 371, 'following': 372, 'generalize': 373, 'i': 374, 'including': 375, 'input': 376, 'issues': 377, 'last': 378, 'margin': 379, 'miniImageNet': 380, 'must': 381, 'neighbor': 382, 'neurons': 383, 'original': 384, 'performed': 385, 'problem': 386, 'red': 387, 'right': 388, 'second': 389, 'several': 390, 'shown': 391, 'shows': 392, 'specificity': 393, 'statistic': 394, 'subset': 395, 'surprising': 396, 'vectors': 397, 'weight': 398, 'y': 399, '15': 400, '19': 401, '40': 402, '450k': 403, '54': 404, '?': 405, 'Acc': 406, 'As': 407, 'Caltech': 408, 'Dept': 409, 'Each': 410, 'MATCHING': 411, 'NCA': 412, 'Nc': 413, 'Networks': 414, 'Our': 415, 'RANDOMSAMPLE': 416, 'RM': 417, 'S': 418, 'Thus': 419, 'Training': 420, 'When': 421, 'adaptation': 422, 'applied': 423, 'attribute': 424, 'average': 425, 'baseA': 426, 'blobs': 427, 'blue': 428, 'bottom': 429, 'call': 430, 'choice': 431, 'choices': 432, 'circles': 433, 'curious': 434, 'define': 435, 'descent': 436, 'difficulties': 437, 'distribution': 438, 'during': 439, 'effects': 440, 'evidence': 441, 'exhibit': 442, 'exp': 443, 'family': 444, 'follow': 445, 'four': 446, 'fragile': 447, 'functions': 448, 'generality': 449, 'greatly': 450, 'improves': 451, 'initialized': 452, 'line': 453, 'literature': 454, 'makes': 455, 'match': 456, 'methods': 457, 'mixture': 458, 'models': 459, 'multiple': 460, 'normalization': 461, 'optimization': 462, 'out': 463, 'paper': 464, 'provide': 465, 'provided': 466, 'quantify': 467, 'rate': 468, 'regular': 469, 's': 470, 'scenario': 471, 'single': 472, 'size': 473, 'statistician': 474, 'straightforward': 475, 'those': 476, 'three': 477, 'versions': 478, 'via': 479, 'white': 480, '000': 481, '100': 482, '2014': 483, '25': 484, '26': 485, '31': 486, '43': 487, '56': 488, '600': 489, '84': 490, '93': 491, 'AlexNet': 492, 'Bengio': 493, 'BnA': 494, 'Computer': 495, 'DA': 496, 'DS': 497, 'Datasets': 498, 'Equation': 499, 'Features': 500, 'Fine': 501, 'KNN': 502, 'Layer': 503, 'Learning': 504, 'NQ': 505, 'Nets': 506, 'OURS': 507, 'One': 508, 'Random': 509, 'SGD': 510, 'Science': 511, 'Select': 512, 'Since': 513, 'WA1': 514, 'accuracies': 515, 'across': 516, 'analysis': 517, 'attributes': 518, 'averaged': 519, 'baselines': 520, 'being': 521, 'bias': 522, 'bird': 523, 'character': 524, 'choose': 525, 'classify': 526, 'clustering': 527, 'come': 528, 'common': 529, 'comparison': 530, 'computation': 531, 'compute': 532, 'dark': 533, 'demonstrate': 534, 'denotes': 535, 'depends': 536, 'details': 537, 'determined': 538, 'directly': 539, 'discussed': 540, 'distant': 541, 'dominate': 542, 'dominates': 543, 'drops': 544, 'eight': 545, 'enable': 546, 'enough': 547, 'entire': 548, 'equivalence': 549, 'estimation': 550, 'expect': 551, 'exponential': 552, 'extend': 553, 'felid': 554, 'final': 555, 'finding': 556, 'five': 557, 'fixed': 558, 'gradient': 559, 'hand': 560, 'having': 561, 'high': 562, 'hyperparameters': 563, 'idea': 564, 'identical': 565, 'improve': 566, 'increases': 567, 'individual': 568, 'inductive': 569, 'initializing': 570, 'iterations': 571, 'keep': 572, 'key': 573, 'knowledge': 574, 'label': 575, 'labels': 576, 'larger': 577, 'learns': 578, 'least': 579, 'length': 580, 'light': 581, 'like': 582, 'limited': 583, 'local': 584, 'making': 585, 'many': 586, 'material': 587, 'meaning': 588, 'means': 589, 'near': 590, 'need': 591, 'observed': 592, 'obtain': 593, 'optimal': 594, 'ours': 595, 'pairs': 596, 'phenomenon': 597, 'pooling': 598, 'posterior': 599, 'previously': 600, 'processing': 601, 'propose': 602, 'pφ': 603, 'quite': 604, 'related': 605, 'respect': 606, 'retraining': 607, 'rotations': 608, 'scenarios': 609, 'seems': 610, 'sense': 611, 'setting': 612, 'showing': 613, 'simpler': 614, 'simply': 615, 'since': 616, 'some': 617, 'splitting': 618, 'study': 619, 'subplot': 620, 'suggests': 621, 'supervised': 622, 'supplementary': 623, 'take': 624, 'thus': 625, 'tune': 626, 'unit': 627, 'untrained': 628, 'useful': 629, 'utilizes': 630, 'various': 631, 'very': 632, 'vk': 633, 'weighted': 634, 'while': 635, 'z': 636, 'φ': 637, '024': 638, '101': 639, '11': 640, '12': 641, '13': 642, '14': 643, '17': 644, '2013': 645, '27': 646, '312': 647, '42': 648, '44': 649, '449': 650, '49': 651, '551': 652, '58': 653, '62': 654, '66': 655, '70': 656, '71': 657, '78': 658, '79': 659, '80': 660, '95': 661, '99': 662, 'ALE': 663, 'Abstract': 664, 'Accuracy': 665, 'Adam': 666, 'Algorithm': 667, 'All': 668, 'An': 669, 'Another': 670, 'Birds': 671, 'Bottom': 672, 'Comparison': 673, 'Cornell': 674, 'DVk': 675, 'Dist': 676, 'Donahue': 677, 'Fei': 678, 'Finally': 679, 'First': 680, 'Further': 681, 'ILSVRC': 682, 'Introduction': 683, 'Krizhevsky': 684, 'LG': 685, 'LMNN': 686, 'Large': 687, 'Lines': 688, 'Mahalanobis': 689, 'Man': 690, 'Mensink': 691, 'Natural': 692, 'Note': 693, 'Of': 694, 'PROTOTYPICAL': 695, 'Proto': 696, 'Qk': 697, 'R': 698, 'Results': 699, 'Such': 700, 'That': 701, 'Their': 702, 'There': 703, 'They': 704, 'Toronto': 705, 'Transfer': 706, 'Tune': 707, 'Twitter': 708, 'UCSD': 709, 'WA2': 710, 'WA3': 711, 'WB1': 712, 'WB2': 713, 'WB3': 714, 'While': 715, 'X': 716, 'able': 717, 'about': 718, 'achieves': 719, 'affected': 720, 'aggregated': 721, 'algorithms': 722, 'allowed': 723, 'allows': 724, 'applying': 725, 'approximate': 726, 'approximately': 727, 'arXiv': 728, 'architectural': 729, 'assigned': 730, 'assigning': 731, 'axis': 732, 'bars': 733, 'batch': 734, 'become': 735, 'beneficial': 736, 'block': 737, 'c1': 738, 'c2': 739, 'c3': 740, 'called': 741, 'carefully': 742, 'cases': 743, 'categories': 744, 'chance': 745, 'characters': 746, 'chopped': 747, 'closer': 748, 'collectively': 749, 'com': 750, 'combination': 751, 'comparable': 752, 'compared': 753, 'computing': 754, 'conditional': 755, 'conducted': 756, 'confidence': 757, 'conjecture': 758, 'connects': 759, 'considering': 760, 'consist': 761, 'contain': 762, 'control': 763, 'controls': 764, 'copy': 765, 'created': 766, 'cs': 767, 'custom': 768, 'decisions': 769, 'decoupled': 770, 'decreases': 771, 'degradation': 772, 'demonstrated': 773, 'depend': 774, 'depending': 775, 'design': 776, 'designed': 777, 'detectors': 778, 'determine': 779, 'developed': 780, 'did': 781, 'differ': 782, 'discriminative': 783, 'distinct': 784, 'divided': 785, 'domain': 786, 'draws': 787, 'dϕ': 788, 'effectively': 789, 'effectiveness': 790, 'efficient': 791, 'elements': 792, 'embeddings': 793, 'empirical': 794, 'empirically': 795, 'entities': 796, 'exact': 797, 'expense': 798, 'experimental': 799, 'experimentally': 800, 'explanation': 801, 'extension': 802, 'extensions': 803, 'fact': 804, 'fully': 805, 'gap': 806, 'generalizing': 807, 'generated': 808, 'generative': 809, 'giving': 810, 'good': 811, 'group': 812, 'grows': 813, 'handle': 814, 'help': 815, 'helpful': 816, 'hexagons': 817, 'implementation': 818, 'improved': 819, 'improvement': 820, 'improving': 821, 'include': 822, 'increased': 823, 'indicate': 824, 'indicates': 825, 'indicating': 826, 'initialization': 827, 'insight': 828, 'intervals': 829, 'involves': 830, 'k0': 831, 'known': 832, 'lead': 833, 'learnable': 834, 'leopard': 835, 'levels': 836, 'lingers': 837, 'lion': 838, 'log': 839, 'longer': 840, 'main': 841, 'mapping': 842, 'max': 843, 'maximize': 844, 'might': 845, 'minimal': 846, 'modeling': 847, 'most': 848, 'naturally': 849, 'nature': 850, 'nearly': 851, 'negatively': 852, 'neighboring': 853, 'nets': 854, 'next': 855, 'nonlinearity': 856, 'normalized': 857, 'note': 858, 'noticing': 859, 'object': 860, 'obtained': 861, 'occur': 862, 'optimize': 863, 'ordinary': 864, 'originally': 865, 'overfit': 866, 'own': 867, 'partitioning': 868, 'performing': 869, 'place': 870, 'plus': 871, 'poorly': 872, 'pre': 873, 'predict': 874, 'predictions': 875, 'previous': 876, 'primarily': 877, 'process': 878, 'produces': 879, 'producing': 880, 'quantified': 881, 'questions': 882, 'quickly': 883, 'reason': 884, 'reference': 885, 'regime': 886, 'relatively': 887, 'relevant': 888, 'remaining': 889, 'report': 890, 'representations': 891, 'represents': 892, 'require': 893, 'retrained': 894, 'reveals': 895, 'rows': 896, 'scratch': 897, 'sections': 898, 'serve': 899, 'severely': 900, 'should': 901, 'showed': 902, 'significant': 903, 'similarity': 904, 'sizes': 905, 'slightly': 906, 'solution': 907, 'specialization': 908, 'species': 909, 'spherical': 910, 'standard': 911, 'statistically': 912, 'still': 913, 'strictly': 914, 'subsets': 915, 'substantial': 916, 'subtracting': 917, 'successful': 918, 'suggesting': 919, 'tend': 920, 'terms': 921, 'therefore': 922, 'third': 923, 'through': 924, 'tiger': 925, 'total': 926, 'transformation': 927, 'transformed': 928, 'true': 929, 'unsupervised': 930, 'up': 931, 'updates': 932, 'us': 933, 'uses': 934, 'utilizing': 935, 'version': 936, 'worse': 937, 'written': 938, 'x1': 939, 'xN': 940, 'y1': 941, 'yN': 942, 'yield': 943, 'yields': 944, 'θ': 945, '00': 946, '05': 947, '05175v2': 948, '09': 949, '0k': 950, '1024': 951, '1200': 952, '1411': 953, '1600': 954, '1623': 955, '167': 956, '1703': 957, '1792v1': 958, '1995': 959, '1The': 960, '2000': 961, '2004': 962, '2013a': 963, '2013b': 964, '2017': 965, '21': 966, '24': 967, '281': 968, '2Note': 969, '37': 970, '3AnA': 971, '4We': 972, '51': 973, '52': 974, '55': 975, '625': 976, '645': 977, '68': 978, '73': 979, '77': 980, '788': 981, '800': 982, '86': 983, '90': 984, '96': 985, '97': 986, 'A8B': 987, 'Additionally': 988, 'Aerospace': 989, 'Also': 990, 'Alternately': 991, 'Although': 992, 'Analysis': 993, 'At': 994, 'B8B': 995, 'BASELINE': 996, 'Because': 997, 'By': 998, 'CLUSTERING': 999, 'CU': 1000, 'Caffe': 1001, 'Can': 1002, 'Caruana': 1003, 'Challenge': 1004, 'Choices': 1005, 'Classes': 1006, 'Clune': 1007, 'Compared': 1008, 'Components': 1009, 'Compute': 1010, 'Conclusion': 1011, 'Conclusions': 1012, 'DNet': 1013, 'Dark': 1014, 'Degradation': 1015, 'Deng': 1016, 'Density': 1017, 'Design': 1018, 'Discussion': 1019, 'Dissimilar': 1020, 'Distance': 1021, 'Dk': 1022, 'Does': 1023, 'Early': 1024, 'Edwards': 1025, 'Egyptian': 1026, 'Engineering': 1027, 'Episode': 1028, 'Error': 1029, 'Estimation': 1030, 'Examples': 1031, 'Experimental': 1032, 'Experiments': 1033, 'Felidae': 1034, 'Fergus': 1035, 'Fisher': 1036, 'Fortunately': 1037, 'Fourth': 1038, 'GPU': 1039, 'Gaussian': 1040, 'Gaussians': 1041, 'Generality': 1042, 'Girshick': 1043, 'Given': 1044, 'Gradient': 1045, 'Here': 1046, 'Hinton': 1047, 'Hod': 1048, 'How': 1049, 'ILSVRC2012': 1050, 'Image': 1051, 'Indeed': 1052, 'Initial': 1053, 'Initialize': 1054, 'Input': 1055, 'Instead': 1056, 'Institute': 1057, 'Into': 1058, 'Intuitively': 1059, 'Jake': 1060, 'Jason': 1061, 'Jeff': 1062, 'Jia': 1063, 'Jun': 1064, 'KL': 1065, 'Kevin': 1066, 'LEARNER': 1067, 'Layers': 1068, 'Le': 1069, 'Learner': 1070, 'Lee': 1071, 'Left': 1072, 'Legendre': 1073, 'Light': 1074, 'Like': 1075, 'Linear': 1076, 'Lipson4': 1077, 'M': 1078, 'META': 1079, 'Many': 1080, 'Measured': 1081, 'Mechanical': 1082, 'Meta': 1083, 'Mixture': 1084, 'Modern': 1085, 'Modifying': 1086, 'Montreal': 1087, 'Moreover': 1088, 'Much': 1089, 'NEAREST': 1090, 'NEIGHBORS': 1091, 'NETS': 1092, 'NEURAL': 1093, 'Neighborhood': 1094, 'Neither': 1095, 'No': 1096, 'Notably': 1097, 'Notation': 1098, 'Nov': 1099, 'Numbered': 1100, 'Operations': 1101, 'Output': 1102, 'Overall': 1103, 'Overview': 1104, 'P': 1105, 'PROTO': 1106, 'Persian': 1107, 'Points': 1108, 'Previously': 1109, 'Prototype': 1110, 'Pseudocode': 1111, 'ReLU': 1112, 'Recent': 1113, 'Recognition': 1114, 'Reed': 1115, 'Reinterpretation': 1116, 'Related': 1117, 'Relative': 1118, 'Research': 1119, 'Richard': 1120, 'Right': 1121, 'Rippel': 1122, 'SAMPLE': 1123, 'STATISTICIAN': 1124, 'SVM': 1125, 'Salakhutdinov': 1126, 'Scale': 1127, 'Second': 1128, 'Separate': 1129, 'Sermanet': 1130, 'Setup': 1131, 'Shot': 1132, 'Siamese': 1133, 'Similar': 1134, 'Similarly': 1135, 'Snell': 1136, 'Specificity': 1137, 'Splitting': 1138, 'Storkey': 1139, 'Swersky': 1140, 'Thanks': 1141, 'Third': 1142, 'Transferability': 1143, 'Two': 1144, 'Unlike': 1145, 'Update': 1146, 'V': 1147, 'Vector': 1148, 'Visual': 1149, 'WA4': 1150, 'WA5': 1151, 'WA6': 1152, 'WA7': 1153, 'WA8': 1154, 'WB4': 1155, 'WB5': 1156, 'WB6': 1157, 'WB7': 1158, 'WB8': 1159, 'Weights': 1160, 'Where': 1161, 'Whereas': 1162, 'With': 1163, 'Work': 1164, 'Wyoming': 1165, 'Y': 1166, 'Yoshua': 1167, 'Yosinski': 1168, 'Zeiler': 1169, 'Zemel': 1170, 'ability': 1171, 'abs': 1172, 'absolute': 1173, 'accommodate': 1174, 'account': 1175, 'achieving': 1176, 'act': 1177, 'activations': 1178, 'adapt': 1179, 'addition': 1180, 'additional': 1181, 'addressing': 1182, 'advance': 1183, 'advantage': 1184, 'advantageous': 1185, 'affect': 1186, 'against': 1187, 'aggregate': 1188, 'aggregation': 1189, 'aim': 1190, 'algorithm': 1191, 'allowing': 1192, 'alone': 1193, 'alphabets': 1194, 'alternative': 1195, 'although': 1196, 'amount': 1197, 'analyze': 1198, 'answers': 1199, 'anything': 1200, 'anywhere': 1201, 'apparent': 1202, 'apparently': 1203, 'appealing': 1204, 'appear': 1205, 'appearance': 1206, 'applicable': 1207, 'applies': 1208, 'apply': 1209, 'arbitrary': 1210, 'around': 1211, 'ask': 1212, 'assess': 1213, 'assign': 1214, 'assignment': 1215, 'associated': 1216, 'assumption': 1217, 'assumptions': 1218, 'attack': 1219, 'attained': 1220, 'attains': 1221, 'attempt': 1222, 'attempts': 1223, 'attention': 1224, 'attributed': 1225, 'augmenting': 1226, 'author': 1227, 'autoencoder': 1228, 'available': 1229, 'b': 1230, 'back': 1231, 'backprop': 1232, 'backpropagate': 1233, 'batches': 1234, 'befits': 1235, 'beginning': 1236, 'behavior': 1237, 'believe': 1238, 'belonging': 1239, 'below': 1240, 'benchmark': 1241, 'benefits': 1242, 'best': 1243, 'beyond': 1244, 'bi': 1245, 'biological': 1246, 'blob': 1247, 'blocks': 1248, 'boosting': 1249, 'bug': 1250, 'cannot': 1251, 'carries': 1252, 'cats': 1253, 'cause': 1254, 'causes': 1255, 'change': 1256, 'characteristics': 1257, 'characterize': 1258, 'cheetah': 1259, 'choosing': 1260, 'ck0': 1261, 'clarity': 1262, 'classconditional': 1263, 'classified': 1264, 'classifiers': 1265, 'closely': 1266, 'clusters': 1267, 'coadaptation': 1268, 'code': 1269, 'collected': 1270, 'comes': 1271, 'commonly': 1272, 'comparing': 1273, 'competition': 1274, 'completely': 1275, 'complex': 1276, 'complicated': 1277, 'component': 1278, 'composed': 1279, 'composition': 1280, 'comprised': 1281, 'comprises': 1282, 'computationally': 1283, 'concise': 1284, 'conclusions': 1285, 'configuration': 1286, 'connecting': 1287, 'connections': 1288, 'consideration': 1289, 'considered': 1290, 'constant': 1291, 'constrain': 1292, 'construct': 1293, 'constructed': 1294, 'constructing': 1295, 'contained': 1296, 'continuous': 1297, 'contrast': 1298, 'contributions': 1299, 'converge': 1300, 'convergence': 1301, 'convex': 1302, 'convolution': 1303, 'core': 1304, 'cost': 1305, 'couple': 1306, 'course': 1307, 'creates': 1308, 'crop': 1309, 'crops': 1310, 'cumulant': 1311, 'currently': 1312, 'cut': 1313, 'cvpr2016': 1314, 'datapoint': 1315, 'days': 1316, 'deal': 1317, 'decay': 1318, 'decline': 1319, 'decoupling': 1320, 'deeper': 1321, 'defined': 1322, 'definition': 1323, 'definitions': 1324, 'degrees': 1325, 'densities': 1326, 'derived': 1327, 'describes': 1328, 'description': 1329, 'descriptions': 1330, 'despite': 1331, 'differences': 1332, 'differentiable': 1333, 'differs': 1334, 'difficult': 1335, 'difficulty': 1336, 'dimension': 1337, 'diminishes': 1338, 'direction': 1339, 'directional': 1340, 'directions': 1341, 'distinction': 1342, 'distributions': 1343, 'divide': 1344, 'document': 1345, 'doesn': 1346, 'dogs': 1347, 'domains': 1348, 'done': 1349, 'dot': 1350, 'dotted': 1351, 'downloaded': 1352, 'draw': 1353, 'drawn': 1354, 'dynamically': 1355, 'dynamics': 1356, 'easier': 1357, 'effective': 1358, 'ellipsoidal': 1359, 'else': 1360, 'elsewhere': 1361, 'embed': 1362, 'emphasize': 1363, 'encode': 1364, 'encoder': 1365, 'encoding': 1366, 'encourages': 1367, 'environment': 1368, 'epochs': 1369, 'equally': 1370, 'errors': 1371, 'etc': 1372, 'eventually': 1373, 'every': 1374, 'excellent': 1375, 'except': 1376, 'exists': 1377, 'expensive': 1378, 'explorations': 1379, 'extends': 1380, 'extensible': 1381, 'extensive': 1382, 'extensively': 1383, 'extra': 1384, 'extracted': 1385, 'extremely': 1386, 'f': 1387, 'failed': 1388, 'fairly': 1389, 'faithful': 1390, 'falls': 1391, 'far': 1392, 'feather': 1393, 'felids': 1394, 'fewer': 1395, 'figure': 1396, 'files': 1397, 'filter': 1398, 'finetuned': 1399, 'fitted': 1400, 'fix': 1401, 'flexibility': 1402, 'flipped': 1403, 'focus': 1404, 'forces': 1405, 'form': 1406, 'formed': 1407, 'forms': 1408, 'formulate': 1409, 'fragilely': 1410, 'future': 1411, 'gaining': 1412, 'gains': 1413, 'generalizes': 1414, 'generally': 1415, 'get': 1416, 'getting': 1417, 'github': 1418, 'goal': 1419, 'grained': 1420, 'grayscale': 1421, 'greater': 1422, 'groups': 1423, 'gϑ': 1424, 'halves': 1425, 'handwritten': 1426, 'hard': 1427, 'held': 1428, 'helps': 1429, 'hierarchy': 1430, 'hinge': 1431, 'hold': 1432, 'holds': 1433, 'horizontally': 1434, 'http': 1435, 'https': 1436, 'human': 1437, 'humans': 1438, 'hyperparameter': 1439, 'hypothesis': 1440, 'hypothesize': 1441, 'ii': 1442, 'illustration': 1443, 'image2': 1444, 'importance': 1445, 'important': 1446, 'imposes': 1447, 'improvements': 1448, 'incorporate': 1449, 'incorporated': 1450, 'increase': 1451, 'indeed': 1452, 'independent': 1453, 'indices': 1454, 'inference': 1455, 'inferred': 1456, 'information': 1457, 'informative': 1458, 'initial': 1459, 'initialize': 1460, 'interact': 1461, 'interactions': 1462, 'interest': 1463, 'interested': 1464, 'interpretation': 1465, 'interpretations': 1466, 'interpreted': 1467, 'introduced': 1468, 'intuitive': 1469, 'involving': 1470, 'issue': 1471, 'itself': 1472, 'jaguar': 1473, 'join': 1474, 'jointly': 1475, 'justify': 1476, 'keeping': 1477, 'know': 1478, 'kz': 1479, 'latter': 1480, 'learner': 1481, 'leave': 1482, 'likewise': 1483, 'linearity': 1484, 'linearly': 1485, 'lines': 1486, 'linger': 1487, 'little': 1488, 'locked': 1489, 'logX': 1490, 'lost': 1491, 'low': 1492, 'lynx': 1493, 'm': 1494, 'manner': 1495, 'marker': 1496, 'measure': 1497, 'mechanism': 1498, 'mentioned': 1499, 'mimic': 1500, 'mini': 1501, 'minimizing': 1502, 'mirrors': 1503, 'mistakes': 1504, 'modal': 1505, 'modern': 1506, 'modifying': 1507, 'monitoring': 1508, 'mountain': 1509, 'multi': 1510, 'multimodal': 1511, 'multiples': 1512, 'naive': 1513, 'namely': 1514, 'negative': 1515, 'neighborhood': 1516, 'net': 1517, 'never': 1518, 'no': 1519, 'nor': 1520, 'normalize': 1521, 'notation': 1522, 'noted': 1523, 'notions': 1524, 'novel': 1525, 'objective': 1526, 'objectives': 1527, 'observation': 1528, 'obtaining': 1529, 'obviates': 1530, 'occurrence': 1531, 'occurs': 1532, 'off': 1533, 'often': 1534, 'old': 1535, 'opposed': 1536, 'optimized': 1537, 'orange': 1538, 'ordering': 1539, 'outline': 1540, 'outperforms': 1541, 'overlapping': 1542, 'parameter': 1543, 'parameterization': 1544, 'parent': 1545, 'part': 1546, 'patterns': 1547, 'perfectly': 1548, 'performances': 1549, 'performs': 1550, 'permissible': 1551, 'persists': 1552, 'phase': 1553, 'plausible': 1554, 'plots': 1555, 'plotted': 1556, 'post': 1557, 'powerful': 1558, 'prediction': 1559, 'preliminary': 1560, 'preparing': 1561, 'presents': 1562, 'prevents': 1563, 'probabilities': 1564, 'probability': 1565, 'proceeds': 1566, 'produced': 1567, 'progress': 1568, 'promising': 1569, 'pronounced': 1570, 'publication': 1571, 'pψ': 1572, 'quantifying': 1573, 'question': 1574, 'raises': 1575, 'ranges': 1576, 'rapidly': 1577, 'rates': 1578, 'raw': 1579, 're': 1580, 'recovers': 1581, 'rectangles': 1582, 'rectification': 1583, 'rediscovered': 1584, 'reedscot': 1585, 'refer': 1586, 'reflect': 1587, 'regardless': 1588, 'regularization': 1589, 'regularize': 1590, 'reinitialize': 1591, 'relate': 1592, 'relates': 1593, 'relative': 1594, 'relearn': 1595, 'relearned': 1596, 'relearning': 1597, 'released': 1598, 'relies': 1599, 'relu': 1600, 'rely': 1601, 'remainder': 1602, 'repeated': 1603, 'replacement': 1604, 'representative': 1605, 'representatives': 1606, 'represented': 1607, 'reproduce': 1608, 'repurpose': 1609, 'required': 1610, 'requires': 1611, 'requiring': 1612, 'researchers': 1613, 'resemble': 1614, 'resembles': 1615, 'resizing': 1616, 'retrain': 1617, 'rigorous': 1618, 'risk': 1619, 'run': 1620, 'runs': 1621, 'sample': 1622, 'sampled': 1623, 'save': 1624, 'say': 1625, 'schedule': 1626, 'scheme': 1627, 'secondary': 1628, 'selecting': 1629, 'semantically': 1630, 'sets': 1631, 'settings': 1632, 'setup': 1633, 'seven': 1634, 'shape': 1635, 'shared': 1636, 'shifted': 1637, 'shown3': 1638, 'significantly': 1639, 'simplicity': 1640, 'simplify': 1641, 'situation': 1642, 'slight': 1643, 'snow': 1644, 'software': 1645, 'somewhere': 1646, 'sophisticated': 1647, 'sparse': 1648, 'special': 1649, 'specifically': 1650, 'specifies': 1651, 'speed': 1652, 'spread': 1653, 'step': 1654, 'stopping': 1655, 'stops': 1656, 'store': 1657, 'strikingly': 1658, 'studied': 1659, 'studies': 1660, 'studying': 1661, 'subject': 1662, 'subsampling': 1663, 'substantially': 1664, 'successfully': 1665, 'successive': 1666, 'suddenly': 1667, 'suffers': 1668, 'suitability': 1669, 'suitable': 1670, 'suited': 1671, 'summarize': 1672, 'summarizes': 1673, 'summary': 1674, 'suspicion': 1675, 'systems': 1676, 't': 1677, 'tabby': 1678, 'tackle': 1679, 'taken': 1680, 'takes': 1681, 'taking': 1682, 'tanh': 1683, 'technique': 1684, 'techniques': 1685, 'tell': 1686, 'tendency': 1687, 'term': 1688, 'tested': 1689, 'tests': 1690, 'text': 1691, 'theart': 1692, 'themselves': 1693, 'thereby': 1694, 'things': 1695, 'though': 1696, 'tool': 1697, 'trains': 1698, 'transferable': 1699, 'treatments': 1700, 'tweak': 1701, 'tweaking': 1702, 'type': 1703, 'types': 1704, 'under': 1705, 'underlying': 1706, 'uniformly': 1707, 'uniquely': 1708, 'unlabeled': 1709, 'until': 1710, 'upon': 1711, 'usual': 1712, 'usually': 1713, 'utilize': 1714, 'v1': 1715, 'v2': 1716, 'v3': 1717, 'variance': 1718, 'variant': 1719, 'variants': 1720, 'variational': 1721, 'vast': 1722, 'versa': 1723, 'versus': 1724, 'vertical': 1725, 'vice': 1726, 'viewed': 1727, 'visual': 1728, 'vital': 1729, 'want': 1730, 'ways': 1731, 'whereas': 1732, 'whole': 1733, 'whose': 1734, 'wide': 1735, 'won': 1736, 'xk': 1737, 'yosinski': 1738, 'µ': 1739, 'θk': 1740, 'ψ': 1741, 'ϕ': 1742}\n",
      "Embeddings shape: torch.Size([1, 512, 128])\n",
      "Positional embeddings shape: torch.Size([1, 512, 128])\n",
      "Logits shape: torch.Size([1, 512, 1743]), Labels shape: torch.Size([512])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (512).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 678\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Labels shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    677\u001b[0m \u001b[38;5;66;03m# Calculate loss using logits for token-level predictions\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    679\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    680\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (512)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # Ensure special tokens are included\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def combined_loss(output, target, model, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)  \n",
    "    regularization_loss = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if isinstance(param, nn.Parameter):  \n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1\n",
    "\n",
    "    return task_loss + regularization_loss\n",
    "\n",
    "# Encoding Transformer Code\n",
    "\n",
    "class AdaptiveDropoutLayer(nn.Module):\n",
    "    def __init__(self, init_dropout_rate=0.1):\n",
    "        super(AdaptiveDropoutLayer, self).__init__()\n",
    "        # Use logit transformation for stability\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.log_alpha)\n",
    "        # Convert p from a tensor to a float\n",
    "        p_value = p.item()  # This extracts the scalar value as a Python float\n",
    "        return nn.functional.dropout(x, p=p_value, training=self.training)\n",
    "\n",
    "\n",
    "class AdaptiveWeightDecayOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, init_l2_strength=0.01):\n",
    "        super().__init__(params, {'lr': lr})\n",
    "        self.log_l2_strength = nn.Parameter(torch.tensor(math.log(init_l2_strength)).float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = torch.exp(self.log_l2_strength)  \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay) \n",
    "                p.update(d_p, group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Create positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension (B x T x C)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Tensor of shape [Batch Size, Sequence Length, Embedding Dimension]\n",
    "        # Adjust positional encoding to match the input size and device\n",
    "        pe = self.pe[:, :x.size(1)]\n",
    "        # Assuming x is on the correct device, pe will be automatically aligned to the same device\n",
    "        return pe\n",
    "\n",
    "\n",
    "class MultiHeadLinformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, k=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k if k is not None else embed_dim // num_heads  # Projection dimension per head\n",
    "\n",
    "        self.key_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.value_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        # Project keys and values\n",
    "        keys = self.key_projections(query)\n",
    "        values = self.value_projections(query)\n",
    "        \n",
    "        # Reshape into [batch_size, num_heads, seq_len, k]\n",
    "        keys = keys.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        values = values.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention (scaled dot-product attention)\n",
    "        # Scaling by the square root of the depth of the key vectors to prevent large values in the dot product\n",
    "        # which could push the softmax function into regions where it has extremely small gradients\n",
    "        keys = keys / (self.k ** 0.5)\n",
    "        attention_scores = torch.softmax(torch.matmul(keys, values.transpose(-2, -1)), dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention_scores, values)\n",
    "        \n",
    "        # Concatenate heads and project back to original embedding dimension\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, self.num_heads * self.k)\n",
    "        out = self.out_projection(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len= 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)[:self.max_seq_len]         \n",
    "        labels = encoded_text.copy()  # Copy encoded text for labels\n",
    "\n",
    "        # Apply dynamic masking\n",
    "        masked_indices = np.random.rand(len(encoded_text)) < 0.15\n",
    "        for i in range(len(encoded_text)):\n",
    "            if masked_indices[i]:\n",
    "                encoded_text[i] = self.tokenizer.vocab[\"[MASK]\"]\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "        labels += [-100] * padding_length  # Use -100 for padding positions\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class AdaptiveEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab,  vocab_size, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "        super(AdaptiveEmbeddingLayer, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab_size\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.large_embed_dim = large_embed_dim\n",
    "        self.small_embed_dim = small_embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.split_vocab(vocab, freq_threshold)  \n",
    "\n",
    "        self.frequent_embeddings = nn.Embedding(num_embeddings=len(self.frequent_vocab), embedding_dim=large_embed_dim)\n",
    "        self.infrequent_embeddings = nn.Embedding(num_embeddings=len(self.infrequent_vocab), embedding_dim=small_embed_dim)\n",
    "        self.infrequent_projection = nn.Linear(small_embed_dim, large_embed_dim)\n",
    "        self.positional_embeddings = PositionalEncoding(large_embed_dim, max_seq_len)\n",
    "\n",
    "\n",
    "    def split_vocab(self, vocab, freq_threshold):\n",
    "        token_counts = [(token, count) for token, count in vocab.items()]\n",
    "        token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "        \n",
    "        self.frequent_vocab = dict(token_counts[:split_point])\n",
    "        self.infrequent_vocab = dict(token_counts[split_point:])\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        device = token_ids.device\n",
    "        seq_len = token_ids.size(1)\n",
    "        batch_size = token_ids.size(0)  # Obtain batch size from token_ids tensor\n",
    "\n",
    "        # Initialize embeddings tensor\n",
    "        embeddings = torch.zeros(token_ids.shape[0], seq_len, self.large_embed_dim, device=device)\n",
    "\n",
    "        # Map token_ids to indices for frequent and infrequent vocab\n",
    "        frequent_indices = torch.zeros_like(token_ids)\n",
    "        infrequent_indices = torch.zeros_like(token_ids)\n",
    "        \n",
    "        for token_id, index in self.vocab.items():\n",
    "            mask = token_ids == token_id\n",
    "            if token_id in self.frequent_vocab:\n",
    "                # Map to index in frequent_vocab\n",
    "                frequent_indices[mask] = self.frequent_vocab[token_id]\n",
    "            elif token_id in self.infrequent_vocab:\n",
    "                # Map to index in infrequent_vocab\n",
    "                infrequent_indices[mask] = self.infrequent_vocab[token_id]\n",
    "\n",
    "        # Create masks for frequent and infrequent tokens\n",
    "        frequent_mask = frequent_indices > 0\n",
    "        infrequent_mask = infrequent_indices > 0\n",
    "\n",
    "        # Embed frequent tokens\n",
    "        if frequent_mask.any():\n",
    "            frequent_embeddings = self.frequent_embeddings(frequent_indices[frequent_mask])\n",
    "            embeddings[frequent_mask] = frequent_embeddings\n",
    "\n",
    "        # Embed and project infrequent tokens\n",
    "        if infrequent_mask.any():\n",
    "            infrequent_embeddings = self.infrequent_embeddings(infrequent_indices[infrequent_mask])\n",
    "            infrequent_embeddings_projected = self.infrequent_projection(infrequent_embeddings)\n",
    "            embeddings[infrequent_mask] = infrequent_embeddings_projected\n",
    "\n",
    "        # Apply positional embeddings\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        position_embeddings = self.positional_embeddings(position_ids)  # Generate for seq_len\n",
    "\n",
    "        # Ensure positional embeddings are broadcastable to the embeddings tensor\n",
    "        # This step may not be necessary if your positional embeddings are already correctly shaped\n",
    "        if position_embeddings.size(0) != batch_size:\n",
    "            position_embeddings = position_embeddings.expand(batch_size, -1, -1)\n",
    "\n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"Positional embeddings shape: {position_embeddings.shape}\")\n",
    "        embeddings += position_embeddings\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadLinformerAttention(embed_dim=d_model, num_heads=nhead)\n",
    "        self.dropout1 = AdaptiveDropoutLayer()  # Use AdaptiveDropoutLayer\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            AdaptiveDropoutLayer(),  # Use AdaptiveDropoutLayer here as well\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.dropout2 = AdaptiveDropoutLayer()  # And here\n",
    " \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2 = self.norm1(src)\n",
    "        attn_output = self.attn(src2)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout2(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab, vocab_size, embedding_dim, max_seq_len, nhead, dim_feedforward, \n",
    "                 freq_threshold, smaller_embed_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab,\n",
    "            vocab_size=vocab_size, \n",
    "            freq_threshold=freq_threshold,  \n",
    "            large_embed_dim=embedding_dim,       \n",
    "            small_embed_dim=smaller_embed_dim,   \n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.encoder = TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward)\n",
    "        self.pooler = Pooler(embedding_dim)  # Retain Pooler for sentence-level representation\n",
    "        # Add an output projection layer for token-level predictions\n",
    "        self.output_projection = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        encoded = self.encoder(embedded, src_mask=attention_mask)\n",
    "        # Get pooled output for sentence-level tasks\n",
    "        pooled_output = self.pooler(encoded)\n",
    "        # Project encoded output to vocabulary size for token-level predictions\n",
    "        logits = self.output_projection(encoded)\n",
    "        return logits, pooled_output\n",
    "\n",
    "# Tokenizing Code\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        node = self.root\n",
    "        subword_ids = []\n",
    "        for char in token:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "                if node.token_id is not None:\n",
    "                    subword_ids.append(node.token_id)\n",
    "                    break  # Assuming one token maps to one subword for simplicity\n",
    "            else:\n",
    "                break  # No further subword match found\n",
    "        if not subword_ids:  # If no subword was found\n",
    "            subword_ids.append(self.unk_token_id if hasattr(self, 'unk_token_id') else 1) \n",
    "        return subword_ids\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(best)  # Track most frequent pair in each iteration\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.split()  # Assuming simple word splitting\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None \n",
    "        self.failure_pops = None\n",
    "\n",
    "    def train(self, corpus):\n",
    "        vocabulary = self._build_vocabulary(corpus)  # Build the word list\n",
    "        self.trie, self.failure_links, self.failure_pops = self._precompute(vocabulary)\n",
    "\n",
    "    def _encode(self, word):\n",
    "        subword_ids = []\n",
    "        current_node = self.trie.root\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "            if char in current_node.children:\n",
    "                current_node = current_node.children[char]\n",
    "                if current_node.token_id:\n",
    "                    subword_ids.append(current_node.token_id)\n",
    "            else:  # No direct match. Implement failure logic\n",
    "                while True:\n",
    "                    if current_node.failure_link is not None:\n",
    "                        current_node = current_node.failure_link\n",
    "\n",
    "                        # Check for pops to skip further backtracking\n",
    "                        for _ in range(current_node.failure_pop):\n",
    "                            current_node = current_node.failure_link\n",
    "\n",
    "                        if char in current_node.children:\n",
    "                            current_node = current_node.children[char]\n",
    "                            if current_node.token_id:\n",
    "                                subword_ids.append(current_node.token_id)\n",
    "                            break  # Successful subword transition \n",
    "\n",
    "                    else:  # No failure link, we're back at root level\n",
    "                        # Handle situation based on your application\n",
    "                        # e.g., append an unknown token \n",
    "                        subword_ids.append(self.unk_token_id)  \n",
    "                        break  \n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        if model_type == \"unigram\":\n",
    "            tokens = self._unigram_tokenize(corpus)  #  Hypothetical tokenize function\n",
    "            vocabulary = self._build_unigram_vocab(tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        # Count token frequencies\n",
    "        token_freqs = collections.Counter(tokens)\n",
    "        # Select the most frequent tokens up to vocab_size\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(token_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction\n",
    "        self.trie = Trie()  # Assuming you have a Trie class as well\n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token)  \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "    def _unigram_tokenize(self, corpus):\n",
    "        \"\"\"\n",
    "        Tokenizes the given corpus into unigram tokens, checking against the built vocabulary.\n",
    "        Tokens not found in the vocabulary are treated as unknowns.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): The text corpus to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list of str: A list of tokens extracted from the corpus, adjusted to fit the vocabulary.\n",
    "        \"\"\"\n",
    "        # Initial tokenization based on word boundaries and punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', corpus)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, actual_vocab_size):\n",
    "        self.vocab = vocab\n",
    "        self.actual_vocab_size = actual_vocab_size\n",
    "        self.unk_token_id = self.vocab.get(\"[UNK]\", 1)  # Get ID of [UNK] \n",
    "        self.max_subword_length = max(len(token) for token in vocab.keys())\n",
    "        self.pattern = re.compile(r'\\b\\w+\\b|[\\s\\.,!?;]')\n",
    "\n",
    "        # Build the Trie\n",
    "        self.trie = Trie()\n",
    "        for token, token_id in vocab.items():\n",
    "            self.trie.insert(token, token_id)\n",
    "\n",
    "    def _find_subwords(self, word):\n",
    "        # 1. Trie Lookup \n",
    "        subword_ids = self.trie.find_subwords(word)\n",
    "\n",
    "        # 2. Fallback to Original Logic \n",
    "        if len(subword_ids) == 1 and subword_ids[0] == self.unk_token_id:  \n",
    "            subwords = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                for j in range(self.max_subword_length, 0, -1):\n",
    "                    subword = word[i:i+j]\n",
    "                    if subword in self.vocab:\n",
    "                        subwords.append(self.vocab[subword])\n",
    "                        i += j\n",
    "                        break \n",
    "                else: \n",
    "                    subwords.append(self.vocab[\"[UNK]\"])\n",
    "                    i += 1\n",
    "            subword_ids = subwords  # Replace with token IDs\n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        token_ids = [self.vocab.get(\"[CLS]\", 1)]  # Use [CLS] token or [UNK] if not found\n",
    "        tokens = self.pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            token_id = self.vocab.get(token, self.vocab.get(\"[UNK]\", 1))  # Fallback to [UNK] if token is not found\n",
    "            token_ids.append(token_id)\n",
    "        token_ids.append(self.vocab.get(\"[SEP]\", 1))  # Use [SEP] token or [UNK] if not found\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "# Training Code\n",
    "\n",
    "# Load corpus and build vocab\n",
    "corpus = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "tokens = tokenize(corpus)\n",
    "vocab = build_vocab(tokens)\n",
    "actual_vocab_size = len(vocab)  # This includes [PAD], [UNK], [CLS], [SEP], [MASK]\n",
    "\n",
    "print(f\"Actual vocabulary size (including special tokens): {actual_vocab_size}\")\n",
    "\n",
    "tokenizer = Tokenizer(vocab=vocab, actual_vocab_size=actual_vocab_size)\n",
    "\n",
    "# Data Loading (Illustrative)\n",
    "train_texts = [corpus]  # Treat your whole sample as one \"document\"\n",
    "train_dataset = TextDataset(train_texts, tokenizer, max_seq_len=512)\n",
    "\n",
    "print(\"Tokenizer unk_token_id:\", tokenizer.unk_token_id) \n",
    "print(\"Tokenizer Vocabulary:\", tokenizer.vocab)\n",
    "\n",
    "\n",
    "\n",
    "device='cpu'\n",
    "freq_threshold_values = [10, 50, 100, 200, 500]  \n",
    "best_validation_accuracy = 0.0 \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for freq_threshold in freq_threshold_values:\n",
    "\n",
    "    # Model instantiation and training setup\n",
    "    model = TransformerModel(\n",
    "        vocab = vocab,\n",
    "        vocab_size=actual_vocab_size,     \n",
    "        embedding_dim=128,\n",
    "        max_seq_len=512,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        freq_threshold=freq_threshold,  # frequency threshold for splitting vocab\n",
    "        smaller_embed_dim=64\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4) \n",
    "    meta_optimizer = AdaptiveWeightDecayOptimizer(model.parameters(), lr=1e-5) \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    meta_update_freq = 5\n",
    "\n",
    "    # Training loop adjusted for the updated model architecture\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)  # Assuming labels are of shape [batch_size, sequence_length]\n",
    "\n",
    "            # Forward pass, model now returns logits and pooled_output\n",
    "            logits, pooled_output = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Correctly reshape logits to match the labels' shape\n",
    "            # Change from [1, 512, vocab_size] to [512, vocab_size] to align with labels\n",
    "            logits = logits.view(-1, logits.size(-1))  # Reshape logits for loss calculation\n",
    "            \n",
    "            labels = labels.view(-1)  # Ensure labels are a flat vector\n",
    "\n",
    "            # Calculate loss using logits for token-level predictions\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Meta-update occasionally\n",
    "            if (i + 1) % meta_update_freq == 0:\n",
    "                meta_optimizer.zero_grad()\n",
    "                # Recalculate or reuse the loss for the meta-update\n",
    "                meta_loss = combined_loss(logits.detach(), labels.detach(), model)\n",
    "                meta_loss.backward()\n",
    "                meta_optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
