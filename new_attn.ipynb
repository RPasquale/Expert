{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (128) must match the existing size (8) at non-singleton dimension 2.  Target sizes: [-1, -1, 128, -1].  Tensor sizes: [2, 32, 8, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 194\u001b[0m\n\u001b[0;32m    190\u001b[0m model \u001b[38;5;241m=\u001b[39m PartitionedLinformerAttentionACT(embed_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, projection_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, partition_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m    192\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m--> 194\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output shape: [2, sequence_length, embed_size]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 160\u001b[0m, in \u001b[0;36mPartitionedLinformerAttentionACT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    155\u001b[0m ponder_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mponder(queries_reshaped)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Ensure ponder_scores are compatible with attention's dimensions\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# This involves adjusting ponder_scores' shape to [N, heads, partition_size, projection_dim // 2]\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Note: This step assumes partition_size is a valid segmentation of the sequence for attention calculations\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m ponder_scores_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mponder_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m#ponder_scores_expanded = ponder_scores.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, attention.size(-1))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m energy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnshd,nshp->nhdp\u001b[39m\u001b[38;5;124m'\u001b[39m, R_queries, C_keys\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (128) must match the existing size (8) at non-singleton dimension 2.  Target sizes: [-1, -1, 128, -1].  Tensor sizes: [2, 32, 8, 1]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def random_projection(matrix, k):\n",
    "\n",
    "    \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "\n",
    "    random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "\n",
    "    return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "\n",
    "\n",
    "def cur_decomposition(matrix, k):\n",
    "    # Ensure k does not exceed the matrix dimensions for both rows and columns\n",
    "    k = min(k, matrix.size(-1), matrix.size(-2))\n",
    "\n",
    "    # Generate indices within the valid range\n",
    "    cols_indices = torch.randperm(matrix.size(-1), device=matrix.device)[:k]\n",
    "    rows_indices = torch.randperm(matrix.size(-2), device=matrix.device)[:k]\n",
    "\n",
    "    # Use torch.index_select for safer indexing\n",
    "    C = torch.index_select(matrix, dim=-1, index=cols_indices)\n",
    "    R = torch.index_select(matrix, dim=-2, index=rows_indices)\n",
    "    # Intersection for U needs careful handling to ensure dimensions match\n",
    "    U = torch.pinverse(C[:, rows_indices, :])\n",
    "\n",
    "    return C, U, R\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PartitionedLinformerAttentionACT(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, heads, sequence_length, projection_dim, partition_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.heads = heads\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "        self.partition_size = partition_size\n",
    "\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "\n",
    "\n",
    "        self.values = nn.Linear(embed_size, heads * self.head_dim, bias=False)\n",
    "        \n",
    "        self.keys = nn.Linear(embed_size, self.projection_dim, bias=False)  \n",
    "        \n",
    "        self.queries = nn.Linear(embed_size, self.projection_dim, bias=False)  \n",
    "\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "\n",
    "        # ACT components\n",
    "\n",
    "        self.ponder = nn.Linear(self.head_dim, 1, bias=True)  # Ponder network to decide halting\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        N, seq_length, _ = x.shape\n",
    "\n",
    "        values = self.values(x)\n",
    "        keys = self.keys(x)\n",
    "        queries = self.queries(x)\n",
    "\n",
    "        keys_proj = random_projection(keys, self.projection_dim // 2)\n",
    "        queries_proj = random_projection(queries, self.projection_dim // 2)\n",
    "\n",
    "\n",
    "        values = values.view(N, seq_length, self.heads, self.head_dim)\n",
    "        keys_proj = keys_proj.view(N, seq_length, self.heads, -1)\n",
    "        queries_proj = queries_proj.view(N, seq_length, self.heads, -1)\n",
    "\n",
    "        attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "\n",
    "        for i in range(0, seq_length, self.partition_size):\n",
    "            partition_end = min((i+1) * self.partition_size, seq_length)\n",
    "            keys_partition = keys_proj[:, i:partition_end, :, :]\n",
    "            queries_partition = queries_proj[:, i:partition_end, :, :]\n",
    "\n",
    "            C_keys, U, R_queries = cur_decomposition(keys_partition, k=self.projection_dim // 2)\n",
    "\n",
    "            queries_flattened = queries_partition.view(N, -1, self.head_dim)\n",
    "            ponder_scores = self.sigmoid(self.ponder(queries_flattened).squeeze(-1))\n",
    "\n",
    "            energy = torch.einsum('nshd,nshp->nhdp', R_queries, C_keys.transpose(-2, -1))\n",
    "            attention = F.softmax(energy / (self.embed_size ** (1 / 2)), dim=-1)\n",
    "\n",
    "            # Now that `attention` is computed, we can correctly shape `ponder_scores_expanded`\n",
    "            ponder_scores_expanded = ponder_scores.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, attention.size(-1))\n",
    "            print(f\"attention shape: {attention.shape}\")\n",
    "            print(f\"ponder_scores shape: {ponder_scores.shape}\")\n",
    "            print(f\"ponder_scores_expanded shape: {ponder_scores_expanded.shape}\")\n",
    "            print(f\"attention_scores shape: {attention_scores.shape}\")\n",
    "            # Apply ponder_scores correctly\n",
    "            attention_scores[:, :, i:partition_end, :] = attention * ponder_scores_expanded\n",
    "\n",
    "        # Reshape and apply final linear transformation to compute output\n",
    "        out = torch.einsum('nhdp,nshd->nshp', attention_scores, values).reshape(N, seq_length, -1)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out'''\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, _ = x.shape\n",
    "\n",
    "        values = self.values(x)\n",
    "        keys = self.keys(x)\n",
    "        queries = self.queries(x)\n",
    "\n",
    "        keys_proj = random_projection(keys, self.projection_dim // 2)\n",
    "        queries_proj = random_projection(queries, self.projection_dim // 2)\n",
    "\n",
    "        values = values.view(N, seq_length, self.heads, self.head_dim)\n",
    "        keys_proj = keys_proj.view(N, seq_length, self.heads, -1)\n",
    "        queries_proj = queries_proj.view(N, seq_length, self.heads, -1)\n",
    "\n",
    "        attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "\n",
    "        for i in range(0, seq_length, self.partition_size):\n",
    "            partition_end = min((i+1) * self.partition_size, seq_length)\n",
    "            keys_partition = keys_proj[:, i:partition_end, :, :]\n",
    "            queries_partition = queries_proj[:, i:partition_end, :, :]\n",
    "\n",
    "            C_keys, U, R_queries = cur_decomposition(keys_partition, k=self.projection_dim // 2)\n",
    "\n",
    "            # Adjusted calculation of ponder_scores\n",
    "            # Calculate ponder_scores per head instead of per flattened queries\n",
    "            # This involves reshaping queries_partition to align with heads and sequence segments\n",
    "            # Note: This step may require further adjustment based on the specific model behavior you're aiming for\n",
    "            queries_reshaped = queries_partition.view(N, -1, self.heads, self.head_dim)\n",
    "            ponder_scores = self.sigmoid(self.ponder(queries_reshaped).squeeze(-1))\n",
    "\n",
    "            # Ensure ponder_scores are compatible with attention's dimensions\n",
    "            # This involves adjusting ponder_scores' shape to [N, heads, partition_size, projection_dim // 2]\n",
    "            # Note: This step assumes partition_size is a valid segmentation of the sequence for attention calculations\n",
    "            ponder_scores_expanded = ponder_scores.unsqueeze(-1).expand(-1, -1, partition_end - i, -1)\n",
    "\n",
    "            #ponder_scores_expanded = ponder_scores.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, attention.size(-1))\n",
    "\n",
    "            energy = torch.einsum('nshd,nshp->nhdp', R_queries, C_keys.transpose(-2, -1))\n",
    "            attention = F.softmax(energy / (self.embed_size ** (1 / 2)), dim=-1)\n",
    "            print(f\"attention shape: {attention.shape}\")\n",
    "            print(f\"ponder_scores shape: {ponder_scores.shape}\")\n",
    "            print(f\"ponder_scores_expanded shape: {ponder_scores_expanded.shape}\")\n",
    "            print(f\"attention_scores shape: {attention_scores.shape}\")\n",
    "            # Correctly apply ponder_scores to attention\n",
    "            attention_scores[:, :, i:partition_end, :] = attention * ponder_scores_expanded\n",
    "\n",
    "        # Reshape and apply final linear transformation to compute output\n",
    "        out = torch.einsum('nhdp,nshd->nshp', attention_scores, values).reshape(N, seq_length, -1)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "model = PartitionedLinformerAttentionACT(embed_size=512, heads=8, sequence_length=1024, projection_dim=256, partition_size=128)\n",
    "\n",
    "input_tensor = torch.rand(2, 1024, 512)\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(output.shape)  # Expected output shape: [2, sequence_length, embed_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2048x16 and 64x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 140\u001b[0m\n\u001b[0;32m    136\u001b[0m model \u001b[38;5;241m=\u001b[39m PartitionedLinformerAttentionACT(embed_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, projection_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, partition_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m    138\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m--> 140\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output shape: [2, sequence_length, embed_size]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 106\u001b[0m, in \u001b[0;36mPartitionedLinformerAttentionACT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    103\u001b[0m C_keys, U, R_queries \u001b[38;5;241m=\u001b[39m cur_decomposition(keys_partition, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    105\u001b[0m queries_partition_flattened \u001b[38;5;241m=\u001b[39m queries_partition\u001b[38;5;241m.\u001b[39mreshape(N \u001b[38;5;241m*\u001b[39m partition_end \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 106\u001b[0m ponder_scores_raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mponder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries_partition_flattened\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m ponder_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(ponder_scores_raw)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Reshape ponder_scores back to match the original partitioning and heads\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Then, calculate the mean across the dimension representing head_dim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2048x16 and 64x1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def random_projection(matrix, k):\n",
    "\n",
    "    \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "\n",
    "    random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "\n",
    "    return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "\n",
    "\n",
    "def cur_decomposition(matrix, k):\n",
    "    # Ensure k does not exceed the matrix dimensions for both rows and columns\n",
    "    k = min(k, matrix.size(-1), matrix.size(-2))\n",
    "\n",
    "    # Generate indices within the valid range\n",
    "    cols_indices = torch.randperm(matrix.size(-1), device=matrix.device)[:k]\n",
    "    rows_indices = torch.randperm(matrix.size(-2), device=matrix.device)[:k]\n",
    "\n",
    "    # Use torch.index_select for safer indexing\n",
    "    C = torch.index_select(matrix, dim=-1, index=cols_indices)\n",
    "    R = torch.index_select(matrix, dim=-2, index=rows_indices)\n",
    "    # Intersection for U needs careful handling to ensure dimensions match\n",
    "    U = torch.pinverse(C[:, rows_indices, :])\n",
    "\n",
    "    return C, U, R\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PartitionedLinformerAttentionACT(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, heads, sequence_length, projection_dim, partition_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.heads = heads\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "        self.partition_size = partition_size\n",
    "\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "\n",
    "\n",
    "        self.values = nn.Linear(embed_size, heads * self.head_dim, bias=False)\n",
    "        \n",
    "        self.keys = nn.Linear(embed_size, self.projection_dim, bias=False)  \n",
    "        \n",
    "        self.queries = nn.Linear(embed_size, self.projection_dim, bias=False)  \n",
    "\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "\n",
    "        # ACT components\n",
    "\n",
    "        self.ponder = nn.Linear(self.head_dim, 1, bias=True)  # Ponder network to decide halting\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, _ = x.shape\n",
    "\n",
    "        values = self.values(x)\n",
    "        keys = self.keys(x)\n",
    "        queries = self.queries(x)\n",
    "\n",
    "        keys_proj = random_projection(keys, self.projection_dim // 2)\n",
    "        queries_proj = random_projection(queries, self.projection_dim // 2)\n",
    "\n",
    "\n",
    "        values = values.view(N, seq_length, self.heads, self.head_dim)\n",
    "        keys_proj = keys_proj.view(N, seq_length, self.heads, -1)\n",
    "        queries_proj = queries_proj.view(N, seq_length, self.heads, -1)\n",
    "\n",
    "        attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "\n",
    "        for i in range(0, seq_length, self.partition_size):\n",
    "            partition_end = min((i+1) * self.partition_size, seq_length)\n",
    "\n",
    "            keys_partition = keys_proj[:, i:partition_end, :, :]\n",
    "            queries_partition = queries_proj[:, i:partition_end, :, :]\n",
    "\n",
    "            C_keys, U, R_queries = cur_decomposition(keys_partition, k=self.projection_dim // 2)\n",
    "\n",
    "            queries_partition_flattened = queries_partition.reshape(N * partition_end * self.heads, -1)\n",
    "            ponder_scores_raw = self.ponder(queries_partition_flattened)\n",
    "            ponder_scores = self.sigmoid(ponder_scores_raw)\n",
    "\n",
    "            # Reshape ponder_scores back to match the original partitioning and heads\n",
    "            # Then, calculate the mean across the dimension representing head_dim\n",
    "            ponder_scores_reshaped = ponder_scores.view(N, partition_end, self.heads, -1).mean(dim=-1)\n",
    "\n",
    "            # Correctly reshape ponder_scores for broadcasting with attention\n",
    "            # Assuming attention mechanism is applied per head and per partitioned segment of the sequence\n",
    "            ponder_scores_expanded = ponder_scores_reshaped.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "            energy = torch.einsum('nshd,nshp->nhdp', R_queries, C_keys.transpose(-2, -1))\n",
    "            attention = F.softmax(energy / (self.embed_size ** (1 / 2)), dim=-1)\n",
    "            print(f\"attention shape: {attention.shape}\")\n",
    "            print(f\"ponder_scores shape: {ponder_scores.shape}\")\n",
    "            print(f\"ponder_scores_expanded shape: {ponder_scores_expanded.shape}\")\n",
    "            print(f\"attention_scores shape: {attention_scores.shape}\")\n",
    "            # Apply ponder_scores correctly\n",
    "            attention_scores[:, :, i:partition_end, :] = attention * ponder_scores_expanded\n",
    "\n",
    "\n",
    "        # Reshape and apply final linear transformation to compute output\n",
    "        out = torch.einsum('nhdp,nshd->nshp', attention_scores, values).reshape(N, seq_length, -1)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "\n",
    "model = PartitionedLinformerAttentionACT(embed_size=512, heads=8, sequence_length=1024, projection_dim=256, partition_size=128)\n",
    "\n",
    "input_tensor = torch.rand(2, 1024, 512)\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(output.shape)  # Expected output shape: [2, sequence_length, embed_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def random_projection(matrix, k):\n",
    "\n",
    "    \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "\n",
    "    random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "\n",
    "    return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "\n",
    "\n",
    "def cur_decomposition(matrix, k):\n",
    "\n",
    "    \"\"\"Performs approximate CUR decomposition on a matrix to identify k columns and rows.\"\"\"\n",
    "\n",
    "    # For simplicity, using a randomized approach to select columns and rows\n",
    "\n",
    "    cols_indices = np.random.choice(matrix.shape[-1], k, replace=False)\n",
    "\n",
    "    rows_indices = np.random.choice(matrix.shape[-2], k, replace=False)\n",
    "\n",
    "    \n",
    "\n",
    "    C = matrix[:, :, cols_indices]\n",
    "\n",
    "    R = matrix[:, rows_indices, :]\n",
    "\n",
    "    U = torch.pinverse(matrix[:, rows_indices][:, :, cols_indices])\n",
    "\n",
    "    \n",
    "\n",
    "    return C, U, R\n",
    "\n",
    "\n",
    "\n",
    "class PartitionedLinformerAttentionACT(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, heads, sequence_length, projection_dim, partition_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.heads = heads\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "        self.partition_size = partition_size\n",
    "\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "        self.keys = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "\n",
    "        self.queries = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "\n",
    "\n",
    "        # For ACT\n",
    "\n",
    "        self.ponder = nn.Linear(self.head_dim, 1)  # Ponder network to decide halting\n",
    "\n",
    "        \n",
    "\n",
    "        # Special initialization for head specialization\n",
    "\n",
    "        for head in range(self.heads):\n",
    "\n",
    "            nn.init.normal_(self.keys.weight[head * self.head_dim:(head + 1) * self.head_dim], mean=0, std=0.02 * (head + 1))\n",
    "\n",
    "            nn.init.normal_(self.queries.weight[head * self.head_dim:(head + 1) * self.head_dim], mean=0, std=0.02 * (head + 1))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        N, seq_length, _ = x.shape\n",
    "\n",
    "        values = self.values(x).view(N, seq_length, self.heads, self.head_dim)\n",
    "\n",
    "        keys = self.keys(x).view(N, seq_length, self.heads, self.projection_dim)\n",
    "\n",
    "        queries = self.queries(x).view(N, seq_length, self.heads, self.projection_dim)\n",
    "\n",
    "\n",
    "\n",
    "        keys = random_projection(keys, self.projection_dim // 2)\n",
    "\n",
    "        queries = random_projection(queries, self.projection_dim // 2)\n",
    "\n",
    "\n",
    "\n",
    "        attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(0, seq_length, self.partition_size):\n",
    "\n",
    "            partition_start = i * self.partition_size\n",
    "\n",
    "            partition_end = min(i + self.partition_size, seq_length)\n",
    "\n",
    "            keys_partition = keys[:, partition_start:partition_end, :, :]\n",
    "\n",
    "            queries_partition = queries[:, partition_start:partition_end, :, :]\n",
    "\n",
    "\n",
    "\n",
    "            # Apply CUR decomposition within each partition for keys and queries\n",
    "\n",
    "            C_keys, U, R_queries = cur_decomposition(keys_partition, k=keys_partition.size(-1) // 2)\n",
    "\n",
    "\n",
    "\n",
    "            # Compute attention scores using CUR components\n",
    "\n",
    "            energy = torch.einsum('nshd,nshp->nhdp', R_queries, C_keys.transpose(-2, -1))\n",
    "\n",
    "            attention = F.softmax(energy / (self.embed_size ** (1 / 2)), dim=-1)\n",
    "\n",
    "            attention_scores[:, :, partition_start:partition_end, :] = attention\n",
    "\n",
    "\n",
    "\n",
    "        out = torch.einsum('nhdp,nshd->nshp', attention_scores, values)\n",
    "\n",
    "        out = out.reshape(N, seq_length, self.heads * self.head_dim)\n",
    "\n",
    "        return self.fc_out(out)\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "model = PartitionedLinformerAttentionACT(embed_size=512, heads=8, sequence_length=1024, projection_dim=256, partition_size=128)\n",
    "\n",
    "input_tensor = torch.rand(2, sequence_length, embed_size)  # Batch size of 2\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape as input to forward: torch.Size([2, 1024, 512])\n",
      "embed size as input to forward: 512\n",
      "values before reshaping: Linear(in_features=64, out_features=64, bias=False)\n",
      "keys before reshaping: Linear(in_features=64, out_features=256, bias=False)\n",
      "queries before reshaping: Linear(in_features=64, out_features=256, bias=False)\n",
      "x_reshaped : torch.Size([2, 1024, 8, 64])\n",
      "values after reshaping: torch.Size([2, 1024, 8, 64])\n",
      "keys after reshaping: torch.Size([2, 1024, 8, 256])\n",
      "queries after reshaping: torch.Size([2, 1024, 8, 256])\n",
      "keys after random_projection: torch.Size([2, 1024, 8, 128])\n",
      "queries after random_projection: torch.Size([2, 1024, 8, 128])\n",
      "attention_scores : torch.Size([2, 8, 1024, 128])\n",
      "START OF LOOP\n",
      "partition_start : 0, partition_end : 128 keys_partition = torch.Size([2, 128, 8, 128]), queries_partition = torch.Size([2, 128, 8, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2048x16 and 64x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 125\u001b[0m\n\u001b[0;32m    121\u001b[0m model \u001b[38;5;241m=\u001b[39m PartitionedLinformerAttentionACT(embed_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, projection_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, partition_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m    123\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, model\u001b[38;5;241m.\u001b[39msequence_length, model\u001b[38;5;241m.\u001b[39membed_size)\n\u001b[1;32m--> 125\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output shape: [2, sequence_length, embed_size]\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMyDataset\u001b[39;00m(Dataset):\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[56], line 102\u001b[0m, in \u001b[0;36mPartitionedLinformerAttentionACT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads):\n\u001b[0;32m    101\u001b[0m     head_queries \u001b[38;5;241m=\u001b[39m queries_reshaped[:, h, :]  \u001b[38;5;66;03m# Select queries for head h\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mponder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_queries\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Apply ponder and squeeze\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     ponder_scores\u001b[38;5;241m.\u001b[39mappend(scores)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Combine ponder scores from all heads\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2048x16 and 64x1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "\n",
    "def random_projection(matrix, k):\n",
    "    \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "    random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "    return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "def cur_decomposition(matrix, k):\n",
    "    \"\"\"Applies CUR decomposition on each head for each batch.\"\"\"\n",
    "    batch_size, seq_length, heads, dim = matrix.shape\n",
    "    # Ensuring k does not exceed dimensions\n",
    "    k = min(k, dim)\n",
    "    C = torch.zeros(batch_size, seq_length, heads, k, device=matrix.device)\n",
    "    R = torch.zeros(batch_size, k, heads, dim, device=matrix.device)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for h in range(heads):\n",
    "            col_indices = np.random.choice(dim, k, replace=False)\n",
    "            row_indices = np.random.choice(seq_length, k, replace=False)\n",
    "            C[b, :, h] = matrix[b, :, h, col_indices]\n",
    "            R[b, :, h] = matrix[b, row_indices, h]\n",
    "    return C, R\n",
    "\n",
    "class PartitionedLinformerAttentionACT(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, heads, sequence_length, projection_dim, partition_size):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.sequence_length = sequence_length\n",
    "        self.projection_dim = projection_dim\n",
    "        self.partition_size = partition_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        # ACT components\n",
    "        #self.ponder = nn.Linear(self.partition_size, 1, bias=True)  # Ponder network to decide halting\n",
    "        self.ponder = nn.Linear(self.head_dim, 1, bias=True)  # Ponder network to decide halting\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        N, seq_length, _ = x.shape\n",
    "        print(f\"x shape as input to forward: {x.shape}\")\n",
    "        print(f\"embed size as input to forward: {self.embed_size}\")\n",
    "        print(f\"values before reshaping: {self.values}\")\n",
    "        print(f\"keys before reshaping: {self.keys}\")\n",
    "        print(f\"queries before reshaping: {self.queries}\")\n",
    "        x_reshaped = x.view(N, seq_length, self.heads, self.head_dim)\n",
    "        print(f\"x_reshaped : {x_reshaped.shape}\")\n",
    "        values = self.values(x_reshaped).view(N, seq_length, self.heads, self.head_dim)\n",
    "        keys = self.keys(x_reshaped).view(N, seq_length, self.heads, self.projection_dim)\n",
    "        queries = self.queries(x_reshaped).view(N, seq_length, self.heads, self.projection_dim)\n",
    "        print(f\"values after reshaping: {values.shape}\")\n",
    "        print(f\"keys after reshaping: {keys.shape}\")\n",
    "        print(f\"queries after reshaping: {queries.shape}\")\n",
    "        # Apply random projection\n",
    "        keys = random_projection(keys, self.projection_dim // 2)\n",
    "        queries = random_projection(queries, self.projection_dim // 2)\n",
    "        print(f\"keys after random_projection: {keys.shape}\")\n",
    "        print(f\"queries after random_projection: {queries.shape}\")\n",
    "        attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "        print(f\"attention_scores : {attention_scores.shape}\")\n",
    "\n",
    "        for i in range(0, seq_length, self.partition_size):\n",
    "            print(f\"START OF LOOP\")\n",
    "            partition_start = i * self.partition_size\n",
    "            partition_end = min(i + self.partition_size, seq_length)\n",
    "            keys_partition = keys[:, partition_start:partition_end, :, :]\n",
    "            queries_partition = queries[:, partition_start:partition_end, :, :]\n",
    "            print(f\"partition_start : {partition_start}, partition_end : {partition_end} keys_partition = {keys_partition.shape}, queries_partition = {queries_partition.shape}\")\n",
    "            # Apply CUR decomposition\n",
    "            C_keys,  R_queries = cur_decomposition(keys_partition, k=keys_partition.size(-1) // 2)\n",
    "            # Apply ponder to each head's queries. Note: you might need to adjust this part based on your specific needs\n",
    "            queries_reshaped = queries_partition.reshape(N * seq_length, self.heads, -1)\n",
    "            ponder_scores = []\n",
    "            for h in range(self.heads):\n",
    "                head_queries = queries_reshaped[:, h, :]  # Select queries for head h\n",
    "                scores = self.sigmoid(self.ponder(head_queries)).squeeze(-1)  # Apply ponder and squeeze\n",
    "                ponder_scores.append(scores)\n",
    "            # Combine ponder scores from all heads\n",
    "            ponder_scores = torch.stack(ponder_scores, dim=1)\n",
    "            # Reshape ponder_scores back to match the original structure\n",
    "            ponder_scores = ponder_scores.view(N, seq_length, self.heads, -1)\n",
    "            # Use ponder scores to weigh the importance of each partition's computation\n",
    "            energy = torch.einsum('nshd,nshp->nhdp', R_queries, C_keys.transpose(-2, -1))\n",
    "            attention = F.softmax(energy / (self.embed_size ** (1 / 2)), dim=-1) * ponder_scores.unsqueeze(-1)\n",
    "            attention_scores[:, :, partition_start:partition_end, :] = attention\n",
    "        out = torch.einsum('nhdp,nshd->nshp', attention_scores, values)\n",
    "        out = out.reshape(N, seq_length, self.heads * self.head_dim)\n",
    "\n",
    "        return self.fc_out(out)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "model = PartitionedLinformerAttentionACT(embed_size=512, heads=8, sequence_length=1024, projection_dim=256, partition_size=128)\n",
    "\n",
    "input_tensor = torch.rand(2, model.sequence_length, model.embed_size)\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(output.shape)  # Expected output shape: [2, sequence_length, embed_size]\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Initialize your data here\n",
    "\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # Return the size of your dataset\n",
    "\n",
    "        return 100  # Example size\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Implement logic to get a single item at a given index\n",
    "\n",
    "        # For simplicity, let's return random tensors\n",
    "\n",
    "        return torch.randn(10, 64), torch.randint(0, 2, (10,))  # Example data\n",
    "\n",
    "\n",
    "# Create a DataLoader with pinned memory\n",
    "\n",
    "dataset = MyDataset()\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(10):  # Example: 10 epochs\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "\n",
    "        with autocast():\n",
    "\n",
    "            predictions = model(inputs)\n",
    "\n",
    "            loss = nn.functional.cross_entropy(predictions, targets)\n",
    "\n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript d has size 64 for operand 1 which does not broadcast with previously seen size 128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m model \u001b[38;5;241m=\u001b[39m CustomLinformer(embed_size, heads, sequence_length, projection_dim, partition_size)\n\u001b[0;32m     80\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, sequence_length, embed_size)\n\u001b[1;32m---> 81\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output shape: [2, sequence_length, embed_size]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[47], line 60\u001b[0m, in \u001b[0;36mCustomLinformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     57\u001b[0m C, R \u001b[38;5;241m=\u001b[39m apply_cur_decomposition(keys_reduced, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Skipping the computation of U for simplification and directly computing the attention scores\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbnhd,bnhd->bnhd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Normalize attention scores\u001b[39;00m\n\u001b[0;32m     64\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): subscript d has size 64 for operand 1 which does not broadcast with previously seen size 128"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def random_projection(matrix, k):\n",
    "    \"\"\"Applies a random projection to reduce dimensionality of the last dimension of the matrix.\"\"\"\n",
    "    _, _, _, dim = matrix.shape\n",
    "    random_matrix = torch.randn(dim, k, device=matrix.device)\n",
    "    return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "def apply_cur_decomposition(matrix, k):\n",
    "    \"\"\"Applies CUR decomposition on each head for each batch.\"\"\"\n",
    "    batch_size, seq_length, heads, dim = matrix.shape\n",
    "    # Ensuring k does not exceed dimensions\n",
    "    k = min(k, dim)\n",
    "    C = torch.zeros(batch_size, seq_length, heads, k, device=matrix.device)\n",
    "    R = torch.zeros(batch_size, k, heads, dim, device=matrix.device)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for h in range(heads):\n",
    "            col_indices = np.random.choice(dim, k, replace=False)\n",
    "            row_indices = np.random.choice(seq_length, k, replace=False)\n",
    "            C[b, :, h] = matrix[b, :, h, col_indices]\n",
    "            R[b, :, h] = matrix[b, row_indices, h]\n",
    "    return C, R\n",
    "\n",
    "class CustomLinformer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, sequence_length, projection_dim, partition_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.sequence_length = sequence_length\n",
    "        self.projection_dim = projection_dim\n",
    "        self.partition_size = partition_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, _ = x.shape\n",
    "        x = x.view(N, seq_length, self.heads, self.head_dim)\n",
    "        \n",
    "        # Applying linear transformations\n",
    "        values = self.values(x)\n",
    "        keys = self.keys(x)\n",
    "        queries = self.queries(x)\n",
    "        \n",
    "        # Reducing dimensionality\n",
    "        keys_reduced = random_projection(keys, self.projection_dim // 2)\n",
    "        queries_reduced = random_projection(queries, self.projection_dim // 2)\n",
    "        \n",
    "        # Applying CUR decomposition\n",
    "        C, R = apply_cur_decomposition(keys_reduced, k=self.projection_dim // 4)\n",
    "        \n",
    "        # Skipping the computation of U for simplification and directly computing the attention scores\n",
    "        attention_scores = torch.einsum('bnhd,bnhd->bnhd', queries_reduced, C)\n",
    "\n",
    "        \n",
    "        # Normalize attention scores\n",
    "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Aggregate values\n",
    "        out = torch.einsum('bnhdm,bndh->bnhm', attention_scores, values)\n",
    "        out = out.reshape(N, seq_length, -1)\n",
    "        \n",
    "        return self.fc_out(out)\n",
    "\n",
    "# Example usage\n",
    "embed_size = 512\n",
    "heads = 8\n",
    "sequence_length = 1024\n",
    "projection_dim = 256\n",
    "partition_size = 128\n",
    "\n",
    "model = CustomLinformer(embed_size, heads, sequence_length, projection_dim, partition_size)\n",
    "input_tensor = torch.rand(2, sequence_length, embed_size)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Expected output shape: [2, sequence_length, embed_size]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
