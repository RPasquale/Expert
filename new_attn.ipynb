{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparseFlash2LinformerAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN FORWARD\n",
      "x input shape: torch.Size([2, 1024, 512])\n",
      "x_reshaped shape: torch.Size([2, 1024, 8, 64])\n",
      "values : torch.Size([2, 1024, 8, 64])\n",
      "queries : torch.Size([2, 1024, 8, 128])\n",
      "keys : torch.Size([2, 1024, 8, 128])\n",
      "attention_scores after random projection: torch.Size([2, 8, 1024, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 0, Partition End 128 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([2, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([2, 128, 8, 128])\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([2, 128, 8, 128])\n",
      "attention shape: torch.Size([2, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 128, Partition End 256 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([2, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([2, 128, 8, 128])\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([2, 128, 8, 128])\n",
      "attention shape: torch.Size([2, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 256, Partition End 384 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([2, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([2, 128, 8, 128])\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([2, 128, 8, 128])\n",
      "attention shape: torch.Size([2, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 384, Partition End 512 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([2, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([2, 128, 8, 128])\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([2, 128, 8, 128])\n",
      "attention shape: torch.Size([2, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 512, Partition End 640 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([2, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([2, 128, 8, 128])\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([2, 128, 8, 128])\n",
      "attention shape: torch.Size([2, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 640, Partition End 768 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([2, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([2, 128, 8, 128])\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([2, 128, 8, 128])\n",
      "attention shape: torch.Size([2, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 768, Partition End 896 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([2, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([2, 128, 8, 128])\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([2, 128, 8, 128])\n",
      "attention shape: torch.Size([2, 128, 8, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 896, Partition End 1024 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "BEFORE 1ST EINSUM:\n",
      "ponder_scores_permuted shape: torch.Size([2, 128, 8, 1])\n",
      "ponder_scores_broadcastable shape: torch.Size([2, 128, 8, 128])\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER 1ST EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "attention_weights shape: torch.Size([2, 128, 8, 128])\n",
      "attention shape: torch.Size([2, 128, 8, 128])\n",
      "values shape: torch.Size([2, 8, 1024, 64])\n",
      "values.reshape(-1, self.head_dim) shape: torch.Size([16384, 64])\n",
      "self.value_projection(values) shape: torch.Size([16384, 128])\n",
      "projected_values shape: torch.Size([2, 8, 1024, 128])\n",
      "2ND EINSUM\n",
      "out shape: torch.Size([2, 8, 1024])\n",
      "torch.Size([2, 8, 1024])\n",
      "BEGIN FORWARD\n",
      "x input shape: torch.Size([32, 10, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 10, 8, 64]' is invalid for input of size 20480",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 153\u001b[0m\n\u001b[0;32m    151\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m--> 153\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(predictions, targets)\n\u001b[0;32m    155\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[139], line 52\u001b[0m, in \u001b[0;36mPartitionedLinformerAttentionACT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m N, seq_length, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 52\u001b[0m x_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_reshaped shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_reshaped\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues(x_reshaped)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[32, 10, 8, 64]' is invalid for input of size 20480"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "\n",
    "def random_projection(matrix, k):\n",
    "    \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "    random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "    return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "def cur_decomposition(matrix, projection_dim):  # Change 'k' argument\n",
    "    \"\"\"Applies CUR decomposition with C matrix dimension aligned to projection dimension.\"\"\"\n",
    "    batch_size, seq_length, heads, dim = matrix.shape\n",
    "    k = min(projection_dim // 2, dim)  # Use projection dimension to determine 'k'\n",
    "    C = torch.zeros(batch_size, seq_length, heads, k, device=matrix.device)\n",
    "    R = torch.zeros(batch_size, k, heads, dim, device=matrix.device)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for h in range(heads):\n",
    "            col_indices = np.random.choice(dim, k, replace=False)\n",
    "            row_indices = np.random.choice(seq_length, k, replace=False)\n",
    "            C[b, :, h] = matrix[b, :, h, col_indices]\n",
    "            R[b, :, h] = matrix[b, row_indices, h]\n",
    "    return C, R\n",
    "\n",
    "class PartitionedLinformerAttentionACT(nn.Module):\n",
    "    def __init__(self, embed_size, heads, sequence_length, projection_dim, partition_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.sequence_length = sequence_length\n",
    "        self.projection_dim = projection_dim\n",
    "        self.partition_size = partition_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.value_projection = nn.Linear(self.head_dim, self.projection_dim//2)  # To project values to match dimensions\n",
    "        self.ponder = nn.Linear(self.partition_size, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"BEGIN FORWARD\")\n",
    "        print(f\"x input shape: {x.shape}\")\n",
    "        N, seq_length, _ = x.shape\n",
    "        x_reshaped = x.view(N, seq_length, self.heads, self.head_dim)\n",
    "        print(f\"x_reshaped shape: {x_reshaped.shape}\")\n",
    "\n",
    "        values = self.values(x_reshaped)\n",
    "        queries = random_projection(self.queries(x_reshaped), self.projection_dim // 2)\n",
    "        keys = random_projection(self.keys(x_reshaped), self.projection_dim // 2 )\n",
    "        print(\"values :\", values.shape)\n",
    "        print(\"queries :\", queries.shape)\n",
    "        print(\"keys :\", keys.shape)\n",
    "\n",
    "        attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "        print(f\"attention_scores after random projection: {attention_scores.shape}\")\n",
    "\n",
    "        for i in range(0, seq_length, self.partition_size):\n",
    "            print(f\"PARTITION START\")\n",
    "            partition_start = i\n",
    "            partition_end = min(i + self.partition_size, seq_length)\n",
    "            keys_part = keys[:, partition_start:partition_end, :, :]\n",
    "            queries_part = queries[:, partition_start:partition_end, :, :]\n",
    "\n",
    "            C_keys, R_queries = cur_decomposition(keys_part, self.projection_dim)\n",
    "            print(\"C_keys shape before return:\", C_keys.shape)\n",
    "\n",
    "            ponder_scores = torch.zeros(N, self.heads, partition_end - partition_start, 1, device=x.device)\n",
    "            print(f\"Partition Start {i}, Partition End {partition_end} , ponder_scores: {ponder_scores.shape}\")\n",
    "\n",
    "            for h in range(self.heads):\n",
    "                #print(f\"HEADS START\")\n",
    "                head_queries = queries_part[:, :, h, :]\n",
    "                #print(f\"head_queries: {head_queries.shape}\")\n",
    "                head_ponder_scores = self.sigmoid(self.ponder(head_queries))\n",
    "                #print(f\"head_ponder_scores: {head_ponder_scores.shape}\")\n",
    "                ponder_scores[:, h, :, 0] = head_ponder_scores.squeeze(-1)\n",
    "\n",
    "            # Correctly expand ponder_scores without adding an unnecessary dimension\n",
    "            print(\"BEFORE 1ST EINSUM:\")\n",
    "            ponder_scores_permuted = ponder_scores.permute(0, 2, 1, 3)  # Move to [2, 128, 8, 1]\n",
    "            print(\"ponder_scores_permuted shape:\", ponder_scores_permuted.shape) \n",
    "            ponder_scores_broadcastable = ponder_scores_permuted.expand(-1, -1, -1, 128)  # Expand to [2, 128, 8, 128]            print(f\"ponder_scores_broadcastable shape: {ponder_scores_broadcastable.shape}\")\n",
    "            print(\"ponder_scores_broadcastable shape:\", ponder_scores_broadcastable.shape) \n",
    "            print(\"queries_part shape:\", queries_part.shape) \n",
    "            print(\"C_keys shape:\", C_keys.shape)\n",
    "            energy = torch.einsum('bnhd,bnhk->bnhd', queries_part, C_keys)\n",
    "            attention_weights = F.softmax(energy, dim=-1)\n",
    "            print(\"AFTER 1ST EINSUM:\")\n",
    "            print(\"energy shape:\", energy.shape) \n",
    "            print(\"attention_weights shape:\", attention_weights.shape)\n",
    "            attention = attention_weights * ponder_scores_broadcastable\n",
    "            print(\"attention shape:\", attention.shape)\n",
    "            attention_corrected = attention.permute(0, 2, 1, 3)\n",
    "            attention_scores[:, :, partition_start:partition_end, :] = attention_corrected\n",
    "\n",
    "        values = values.permute(0, 2, 1, 3)  # Swap heads and seq_length to bring heads next to head_dim\n",
    "        print(\"values shape:\", values.shape)\n",
    "        values = values.reshape(-1, self.head_dim)  # Flatten to [N*heads*seq_length, head_dim] for linear layer\n",
    "        print(\"values.reshape(-1, self.head_dim) shape:\", values.shape)\n",
    "        projected_values = self.value_projection(values)  # Now [N*heads*seq_length, projection_dim / 2]\n",
    "        print(\"self.value_projection(values) shape:\", projected_values.shape)\n",
    "        projected_values = projected_values.view(N, self.heads, seq_length, self.projection_dim // 2)\n",
    "        print(\"projected_values shape:\", projected_values.shape)\n",
    "\n",
    "        print(f\"2ND EINSUM\")\n",
    "        # Combine attention_scores and projected_values then pass through the final linear layer\n",
    "        out = torch.einsum('bnhp,bnhp->bnh', attention_scores, projected_values)\n",
    "        print(\"out shape:\", out.shape)\n",
    "       \n",
    "        return out\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = PartitionedLinformerAttentionACT(embed_size=512, heads=8, sequence_length=1024, projection_dim=256, partition_size=128)\n",
    "input_tensor = torch.rand(2, model.sequence_length, model.embed_size)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  \n",
    "\n",
    "\n",
    "# Create a DataLoader with pinned memory\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize your data here\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # Return the size of your dataset\n",
    "        return 100  # Example size\n",
    "    def __getitem__(self, idx):\n",
    "        # Implement logic to get a single item at a given index\n",
    "        # For simplicity, let's return random tensors\n",
    "        return torch.randn(10, 64), torch.randint(0, 2, (10,))  # Example data\n",
    "\n",
    "dataset = MyDataset()\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(10):  # Example: 10 epochs\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            predictions = model(inputs)\n",
    "            loss = nn.functional.cross_entropy(predictions, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
