{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape as input to forward: torch.Size([2, 1024, 512])\n",
      "embed size as input to forward: 512\n",
      "x_reshaped = x.view(N, seq_length, self.heads, self.head_dim): torch.Size([2, 1024, 8, 64])\n",
      "values: torch.Size([2, 1024, 8, 64]), queries: torch.Size([2, 1024, 8, 256]), keys: torch.Size([2, 1024, 8, 256])\n",
      "queries after random projection: torch.Size([2, 1024, 8, 128]), keys after random projection: torch.Size([2, 1024, 8, 128])\n",
      "attention_scores after random projection: torch.Size([2, 8, 1024, 128])\n",
      "PARTITION START\n",
      "Partition Start 0, Partition End 128 ,keys_partition: torch.Size([2, 128, 8, 128]), queries_partition: torch.Size([2, 128, 8, 128]), ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores shape before view: torch.Size([2, 128, 1])\n",
      "head_ponder_scores_reshaped shape : torch.Size([2, 128, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[2, 128, 64]}, size=[2, 128]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 110\u001b[0m\n\u001b[0;32m    108\u001b[0m model \u001b[38;5;241m=\u001b[39m PartitionedLinformerAttentionACT(embed_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, projection_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, partition_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m    109\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, model\u001b[38;5;241m.\u001b[39msequence_length, model\u001b[38;5;241m.\u001b[39membed_size)\n\u001b[1;32m--> 110\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output shape: [2, sequence_length, embed_size]\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Create a DataLoader with pinned memory\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[83], line 94\u001b[0m, in \u001b[0;36mPartitionedLinformerAttentionACT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_ponder_scores_reshaped shape : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead_ponder_scores_reshaped\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# Assign the reshaped ponder scores back to the corresponding head in the ponder_scores tensor\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[43mponder_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m head_ponder_scores_reshaped \n\u001b[0;32m     96\u001b[0m energy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbnhdp,bnhpd->bnhdp\u001b[39m\u001b[38;5;124m'\u001b[39m, queries_partition, C_keys\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     97\u001b[0m attention \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(energy, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ponder_scores\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[2, 128, 64]}, size=[2, 128]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "\n",
    "def random_projection(matrix, k):\n",
    "    \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "    random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "    return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "def cur_decomposition(matrix, k):\n",
    "    \"\"\"Applies CUR decomposition on each head for each batch.\"\"\"\n",
    "    batch_size, seq_length, heads, dim = matrix.shape\n",
    "    # Ensuring k does not exceed dimensions\n",
    "    k = min(k, dim)\n",
    "    C = torch.zeros(batch_size, seq_length, heads, k, device=matrix.device)\n",
    "    R = torch.zeros(batch_size, k, heads, dim, device=matrix.device)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for h in range(heads):\n",
    "            col_indices = np.random.choice(dim, k, replace=False)\n",
    "            row_indices = np.random.choice(seq_length, k, replace=False)\n",
    "            C[b, :, h] = matrix[b, :, h, col_indices]\n",
    "            R[b, :, h] = matrix[b, row_indices, h]\n",
    "    return C, R\n",
    "\n",
    "class PartitionedLinformerAttentionACT(nn.Module):\n",
    "    def __init__(self, embed_size, heads, sequence_length, projection_dim, partition_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.sequence_length = sequence_length\n",
    "        self.projection_dim = projection_dim\n",
    "        self.partition_size = partition_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        self.ponder = nn.Linear(self.partition_size, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, _ = x.shape\n",
    "        print(f\"x shape as input to forward: {x.shape}\")\n",
    "        print(f\"embed size as input to forward: {self.embed_size}\")\n",
    "\n",
    "        x_reshaped = x.view(N, seq_length, self.heads, self.head_dim)\n",
    "        print(f\"x_reshaped = x.view(N, seq_length, self.heads, self.head_dim): {x_reshaped.shape}\")\n",
    "\n",
    "        values = self.values(x_reshaped).view(N, seq_length, self.heads, self.head_dim)\n",
    "        keys = self.keys(x_reshaped).view(N, seq_length, self.heads, self.projection_dim)\n",
    "        queries = self.queries(x_reshaped).view(N, seq_length, self.heads, self.projection_dim)\n",
    "        print(f\"values: {values.shape}, queries: {queries.shape}, keys: {keys.shape}\")\n",
    "\n",
    "        keys = random_projection(keys, self.projection_dim // 2)\n",
    "        queries = random_projection(queries, self.projection_dim // 2)\n",
    "        print(f\"queries after random projection: {queries.shape}, keys after random projection: {keys.shape}\")\n",
    "        attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "        print(f\"attention_scores after random projection: {attention_scores.shape}\")\n",
    "\n",
    "        for i in range(0, seq_length, self.partition_size):\n",
    "            print(f\"PARTITION START\")\n",
    "            partition_start = i\n",
    "            partition_end = min(i + self.partition_size, seq_length)\n",
    "            keys_partition = keys[:, partition_start:partition_end, :, :]\n",
    "            queries_partition = queries[:, partition_start:partition_end, :, :]\n",
    "\n",
    "            C_keys, R_queries = cur_decomposition(keys_partition, k=keys_partition.size(-1) // 2)\n",
    "\n",
    "            ponder_scores = torch.zeros(N, self.heads, partition_end - partition_start, 1, device=x.device)\n",
    "            print(f\"Partition Start {i}, Partition End {partition_end} ,keys_partition: {keys_partition.shape}, queries_partition: {queries_partition.shape}, ponder_scores: {ponder_scores.shape}\")\n",
    "\n",
    "            for h in range(self.heads):\n",
    "                print(f\"HEADS START\")\n",
    "                head_queries = queries_partition[:, :, h, :]\n",
    "                print(f\"head_queries: {head_queries.shape}\")\n",
    "\n",
    "                # Apply the ponder network\n",
    "                head_ponder_scores = self.sigmoid(self.ponder(head_queries))\n",
    "                print(f\"head_ponder_scores shape before view: {head_ponder_scores.shape}\")\n",
    "                # Reshape head_ponder_scores back to match the original batch and partition sizes\n",
    "                # and expand the last dimension to match the expected shape for broadcasting\n",
    "                head_ponder_scores_reshaped = head_ponder_scores.view(N, partition_end - partition_start, -1).expand(-1, -1, self.head_dim)\n",
    "                print(f\"head_ponder_scores_reshaped shape : {head_ponder_scores_reshaped.shape}\")\n",
    "\n",
    "                # Assign the reshaped ponder scores back to the corresponding head in the ponder_scores tensor\n",
    "                ponder_scores[:, h, :, 0] = head_ponder_scores_reshaped \n",
    "\n",
    "            energy = torch.einsum('bnhdp,bnhpd->bnhdp', queries_partition, C_keys.transpose(-2, -1))\n",
    "            attention = F.softmax(energy, dim=-1) * ponder_scores\n",
    "            attention_scores[:, :, partition_start:partition_end, :] = attention\n",
    "\n",
    "        out = torch.einsum('bnhdp,bnhpd->bnshp', attention_scores, values).reshape(N, seq_length, -1)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "model = PartitionedLinformerAttentionACT(embed_size=512, heads=8, sequence_length=1024, projection_dim=256, partition_size=128)\n",
    "input_tensor = torch.rand(2, model.sequence_length, model.embed_size)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Expected output shape: [2, sequence_length, embed_size]\n",
    "\n",
    "\n",
    "# Create a DataLoader with pinned memory\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Initialize your data here\n",
    "\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # Return the size of your dataset\n",
    "\n",
    "        return 100  # Example size\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Implement logic to get a single item at a given index\n",
    "\n",
    "        # For simplicity, let's return random tensors\n",
    "\n",
    "        return torch.randn(10, 64), torch.randint(0, 2, (10,))  # Example data\n",
    "\n",
    "dataset = MyDataset()\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(10):  # Example: 10 epochs\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            predictions = model(inputs)\n",
    "            loss = nn.functional.cross_entropy(predictions, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape as input to forward: torch.Size([2, 1024, 512])\n",
      "embed size as input to forward: 512\n",
      "x_reshaped = x.view(N, seq_length, self.heads, self.head_dim): torch.Size([2, 1024, 8, 64])\n",
      "values: torch.Size([2, 1024, 8, 64]), queries: torch.Size([2, 1024, 8, 128]), keys: torch.Size([2, 1024, 8, 128])\n",
      "attention_scores after random projection: torch.Size([2, 8, 1024, 128])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 0, Partition End 128 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "BEFORE EINSUM:\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "ponder_scores shape: torch.Size([2, 128, 8, 1])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 128, Partition End 256 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "BEFORE EINSUM:\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "ponder_scores shape: torch.Size([2, 128, 8, 1])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 256, Partition End 384 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "BEFORE EINSUM:\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "ponder_scores shape: torch.Size([2, 128, 8, 1])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 384, Partition End 512 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "BEFORE EINSUM:\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "ponder_scores shape: torch.Size([2, 128, 8, 1])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 512, Partition End 640 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "BEFORE EINSUM:\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "ponder_scores shape: torch.Size([2, 128, 8, 1])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 640, Partition End 768 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "BEFORE EINSUM:\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "ponder_scores shape: torch.Size([2, 128, 8, 1])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 768, Partition End 896 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "BEFORE EINSUM:\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "ponder_scores shape: torch.Size([2, 128, 8, 1])\n",
      "PARTITION START\n",
      "C_keys shape before return: torch.Size([2, 128, 8, 128])\n",
      "Partition Start 896, Partition End 1024 , ponder_scores: torch.Size([2, 8, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "HEADS START\n",
      "head_queries: torch.Size([2, 128, 128])\n",
      "head_ponder_scores: torch.Size([2, 128, 1])\n",
      "BEFORE EINSUM:\n",
      "queries_part shape: torch.Size([2, 128, 8, 128])\n",
      "C_keys shape: torch.Size([2, 128, 8, 128])\n",
      "AFTER EINSUM:\n",
      "energy shape: torch.Size([2, 128, 8, 128])\n",
      "ponder_scores shape: torch.Size([2, 128, 8, 1])\n",
      "attention_scores shape: torch.Size([2, 8, 1024, 128])\n",
      "values shape: torch.Size([2, 1024, 8, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript n has size 1024 for operand 1 which does not broadcast with previously seen size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m model \u001b[38;5;241m=\u001b[39m PartitionedLinformerAttentionACT(embed_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, projection_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, partition_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m    115\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, model\u001b[38;5;241m.\u001b[39msequence_length, model\u001b[38;5;241m.\u001b[39membed_size)\n\u001b[1;32m--> 116\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output shape: [2, sequence_length, embed_size]\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Create a DataLoader with pinned memory\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[107], line 109\u001b[0m, in \u001b[0;36mPartitionedLinformerAttentionACT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_scores shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, attention_scores\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 109\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbnhp,bnhp->bnh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(out)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\torch\\functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): subscript n has size 1024 for operand 1 which does not broadcast with previously seen size 8"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "\n",
    "def random_projection(matrix, k):\n",
    "    \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "    random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "    return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "def cur_decomposition(matrix, projection_dim):  # Change 'k' argument\n",
    "    \"\"\"Applies CUR decomposition with C matrix dimension aligned to projection dimension.\"\"\"\n",
    "    batch_size, seq_length, heads, dim = matrix.shape\n",
    "    k = min(projection_dim // 2, dim)  # Use projection dimension to determine 'k'\n",
    "    C = torch.zeros(batch_size, seq_length, heads, k, device=matrix.device)\n",
    "    R = torch.zeros(batch_size, k, heads, dim, device=matrix.device)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for h in range(heads):\n",
    "            col_indices = np.random.choice(dim, k, replace=False)\n",
    "            row_indices = np.random.choice(seq_length, k, replace=False)\n",
    "            C[b, :, h] = matrix[b, :, h, col_indices]\n",
    "            R[b, :, h] = matrix[b, row_indices, h]\n",
    "    return C, R\n",
    "\n",
    "class PartitionedLinformerAttentionACT(nn.Module):\n",
    "    def __init__(self, embed_size, heads, sequence_length, projection_dim, partition_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.sequence_length = sequence_length\n",
    "        self.projection_dim = projection_dim\n",
    "        self.partition_size = partition_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        self.ponder = nn.Linear(self.partition_size, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, _ = x.shape\n",
    "        print(f\"x shape as input to forward: {x.shape}\")\n",
    "        print(f\"embed size as input to forward: {self.embed_size}\")\n",
    "\n",
    "        x_reshaped = x.view(N, seq_length, self.heads, self.head_dim)\n",
    "        print(f\"x_reshaped = x.view(N, seq_length, self.heads, self.head_dim): {x_reshaped.shape}\")\n",
    "\n",
    "        values = self.values(x_reshaped)  # Don't need the extra view\n",
    "        queries = random_projection(self.queries(x_reshaped), self.projection_dim // 2)\n",
    "        keys = random_projection(self.keys(x_reshaped), self.projection_dim // 2)\n",
    "        print(f\"values: {values.shape}, queries: {queries.shape}, keys: {keys.shape}\")\n",
    "\n",
    "        attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim // 2, device=x.device)\n",
    "        print(f\"attention_scores after random projection: {attention_scores.shape}\")\n",
    "\n",
    "        for i in range(0, seq_length, self.partition_size):\n",
    "            print(f\"PARTITION START\")\n",
    "            partition_start = i\n",
    "            partition_end = min(i + self.partition_size, seq_length)\n",
    "            \n",
    "            # Extract partition slices\n",
    "            keys_part = keys[:, partition_start:partition_end, :, :]\n",
    "            queries_part = queries[:, partition_start:partition_end, :, :]\n",
    "\n",
    "            C_keys, R_queries = cur_decomposition(keys_part, self.projection_dim)\n",
    "            print(\"C_keys shape before return:\", C_keys.shape)\n",
    "\n",
    "            ponder_scores = torch.zeros(N, self.heads, partition_end - partition_start, 1, device=x.device)\n",
    "            print(f\"Partition Start {i}, Partition End {partition_end} , ponder_scores: {ponder_scores.shape}\")\n",
    "\n",
    "            for h in range(self.heads):\n",
    "                print(f\"HEADS START\")\n",
    "            \n",
    "                head_queries = queries_part[:, :, h, :]\n",
    "                print(f\"head_queries: {head_queries.shape}\")\n",
    "\n",
    "                head_ponder_scores = self.sigmoid(self.ponder(head_queries))  # Shape: [N, partition_size, 1]\n",
    "                print(f\"head_ponder_scores: {head_ponder_scores.shape}\")\n",
    "\n",
    "                ponder_scores[:, h, :, 0] = head_ponder_scores[:, :, 0]  # Assign only the relevant scores \n",
    "\n",
    "            ponder_scores = ponder_scores.permute(0, 2, 1, 3)  # New shape: [batch, partition_size, heads, 1]\n",
    "            print(\"BEFORE EINSUM:\")\n",
    "            print(\"queries_part shape:\", queries_part.shape) \n",
    "            print(\"C_keys shape:\", C_keys.shape)\n",
    "            ponder_scores_broadcastable = ponder_scores.expand(-1, -1, -1, self.projection_dim // 2)\n",
    "\n",
    "            energy = torch.einsum('bnhd,bnhk->bnhd', queries_part, C_keys)\n",
    "            print(\"AFTER EINSUM:\")\n",
    "            print(f\"energy shape: {energy.shape}\")\n",
    "            print(f\"ponder_scores shape: {ponder_scores.shape}\")\n",
    "            # Apply softmax over the last dimension of energy\n",
    "            attention_weights = F.softmax(energy, dim=-1)\n",
    "\n",
    "            # Multiply by broadcasted ponder_scores\n",
    "            attention = attention_weights * ponder_scores_broadcastable\n",
    "            attention_scores[:, :, partition_start:partition_end, :] = attention.transpose(1, 2)\n",
    "        print(\"attention_scores shape:\", attention_scores.shape)\n",
    "        print(\"values shape:\", values.shape)\n",
    "\n",
    "        out = torch.einsum('bnhp,bnhp->bnh', attention_scores, values)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = PartitionedLinformerAttentionACT(embed_size=512, heads=8, sequence_length=1024, projection_dim=256, partition_size=128)\n",
    "input_tensor = torch.rand(2, model.sequence_length, model.embed_size)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Expected output shape: [2, sequence_length, embed_size]\n",
    "\n",
    "\n",
    "# Create a DataLoader with pinned memory\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Initialize your data here\n",
    "\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # Return the size of your dataset\n",
    "\n",
    "        return 100  # Example size\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Implement logic to get a single item at a given index\n",
    "\n",
    "        # For simplicity, let's return random tensors\n",
    "\n",
    "        return torch.randn(10, 64), torch.randint(0, 2, (10,))  # Example data\n",
    "\n",
    "dataset = MyDataset()\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(10):  # Example: 10 epochs\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            predictions = model(inputs)\n",
    "            loss = nn.functional.cross_entropy(predictions, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "\n",
    "def random_projection(matrix, k):\n",
    "    \"\"\"Random projection to reduce dimensionality of matrix to k dimensions.\"\"\"\n",
    "    random_matrix = torch.randn(matrix.size(-1), k, device=matrix.device)\n",
    "    return torch.matmul(matrix, random_matrix)\n",
    "\n",
    "def cur_decomposition(matrix, projection_dim):  # Change 'k' argument\n",
    "    \"\"\"Applies CUR decomposition with C matrix dimension aligned to projection dimension.\"\"\"\n",
    "    batch_size, seq_length, heads, dim = matrix.shape\n",
    "    k = min(projection_dim // 2, dim)  # Use projection dimension to determine 'k'\n",
    "    C = torch.zeros(batch_size, seq_length, heads, k, device=matrix.device)\n",
    "    R = torch.zeros(batch_size, k, heads, dim, device=matrix.device)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for h in range(heads):\n",
    "            col_indices = np.random.choice(dim, k, replace=False)\n",
    "            row_indices = np.random.choice(seq_length, k, replace=False)\n",
    "            C[b, :, h] = matrix[b, :, h, col_indices]\n",
    "            R[b, :, h] = matrix[b, row_indices, h]\n",
    "    return C, R\n",
    "\n",
    "class PartitionedLinformerAttentionACT(nn.Module):\n",
    "    def __init__(self, embed_size, heads, sequence_length, projection_dim, partition_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.sequence_length = sequence_length\n",
    "        self.projection_dim = projection_dim\n",
    "        self.partition_size = partition_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        # Linear transformations\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.projection_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        self.value_projection = nn.Linear(self.head_dim, self.projection_dim)  # New linear transformation to match dimensions\n",
    "        self.ponder = nn.Linear(self.partition_size, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, _ = x.shape\n",
    "        x_reshaped = x.view(N, seq_length, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(x_reshaped)\n",
    "        queries = random_projection(self.queries(x_reshaped), self.projection_dim // 2)\n",
    "        keys = random_projection(self.keys(x_reshaped), self.projection_dim // 2)\n",
    "        \n",
    "        # Projecting values to match the projection dimension\n",
    "        projected_values = self.value_projection(values)  # Transforming values here\n",
    "\n",
    "        attention_scores = torch.zeros(N, self.heads, seq_length, self.projection_dim, device=x.device)\n",
    "        print(f\"attention_scores after random projection: {attention_scores.shape}\")\n",
    "\n",
    "        for i in range(0, seq_length, self.partition_size):\n",
    "            print(f\"PARTITION START\")\n",
    "            partition_start = i\n",
    "            partition_end = min(i + self.partition_size, seq_length)\n",
    "            \n",
    "            # Extract partition slices\n",
    "            keys_part = keys[:, partition_start:partition_end, :, :]\n",
    "            queries_part = queries[:, partition_start:partition_end, :, :]\n",
    "\n",
    "            C_keys, R_queries = cur_decomposition(keys_part, self.projection_dim)\n",
    "            print(\"C_keys shape before return:\", C_keys.shape)\n",
    "\n",
    "            ponder_scores = torch.zeros(N, self.heads, partition_end - partition_start, 1, device=x.device)\n",
    "            print(f\"Partition Start {i}, Partition End {partition_end} , ponder_scores: {ponder_scores.shape}\")\n",
    "\n",
    "            for h in range(self.heads):\n",
    "                print(f\"HEADS START\")\n",
    "            \n",
    "                head_queries = queries_part[:, :, h, :]\n",
    "                print(f\"head_queries: {head_queries.shape}\")\n",
    "\n",
    "                head_ponder_scores = self.sigmoid(self.ponder(head_queries))  # Shape: [N, partition_size, 1]\n",
    "                print(f\"head_ponder_scores: {head_ponder_scores.shape}\")\n",
    "\n",
    "                ponder_scores[:, h, :, 0] = head_ponder_scores[:, :, 0]  # Assign only the relevant scores \n",
    "\n",
    "            ponder_scores = ponder_scores.permute(0, 2, 1, 3)  # New shape: [batch, partition_size, heads, 1]\n",
    "            print(\"BEFORE EINSUM:\")\n",
    "            print(\"queries_part shape:\", queries_part.shape) \n",
    "            print(\"C_keys shape:\", C_keys.shape)\n",
    "            ponder_scores_broadcastable = ponder_scores.expand(-1, -1, -1, self.projection_dim // 2)\n",
    "\n",
    "            energy = torch.einsum('bnhd,bnhk->bnhd', queries_part, C_keys)\n",
    "            print(\"AFTER EINSUM:\")\n",
    "            print(f\"energy shape: {energy.shape}\")\n",
    "            print(f\"ponder_scores shape: {ponder_scores.shape}\")\n",
    "            # Apply softmax over the last dimension of energy\n",
    "            attention_weights = F.softmax(energy, dim=-1)\n",
    "\n",
    "            # Multiply by broadcasted ponder_scores\n",
    "            attention = attention_weights * ponder_scores_broadcastable\n",
    "            attention_scores[:, :, partition_start:partition_end, :] = attention.transpose(1, 2)\n",
    "        print(\"attention_scores shape:\", attention_scores.shape)\n",
    "        print(\"values shape:\", values.shape)\n",
    "\n",
    "        out = torch.einsum('bnhp,bnhp->bnh', attention_scores, projected_values)\n",
    "        return self.fc_out(out.view(N, seq_length, -1))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = PartitionedLinformerAttentionACT(embed_size=512, heads=8, sequence_length=1024, projection_dim=256, partition_size=128)\n",
    "input_tensor = torch.rand(2, model.sequence_length, model.embed_size)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Expected output shape: [2, sequence_length, embed_size]\n",
    "\n",
    "\n",
    "# Create a DataLoader with pinned memory\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Initialize your data here\n",
    "\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # Return the size of your dataset\n",
    "\n",
    "        return 100  # Example size\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Implement logic to get a single item at a given index\n",
    "\n",
    "        # For simplicity, let's return random tensors\n",
    "\n",
    "        return torch.randn(10, 64), torch.randint(0, 2, (10,))  # Example data\n",
    "\n",
    "dataset = MyDataset()\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(10):  # Example: 10 epochs\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            predictions = model(inputs)\n",
    "            loss = nn.functional.cross_entropy(predictions, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
