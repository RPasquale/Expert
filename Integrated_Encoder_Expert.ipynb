{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # Ensure special tokens are included\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def combined_loss(output, target, model, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)  \n",
    "    regularization_loss = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if isinstance(param, nn.Parameter):  \n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1\n",
    "\n",
    "    return task_loss + regularization_loss\n",
    "\n",
    "# Encoding Transformer Code\n",
    "\n",
    "class AdaptiveDropoutLayer(nn.Module):\n",
    "    def __init__(self, init_dropout_rate=0.1):\n",
    "        super(AdaptiveDropoutLayer, self).__init__()\n",
    "        # Use logit transformation for stability\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.log_alpha)\n",
    "        # Convert p from a tensor to a float\n",
    "        p_value = p.item()  # This extracts the scalar value as a Python float\n",
    "        return nn.functional.dropout(x, p=p_value, training=self.training)\n",
    "\n",
    "\n",
    "class AdaptiveWeightDecayOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, init_l2_strength=0.01):\n",
    "        super().__init__(params, {'lr': lr})\n",
    "        self.log_l2_strength = nn.Parameter(torch.tensor(math.log(init_l2_strength)).float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = torch.exp(self.log_l2_strength)  \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay) \n",
    "                p.update(d_p, group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Create positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension (B x T x C)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Tensor of shape [Batch Size, Sequence Length, Embedding Dimension]\n",
    "        # Adjust positional encoding to match the input size and device\n",
    "        pe = self.pe[:, :x.size(1)]\n",
    "        # Assuming x is on the correct device, pe will be automatically aligned to the same device\n",
    "        return pe\n",
    "\n",
    "\n",
    "class MultiHeadLinformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, k=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k if k is not None else embed_dim // num_heads  # Projection dimension per head\n",
    "\n",
    "        self.key_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.value_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        # Project keys and values\n",
    "        keys = self.key_projections(query)\n",
    "        values = self.value_projections(query)\n",
    "        \n",
    "        # Reshape into [batch_size, num_heads, seq_len, k]\n",
    "        keys = keys.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        values = values.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention (scaled dot-product attention)\n",
    "        # Scaling by the square root of the depth of the key vectors to prevent large values in the dot product\n",
    "        # which could push the softmax function into regions where it has extremely small gradients\n",
    "        keys = keys / (self.k ** 0.5)\n",
    "        attention_scores = torch.softmax(torch.matmul(keys, values.transpose(-2, -1)), dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention_scores, values)\n",
    "        \n",
    "        # Concatenate heads and project back to original embedding dimension\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, self.num_heads * self.k)\n",
    "        out = self.out_projection(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len= 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)[:self.max_seq_len]         \n",
    "        labels = encoded_text.copy()  # Copy encoded text for labels\n",
    "\n",
    "        # Apply dynamic masking\n",
    "        masked_indices = np.random.rand(len(encoded_text)) < 0.15\n",
    "        for i in range(len(encoded_text)):\n",
    "            if masked_indices[i]:\n",
    "                encoded_text[i] = self.tokenizer.vocab[\"[MASK]\"]\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "        labels += [-100] * padding_length  # Use -100 for padding positions\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class AdaptiveEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab,  vocab_size, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "        super(AdaptiveEmbeddingLayer, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab_size\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.large_embed_dim = large_embed_dim\n",
    "        self.small_embed_dim = small_embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.split_vocab(vocab, freq_threshold)  \n",
    "\n",
    "        self.frequent_embeddings = nn.Embedding(num_embeddings=len(self.frequent_vocab), embedding_dim=large_embed_dim)\n",
    "        self.infrequent_embeddings = nn.Embedding(num_embeddings=len(self.infrequent_vocab), embedding_dim=small_embed_dim)\n",
    "        self.infrequent_projection = nn.Linear(small_embed_dim, large_embed_dim)\n",
    "        self.positional_embeddings = PositionalEncoding(large_embed_dim, max_seq_len)\n",
    "\n",
    "\n",
    "    def split_vocab(self, vocab, freq_threshold):\n",
    "        token_counts = [(token, count) for token, count in vocab.items()]\n",
    "        token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "        \n",
    "        self.frequent_vocab = dict(token_counts[:split_point])\n",
    "        self.infrequent_vocab = dict(token_counts[split_point:])\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        device = token_ids.device\n",
    "        seq_len = token_ids.size(1)\n",
    "        batch_size = token_ids.size(0)  # Obtain batch size from token_ids tensor\n",
    "\n",
    "        # Initialize embeddings tensor\n",
    "        embeddings = torch.zeros(token_ids.shape[0], seq_len, self.large_embed_dim, device=device)\n",
    "\n",
    "        # Map token_ids to indices for frequent and infrequent vocab\n",
    "        frequent_indices = torch.zeros_like(token_ids)\n",
    "        infrequent_indices = torch.zeros_like(token_ids)\n",
    "        \n",
    "        for token_id, index in self.vocab.items():\n",
    "            mask = token_ids == token_id\n",
    "            if token_id in self.frequent_vocab:\n",
    "                # Map to index in frequent_vocab\n",
    "                frequent_indices[mask] = self.frequent_vocab[token_id]\n",
    "            elif token_id in self.infrequent_vocab:\n",
    "                # Map to index in infrequent_vocab\n",
    "                infrequent_indices[mask] = self.infrequent_vocab[token_id]\n",
    "\n",
    "        # Create masks for frequent and infrequent tokens\n",
    "        frequent_mask = frequent_indices > 0\n",
    "        infrequent_mask = infrequent_indices > 0\n",
    "\n",
    "        # Embed frequent tokens\n",
    "        if frequent_mask.any():\n",
    "            frequent_embeddings = self.frequent_embeddings(frequent_indices[frequent_mask])\n",
    "            embeddings[frequent_mask] = frequent_embeddings\n",
    "\n",
    "        # Embed and project infrequent tokens\n",
    "        if infrequent_mask.any():\n",
    "            infrequent_embeddings = self.infrequent_embeddings(infrequent_indices[infrequent_mask])\n",
    "            infrequent_embeddings_projected = self.infrequent_projection(infrequent_embeddings)\n",
    "            embeddings[infrequent_mask] = infrequent_embeddings_projected\n",
    "\n",
    "        # Apply positional embeddings\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        position_embeddings = self.positional_embeddings(position_ids)  # Generate for seq_len\n",
    "\n",
    "        # Ensure positional embeddings are broadcastable to the embeddings tensor\n",
    "        # This step may not be necessary if your positional embeddings are already correctly shaped\n",
    "        if position_embeddings.size(0) != batch_size:\n",
    "            position_embeddings = position_embeddings.expand(batch_size, -1, -1)\n",
    "\n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"Positional embeddings shape: {position_embeddings.shape}\")\n",
    "        embeddings += position_embeddings\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadLinformerAttention(embed_dim=d_model, num_heads=nhead)\n",
    "        self.dropout1 = AdaptiveDropoutLayer()  # Use AdaptiveDropoutLayer\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            AdaptiveDropoutLayer(),  # Use AdaptiveDropoutLayer here as well\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.dropout2 = AdaptiveDropoutLayer()  # And here\n",
    " \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2 = self.norm1(src)\n",
    "        attn_output = self.attn(src2)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout2(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab, vocab_size, embedding_dim, max_seq_len, nhead, dim_feedforward, \n",
    "                 freq_threshold, smaller_embed_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab,\n",
    "            vocab_size=vocab_size, \n",
    "            freq_threshold=freq_threshold,  \n",
    "            large_embed_dim=embedding_dim,       \n",
    "            small_embed_dim=smaller_embed_dim,   \n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.encoder = TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward)\n",
    "        self.pooler = Pooler(embedding_dim)  # Retain Pooler for sentence-level representation\n",
    "        # Add an output projection layer for token-level predictions\n",
    "        self.output_projection = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        encoded = self.encoder(embedded, src_mask=attention_mask)\n",
    "        # Get pooled output for sentence-level tasks\n",
    "        pooled_output = self.pooler(encoded)\n",
    "        # Project encoded output to vocabulary size for token-level predictions\n",
    "        logits = self.output_projection(encoded)\n",
    "        return logits, pooled_output\n",
    "\n",
    "# Tokenizing Code\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        node = self.root\n",
    "        subword_ids = []\n",
    "        for char in token:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "                if node.token_id is not None:\n",
    "                    subword_ids.append(node.token_id)\n",
    "                    break  # Assuming one token maps to one subword for simplicity\n",
    "            else:\n",
    "                break  # No further subword match found\n",
    "        if not subword_ids:  # If no subword was found\n",
    "            subword_ids.append(self.unk_token_id if hasattr(self, 'unk_token_id') else 1) \n",
    "        return subword_ids\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(best)  # Track most frequent pair in each iteration\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.split()  # Assuming simple word splitting\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None \n",
    "        self.failure_pops = None\n",
    "\n",
    "    def train(self, corpus):\n",
    "        vocabulary = self._build_vocabulary(corpus)  # Build the word list\n",
    "        self.trie, self.failure_links, self.failure_pops = self._precompute(vocabulary)\n",
    "\n",
    "    def _encode(self, word):\n",
    "        subword_ids = []\n",
    "        current_node = self.trie.root\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "            if char in current_node.children:\n",
    "                current_node = current_node.children[char]\n",
    "                if current_node.token_id:\n",
    "                    subword_ids.append(current_node.token_id)\n",
    "            else:  # No direct match. Implement failure logic\n",
    "                while True:\n",
    "                    if current_node.failure_link is not None:\n",
    "                        current_node = current_node.failure_link\n",
    "\n",
    "                        # Check for pops to skip further backtracking\n",
    "                        for _ in range(current_node.failure_pop):\n",
    "                            current_node = current_node.failure_link\n",
    "\n",
    "                        if char in current_node.children:\n",
    "                            current_node = current_node.children[char]\n",
    "                            if current_node.token_id:\n",
    "                                subword_ids.append(current_node.token_id)\n",
    "                            break  # Successful subword transition \n",
    "\n",
    "                    else:  # No failure link, we're back at root level\n",
    "                        # Handle situation based on your application\n",
    "                        # e.g., append an unknown token \n",
    "                        subword_ids.append(self.unk_token_id)  \n",
    "                        break  \n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        if model_type == \"unigram\":\n",
    "            tokens = self._unigram_tokenize(corpus)  #  Hypothetical tokenize function\n",
    "            vocabulary = self._build_unigram_vocab(tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        # Count token frequencies\n",
    "        token_freqs = collections.Counter(tokens)\n",
    "        # Select the most frequent tokens up to vocab_size\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(token_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction\n",
    "        self.trie = Trie()  # Assuming you have a Trie class as well\n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token)  \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "    def _unigram_tokenize(self, corpus):\n",
    "        \"\"\"\n",
    "        Tokenizes the given corpus into unigram tokens, checking against the built vocabulary.\n",
    "        Tokens not found in the vocabulary are treated as unknowns.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): The text corpus to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list of str: A list of tokens extracted from the corpus, adjusted to fit the vocabulary.\n",
    "        \"\"\"\n",
    "        # Initial tokenization based on word boundaries and punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', corpus)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, actual_vocab_size):\n",
    "        self.vocab = vocab\n",
    "        self.actual_vocab_size = actual_vocab_size\n",
    "        self.unk_token_id = self.vocab.get(\"[UNK]\", 1)  # Get ID of [UNK] \n",
    "        self.max_subword_length = max(len(token) for token in vocab.keys())\n",
    "        self.pattern = re.compile(r'\\b\\w+\\b|[\\s\\.,!?;]')\n",
    "\n",
    "        # Build the Trie\n",
    "        self.trie = Trie()\n",
    "        for token, token_id in vocab.items():\n",
    "            self.trie.insert(token, token_id)\n",
    "\n",
    "    def _find_subwords(self, word):\n",
    "        # 1. Trie Lookup \n",
    "        subword_ids = self.trie.find_subwords(word)\n",
    "\n",
    "        # 2. Fallback to Original Logic \n",
    "        if len(subword_ids) == 1 and subword_ids[0] == self.unk_token_id:  \n",
    "            subwords = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                for j in range(self.max_subword_length, 0, -1):\n",
    "                    subword = word[i:i+j]\n",
    "                    if subword in self.vocab:\n",
    "                        subwords.append(self.vocab[subword])\n",
    "                        i += j\n",
    "                        break \n",
    "                else: \n",
    "                    subwords.append(self.vocab[\"[UNK]\"])\n",
    "                    i += 1\n",
    "            subword_ids = subwords  # Replace with token IDs\n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        token_ids = [self.vocab.get(\"[CLS]\", 1)]  # Use [CLS] token or [UNK] if not found\n",
    "        tokens = self.pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            token_id = self.vocab.get(token, self.vocab.get(\"[UNK]\", 1))  # Fallback to [UNK] if token is not found\n",
    "            token_ids.append(token_id)\n",
    "        token_ids.append(self.vocab.get(\"[SEP]\", 1))  # Use [SEP] token or [UNK] if not found\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "# Training Code\n",
    "\n",
    "# Load corpus and build vocab\n",
    "corpus = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "tokens = tokenize(corpus)\n",
    "vocab = build_vocab(tokens)\n",
    "actual_vocab_size = len(vocab)  # This includes [PAD], [UNK], [CLS], [SEP], [MASK]\n",
    "\n",
    "print(f\"Actual vocabulary size (including special tokens): {actual_vocab_size}\")\n",
    "\n",
    "tokenizer = Tokenizer(vocab=vocab, actual_vocab_size=actual_vocab_size)\n",
    "\n",
    "# Data Loading (Illustrative)\n",
    "train_texts = [corpus]  # Treat your whole sample as one \"document\"\n",
    "train_dataset = TextDataset(train_texts, tokenizer, max_seq_len=512)\n",
    "\n",
    "print(\"Tokenizer unk_token_id:\", tokenizer.unk_token_id) \n",
    "print(\"Tokenizer Vocabulary:\", tokenizer.vocab)\n",
    "\n",
    "\n",
    "\n",
    "device='cpu'\n",
    "freq_threshold_values = [10, 50, 100, 200, 500]  \n",
    "best_validation_accuracy = 0.0 \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for freq_threshold in freq_threshold_values:\n",
    "\n",
    "    # Model instantiation and training setup\n",
    "    model = TransformerModel(\n",
    "        vocab = vocab,\n",
    "        vocab_size=actual_vocab_size,     \n",
    "        embedding_dim=128,\n",
    "        max_seq_len=512,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        freq_threshold=freq_threshold,  # frequency threshold for splitting vocab\n",
    "        smaller_embed_dim=64\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4) \n",
    "    meta_optimizer = AdaptiveWeightDecayOptimizer(model.parameters(), lr=1e-5) \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    meta_update_freq = 5\n",
    "\n",
    "    # Training loop adjusted for the updated model architecture\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)  # Assuming labels are of shape [batch_size, sequence_length]\n",
    "\n",
    "            # Forward pass, model now returns logits and pooled_output\n",
    "            logits, pooled_output = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Correctly reshape logits to match the labels' shape\n",
    "            # Change from [1, 512, vocab_size] to [512, vocab_size] to align with labels\n",
    "            logits = logits.view(-1, logits.size(-1))  # Reshape logits for loss calculation\n",
    "            \n",
    "            labels = labels.view(-1)  # Ensure labels are a flat vector\n",
    "\n",
    "            # Calculate loss using logits for token-level predictions\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Meta-update occasionally\n",
    "            if (i + 1) % meta_update_freq == 0:\n",
    "                meta_optimizer.zero_grad()\n",
    "                # Recalculate or reuse the loss for the meta-update\n",
    "                meta_loss = combined_loss(logits.detach(), labels.detach(), model)\n",
    "                meta_loss.backward()\n",
    "                meta_optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "# Save model state\n",
    "model_path = \"D:\\\\EXPERT_WEIGHTS\\\\encoding_transformer.bin\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save tokenizer using pickle for simplicity\n",
    "import pickle\n",
    "tokenizer_path = \"D:\\\\EXPERT_WEIGHTS\\\\tokenizer.pkl\"\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "\n",
    "# Assuming TransformerModel and Tokenizer classes are defined in the scope\n",
    "\n",
    "# Load the model\n",
    "model_loaded = TransformerModel(\n",
    "    vocab=vocab,  # Ensure `vocab` is loaded or defined in the scope\n",
    "    vocab_size=actual_vocab_size,\n",
    "    embedding_dim=128,\n",
    "    max_seq_len=512,\n",
    "    nhead=8,\n",
    "    dim_feedforward=2048,\n",
    "    freq_threshold=freq_threshold,  # Define or load `freq_threshold` as appropriate\n",
    "    smaller_embed_dim=64\n",
    ")\n",
    "model_loaded.load_state_dict(torch.load(\"D:\\\\EXPERT_WEIGHTS\\\\encoding_transformer.bin\"))\n",
    "model_loaded.eval()  # Set to evaluation mode\n",
    "\n",
    "# Load the tokenizer\n",
    "with open(\"D:\\\\EXPERT_WEIGHTS\\\\tokenizer.pkl\", 'rb') as f:\n",
    "    tokenizer_loaded = pickle.load(f)\n",
    "\n",
    "\n",
    "# Example text\n",
    "text = \"Here is some text to encode\"\n",
    "\n",
    "# Tokenize the input\n",
    "encoded_input = tokenizer_loaded.encode(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "# Predict with your model\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "    output = model_loaded(encoded_input)\n",
    "\n",
    "# Process the output as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Set the HF_HOME environment variable to a new cache directory on the D drive\n",
    "os.environ['HF_HOME'] = 'D:/hf_datasets_cache'\n",
    "\n",
    "class ExpertConfig:\n",
    "    def __init__(self, seq_len=512, head_dim=64, block_size=64, sparsity_factor=4,\n",
    "                 input_dim=512, num_experts=3, vocab_size=30522, embed_size=256,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0.1,\n",
    "                 max_length=512,  # Updated to match seq_len for consistency\n",
    "                 rank=16, device='cpu',\n",
    "                 mamba_model_path='D:\\\\EXPERT_WEIGHTS\\\\mamba_model_weights.pth',\n",
    "                 rag_model_path='D:\\\\EXPERT_WEIGHTS\\\\rag_model_weights.pth',\n",
    "                 context_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\context_encoder.pth',\n",
    "                 language_model_path='D:\\\\EXPERT_WEIGHTS\\\\language_model.pth',\n",
    "                 question_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\question_encoder.pth',\n",
    "                 dpo_model_path='D:\\\\EXPERT_WEIGHTS\\\\dpo_model_weights.pth',\n",
    "                 model_name='bert-base-uncased', embedding_dim=768,\n",
    "                 alpha=1, quantization_bits=8, tokenizer_name='bert-base-uncased',\n",
    "                 d_model=512, d_state=2048, d_conv=3, expansion_factor=2, \n",
    "                 clip_gradient = 1.0, mamba_learning_rate = 5e-4, weight_decay = 0.1,\n",
    "                 warmup_steps = 10, total_mamba_steps = 100\n",
    "                ):\n",
    "\n",
    "        # Common hyperparameters\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        self.input_dim = input_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length  # Ensure this is properly reflected in model components\n",
    "        self.rank = rank\n",
    "\n",
    "        # Model paths and device\n",
    "        self.mamba_model_path = mamba_model_path\n",
    "        self.rag_model_path = rag_model_path\n",
    "        self.context_encoder_path = context_encoder_path\n",
    "        self.language_model_path = language_model_path\n",
    "        self.question_encoder_path = question_encoder_path\n",
    "        self.dpo_model_path = dpo_model_path\n",
    "        self.device = device\n",
    "\n",
    "        # Unique hyperparameters\n",
    "        self.num_experts = num_experts\n",
    "        self.model_name = model_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.clip_gradient = clip_gradient\n",
    "        self.mamba_learning_rate = mamba_learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_mamba_steps = total_mamba_steps\n",
    "\n",
    "        # PDFs (unchanged)\n",
    "        self.pdf_file_paths = [\n",
    "            r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "            r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf'\n",
    "        ]\n",
    "        \n",
    "        # Preserving original dataset loading functionality\n",
    "        self.rag_dataset = Expert.TransformerRAG.create_dataset_from_pdfs(self.pdf_file_paths)\n",
    "\n",
    "    def validate(self):\n",
    "        assert self.seq_len % self.block_size == 0, \"seq_len must be divisible by block_size\"\n",
    "        assert self.max_length >= self.seq_len, \"max_length should be equal to or greater than seq_len\"\n",
    "\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    # Load model weights function\n",
    "    def load_model_weights(model, model_path):\n",
    "        #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        checkpoint = torch.load(model_path, map_location=config.device)\n",
    "\n",
    "        if isinstance(checkpoint, dict):\n",
    "            # Check for 'state_dict' or 'model_state_dict' keys\n",
    "            if 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            elif 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                # If no known key is found, try loading it as a raw state dictionary\n",
    "                try:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                except RuntimeError as e:\n",
    "                    raise ValueError(f\"Error loading state dict: {e}\")\n",
    "        elif isinstance(checkpoint, nn.Module):\n",
    "            # If the checkpoint is a model object, assign it directly\n",
    "            model = checkpoint\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    ###############################\n",
    "    # Flash2_Attention\n",
    "    class FlashAttention2(nn.Module):\n",
    "        def __init__(self, sequence_length, head_dimension, block_size):\n",
    "            super(Expert.FlashAttention2, self).__init__()\n",
    "            self.block_size = block_size\n",
    "            # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "            assert sequence_length % block_size == 0\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            # Partitioning of inputs\n",
    "            Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "            # Efficient computation of the attention mechanism\n",
    "            outputs = []\n",
    "            for i, Q_block in enumerate(Q_blocks):\n",
    "                output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "                outputs.append(output_block)\n",
    "\n",
    "            # Concatenating the processed blocks\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "            return output\n",
    "\n",
    "        def partition_inputs(self, Q, K, V):\n",
    "            # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "            Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "            K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "            V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "            return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "        def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "            # Process each block efficiently as per FLASH2's optimized method\n",
    "            # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "            output_blocks = []\n",
    "            for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "                attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "                attention_scores = self.online_softmax(attention_scores)\n",
    "                output_block = torch.matmul(attention_scores, V_block)\n",
    "                output_blocks.append(output_block)\n",
    "\n",
    "            # Summing up the results from each block\n",
    "            output_block_sum = sum(output_blocks)\n",
    "            return output_block_sum\n",
    "\n",
    "        def online_softmax(self, scores, chunk_size=128):\n",
    "            # Apply softmax in chunks for large sequences\n",
    "            softmaxed_scores = []\n",
    "            for i in range(0, scores.size(0), chunk_size):\n",
    "                chunk = scores[i:i + chunk_size, :]\n",
    "                softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "                softmaxed_scores.append(softmaxed_chunk)\n",
    "            return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "    ###############################\n",
    "    # SparseFlash2_Attention\n",
    "    class SparseFlash2Attention(nn.Module):\n",
    "        def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "            super().__init__()\n",
    "            self.flash_attention = Expert.FlashAttention2(seq_len, head_dim, blk_size)\n",
    "            self.seq_len = seq_len\n",
    "            self.head_dim = head_dim\n",
    "            self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "            self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        def generate_sparsity_mask(self):\n",
    "            mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "            step = self.sparsity_factor\n",
    "            for i in range(0, self.seq_len, step):\n",
    "                mask[i:i + step, :] = 1\n",
    "            return mask.bool()\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "            # Reshape output to be 3D for batch matrix multiplication\n",
    "            output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "            sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "            # Apply the sparsity mask to the output\n",
    "            sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "            output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "            # Reshape the output back to 2D\n",
    "            output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "            return output\n",
    "\n",
    "    ###############################\n",
    "    # MAMBA\n",
    "    # RoPE\n",
    "    class RotaryPositionalEncoding(nn.Module):\n",
    "        def __init__(self, dim, max_len=5000):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "            t = torch.arange(max_len).type_as(inv_freq)\n",
    "            freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "            self.register_buffer('sin', freqs.sin())\n",
    "            self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "        def forward(self, x):\n",
    "            n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "            sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "            # Apply RoPE to even and odd indices separately\n",
    "            x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "    # SWIGLU\n",
    "    class SwiGLU(nn.Module):\n",
    "        def __init__(self, dim_in, dim_out):\n",
    "            super(Expert.SwiGLU, self).__init__()\n",
    "            self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "            self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "        def forward(self, x):\n",
    "            gate = torch.sigmoid(self.fc2(x))\n",
    "            return self.fc1(x) * gate\n",
    "\n",
    "    class SimplifiedMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "            \n",
    "            self.feedforward = nn.Sequential(\n",
    "                nn.Linear(self.d_model, self.d_state),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(self.d_state, self.d_model)\n",
    "            )\n",
    "            self.input_embedding = nn.Linear(self.d_model, self.d_model)\n",
    "            self.convs = nn.Sequential(*[nn.Conv1d(self.d_model, self.d_model, kernel_size=self.d_conv, padding=(self.d_conv // 2)) for _ in range(self.num_layers)])\n",
    "            self.swiglu = Expert.SwiGLU(self.d_model, self.d_model)\n",
    "            self.output_projection = nn.Linear(self.d_model, self.d_model * self.expansion_factor)\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "            nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "            nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "            nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "        def forward(self, inputs, attention_mask=None):\n",
    "            print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "            # Apply the attention mask if provided\n",
    "            if attention_mask is not None:\n",
    "                inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "            projected_inputs = self.input_embedding(inputs)\n",
    "            print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            for conv in self.convs:\n",
    "                projected_inputs = conv(projected_inputs)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = self.swiglu(projected_inputs)\n",
    "            print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "            output = self.output_projection(projected_inputs)\n",
    "            print(\"output shape:\", output.shape)\n",
    "\n",
    "            return output\n",
    "\n",
    "    class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.vocab_size = config.vocab_size\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "            self.pos_encoder = Expert.RotaryPositionalEncoding(self.d_model)\n",
    "            self.simplified_mamba = Expert.SimplifiedMAMBA(config)\n",
    "            self.output_projection = nn.Linear(self.d_model * 2, self.vocab_size)  # Adjust if needed\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = 1.0\n",
    "\n",
    "            nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "        def forward(self, input_values, attention_mask):\n",
    "            embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "            embedded = self.pos_encoder(embedded)\n",
    "            simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "            logits = self.output_projection(simplified_mamba_output)\n",
    "            return logits\n",
    "\n",
    "    ###############################\n",
    "    # Switch Router\n",
    "\n",
    "    class SwitchRouter(nn.Module):\n",
    "        CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "        class SwitchGate(nn.Module):\n",
    "            def __init__(self, input_dim, num_experts):\n",
    "                super(Expert.SwitchRouter.SwitchGate, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "                self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "                return gate_scores\n",
    "\n",
    "        def __init__(self, config):\n",
    "            super(Expert.SwitchRouter, self).__init__()\n",
    "            self.config = config\n",
    "            self.device = config.device\n",
    "            self.router = self.SwitchGate(config.input_dim, config.num_experts).to(config.device)\n",
    "            self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "            self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.transformer_dpo = Expert.DPO(self.lmt, config.device,config.embed_size).to(config.device)\n",
    "            self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "            self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "            self.input_embedding = nn.Linear(config.input_dim, config.input_dim)\n",
    "\n",
    "        def forward(self, x, attention_mask, context_texts, question_text):\n",
    "            x = x.to(self.device)\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.float()\n",
    "\n",
    "            x = self.input_embedding(x)\n",
    "            gate_scores = self.router(x)\n",
    "            expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "            final_output = torch.zeros_like(x)\n",
    "\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                mask = expert_indices == i\n",
    "                if mask.any():\n",
    "                    selected_inputs = x[mask]\n",
    "                    selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                    if isinstance(expert, Expert.TransformerRAG):\n",
    "                        expert_output = expert.forward(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                    else:\n",
    "                        expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                    final_output[mask] = expert_output\n",
    "\n",
    "            aux_loss = self.auxiliary_loss(gate_scores)\n",
    "            return final_output, aux_loss\n",
    "\n",
    "        def auxiliary_loss(self, gate_scores):\n",
    "            expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "            loss_balancing = torch.std(expert_load)\n",
    "            return loss_balancing\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "            capacity_factor_tensor = torch.tensor([Expert.SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "            capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "            expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "            for idx in range(len(expert_indices)):\n",
    "                selected_expert = expert_indices[idx]\n",
    "                if expert_counts[selected_expert] < capacities[0]:\n",
    "                    expert_counts[selected_expert] += 1\n",
    "                else:\n",
    "                    available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                    if len(available_experts) > 0:\n",
    "                        alternative_expert = available_experts[0]\n",
    "                        expert_indices[idx] = alternative_expert\n",
    "                        expert_counts[alternative_expert] += 1\n",
    "                    else:\n",
    "                        print(\"No available experts to reroute. Handling overflow.\")\n",
    "            return expert_indices\n",
    "    \n",
    "    ###############################\n",
    "    # RAG\n",
    "    class CustomDPRContextEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.CustomDPRContextEncoder, self).__init__()  \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            context_embeddings = self.embedding_layer(pooled_output)\n",
    "            return context_embeddings\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.DPRQuestionEncoder, self).__init__() \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, **kwargs):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            embeddings = self.embedding_layer(pooled_output)\n",
    "            return embeddings\n",
    "\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(Expert.TransformerRAG, self).__init__()\n",
    "            self.config = config\n",
    "            self.context_encoder = Expert.CustomDPRContextEncoder(config.embedding_dim).to(config.device)\n",
    "            self.language_model = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.question_encoder = Expert.DPRQuestionEncoder(config.embedding_dim).to(config.device)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "            \n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "                \n",
    "                aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = context['input_ids'].to(config.device)\n",
    "                    context_attention_mask = context['attention_mask'].to(config.device)\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "                \n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "            \n",
    "            question_input_ids = question_input_ids.to(config.device).long()\n",
    "            question_attention_mask = question_attention_mask.to(config.device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "            \n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=config.device))\n",
    "            \n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(config.device) for k, v in tokenized_combined_input.items()}\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            \n",
    "            return response\n",
    "\n",
    "        @staticmethod\n",
    "        def extract_text_from_pdf(file_path):\n",
    "            text = \"\"\n",
    "            with fitz.open(file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            return text\n",
    "\n",
    "        @staticmethod\n",
    "        def split_into_chunks(text, chunk_size):\n",
    "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        @staticmethod\n",
    "        def preprocess_text(text, max_length=512):\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            chunk_size = max_length - 50\n",
    "            text_chunks = Expert.TransformerRAG.split_into_chunks(text, chunk_size)\n",
    "            processed_chunks = []\n",
    "            for chunk in text_chunks:\n",
    "                tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "                processed_chunk = {\n",
    "                    'input_ids': tokenized_output['input_ids'],\n",
    "                    'attention_mask': tokenized_output['attention_mask']\n",
    "                }\n",
    "                processed_chunks.append(processed_chunk)\n",
    "            return processed_chunks\n",
    "\n",
    "        @staticmethod\n",
    "        def create_dataset_from_pdfs(pdf_file_paths):\n",
    "            dataset = []\n",
    "            for file_path in pdf_file_paths:\n",
    "                text = Expert.TransformerRAG.extract_text_from_pdf(file_path)\n",
    "                processed_text = Expert.TransformerRAG.preprocess_text(text)\n",
    "                dataset.append(processed_text)\n",
    "            return dataset\n",
    "\n",
    "        def retrieve_contexts(self, dataset, query_embedding, top_k=5):\n",
    "            similarity_scores = []\n",
    "            for context in dataset:\n",
    "                context_input_ids = context['input_ids'].to(self.config.device)\n",
    "                context_attention_mask = context['attention_mask'].to(self.config.device)\n",
    "                # Use the class's context_encoder\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "                similarity_scores.append(similarity.squeeze().item())\n",
    "            top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "            top_contexts = [dataset[i] for i in top_k_indices]\n",
    "            return top_contexts\n",
    "\n",
    "        def rag_retrieve_and_generate(self, dataset, query):\n",
    "            # Tokenize the query using the class's tokenizer\n",
    "            tokenized_query = self.tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
    "            input_ids = tokenized_query['input_ids']\n",
    "            attention_mask = tokenized_query['attention_mask']\n",
    "            # Use the class's question_encoder\n",
    "            encoded_query = self.question_encoder(input_ids, attention_mask)\n",
    "            relevant_contexts = self.retrieve_contexts(dataset, encoded_query)\n",
    "            # Assuming generate_response is a method of LanguageModelTransformer that accepts tokenized contexts and generates a response\n",
    "            response = self.language_model.generate_response(relevant_contexts)\n",
    "            return response\n",
    "\n",
    "    ###############################\n",
    "    # LORA\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "            super(Expert.LORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "\n",
    "            # Original weight and bias of the linear layer\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # LORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # QLORA\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "            super(Expert.QLORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "            self.quantization_bits = quantization_bits\n",
    "\n",
    "            # Original weight and bias\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # QLORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    ###############################\n",
    "    # Language Model Transformer\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, embed_size, heads):\n",
    "            super(Expert.MultiHeadAttention, self).__init__()\n",
    "            self.embed_size = embed_size\n",
    "            self.heads = heads\n",
    "            self.head_dim = embed_size // heads\n",
    "\n",
    "            assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "            ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        def forward(self, values, keys, query, mask):\n",
    "            N = query.shape[0]\n",
    "            value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "            # Split the embedding into self.heads different pieces\n",
    "            values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "            queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            # Einsum does the matrix multiplication for query*keys for each training example\n",
    "            # with every other training example, then sum it up\n",
    "            attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "                N, query_len, self.heads * self.head_dim\n",
    "            )\n",
    "\n",
    "            out = self.fc_out(out)\n",
    "            return out\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "            super(Expert.TransformerBlock, self).__init__()\n",
    "            self.attention = Expert.MultiHeadAttention(embed_size, heads)\n",
    "            self.norm1 = nn.LayerNorm(embed_size)\n",
    "            self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                Expert.LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, value, key, query, mask):\n",
    "            attention = self.attention(value, key, query, mask)\n",
    "            x = self.dropout(self.norm1(attention + query))\n",
    "            forward = self.feed_forward(x)\n",
    "            out = self.dropout(self.norm2(forward + x))\n",
    "            return out\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "            super(Expert.LanguageModelDecoder, self).__init__()\n",
    "            self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "            # Adding BatchNorm layers\n",
    "            self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "            self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    Expert.TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # QLORA layers\n",
    "            self.qlora_feed_forward = nn.Sequential(\n",
    "                Expert.QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "            self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "            self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, trg_mask):\n",
    "            N, seq_length = x.shape\n",
    "            if seq_length > self.position_embedding.num_embeddings:\n",
    "                raise ValueError(f\"Sequence length {seq_length} exceeds maximum allowed {self.position_embedding.num_embeddings}\")\n",
    "            positions = torch.arange(0, seq_length).expand(N, seq_length).to(config.device)\n",
    "            x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "            print(f\"Max position index: {positions.max().item()}, Position Embedding Size: {self.position_embedding.num_embeddings}\")\n",
    "\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn1(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, trg_mask)\n",
    "                if self.use_qlora:\n",
    "                    x = self.qlora_feed_forward(x)\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            out = self.fc_out(x)\n",
    "            #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "            return out\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.use_qlora = use_qlora\n",
    "\n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.vocab_size = config.vocab_size  # Use vocab_size from ExpertConfig\n",
    "            self.embed_size = config.embed_size  # Use embed_size from ExpertConfig\n",
    "            self.num_layers = config.num_layers  # Use num_layers from ExpertConfig\n",
    "            self.forward_expansion = config.forward_expansion  # Use forward_expansion from ExpertConfig\n",
    "            self.heads = config.heads  # Use heads from ExpertConfig\n",
    "            self.dropout = config.dropout  # Use dropout from ExpertConfig\n",
    "            self.max_length = config.max_length  # Use max_length from ExpertConfig\n",
    "            self.rank = config.rank  # Use rank from ExpertConfig\n",
    "            self.tokenizer_name = config.tokenizer_name  # Use tokenizer_name from ExpertConfig\n",
    "\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "            self.decoder = Expert.LanguageModelDecoder(\n",
    "                self.vocab_size,\n",
    "                self.embed_size,\n",
    "                self.num_layers,\n",
    "                self.heads,\n",
    "                self.forward_expansion,\n",
    "                self.dropout,\n",
    "                self.max_length,\n",
    "                self.rank,\n",
    "            )\n",
    "\n",
    "        def forward(self, trg, attention_mask=None):  # Remove attention_mask here since it's not used\n",
    "            print(f\"Input shape to LanguageModelTransformer: {trg.shape}\")\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            out = self.decoder(trg, trg_mask)  # Do not pass attention_mask here\n",
    "            print(f\"Language Model Transformer out shape: {out.shape}\")\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len).to(trg.device)\n",
    "            return trg_mask\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = [self.tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            return response\n",
    "\n",
    "    ###############################\n",
    "    # DPO\n",
    "    class DPO(nn.Module):\n",
    "        def __init__(self, language_model, device, embed_size):\n",
    "            super(Expert.DPO, self).__init__()\n",
    "            self.language_model = language_model\n",
    "            self.device = device\n",
    "            # Assuming embed_size is accessible and correct\n",
    "            self.projection = nn.Linear(language_model.vocab_size, embed_size)  # Project from vocab_size to embed_size\n",
    "            self.classifier = nn.Linear(embed_size, 2)  # Assuming embed_size is accessible\n",
    "\n",
    "        def forward(self, input_ids_question, input_ids_chosen=None, input_ids_rejected=None, labels=None):\n",
    "            combined_input_ids = torch.cat((input_ids_question, input_ids_chosen, input_ids_rejected), dim=1)\n",
    "\n",
    "            # Assuming combined_input_ids has shape [batch_size, sequence_length]\n",
    "            logits = self.language_model(combined_input_ids)  # Output shape: [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "            # Project logits to embedding space before pooling\n",
    "            projected_logits = self.projection(logits)  # New shape: [batch_size, sequence_length, embed_size]\n",
    "            \n",
    "            # Apply global mean pooling across the sequence length dimension\n",
    "            pooled_logits = projected_logits.mean(dim=1)  # New shape: [batch_size, embed_size]\n",
    "\n",
    "            # Pass the pooled representation through the classifier\n",
    "            predictions = self.classifier(pooled_logits)  # New shape: [batch_size, 2]\n",
    "\n",
    "            # Calculate loss if labels are provided\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(predictions, labels)\n",
    "\n",
    "            return predictions, loss\n",
    "\n",
    "\n",
    "    def __init__(self, config: ExpertConfig):\n",
    "        super().__init__()\n",
    "        # Validate configuration\n",
    "        config.validate()\n",
    "\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(\n",
    "            config.seq_len, \n",
    "            config.head_dim, \n",
    "            config.block_size, \n",
    "            config.sparsity_factor\n",
    "        )\n",
    "\n",
    "        # Inside the Expert class definition\n",
    "        self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "\n",
    "        self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "\n",
    "        # Corrected instantiation of SimplifiedLanguageModelMAMBA\n",
    "        self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "\n",
    "        # Now initialize DPO with the language model transformer\n",
    "        self.transformer_dpo = Expert.DPO(self.lmt, config.device, config.embed_size).to(config.device)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(config.input_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = Expert.SwitchRouter(config)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = Expert.QLORALayer(config.input_dim, config.input_dim, config.rank)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, aux_loss = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "        \"\"\"\n",
    "        Calculate a new alpha value based on the current loss.\n",
    "        \"\"\"\n",
    "        if current_loss >= initial_loss:\n",
    "            return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "        loss_ratio = current_loss / initial_loss\n",
    "        alpha_range = initial_alpha - final_alpha\n",
    "        new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "        return new_alpha  \n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Linear warmup with cosine decay\n",
    "        scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "        return optimizer, scheduler\n",
    "    ###############################\n",
    "    # TRAINING METHODS\n",
    "\n",
    "    # DPO Training\n",
    "    def train_dpo(self, train_loader, optimizer, config, save_path):\n",
    "            self.train()  # Set the model to training mode\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                input_ids_question = batch['input_ids_question'].to(config.device)\n",
    "                input_ids_chosen = batch['input_ids_chosen'].to(config.device)\n",
    "                input_ids_rejected = batch['input_ids_rejected'].to(config.device)\n",
    "                labels = batch['labels'].to(config.device)\n",
    "                print(f\"train_dpo input_ids_question: {input_ids_question.shape}\")\n",
    "                print(f\"train_dpo input_ids_chosen: {input_ids_chosen.shape}\")\n",
    "                print(f\"train_dpo input_ids_rejected: {input_ids_rejected.shape}\")\n",
    "                print(f\"train_dpo labels: {labels.shape}\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                logit, loss = self.transformer_dpo(input_ids_question, input_ids_chosen, input_ids_rejected, labels)\n",
    "                print(f\"Logits shape: {logit.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            print(f\"Training complete. Average Loss: {average_loss}\")\n",
    "            \n",
    "            # Save the model\n",
    "            torch.save(self.transformer_dpo.state_dict(), save_path)\n",
    "\n",
    "            return average_loss\n",
    "\n",
    "    # RAG Training\n",
    "    def train_language_model_rag(self, model, save_path, train_loader, device, num_epochs=5, lr=1e-8, weight_decay=1e-4):\n",
    "        # Enable QLORA during training\n",
    "        model.decoder.toggle_qlora(True)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "\n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                targets = batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        # Toggle QLORA off after training\n",
    "        model.decoder.toggle_qlora(False)\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"Training Complete\")\n",
    "        return model, average_loss\n",
    "\n",
    "    # DPR Training\n",
    "    def train_dpr_encoders(self, train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs , context_save_path, question_save_path):\n",
    "        loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(train_data[\"queries\"])):\n",
    "                query = train_data[\"queries\"][i]\n",
    "                context = train_data[\"contexts\"][i]\n",
    "\n",
    "                # Ensure query is a string\n",
    "                if not isinstance(query, str):\n",
    "                    raise ValueError(\"Query must be a string.\")\n",
    "                tokenized_query = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                # Ensure context is a string\n",
    "                if isinstance(context, dict):\n",
    "                    # If context is a dictionary, extract the text content. This is a placeholder and might need adjustment.\n",
    "                    context_text = context.get(\"text\", \"\")\n",
    "                elif isinstance(context, str):\n",
    "                    context_text = context\n",
    "                else:\n",
    "                    raise ValueError(\"Context must be a string or a dictionary containing a text field.\")\n",
    "                tokenized_context = tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                question_embeddings = question_encoder(input_ids=tokenized_query['input_ids'], attention_mask=tokenized_query['attention_mask'])\n",
    "                context_embeddings = context_encoder(input_ids=tokenized_context['input_ids'], attention_mask=tokenized_context['attention_mask'])\n",
    "\n",
    "                # The labels tensor should have the same first dimension size as the input tensors\n",
    "                labels = torch.tensor([1.0] * question_embeddings.size(0), dtype=torch.float).to(question_embeddings.device)\n",
    "\n",
    "                loss = loss_function(question_embeddings, context_embeddings, labels)\n",
    "\n",
    "                optimizer_context.zero_grad()\n",
    "                optimizer_question.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_context.step()\n",
    "                optimizer_question.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_data['queries'])}\")\n",
    "        average_loss = total_loss / len(train_data['queries'])\n",
    "        torch.save(context_encoder.state_dict(), context_save_path)\n",
    "        torch.save(question_encoder.state_dict(), question_save_path)\n",
    "        return (context_encoder, question_encoder), average_loss\n",
    "       \n",
    "    # LMT Training\n",
    "    def train_language_model_transformer(self, train_loader, device, vocab_size, save_path):\n",
    "        model = self.lmt\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "        num_epochs = 5\n",
    "        \n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            model.decoder.toggle_qlora(True)\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs, targets = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        return model, average_loss\n",
    "\n",
    "    # MAMBA Training\n",
    "    def train_mamba(self, train_loader, num_epochs, config):\n",
    "            # Initialize the optimizer and scheduler with MAMBA model parameters\n",
    "            optimizer, scheduler = self.setup_optimizer(self.mamba, \n",
    "                                                        config.mamba_learning_rate, \n",
    "                                                        config.weight_decay, \n",
    "                                                        config.warmup_steps, \n",
    "                                                        config.total_mamba_steps)\n",
    "\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            progress_bar = tqdm(range(num_epochs))\n",
    "\n",
    "            for epoch in progress_bar:\n",
    "                self.mamba.train()\n",
    "                total_loss = 0\n",
    "\n",
    "                for batch in train_loader:\n",
    "                    input_values, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "                    input_values = input_values.to(config.device)\n",
    "                    attention_mask = attention_mask.to(config.device)\n",
    "                    labels = labels.to(config.device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass through MAMBA model\n",
    "                    outputs = self.mamba(input_values, attention_mask)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = loss_fn(outputs.view(-1, config.vocab_size), labels.view(-1))\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Clip gradients and perform an optimization step\n",
    "                    torch.nn.utils.clip_grad_norm_(self.mamba.parameters(), config.clip_gradient)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save the trained MAMBA model\n",
    "            torch.save(self.mamba.state_dict(), config.mamba_model_path)\n",
    "            print(f\"MAMBA Training Complete. Model saved to {config.mamba_model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Full Expert Training\n",
    "    def train_expert(self, train_loader, train_data, optimizer, main_loss_function, aux_loss_weight, device,save_path, accumulation_steps=4, num_epochs=5):\n",
    "            self.train()  # Set the model to training mode\n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0\n",
    "                optimizer.zero_grad()  # Initialize gradients to zero\n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "                    inputs, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "\n",
    "                    # Calculate start and end indices for current batch in train_data\n",
    "                    start_idx = batch_idx * batch['input_ids'].size(0)\n",
    "                    end_idx = start_idx + batch['input_ids'].size(0)\n",
    "\n",
    "                    # Extract current_queries and current_contexts for the batch\n",
    "                    current_queries = train_data['queries'][start_idx:end_idx]\n",
    "                    current_contexts = train_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "                    # Call to the model forward function\n",
    "                    outputs, aux_loss = self(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "                    # Calculate loss and accumulate\n",
    "                    main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                    loss = (main_loss + aux_loss_weight * aux_loss) / accumulation_steps\n",
    "                    loss.backward()\n",
    "\n",
    "                    if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    total_loss += loss.item() * accumulation_steps  # Scale back up\n",
    "\n",
    "                average_loss = total_loss / len(train_loader)\n",
    "                print(f'End of Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            torch.save(self.state_dict(), save_path)\n",
    "            return self, average_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Language Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Assuming the Expert class and ExpertConfig class have already been defined\n",
    "\n",
    "# Initialize configuration\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "\n",
    "# Initialize Expert system\n",
    "expert_system = Expert(config)\n",
    "\n",
    "# Load Wikipedia dataset and preprocess\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:1%]\")  # Using 1% of the data for demonstration\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=config.max_length)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])  # Remove original text to only keep tokenized versions\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(tokenized_datasets, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define save path for the trained model\n",
    "save_path = 'D:/EXPERT_WEIGHTS/lmt_expert_trained.pth'\n",
    "\n",
    "\n",
    "# Train the LMT sub-model within the Expert system\n",
    "trained_model, average_loss = expert_system.train_language_model_transformer(\n",
    "    train_loader=train_loader, \n",
    "    device=config.device, \n",
    "    vocab_size=config.vocab_size, \n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "print(f\"Training complete. Model saved to {save_path}. Average Loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Training transformer_with_dpo\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def preprocess_dpo_data(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    vocab_size = len(tokenizer.vocab)  # Make sure this matches your embedding layer's vocab size\n",
    "\n",
    "    # Define max sequence length\n",
    "    max_seq_length = 512 // 3\n",
    "\n",
    "    # Tokenize 'question', 'chosen', and 'rejected' fields\n",
    "    tokenized_questions = tokenizer(examples['question'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_chosen = tokenizer(examples['chosen'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_rejected = tokenizer(examples['rejected'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "\n",
    "    # Prepare the labels: 1 for 'chosen' and 0 for 'rejected'\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(len(examples['question']))]\n",
    "\n",
    "    return {\n",
    "        'input_ids_question': tokenized_questions['input_ids'],\n",
    "        'attention_mask_question': tokenized_questions['attention_mask'],\n",
    "        'input_ids_chosen': tokenized_chosen['input_ids'],\n",
    "        'attention_mask_chosen': tokenized_chosen['attention_mask'],\n",
    "        'input_ids_rejected': tokenized_rejected['input_ids'],\n",
    "        'attention_mask_rejected': tokenized_rejected['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Load the DPO dataset from Hugging Face\n",
    "dpo_dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "\n",
    "# Apply the preprocessing to the dataset\n",
    "dpo_dataset = dpo_dataset.map(preprocess_dpo_data, batched=True)\n",
    "\n",
    "\n",
    "# You can convert to PyTorch tensors after processing\n",
    "dpo_dataset.set_format(type='torch', columns=['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(dpo_dataset['train'], batch_size=2, shuffle=True)\n",
    "\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "expert_model = Expert(config)\n",
    "\n",
    "model_vocab_size = expert_model.lmt.decoder.word_embedding.num_embeddings\n",
    "print(f\"Model vocab size: {model_vocab_size}\")\n",
    "print(f\"Model embedding size: {expert_model.lmt.decoder.word_embedding.embedding_dim}\")\n",
    "print(f\"Configured max length: {config.max_length}\")\n",
    "\n",
    "optimizer = AdamW(expert_model.parameters(), lr=1e-5)\n",
    "save_path = 'D:/EXPERT_WEIGHTS/dpo_model.pth'\n",
    "\n",
    "# Train the DPO model\n",
    "label_column = 'labels'\n",
    "input_columns = ['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
    "# Assuming expert_model is an instance of Expert\n",
    "avg_loss = expert_model.train_dpo(train_loader, optimizer, config, save_path)\n",
    "\n",
    "# Save the model\n",
    "torch.save(expert_model.transformer_dpo.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Fast WordPiece Tokenization\n",
      "Decoded: Fast  Fast  tokenization\n",
      "\n",
      "Original: Xinying Song† Alex Salcianu† Yang Song‡∗ Dave Dopson† Denny Zhou†\n",
      "Decoded: Fast  Fast  †  Fast  salcianu  †  Fast  Fast  ‡  ∗  Fast  Fast  †  Fast  Fast  †\n",
      "\n",
      "Original: †Google Research, Mountain View, CA\n",
      "Decoded: †  google  Fast  ,  mountain  Fast  ,  Fast\n",
      "\n",
      "Original: †{xysong,salcianu,ddopson,dennyzhou}@google.com\n",
      "Decoded: †  {  xysong  ,  salcianu  ,  ddopson  ,  dennyzhou  }  @  google  .  com\n",
      "\n",
      "Original: ‡Kuaishou Technology, Beijing, China\n",
      "Decoded: ‡  kuaishou  Fast  ,  Fast  ,  Fast\n",
      "\n",
      "Original: ‡yangsong@kuaishou.com\n",
      "Decoded: ‡  yangsong  @  kuaishou  .  com\n",
      "\n",
      "Original: Abstract\n",
      "Decoded: Fast\n",
      "\n",
      "Original: Tokenization is a fundamental preprocessing\n",
      "Decoded: tokenization  is  a  fundamental  preprocessing\n",
      "\n",
      "Original: step for almost all NLP tasks. In this paper,\n",
      "Decoded: step  for  almost  all  Fast  tasks  .  in  this  paper  ,\n",
      "\n",
      "Original: we propose efficient algorithms for the WordPiece tokenization used in BERT, from singleword tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a longest-matchfirst strategy, known as maximum matching.\n",
      "Decoded: we  propose  efficient  algorithms  for  the  Fast  tokenization  used  in  bert  ,  from  singleword  tokenization  to  general  text  (  e  .  g  .  ,  sentence  )  tokenization  .  when  tokenizing  a  single  word  ,  Fast  uses  a  longest  -  matchfirst  strategy  ,  known  as  maximum  matching  .\n",
      "\n",
      "Original: The best known algorithms so far are 푂(푛\n",
      "Decoded: the  best  known  algorithms  so  far  are  푂  (  푛\n",
      "\n",
      "Original: 2\n",
      "Decoded: 2\n",
      "\n",
      "Original: )\n",
      "Decoded: )\n",
      "\n",
      "Original: (where 푛 is the input length) or 푂(푛푚) (where\n",
      "Decoded: (  where  푛  is  the  input  length  )  or  푂  (  푛푚  )  (  where\n",
      "\n",
      "Original: 푚 is the maximum vocabulary token length).\n",
      "Decoded: 푚  is  the  maximum  vocabulary  token  length  )  .\n",
      "\n",
      "Original: We propose a novel algorithm whose tokenization complexity is strictly 푂(푛). Our method is\n",
      "Decoded: we  propose  a  novel  algorithm  whose  tokenization  complexity  is  strictly  푂  (  푛  )  .  our  method  is\n",
      "\n",
      "Original: inspired by the Aho-Corasick algorithm. We\n",
      "Decoded: inspired  by  the  Fast  -  Fast  algorithm  .  we\n",
      "\n",
      "Original: introduce additional linkages on top of the trie\n",
      "Decoded: introduce  additional  linkages  on  top  of  the  trie\n",
      "\n",
      "Original: built from the vocabulary, allowing smart transitions when the trie matching cannot continue.\n",
      "Decoded: built  from  the  vocabulary  ,  allowing  smart  transitions  when  the  trie  matching  cannot  continue  .\n",
      "\n",
      "Original: For general text, we further propose an algorithm that combines pre-tokenization (splitting\n",
      "Decoded: for  general  text  ,  we  further  propose  an  algorithm  that  combines  pre  -  tokenization  (  splitting\n",
      "\n",
      "Original: the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our method is 8.2x faster\n",
      "Decoded: the  text  into  words  )  and  our  linear  -  time  Fast  method  into  a  single  pass  .  experimental  results  show  that  our  method  is  8  .  2x  faster\n",
      "\n",
      "Original: than HuggingFace Tokenizers and 5.1x faster\n",
      "Decoded: than  Fast  Fast  and  5  .  1x  faster\n",
      "\n",
      "Original: than TensorFlow Text on average for general\n",
      "Decoded: than  tensorflow  text  on  average  for  general\n",
      "\n",
      "Original: text tokenization.\n",
      "Decoded: text  tokenization  .\n",
      "\n",
      "Original: 1 Introduction\n",
      "Decoded: 1  Fast\n",
      "\n",
      "Original: Tokenization is the process of splitting text into\n",
      "Decoded: tokenization  is  the  process  of  splitting  text  into\n",
      "\n",
      "Original: smaller units called tokens (e.g., words). It is a\n",
      "Decoded: smaller  units  called  tokens  (  e  .  g  .  ,  words  )  .  it  is  a\n",
      "\n",
      "Original: fundamental preprocessing step for almost all NLP\n",
      "Decoded: fundamental  preprocessing  step  for  almost  all  Fast\n",
      "\n",
      "Original: applications: sentiment analysis, question answering, machine translation, information retrieval, etc.\n",
      "Decoded: applications  :  sentiment  analysis  ,  question  answering  ,  machine  translation  ,  information  retrieval  ,  etc  .\n",
      "\n",
      "Original: Modern NLP models like BERT (Devlin et al.,\n",
      "Decoded: modern  Fast  models  like  bert  (  Fast  et  al  .  ,\n",
      "\n",
      "Original: 2019), GPT-3 (Brown et al., 2020), and XLNet (Yang et al., 2019) tokenize text into subword units (Schuster and Nakajima, 2012; Sennrich\n",
      "Decoded: 2019  )  ,  Fast  -  3  (  Fast  et  al  .  ,  2020  )  ,  and  Fast  (  Fast  et  al  .  ,  2019  )  tokenize  text  into  subword  units  (  Fast  and  Fast  ,  2012  ;  Fast\n",
      "\n",
      "Original: et al., 2016; Kudo, 2018). As a midpoint between\n",
      "Decoded: et  al  .  ,  2016  ;  Fast  ,  2018  )  .  as  a  midpoint  between\n",
      "\n",
      "Original: words and characters, subword units retain linguistic meaning (like morphemes), while alleviating\n",
      "Decoded: words  and  characters  ,  subword  units  retain  linguistic  meaning  (  like  morphemes  )  ,  while  alleviating\n",
      "\n",
      "Original: out-of-vocabulary situations even with a relatively\n",
      "Decoded: out  -  of  -  vocabulary  situations  even  with  a  relatively\n",
      "\n",
      "Original: small-size vocabulary.\n",
      "Decoded: small  -  size  vocabulary  .\n",
      "\n",
      "Original: ∗ Research conducted while working at Google.\n",
      "Decoded: ∗  Fast  conducted  while  working  at  google  .\n",
      "\n",
      "Original: In this paper, we propose efficient algorithms\n",
      "Decoded: in  this  paper  ,  we  propose  efficient  algorithms\n",
      "\n",
      "Original: for WordPiece, the subword tokenization used in\n",
      "Decoded: for  Fast  ,  the  subword  tokenization  used  in\n",
      "\n",
      "Original: BERT (Devlin et al., 2019). Given Unicode text\n",
      "Decoded: bert  (  Fast  et  al  .  ,  2019  )  .  given  Fast  text\n",
      "\n",
      "Original: that has already been cleaned up and normalized,\n",
      "Decoded: that  has  already  been  cleaned  up  and  normalized  ,\n",
      "\n",
      "Original: WordPiece has two steps: (1) pre-tokenize the text\n",
      "Decoded: Fast  has  two  steps  :  (  1  )  pre  -  tokenize  the  text\n",
      "\n",
      "Original: into words (by splitting on punctuation and whitespaces), and (2) tokenize each word into wordpieces.\n",
      "Decoded: into  words  (  by  splitting  on  punctuation  and  whitespaces  )  ,  and  (  2  )  tokenize  each  word  into  wordpieces  .\n",
      "\n",
      "Original: For single-word tokenization, WordPiece uses\n",
      "Decoded: for  single  -  word  tokenization  ,  Fast  uses\n",
      "\n",
      "Original: a greedy longest-match-first strategy: iteratively\n",
      "Decoded: a  greedy  longest  -  match  -  first  strategy  :  iteratively\n",
      "\n",
      "Original: pick the longest prefix of the remaining text that\n",
      "Decoded: pick  the  longest  prefix  of  the  remaining  text  that\n",
      "\n",
      "Original: matches a vocabulary token. This is well-known as\n",
      "Decoded: matches  a  vocabulary  token  .  this  is  well  -  known  as\n",
      "\n",
      "Original: Maximum Matching or MaxMatch (Palmer, 2000),\n",
      "Decoded: maximum  matching  or  Fast  (  Fast  ,  2000  )  ,\n",
      "\n",
      "Original: which has also been used for Chinese word segmentation since 1980s (Liu and Liang, 1986).\n",
      "Decoded: which  has  also  been  used  for  Fast  word  segmentation  since  1980s  (  Fast  and  Fast  ,  1986  )  .\n",
      "\n",
      "Original: Despite its wide use in NLP for decades, to the\n",
      "Decoded: despite  its  wide  use  in  Fast  for  decades  ,  to  the\n",
      "\n",
      "Original: best of our knowledge, the most efficient MaxMatch algorithms so far are 푂(푛\n",
      "Decoded: best  of  our  knowledge  ,  the  most  efficient  Fast  algorithms  so  far  are  푂  (  푛\n",
      "\n",
      "Original: 2\n",
      "Decoded: 2\n",
      "\n",
      "Original: ) (where 푛 is the\n",
      "Decoded: )  (  where  푛  is  the\n",
      "\n",
      "Original: input word length) or 푂(푛푚) (where 푚 is the maximum vocabulary token length) (see Section 2).\n",
      "Decoded: input  word  length  )  or  푂  (  푛푚  )  (  where  푚  is  the  maximum  vocabulary  token  length  )  (  see  section  2  )  .\n",
      "\n",
      "Original: It’s worth noting that the latter has a vocabularyspecific multiplicative factor 푚, which can be large\n",
      "Decoded: it  ’  s  worth  noting  that  the  latter  has  a  vocabularyspecific  multiplicative  factor  푚  ,  which  can  be  large\n",
      "\n",
      "Original: when the vocabulary contains long words.\n",
      "Decoded: when  the  vocabulary  contains  long  words  .\n",
      "\n",
      "Original: We propose LinMaxMatch, a novel MaxMatch\n",
      "Decoded: we  propose  Fast  ,  a  novel  Fast\n",
      "\n",
      "Original: algorithm for WordPiece tokenization, whose\n",
      "Decoded: algorithm  for  Fast  tokenization  ,  whose\n",
      "\n",
      "Original: tokenization time is strictly 푂(푛) without any\n",
      "Decoded: tokenization  time  is  strictly  푂  (  푛  )  without  any\n",
      "\n",
      "Original: vocabulary-specific multiplicative factors. Inspired\n",
      "Decoded: vocabulary  -  specific  multiplicative  factors  .  inspired\n",
      "\n",
      "Original: by the Aho-Corasick algorithm (Aho and Corasick, 1975), we organize vocabulary tokens in a\n",
      "Decoded: by  the  Fast  -  Fast  algorithm  (  Fast  and  Fast  ,  1975  )  ,  we  organize  vocabulary  tokens  in  a\n",
      "\n",
      "Original: trie (Fredkin, 1960) and introduce precomputed\n",
      "Decoded: trie  (  Fast  ,  1960  )  and  introduce  precomputed\n",
      "\n",
      "Original: failure links and failure pops. During tokenization, if an input character does not match any trie\n",
      "Decoded: failure  links  and  failure  pops  .  during  tokenization  ,  if  an  input  character  does  not  match  any  trie\n",
      "\n",
      "Original: edge, we perform smart transitions to avoid backtracking to earlier input characters. This involves\n",
      "Decoded: edge  ,  we  perform  smart  transitions  to  avoid  backtracking  to  earlier  input  characters  .  this  involves\n",
      "\n",
      "Original: collecting the recognized tokens (i.e., failure pops)\n",
      "Decoded: collecting  the  recognized  tokens  (  i  .  e  .  ,  failure  pops  )\n",
      "\n",
      "Original: and moving to a trie node (via the failure link),\n",
      "Decoded: and  moving  to  a  trie  node  (  via  the  failure  link  )  ,\n",
      "\n",
      "Original: from where we continue to match the same character (Section 3).\n",
      "Decoded: from  where  we  continue  to  match  the  same  character  (  section  3  )  .\n",
      "\n",
      "Original: For general text tokenization, referred to as\n",
      "Decoded: for  general  text  tokenization  ,  referred  to  as\n",
      "\n",
      "Original: end-to-end tokenization in this paper, we propose\n",
      "Decoded: end  -  to  -  end  tokenization  in  this  paper  ,  we  propose\n",
      "\n",
      "Original: E2E WordPiece, an end-to-end algorithm that combines pre-tokenization and WordPiece tokenization\n",
      "Decoded: Fast  Fast  ,  an  end  -  to  -  end  algorithm  that  combines  pre  -  tokenization  and  Fast  tokenization\n",
      "\n",
      "Original: arXiv:2012.15524v3 [cs.CL] 5 Oct 2021\n",
      "Decoded: Fast  :  2012  .  15524v3  [  cs  .  Fast  ]  5  Fast  2021\n",
      "\n",
      "Original: into a single, linear-time pass (Section 4).\n",
      "Decoded: into  a  single  ,  linear  -  time  pass  (  section  4  )  .\n",
      "\n",
      "Original: Experimental results show that our method is\n",
      "Decoded: experimental  results  show  that  our  method  is\n",
      "\n",
      "Original: 8.2x faster than HuggingFace Tokenizers (HuggingFace, 2020) and 5.1x faster than TensorFlow Text (Google, 2020) on average for general\n",
      "Decoded: 8  .  2x  faster  than  Fast  Fast  (  Fast  ,  2020  )  and  5  .  1x  faster  than  tensorflow  text  (  google  ,  2020  )  on  average  for  general\n",
      "\n",
      "Original: text tokenization (Section 5).\n",
      "Decoded: text  tokenization  (  section  5  )  .\n",
      "\n",
      "Original: Although tokenization is relatively faster than\n",
      "Decoded: although  tokenization  is  relatively  faster  than\n",
      "\n",
      "Original: other steps, it’s still worth improving the performance: Tokenization is a prerequisite step for\n",
      "Decoded: other  steps  ,  it  ’  s  still  worth  improving  the  performance  :  tokenization  is  a  prerequisite  step  for\n",
      "\n",
      "Original: almost all NLP tasks, and any improvement on\n",
      "Decoded: almost  all  Fast  tasks  ,  and  any  improvement  on\n",
      "\n",
      "Original: its efficiency helps reduce the latency of the entire inference. One potential impact of the work,\n",
      "Decoded: its  efficiency  helps  reduce  the  latency  of  the  entire  inference  .  one  potential  impact  of  the  work  ,\n",
      "\n",
      "Original: for example, is on mobile NLP applications. Ondevice models are generally highly optimized for\n",
      "Decoded: for  example  ,  is  on  mobile  Fast  applications  .  Fast  models  are  generally  highly  optimized  for\n",
      "\n",
      "Original: reducing latency, e.g., by distilling or compressing\n",
      "Decoded: reducing  latency  ,  e  .  g  .  ,  by  distilling  or  compressing\n",
      "\n",
      "Original: larger models. Thus, the impact of tokenization\n",
      "Decoded: larger  models  .  thus  ,  the  impact  of  tokenization\n",
      "\n",
      "Original: can be significant here. Another impact is on aggregate computational savings for Web services\n",
      "Decoded: can  be  significant  here  .  another  impact  is  on  aggregate  computational  savings  for  Fast  services\n",
      "\n",
      "Original: like Google, Facebook, Twitter, etc. For example,\n",
      "Decoded: like  google  ,  Fast  ,  Fast  ,  etc  .  for  example  ,\n",
      "\n",
      "Original: Google uses BERT to power its Web search nowadays.1 Google serves billions of search queries per\n",
      "Decoded: google  uses  bert  to  power  its  Fast  search  nowadays  .  1  google  serves  billions  of  search  queries  per\n",
      "\n",
      "Original: day, and it processes hundreds of trillions of Web\n",
      "Decoded: day  ,  and  it  processes  hundreds  of  trillions  of  Fast\n",
      "\n",
      "Original: pages in index building. By employing a faster\n",
      "Decoded: pages  in  index  building  .  by  employing  a  faster\n",
      "\n",
      "Original: tokenization system, the aggregate computational\n",
      "Decoded: tokenization  system  ,  the  aggregate  computational\n",
      "\n",
      "Original: savings would be material, which also benefits the\n",
      "Decoded: savings  would  be  material  ,  which  also  benefits  the\n",
      "\n",
      "Original: environment (for less power consumption).\n",
      "Decoded: environment  (  for  less  power  consumption  )  .\n",
      "\n",
      "Original: This paper also makes a theoretical contribution. The proposed LinMaxMatch algorithm solves\n",
      "Decoded: this  paper  also  makes  a  theoretical  contribution  .  the  proposed  Fast  algorithm  solves\n",
      "\n",
      "Original: the decades-old MaxMatch problem in the optimal\n",
      "Decoded: the  decades  -  old  Fast  problem  in  the  optimal\n",
      "\n",
      "Original: 푂(푛) time, and the idea is applicable to other string\n",
      "Decoded: 푂  (  푛  )  time  ,  and  the  idea  is  applicable  to  other  string\n",
      "\n",
      "Original: matching or rewriting problems (Section 3.6).\n",
      "Decoded: matching  or  rewriting  problems  (  section  3  .  6  )  .\n",
      "\n",
      "Original: The code will be available at https://www.\n",
      "Decoded: the  code  will  be  available  at  https  :  /  /  www  .\n",
      "\n",
      "Original: tensorflow.org/text.\n",
      "Decoded: tensorflow  .  org  /  text  .\n",
      "\n",
      "Original: 2 Related Work\n",
      "Decoded: 2  related  work\n",
      "\n",
      "Original: Maximum Matching (or MaxMatch) has been used\n",
      "Decoded: maximum  matching  (  or  Fast  )  has  been  used\n",
      "\n",
      "Original: for Chinese word segmentation (CWS) since the\n",
      "Decoded: for  Fast  word  segmentation  (  Fast  )  since  the\n",
      "\n",
      "Original: 1980s (Liu and Liang, 1986; Palmer, 2000). Recent\n",
      "Decoded: 1980s  (  Fast  and  Fast  ,  1986  ;  Fast  ,  2000  )  .  recent\n",
      "\n",
      "Original: CWS work focuses on machine learning-based segmentation approaches, but MaxMatch remains a\n",
      "Decoded: Fast  work  focuses  on  machine  learning  -  based  segmentation  approaches  ,  but  Fast  remains  a\n",
      "\n",
      "Original: Fast WordPiece Tokenization\n",
      "Decoded: Fast  Fast  tokenization\n",
      "\n",
      "Original: Xinying Song† Alex Salcianu† Yang Song‡∗ Dave Dopson† Denny Zhou†\n",
      "Decoded: Fast  Fast  †  Fast  salcianu  †  Fast  Fast  ‡  ∗  Fast  Fast  †  Fast  Fast  †\n",
      "\n",
      "Original: †Google Research, Mountain View, CA\n",
      "Decoded: †  google  Fast  ,  mountain  Fast  ,  Fast\n",
      "\n",
      "Original: †{xysong,salcianu,ddopson,dennyzhou}@google.com\n",
      "Decoded: †  {  xysong  ,  salcianu  ,  ddopson  ,  dennyzhou  }  @  google  .  com\n",
      "\n",
      "Original: ‡Kuaishou Technology, Beijing, China\n",
      "Decoded: ‡  kuaishou  Fast  ,  Fast  ,  Fast\n",
      "\n",
      "Original: ‡yangsong@kuaishou.com\n",
      "Decoded: ‡  yangsong  @  kuaishou  .  com\n",
      "\n",
      "Original: Abstract\n",
      "Decoded: Fast\n",
      "\n",
      "Original: Tokenization is a fundamental preprocessing\n",
      "Decoded: tokenization  is  a  fundamental  preprocessing\n",
      "\n",
      "Original: step for almost all NLP tasks. In this paper,\n",
      "Decoded: step  for  almost  all  Fast  tasks  .  in  this  paper  ,\n",
      "\n",
      "Original: we propose efficient algorithms for the WordPiece tokenization used in BERT, from singleword tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a longest-matchfirst strategy, known as maximum matching.\n",
      "Decoded: we  propose  efficient  algorithms  for  the  Fast  tokenization  used  in  bert  ,  from  singleword  tokenization  to  general  text  (  e  .  g  .  ,  sentence  )  tokenization  .  when  tokenizing  a  single  word  ,  Fast  uses  a  longest  -  matchfirst  strategy  ,  known  as  maximum  matching  .\n",
      "\n",
      "Original: The best known algorithms so far are 푂(푛\n",
      "Decoded: the  best  known  algorithms  so  far  are  푂  (  푛\n",
      "\n",
      "Original: 2\n",
      "Decoded: 2\n",
      "\n",
      "Original: )\n",
      "Decoded: )\n",
      "\n",
      "Original: (where 푛 is the input length) or 푂(푛푚) (where\n",
      "Decoded: (  where  푛  is  the  input  length  )  or  푂  (  푛푚  )  (  where\n",
      "\n",
      "Original: 푚 is the maximum vocabulary token length).\n",
      "Decoded: 푚  is  the  maximum  vocabulary  token  length  )  .\n",
      "\n",
      "Original: We propose a novel algorithm whose tokenization complexity is strictly 푂(푛). Our method is\n",
      "Decoded: we  propose  a  novel  algorithm  whose  tokenization  complexity  is  strictly  푂  (  푛  )  .  our  method  is\n",
      "\n",
      "Original: inspired by the Aho-Corasick algorithm. We\n",
      "Decoded: inspired  by  the  Fast  -  Fast  algorithm  .  we\n",
      "\n",
      "Original: introduce additional linkages on top of the trie\n",
      "Decoded: introduce  additional  linkages  on  top  of  the  trie\n",
      "\n",
      "Original: built from the vocabulary, allowing smart transitions when the trie matching cannot continue.\n",
      "Decoded: built  from  the  vocabulary  ,  allowing  smart  transitions  when  the  trie  matching  cannot  continue  .\n",
      "\n",
      "Original: For general text, we further propose an algorithm that combines pre-tokenization (splitting\n",
      "Decoded: for  general  text  ,  we  further  propose  an  algorithm  that  combines  pre  -  tokenization  (  splitting\n",
      "\n",
      "Original: the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our method is 8.2x faster\n",
      "Decoded: the  text  into  words  )  and  our  linear  -  time  Fast  method  into  a  single  pass  .  experimental  results  show  that  our  method  is  8  .  2x  faster\n",
      "\n",
      "Original: than HuggingFace Tokenizers and 5.1x faster\n",
      "Decoded: than  Fast  Fast  and  5  .  1x  faster\n",
      "\n",
      "Original: than TensorFlow Text on average for general\n",
      "Decoded: than  tensorflow  text  on  average  for  general\n",
      "\n",
      "Original: text tokenization.\n",
      "Decoded: text  tokenization  .\n",
      "\n",
      "Original: 1 Introduction\n",
      "Decoded: 1  Fast\n",
      "\n",
      "Original: Tokenization is the process of splitting text into\n",
      "Decoded: tokenization  is  the  process  of  splitting  text  into\n",
      "\n",
      "Original: smaller units called tokens (e.g., words). It is a\n",
      "Decoded: smaller  units  called  tokens  (  e  .  g  .  ,  words  )  .  it  is  a\n",
      "\n",
      "Original: fundamental preprocessing step for almost all NLP\n",
      "Decoded: fundamental  preprocessing  step  for  almost  all  Fast\n",
      "\n",
      "Original: applications: sentiment analysis, question answering, machine translation, information retrieval, etc.\n",
      "Decoded: applications  :  sentiment  analysis  ,  question  answering  ,  machine  translation  ,  information  retrieval  ,  etc  .\n",
      "\n",
      "Original: Modern NLP models like BERT (Devlin et al.,\n",
      "Decoded: modern  Fast  models  like  bert  (  Fast  et  al  .  ,\n",
      "\n",
      "Original: 2019), GPT-3 (Brown et al., 2020), and XLNet (Yang et al., 2019) tokenize text into subword units (Schuster and Nakajima, 2012; Sennrich\n",
      "Decoded: 2019  )  ,  Fast  -  3  (  Fast  et  al  .  ,  2020  )  ,  and  Fast  (  Fast  et  al  .  ,  2019  )  tokenize  text  into  subword  units  (  Fast  and  Fast  ,  2012  ;  Fast\n",
      "\n",
      "Original: et al., 2016; Kudo, 2018). As a midpoint between\n",
      "Decoded: et  al  .  ,  2016  ;  Fast  ,  2018  )  .  as  a  midpoint  between\n",
      "\n",
      "Original: words and characters, subword units retain linguistic meaning (like morphemes), while alleviating\n",
      "Decoded: words  and  characters  ,  subword  units  retain  linguistic  meaning  (  like  morphemes  )  ,  while  alleviating\n",
      "\n",
      "Original: out-of-vocabulary situations even with a relatively\n",
      "Decoded: out  -  of  -  vocabulary  situations  even  with  a  relatively\n",
      "\n",
      "Original: small-size vocabulary.\n",
      "Decoded: small  -  size  vocabulary  .\n",
      "\n",
      "Original: ∗ Research conducted while working at Google.\n",
      "Decoded: ∗  Fast  conducted  while  working  at  google  .\n",
      "\n",
      "Original: In this paper, we propose efficient algorithms\n",
      "Decoded: in  this  paper  ,  we  propose  efficient  algorithms\n",
      "\n",
      "Original: for WordPiece, the subword tokenization used in\n",
      "Decoded: for  Fast  ,  the  subword  tokenization  used  in\n",
      "\n",
      "Original: BERT (Devlin et al., 2019). Given Unicode text\n",
      "Decoded: bert  (  Fast  et  al  .  ,  2019  )  .  given  Fast  text\n",
      "\n",
      "Original: that has already been cleaned up and normalized,\n",
      "Decoded: that  has  already  been  cleaned  up  and  normalized  ,\n",
      "\n",
      "Original: WordPiece has two steps: (1) pre-tokenize the text\n",
      "Decoded: Fast  has  two  steps  :  (  1  )  pre  -  tokenize  the  text\n",
      "\n",
      "Original: into words (by splitting on punctuation and whitespaces), and (2) tokenize each word into wordpieces.\n",
      "Decoded: into  words  (  by  splitting  on  punctuation  and  whitespaces  )  ,  and  (  2  )  tokenize  each  word  into  wordpieces  .\n",
      "\n",
      "Original: For single-word tokenization, WordPiece uses\n",
      "Decoded: for  single  -  word  tokenization  ,  Fast  uses\n",
      "\n",
      "Original: a greedy longest-match-first strategy: iteratively\n",
      "Decoded: a  greedy  longest  -  match  -  first  strategy  :  iteratively\n",
      "\n",
      "Original: pick the longest prefix of the remaining text that\n",
      "Decoded: pick  the  longest  prefix  of  the  remaining  text  that\n",
      "\n",
      "Original: matches a vocabulary token. This is well-known as\n",
      "Decoded: matches  a  vocabulary  token  .  this  is  well  -  known  as\n",
      "\n",
      "Original: Maximum Matching or MaxMatch (Palmer, 2000),\n",
      "Decoded: maximum  matching  or  Fast  (  Fast  ,  2000  )  ,\n",
      "\n",
      "Original: which has also been used for Chinese word segmentation since 1980s (Liu and Liang, 1986).\n",
      "Decoded: which  has  also  been  used  for  Fast  word  segmentation  since  1980s  (  Fast  and  Fast  ,  1986  )  .\n",
      "\n",
      "Original: Despite its wide use in NLP for decades, to the\n",
      "Decoded: despite  its  wide  use  in  Fast  for  decades  ,  to  the\n",
      "\n",
      "Original: best of our knowledge, the most efficient MaxMatch algorithms so far are 푂(푛\n",
      "Decoded: best  of  our  knowledge  ,  the  most  efficient  Fast  algorithms  so  far  are  푂  (  푛\n",
      "\n",
      "Original: 2\n",
      "Decoded: 2\n",
      "\n",
      "Original: ) (where 푛 is the\n",
      "Decoded: )  (  where  푛  is  the\n",
      "\n",
      "Original: input word length) or 푂(푛푚) (where 푚 is the maximum vocabulary token length) (see Section 2).\n",
      "Decoded: input  word  length  )  or  푂  (  푛푚  )  (  where  푚  is  the  maximum  vocabulary  token  length  )  (  see  section  2  )  .\n",
      "\n",
      "Original: It’s worth noting that the latter has a vocabularyspecific multiplicative factor 푚, which can be large\n",
      "Decoded: it  ’  s  worth  noting  that  the  latter  has  a  vocabularyspecific  multiplicative  factor  푚  ,  which  can  be  large\n",
      "\n",
      "Original: when the vocabulary contains long words.\n",
      "Decoded: when  the  vocabulary  contains  long  words  .\n",
      "\n",
      "Original: We propose LinMaxMatch, a novel MaxMatch\n",
      "Decoded: we  propose  Fast  ,  a  novel  Fast\n",
      "\n",
      "Original: algorithm for WordPiece tokenization, whose\n",
      "Decoded: algorithm  for  Fast  tokenization  ,  whose\n",
      "\n",
      "Original: tokenization time is strictly 푂(푛) without any\n",
      "Decoded: tokenization  time  is  strictly  푂  (  푛  )  without  any\n",
      "\n",
      "Original: vocabulary-specific multiplicative factors. Inspired\n",
      "Decoded: vocabulary  -  specific  multiplicative  factors  .  inspired\n",
      "\n",
      "Original: by the Aho-Corasick algorithm (Aho and Corasick, 1975), we organize vocabulary tokens in a\n",
      "Decoded: by  the  Fast  -  Fast  algorithm  (  Fast  and  Fast  ,  1975  )  ,  we  organize  vocabulary  tokens  in  a\n",
      "\n",
      "Original: trie (Fredkin, 1960) and introduce precomputed\n",
      "Decoded: trie  (  Fast  ,  1960  )  and  introduce  precomputed\n",
      "\n",
      "Original: failure links and failure pops. During tokenization, if an input character does not match any trie\n",
      "Decoded: failure  links  and  failure  pops  .  during  tokenization  ,  if  an  input  character  does  not  match  any  trie\n",
      "\n",
      "Original: edge, we perform smart transitions to avoid backtracking to earlier input characters. This involves\n",
      "Decoded: edge  ,  we  perform  smart  transitions  to  avoid  backtracking  to  earlier  input  characters  .  this  involves\n",
      "\n",
      "Original: collecting the recognized tokens (i.e., failure pops)\n",
      "Decoded: collecting  the  recognized  tokens  (  i  .  e  .  ,  failure  pops  )\n",
      "\n",
      "Original: and moving to a trie node (via the failure link),\n",
      "Decoded: and  moving  to  a  trie  node  (  via  the  failure  link  )  ,\n",
      "\n",
      "Original: from where we continue to match the same character (Section 3).\n",
      "Decoded: from  where  we  continue  to  match  the  same  character  (  section  3  )  .\n",
      "\n",
      "Original: For general text tokenization, referred to as\n",
      "Decoded: for  general  text  tokenization  ,  referred  to  as\n",
      "\n",
      "Original: end-to-end tokenization in this paper, we propose\n",
      "Decoded: end  -  to  -  end  tokenization  in  this  paper  ,  we  propose\n",
      "\n",
      "Original: E2E WordPiece, an end-to-end algorithm that combines pre-tokenization and WordPiece tokenization\n",
      "Decoded: Fast  Fast  ,  an  end  -  to  -  end  algorithm  that  combines  pre  -  tokenization  and  Fast  tokenization\n",
      "\n",
      "Original: arXiv:2012.15524v3 [cs.CL] 5 Oct 2021\n",
      "Decoded: Fast  :  2012  .  15524v3  [  cs  .  Fast  ]  5  Fast  2021\n",
      "\n",
      "Original: into a single, linear-time pass (Section 4).\n",
      "Decoded: into  a  single  ,  linear  -  time  pass  (  section  4  )  .\n",
      "\n",
      "Original: Experimental results show that our method is\n",
      "Decoded: experimental  results  show  that  our  method  is\n",
      "\n",
      "Original: 8.2x faster than HuggingFace Tokenizers (HuggingFace, 2020) and 5.1x faster than TensorFlow Text (Google, 2020) on average for general\n",
      "Decoded: 8  .  2x  faster  than  Fast  Fast  (  Fast  ,  2020  )  and  5  .  1x  faster  than  tensorflow  text  (  google  ,  2020  )  on  average  for  general\n",
      "\n",
      "Original: text tokenization (Section 5).\n",
      "Decoded: text  tokenization  (  section  5  )  .\n",
      "\n",
      "Original: Although tokenization is relatively faster than\n",
      "Decoded: although  tokenization  is  relatively  faster  than\n",
      "\n",
      "Original: other steps, it’s still worth improving the performance: Tokenization is a prerequisite step for\n",
      "Decoded: other  steps  ,  it  ’  s  still  worth  improving  the  performance  :  tokenization  is  a  prerequisite  step  for\n",
      "\n",
      "Original: almost all NLP tasks, and any improvement on\n",
      "Decoded: almost  all  Fast  tasks  ,  and  any  improvement  on\n",
      "\n",
      "Original: its efficiency helps reduce the latency of the entire inference. One potential impact of the work,\n",
      "Decoded: its  efficiency  helps  reduce  the  latency  of  the  entire  inference  .  one  potential  impact  of  the  work  ,\n",
      "\n",
      "Original: for example, is on mobile NLP applications. Ondevice models are generally highly optimized for\n",
      "Decoded: for  example  ,  is  on  mobile  Fast  applications  .  Fast  models  are  generally  highly  optimized  for\n",
      "\n",
      "Original: reducing latency, e.g., by distilling or compressing\n",
      "Decoded: reducing  latency  ,  e  .  g  .  ,  by  distilling  or  compressing\n",
      "\n",
      "Original: larger models. Thus, the impact of tokenization\n",
      "Decoded: larger  models  .  thus  ,  the  impact  of  tokenization\n",
      "\n",
      "Original: can be significant here. Another impact is on aggregate computational savings for Web services\n",
      "Decoded: can  be  significant  here  .  another  impact  is  on  aggregate  computational  savings  for  Fast  services\n",
      "\n",
      "Original: like Google, Facebook, Twitter, etc. For example,\n",
      "Decoded: like  google  ,  Fast  ,  Fast  ,  etc  .  for  example  ,\n",
      "\n",
      "Original: Google uses BERT to power its Web search nowadays.1 Google serves billions of search queries per\n",
      "Decoded: google  uses  bert  to  power  its  Fast  search  nowadays  .  1  google  serves  billions  of  search  queries  per\n",
      "\n",
      "Original: day, and it processes hundreds of trillions of Web\n",
      "Decoded: day  ,  and  it  processes  hundreds  of  trillions  of  Fast\n",
      "\n",
      "Original: pages in index building. By employing a faster\n",
      "Decoded: pages  in  index  building  .  by  employing  a  faster\n",
      "\n",
      "Original: tokenization system, the aggregate computational\n",
      "Decoded: tokenization  system  ,  the  aggregate  computational\n",
      "\n",
      "Original: savings would be material, which also benefits the\n",
      "Decoded: savings  would  be  material  ,  which  also  benefits  the\n",
      "\n",
      "Original: environment (for less power consumption).\n",
      "Decoded: environment  (  for  less  power  consumption  )  .\n",
      "\n",
      "Original: This paper also makes a theoretical contribution. The proposed LinMaxMatch algorithm solves\n",
      "Decoded: this  paper  also  makes  a  theoretical  contribution  .  the  proposed  Fast  algorithm  solves\n",
      "\n",
      "Original: the decades-old MaxMatch problem in the optimal\n",
      "Decoded: the  decades  -  old  Fast  problem  in  the  optimal\n",
      "\n",
      "Original: 푂(푛) time, and the idea is applicable to other string\n",
      "Decoded: 푂  (  푛  )  time  ,  and  the  idea  is  applicable  to  other  string\n",
      "\n",
      "Original: matching or rewriting problems (Section 3.6).\n",
      "Decoded: matching  or  rewriting  problems  (  section  3  .  6  )  .\n",
      "\n",
      "Original: The code will be available at https://www.\n",
      "Decoded: the  code  will  be  available  at  https  :  /  /  www  .\n",
      "\n",
      "Original: tensorflow.org/text.\n",
      "Decoded: tensorflow  .  org  /  text  .\n",
      "\n",
      "Original: 2 Related Work\n",
      "Decoded: 2  related  work\n",
      "\n",
      "Original: Maximum Matching (or MaxMatch) has been used\n",
      "Decoded: maximum  matching  (  or  Fast  )  has  been  used\n",
      "\n",
      "Original: for Chinese word segmentation (CWS) since the\n",
      "Decoded: for  Fast  word  segmentation  (  Fast  )  since  the\n",
      "\n",
      "Original: 1980s (Liu and Liang, 1986; Palmer, 2000). Recent\n",
      "Decoded: 1980s  (  Fast  and  Fast  ,  1986  ;  Fast  ,  2000  )  .  recent\n",
      "\n",
      "Original: CWS work focuses on machine learning-based segmentation approaches, but MaxMatch remains a\n",
      "Decoded: Fast  work  focuses  on  machine  learning  -  based  segmentation  approaches  ,  but  Fast  remains  a\n",
      "\n",
      "vocab ; 2566 , new_vocab: 2566\n",
      "Sample Vocabulary Check:\n",
      "Fast: 0\n",
      "WordPiece: 1\n",
      "Tokenization: 2\n",
      "Xinying: 3\n",
      "Song: 4\n",
      "†: 5\n",
      "Alex: 6\n",
      "Salcianu: 7\n",
      "Yang: 8\n",
      "‡: 9\n",
      "Found 0 subtokens in vocabulary.\n",
      "Trie Construction Completed Successfully\n",
      "Trie built successfully.\n",
      "Testing ': Fast WordPiece Tokenization\n",
      "Tokenized: ['tokenization']\n",
      "\n",
      "Testing ': Xinying Song† Alex Salcianu† Yang Song‡∗ Dave Dopson† Denny Zhou†\n",
      "Tokenized: ['xi', 'n', 'yi', 'n', 'g', 'son', 'g', '†', 'al', 'salcianu', '†', 'son', 'g', '‡', '∗', None, 'do', None, 'son', '†', 'z', None, None, '†']\n",
      "\n",
      "Testing ': †Google Research, Mountain View, CA\n",
      "Tokenized: ['†', 'google', ',', 'mountain', ',']\n",
      "\n",
      "Testing ': †{xysong,salcianu,ddopson,dennyzhou}@google.com\n",
      "Tokenized: ['†', '{', 'xysong', ',', 'salcianu', ',', 'ddopson', ',', 'dennyzhou', '}', '@', 'google', '.', 'com']\n",
      "\n",
      "Testing ': ‡Kuaishou Technology, Beijing, China\n",
      "Tokenized: ['‡', 'kuaishou', None, None, 'o', 'g', 'y', ',', None, None, 'in', 'g', ',', None, 'in', 'a']\n",
      "\n",
      "Testing ': ‡yangsong@kuaishou.com\n",
      "Tokenized: ['‡', 'yangsong', '@', 'kuaishou', '.', 'com']\n",
      "\n",
      "Testing ': Abstract\n",
      "Tokenized: ['abs', None, 'c', 't']\n",
      "\n",
      "Testing ': Tokenization is a fundamental preprocessing\n",
      "Tokenized: ['tokenization', 'is', 'a', 'fundamental', 'preprocessing']\n",
      "\n",
      "Testing ': step for almost all NLP tasks. In this paper,\n",
      "Tokenized: ['step', 'for', 'almost', 'all', 'n', None, 'tasks', '.', 'in', 'this', 'paper', ',']\n",
      "\n",
      "Testing ': we propose efficient algorithms for the WordPiece tokenization used in BERT, from singleword tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a longest-matchfirst strategy, known as maximum matching.\n",
      "Tokenized: ['we', 'propose', 'efficient', 'algorithms', 'for', 'the', 'tokenization', 'used', 'in', 'bert', ',', 'from', 'singleword', 'tokenization', 'to', 'general', 'text', '(', 'e', '.', 'g', '.', ',', 'sentence', ')', 'tokenization', '.', 'when', 'tokenizing', 'a', 'single', 'word', ',', 'uses', 'a', 'longest', '-', 'matchfirst', 'strategy', ',', 'known', 'as', 'maximum', 'matching', '.']\n",
      "\n",
      "Testing ': The best known algorithms so far are 푂(푛\n",
      "Tokenized: ['the', 'best', 'known', 'algorithms', 'so', 'far', 'are', '푂', '(', '푛']\n",
      "\n",
      "Testing ': 2\n",
      "Tokenized: ['2']\n",
      "\n",
      "Testing ': )\n",
      "Tokenized: [')']\n",
      "\n",
      "Testing ': (where 푛 is the input length) or 푂(푛푚) (where\n",
      "Tokenized: ['(', 'where', '푛', 'is', 'the', 'input', 'length', ')', 'or', '푂', '(', '푛푚', ')', '(', 'where']\n",
      "\n",
      "Testing ': 푚 is the maximum vocabulary token length).\n",
      "Tokenized: ['푚', 'is', 'the', 'maximum', 'vocabulary', 'token', 'length', ')', '.']\n",
      "\n",
      "Testing ': We propose a novel algorithm whose tokenization complexity is strictly 푂(푛). Our method is\n",
      "Tokenized: ['we', 'propose', 'a', 'novel', 'algorithm', 'whose', 'tokenization', 'complexity', 'is', 'strictly', '푂', '(', '푛', ')', '.', 'our', 'method', 'is']\n",
      "\n",
      "Testing ': inspired by the Aho-Corasick algorithm. We\n",
      "Tokenized: ['inspired', 'by', 'the', 'a', None, '-', None, 'as', 'i', 'ck', 'algorithm', '.', 'we']\n",
      "\n",
      "Testing ': introduce additional linkages on top of the trie\n",
      "Tokenized: ['introduce', 'additional', 'linkages', 'on', 'top', 'of', 'the', 'trie']\n",
      "\n",
      "Testing ': built from the vocabulary, allowing smart transitions when the trie matching cannot continue.\n",
      "Tokenized: ['built', 'from', 'the', 'vocabulary', ',', 'allowing', 'smart', 'transitions', 'when', 'the', 'trie', 'matching', 'cannot', 'continue', '.']\n",
      "\n",
      "Testing ': For general text, we further propose an algorithm that combines pre-tokenization (splitting\n",
      "Tokenized: ['for', 'general', 'text', ',', 'we', 'further', 'propose', 'an', 'algorithm', 'that', 'combines', 'pre', '-', 'tokenization', '(', 'splitting']\n",
      "\n",
      "Testing ': the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our method is 8.2x faster\n",
      "Tokenized: ['the', 'text', 'into', 'words', ')', 'and', 'our', 'linear', '-', 'time', 'method', 'into', 'a', 'single', 'pass', '.', 'experimental', 'results', 'show', 'that', 'our', 'method', 'is', '8', '.', '2x', 'faster']\n",
      "\n",
      "Testing ': than HuggingFace Tokenizers and 5.1x faster\n",
      "Tokenized: ['than', None, 'g', None, 'n', 'g', None, 'e', 'tokenizer', 's', 'and', '5', '.', '1x', 'faster']\n",
      "\n",
      "Testing ': than TensorFlow Text on average for general\n",
      "Tokenized: ['than', 'tensorflow', 'text', 'on', 'average', 'for', 'general']\n",
      "\n",
      "Testing ': text tokenization.\n",
      "Tokenized: ['text', 'tokenization', '.']\n",
      "\n",
      "Testing ': 1 Introduction\n",
      "Tokenized: ['1', None, None, 'on']\n",
      "\n",
      "Testing ': Tokenization is the process of splitting text into\n",
      "Tokenized: ['tokenization', 'is', 'the', 'process', 'of', 'splitting', 'text', 'into']\n",
      "\n",
      "Testing ': smaller units called tokens (e.g., words). It is a\n",
      "Tokenized: ['smaller', 'units', 'called', 'tokens', '(', 'e', '.', 'g', '.', ',', 'words', ')', '.', 'it', 'is', 'a']\n",
      "\n",
      "Testing ': fundamental preprocessing step for almost all NLP\n",
      "Tokenized: ['fundamental', 'preprocessing', 'step', 'for', 'almost', 'all', 'n', None]\n",
      "\n",
      "Testing ': applications: sentiment analysis, question answering, machine translation, information retrieval, etc.\n",
      "Tokenized: ['applications', ':', 'sentiment', 'analysis', ',', 'question', 'answering', ',', 'machine', 'translation', ',', 'information', 'retrieval', ',', 'etc', '.']\n",
      "\n",
      "Testing ': Modern NLP models like BERT (Devlin et al.,\n",
      "Tokenized: ['modern', 'n', None, 'models', 'like', 'bert', '(', None, 'et', 'al', '.', ',']\n",
      "\n",
      "Testing ': 2019), GPT-3 (Brown et al., 2020), and XLNet (Yang et al., 2019) tokenize text into subword units (Schuster and Nakajima, 2012; Sennrich\n",
      "Tokenized: ['2019', ')', ',', 'g', None, 't', '-', '3', '(', None, 'own', 'et', 'al', '.', ',', '2020', ')', ',', 'and', 'x', None, 'net', '(', 'et', 'al', '.', ',', '2019', ')', 'tokenize', 'text', 'into', 'subword', 'units', '(', None, 'us', 'and', None, 'k', 'a', None, ',', '2012', ';', None, 'n', None]\n",
      "\n",
      "Testing ': et al., 2016; Kudo, 2018). As a midpoint between\n",
      "Tokenized: ['et', 'al', '.', ',', '2016', ';', None, 'do', ',', '2018', ')', '.', 'as', 'a', 'midpoint', 'between']\n",
      "\n",
      "Testing ': words and characters, subword units retain linguistic meaning (like morphemes), while alleviating\n",
      "Tokenized: ['words', 'and', 'characters', ',', 'subword', 'units', 'retain', 'linguistic', 'meaning', '(', 'like', 'morphemes', ')', ',', 'while', 'alleviating']\n",
      "\n",
      "Testing ': out-of-vocabulary situations even with a relatively\n",
      "Tokenized: ['out', '-', 'of', '-', 'vocabulary', 'situations', 'even', 'with', 'a', 'relatively']\n",
      "\n",
      "Testing ': small-size vocabulary.\n",
      "Tokenized: ['small', '-', 'size', 'vocabulary', '.']\n",
      "\n",
      "Testing ': ∗ Research conducted while working at Google.\n",
      "Tokenized: ['∗', 'conducted', 'while', 'working', 'at', 'google', '.']\n",
      "\n",
      "Testing ': In this paper, we propose efficient algorithms\n",
      "Tokenized: ['in', 'this', 'paper', ',', 'we', 'propose', 'efficient', 'algorithms']\n",
      "\n",
      "Testing ': for WordPiece, the subword tokenization used in\n",
      "Tokenized: ['for', ',', 'the', 'subword', 'tokenization', 'used', 'in']\n",
      "\n",
      "Testing ': BERT (Devlin et al., 2019). Given Unicode text\n",
      "Tokenized: ['bert', '(', None, 'et', 'al', '.', ',', '2019', ')', '.', 'given', None, 'code', 'text']\n",
      "\n",
      "Testing ': that has already been cleaned up and normalized,\n",
      "Tokenized: ['that', 'has', 'already', 'been', 'cleaned', 'up', 'and', 'normalized', ',']\n",
      "\n",
      "Testing ': WordPiece has two steps: (1) pre-tokenize the text\n",
      "Tokenized: ['has', 'two', 'steps', ':', '(', '1', ')', 'pre', '-', 'tokenize', 'the', 'text']\n",
      "\n",
      "Testing ': into words (by splitting on punctuation and whitespaces), and (2) tokenize each word into wordpieces.\n",
      "Tokenized: ['into', 'words', '(', 'by', 'splitting', 'on', 'punctuation', 'and', 'whitespaces', ')', ',', 'and', '(', '2', ')', 'tokenize', 'each', 'word', 'into', 'wordpieces', '.']\n",
      "\n",
      "Testing ': For single-word tokenization, WordPiece uses\n",
      "Tokenized: ['for', 'single', '-', 'word', 'tokenization', ',', 'uses']\n",
      "\n",
      "Testing ': a greedy longest-match-first strategy: iteratively\n",
      "Tokenized: ['a', 'greedy', 'longest', '-', 'match', '-', 'first', 'strategy', ':', 'iteratively']\n",
      "\n",
      "Testing ': pick the longest prefix of the remaining text that\n",
      "Tokenized: ['pick', 'the', 'longest', 'prefix', 'of', 'the', 'remaining', 'text', 'that']\n",
      "\n",
      "Testing ': matches a vocabulary token. This is well-known as\n",
      "Tokenized: ['matches', 'a', 'vocabulary', 'token', '.', 'this', 'is', 'well', '-', 'known', 'as']\n",
      "\n",
      "Testing ': Maximum Matching or MaxMatch (Palmer, 2000),\n",
      "Tokenized: ['maximum', 'matching', 'or', 'max', 'match', '(', None, None, ',', '2000', ')', ',']\n",
      "\n",
      "Testing ': which has also been used for Chinese word segmentation since 1980s (Liu and Liang, 1986).\n",
      "Tokenized: ['which', 'has', 'also', 'been', 'used', 'for', None, 'in', None, 'e', 'word', 'segmentation', 'since', '1980s', '(', None, 'and', None, 'an', 'g', ',', '1986', ')', '.']\n",
      "\n",
      "Testing ': Despite its wide use in NLP for decades, to the\n",
      "Tokenized: ['despite', 'its', 'wide', 'use', 'in', 'n', None, 'for', 'decades', ',', 'to', 'the']\n",
      "\n",
      "Testing ': best of our knowledge, the most efficient MaxMatch algorithms so far are 푂(푛\n",
      "Tokenized: ['best', 'of', 'our', 'knowledge', ',', 'the', 'most', 'efficient', 'max', 'match', 'algorithms', 'so', 'far', 'are', '푂', '(', '푛']\n",
      "\n",
      "Testing ': 2\n",
      "Tokenized: ['2']\n",
      "\n",
      "Testing ': ) (where 푛 is the\n",
      "Tokenized: [')', '(', 'where', '푛', 'is', 'the']\n",
      "\n",
      "Testing ': input word length) or 푂(푛푚) (where 푚 is the maximum vocabulary token length) (see Section 2).\n",
      "Tokenized: ['input', 'word', 'length', ')', 'or', '푂', '(', '푛푚', ')', '(', 'where', '푚', 'is', 'the', 'maximum', 'vocabulary', 'token', 'length', ')', '(', 'see', 'section', '2', ')', '.']\n",
      "\n",
      "Testing ': It’s worth noting that the latter has a vocabularyspecific multiplicative factor 푚, which can be large\n",
      "Tokenized: ['it', '’', 's', 'worth', 'noting', 'that', 'the', 'latter', 'has', 'a', 'vocabularyspecific', 'multiplicative', 'factor', '푚', ',', 'which', 'can', 'be', 'large']\n",
      "\n",
      "Testing ': when the vocabulary contains long words.\n",
      "Tokenized: ['when', 'the', 'vocabulary', 'contains', 'long', 'words', '.']\n",
      "\n",
      "Testing ': We propose LinMaxMatch, a novel MaxMatch\n",
      "Tokenized: ['we', 'propose', None, 'max', 'match', ',', 'a', 'novel', 'max', 'match']\n",
      "\n",
      "Testing ': algorithm for WordPiece tokenization, whose\n",
      "Tokenized: ['algorithm', 'for', 'tokenization', ',', 'whose']\n",
      "\n",
      "Testing ': tokenization time is strictly 푂(푛) without any\n",
      "Tokenized: ['tokenization', 'time', 'is', 'strictly', '푂', '(', '푛', ')', 'without', 'any']\n",
      "\n",
      "Testing ': vocabulary-specific multiplicative factors. Inspired\n",
      "Tokenized: ['vocabulary', '-', 'specific', 'multiplicative', 'factors', '.', 'inspired']\n",
      "\n",
      "Testing ': by the Aho-Corasick algorithm (Aho and Corasick, 1975), we organize vocabulary tokens in a\n",
      "Tokenized: ['by', 'the', 'a', None, '-', None, 'as', 'i', 'ck', 'algorithm', '(', 'a', 'and', None, 'as', 'i', 'ck', ',', '1975', ')', ',', 'we', 'organize', 'vocabulary', 'tokens', 'in', 'a']\n",
      "\n",
      "Testing ': trie (Fredkin, 1960) and introduce precomputed\n",
      "Tokenized: ['trie', '(', None, 'd', ',', '1960', ')', 'and', 'introduce', 'precomputed']\n",
      "\n",
      "Testing ': failure links and failure pops. During tokenization, if an input character does not match any trie\n",
      "Tokenized: ['failure', 'links', 'and', 'failure', 'pops', '.', 'during', 'tokenization', ',', 'if', 'an', 'input', 'character', 'does', 'not', 'match', 'any', 'trie']\n",
      "\n",
      "Testing ': edge, we perform smart transitions to avoid backtracking to earlier input characters. This involves\n",
      "Tokenized: ['edge', ',', 'we', 'perform', 'smart', 'transitions', 'to', 'avoid', 'backtracking', 'to', 'earlier', 'input', 'characters', '.', 'this', 'involves']\n",
      "\n",
      "Testing ': collecting the recognized tokens (i.e., failure pops)\n",
      "Tokenized: ['collecting', 'the', 'recognized', 'tokens', '(', 'i', '.', 'e', '.', ',', 'failure', 'pops', ')']\n",
      "\n",
      "Testing ': and moving to a trie node (via the failure link),\n",
      "Tokenized: ['and', 'moving', 'to', 'a', 'trie', 'node', '(', 'via', 'the', 'failure', 'link', ')', ',']\n",
      "\n",
      "Testing ': from where we continue to match the same character (Section 3).\n",
      "Tokenized: ['from', 'where', 'we', 'continue', 'to', 'match', 'the', 'same', 'character', '(', 'section', '3', ')', '.']\n",
      "\n",
      "Testing ': For general text tokenization, referred to as\n",
      "Tokenized: ['for', 'general', 'text', 'tokenization', ',', 'referred', 'to', 'as']\n",
      "\n",
      "Testing ': end-to-end tokenization in this paper, we propose\n",
      "Tokenized: ['end', '-', 'to', '-', 'end', 'tokenization', 'in', 'this', 'paper', ',', 'we', 'propose']\n",
      "\n",
      "Testing ': E2E WordPiece, an end-to-end algorithm that combines pre-tokenization and WordPiece tokenization\n",
      "Tokenized: ['e', '2', 'e', ',', 'an', 'end', '-', 'to', '-', 'end', 'algorithm', 'that', 'combines', 'pre', '-', 'tokenization', 'and', 'tokenization']\n",
      "\n",
      "Testing ': arXiv:2012.15524v3 [cs.CL] 5 Oct 2021\n",
      "Tokenized: [None, 'xi', None, ':', '2012', '.', '15524v3', '[', 'cs', '.', None, ']', '5', None, 't', '2021']\n",
      "\n",
      "Testing ': into a single, linear-time pass (Section 4).\n",
      "Tokenized: ['into', 'a', 'single', ',', 'linear', '-', 'time', 'pass', '(', 'section', '4', ')', '.']\n",
      "\n",
      "Testing ': Experimental results show that our method is\n",
      "Tokenized: ['experimental', 'results', 'show', 'that', 'our', 'method', 'is']\n",
      "\n",
      "Testing ': 8.2x faster than HuggingFace Tokenizers (HuggingFace, 2020) and 5.1x faster than TensorFlow Text (Google, 2020) on average for general\n",
      "Tokenized: ['8', '.', '2x', 'faster', 'than', None, 'g', None, 'n', 'g', None, 'e', 'tokenizer', 's', '(', None, 'g', None, 'n', 'g', None, 'e', ',', '2020', ')', 'and', '5', '.', '1x', 'faster', 'than', 'tensorflow', 'text', '(', 'google', ',', '2020', ')', 'on', 'average', 'for', 'general']\n",
      "\n",
      "Testing ': text tokenization (Section 5).\n",
      "Tokenized: ['text', 'tokenization', '(', 'section', '5', ')', '.']\n",
      "\n",
      "Testing ': Although tokenization is relatively faster than\n",
      "Tokenized: ['although', 'tokenization', 'is', 'relatively', 'faster', 'than']\n",
      "\n",
      "Testing ': other steps, it’s still worth improving the performance: Tokenization is a prerequisite step for\n",
      "Tokenized: ['other', 'steps', ',', 'it', '’', 's', 'still', 'worth', 'improving', 'the', 'performance', ':', 'tokenization', 'is', 'a', 'prerequisite', 'step', 'for']\n",
      "\n",
      "Testing ': almost all NLP tasks, and any improvement on\n",
      "Tokenized: ['almost', 'all', 'n', None, 'tasks', ',', 'and', 'any', 'improvement', 'on']\n",
      "\n",
      "Testing ': its efficiency helps reduce the latency of the entire inference. One potential impact of the work,\n",
      "Tokenized: ['its', 'efficiency', 'helps', 'reduce', 'the', 'latency', 'of', 'the', 'entire', 'inference', '.', 'one', 'potential', 'impact', 'of', 'the', 'work', ',']\n",
      "\n",
      "Testing ': for example, is on mobile NLP applications. Ondevice models are generally highly optimized for\n",
      "Tokenized: ['for', 'example', ',', 'is', 'on', 'mobile', 'n', None, 'applications', '.', 'on', None, 'i', 'c', 'e', 'models', 'are', 'generally', 'highly', 'optimized', 'for']\n",
      "\n",
      "Testing ': reducing latency, e.g., by distilling or compressing\n",
      "Tokenized: ['reducing', 'latency', ',', 'e', '.', 'g', '.', ',', 'by', 'distilling', 'or', 'compressing']\n",
      "\n",
      "Testing ': larger models. Thus, the impact of tokenization\n",
      "Tokenized: ['larger', 'models', '.', 'thus', ',', 'the', 'impact', 'of', 'tokenization']\n",
      "\n",
      "Testing ': can be significant here. Another impact is on aggregate computational savings for Web services\n",
      "Tokenized: ['can', 'be', 'significant', 'here', '.', 'another', 'impact', 'is', 'on', 'aggregate', 'computational', 'savings', 'for', 'we', 'b', 'services']\n",
      "\n",
      "Testing ': like Google, Facebook, Twitter, etc. For example,\n",
      "Tokenized: ['like', 'google', ',', None, 'e', None, 'k', ',', None, 'it', ',', 'etc', '.', 'for', 'example', ',']\n",
      "\n",
      "Testing ': Google uses BERT to power its Web search nowadays.1 Google serves billions of search queries per\n",
      "Tokenized: ['google', 'uses', 'bert', 'to', 'power', 'its', 'we', 'b', 'search', 'nowadays', '.', '1', 'google', 'serves', 'billions', 'of', 'search', 'queries', 'per']\n",
      "\n",
      "Testing ': day, and it processes hundreds of trillions of Web\n",
      "Tokenized: ['day', ',', 'and', 'it', 'processes', 'hundreds', 'of', 'trillions', 'of', 'we', 'b']\n",
      "\n",
      "Testing ': pages in index building. By employing a faster\n",
      "Tokenized: ['pages', 'in', 'index', 'building', '.', 'by', 'employing', 'a', 'faster']\n",
      "\n",
      "Testing ': tokenization system, the aggregate computational\n",
      "Tokenized: ['tokenization', 'system', ',', 'the', 'aggregate', 'computational']\n",
      "\n",
      "Testing ': savings would be material, which also benefits the\n",
      "Tokenized: ['savings', 'would', 'be', 'material', ',', 'which', 'also', 'benefits', 'the']\n",
      "\n",
      "Testing ': environment (for less power consumption).\n",
      "Tokenized: ['environment', '(', 'for', 'less', 'power', 'consumption', ')', '.']\n",
      "\n",
      "Testing ': This paper also makes a theoretical contribution. The proposed LinMaxMatch algorithm solves\n",
      "Tokenized: ['this', 'paper', 'also', 'makes', 'a', 'theoretical', 'contribution', '.', 'the', 'proposed', None, 'max', 'match', 'algorithm', 'solves']\n",
      "\n",
      "Testing ': the decades-old MaxMatch problem in the optimal\n",
      "Tokenized: ['the', 'decades', '-', 'old', 'max', 'match', 'problem', 'in', 'the', 'optimal']\n",
      "\n",
      "Testing ': 푂(푛) time, and the idea is applicable to other string\n",
      "Tokenized: ['푂', '(', '푛', ')', 'time', ',', 'and', 'the', 'idea', 'is', 'applicable', 'to', 'other', 'string']\n",
      "\n",
      "Testing ': matching or rewriting problems (Section 3.6).\n",
      "Tokenized: ['matching', 'or', 'rewriting', 'problems', '(', 'section', '3', '.', '6', ')', '.']\n",
      "\n",
      "Testing ': The code will be available at https://www.\n",
      "Tokenized: ['the', 'code', 'will', 'be', 'available', 'at', 'https', ':', '/', '/', 'www', '.']\n",
      "\n",
      "Testing ': tensorflow.org/text.\n",
      "Tokenized: ['tensorflow', '.', 'org', '/', 'text', '.']\n",
      "\n",
      "Testing ': 2 Related Work\n",
      "Tokenized: ['2', 'related', 'work']\n",
      "\n",
      "Testing ': Maximum Matching (or MaxMatch) has been used\n",
      "Tokenized: ['maximum', 'matching', '(', 'or', 'max', 'match', ')', 'has', 'been', 'used']\n",
      "\n",
      "Testing ': for Chinese word segmentation (CWS) since the\n",
      "Tokenized: ['for', None, 'in', None, 'e', 'word', 'segmentation', '(', 'c', None, 's', ')', 'since', 'the']\n",
      "\n",
      "Testing ': 1980s (Liu and Liang, 1986; Palmer, 2000). Recent\n",
      "Tokenized: ['1980s', '(', None, 'and', None, 'an', 'g', ',', '1986', ';', None, None, ',', '2000', ')', '.', 'recent']\n",
      "\n",
      "Testing ': CWS work focuses on machine learning-based segmentation approaches, but MaxMatch remains a\n",
      "Tokenized: ['c', None, 's', 'work', 'focuses', 'on', 'machine', 'learning', '-', 'based', 'segmentation', 'approaches', ',', 'but', 'max', 'match', 'remains', 'a']\n",
      "\n",
      "Trie Construction Completed Successfully\n",
      "Trie built successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b098ee701374f8285b99b6b6e2a4393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 4 named input_ids_question expected length 1000 but got length 170",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 430\u001b[0m\n\u001b[0;32m    428\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing to the dataset with your WordPiece tokenizer\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m dpo_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdpo_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_dpo_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordpiece_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# You can convert to PyTorch tensors after processing\u001b[39;00m\n\u001b[0;32m    434\u001b[0m dpo_dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\dataset_dict.py:853\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 853\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\dataset_dict.py:854\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    853\u001b[0m     {\n\u001b[1;32m--> 854\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    874\u001b[0m     }\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3091\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   3092\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3093\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3094\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3095\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3096\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3097\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3098\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3099\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:3493\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3491\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(batch))\n\u001b[0;32m   3492\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3493\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3494\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_writer.py:558\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    556\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[0;32m    557\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m--> 558\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\pyarrow\\table.pxi:3674\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\pyarrow\\table.pxi:2837\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Column 4 named input_ids_question expected length 1000 but got length 170"
     ]
    }
   ],
   "source": [
    "# Test on larger text\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None\n",
    "        self.frequency = 0\n",
    "        self.failure_link = None\n",
    "        self.is_end = False  # Add is_end attribute to mark the end of a word\n",
    "        self.token = None  # Add token attribute to store the token associated with the node\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self, unk_token_id=0):\n",
    "        self.root = TrieNode()\n",
    "        self.unk_token_id = unk_token_id\n",
    "\n",
    "    def insert(self, token, token_id, frequency):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "        node.frequency = frequency\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        \"\"\"Finds the most probable subwords based on frequency.\"\"\"\n",
    "        node = self.root\n",
    "        best_subwords = []\n",
    "\n",
    "        def dfs(current_node, subword='', collected_subwords=[]):\n",
    "            if current_node.token_id is not None:\n",
    "                # Update to correctly calculate total_frequency based on the structure of collected_subwords\n",
    "                total_frequency = sum(n.frequency for _, _, n in collected_subwords) + current_node.frequency\n",
    "                probability = current_node.frequency / total_frequency if total_frequency else 0\n",
    "                collected_subwords.append((subword, probability, current_node))\n",
    "\n",
    "            for char, next_node in current_node.children.items():\n",
    "                dfs(next_node, subword + char, list(collected_subwords))  # Create a copy of the list to avoid shared state\n",
    "\n",
    "        dfs(node)\n",
    "        best_subwords = sorted(best_subwords, key=lambda x: x[1], reverse=True)\n",
    "        return [subword for subword, _, _ in best_subwords][:5] or [self.unk_token_id]\n",
    "\n",
    "\n",
    "    def compute_failure_links(self):\n",
    "        root = self.root\n",
    "        root.failure_link = root  # Root's failure link points to itself\n",
    "        queue = [root]\n",
    "\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)\n",
    "\n",
    "                # Follow failure link to find the longest suffix for the child_node\n",
    "                failure_candidate = current_node.failure_link\n",
    "                while failure_candidate != root and char not in failure_candidate.children:\n",
    "                    failure_candidate = failure_candidate.failure_link\n",
    "                child_node.failure_link = failure_candidate.children.get(char, root)\n",
    "\n",
    "\n",
    "class SimpleSentencePiece:\n",
    "    def __init__(self, model_type=\"bpe\", vocab_size=8000):\n",
    "        self.vocab = {}\n",
    "        self.id_to_subword = {}\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.unk_token_id = 0\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model = None if model_type == \"bpe\" else None\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def train(self, text):\n",
    "        if self.model_type == \"bpe\":\n",
    "            self.model = BPE(num_merges=self.vocab_size, unk_token_id=self.unk_token_id)\n",
    "            self.model.train(text)\n",
    "            self.vocab = self.model.vocab\n",
    "            self.id_to_subword = {i: word for word, i in self.vocab.items()}\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported yet.\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.preprocess_text(text)  # Preprocess text before encoding\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        return self.model.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if not self.id_to_subword:\n",
    "            raise ValueError(\"Vocabulary is empty. Ensure the model is trained first.\")\n",
    "        text = \" \".join([self.id_to_subword.get(id_, self.unk_token) for id_ in ids])\n",
    "        text = text.replace(\" </w>\", \"\").replace(\"</w>\", \" \").strip()\n",
    "        return text\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Convert text to lowercase to ensure case insensitivity\n",
    "        text = text.lower()\n",
    "        # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "        text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Trim leading and trailing spaces\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        model_data = {\n",
    "            'vocab': self.vocab,\n",
    "            'id_to_subword': self.id_to_subword,\n",
    "            'model_type': self.model_type,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            # Potentially include other relevant attributes\n",
    "        }\n",
    "        # Save the high-level tokenizer settings\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(model_data, f)\n",
    "        \n",
    "        # Now save the BPE model specifically\n",
    "        if self.model_type == \"bpe\" and self.model:\n",
    "            self.model.save_model(filepath + \"_bpe\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        self.vocab = model_data['vocab']\n",
    "        self.id_to_subword = model_data['id_to_subword']\n",
    "        self.model_type = model_data['model_type']\n",
    "        self.vocab_size = model_data['vocab_size']\n",
    "        \n",
    "        # Assuming model_type is still \"bpe\", we now load the BPE model\n",
    "        if self.model_type == \"bpe\":\n",
    "            self.model = BPE(self.vocab_size, self.unk_token_id)\n",
    "            self.model.load_model(filepath + \"_bpe\")\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, num_merges=100, unk_token_id=0):  # Accept unk_token_id parameter\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        self.num_merges = num_merges\n",
    "        self.unk_token_id = unk_token_id  # Store the unknown token ID\n",
    "\n",
    "    def train(self, text):\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "        vocab = collections.Counter(words)\n",
    "        vocab = {word + '</w>': count for word, count in vocab.items()}\n",
    "        \n",
    "        for _ in range(self.num_merges):  # Use the num_merges from the instance variable\n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best, vocab)\n",
    "            self.merges.append(best)\n",
    "\n",
    "        self.vocab = {word: i for i, word in enumerate(vocab.keys())}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stats(vocab):\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[symbols[i], symbols[i+1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_vocab(pair, vocab):\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in vocab:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = vocab[word]\n",
    "        return v_out\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text into subwords using learned BPE merges.\"\"\"\n",
    "        encoded_tokens = []\n",
    "        for word in re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE):\n",
    "            word += '</w>'\n",
    "            subwords = [word]  # Start with the entire word as one subword\n",
    "            for merge in self.merges:\n",
    "                new_subwords = []\n",
    "                for subword in subwords:\n",
    "                    # If the merge is in subword, split it; otherwise, keep it as is\n",
    "                    if ' '.join(merge) in subword:\n",
    "                        new_subwords.extend(subword.replace(' '.join(merge), ''.join(merge)).split(' '))\n",
    "                    else:\n",
    "                        new_subwords.append(subword)\n",
    "                subwords = new_subwords\n",
    "            encoded_tokens.extend(subwords)\n",
    "        return [self.vocab.get(token, self.unk_token_id) for token in encoded_tokens]\n",
    "    \n",
    "        # New method to save trained model\n",
    "    def save_model(self, filepath):\n",
    "        bpe_data = {\n",
    "            'merges': self.merges,\n",
    "            'vocab': self.vocab,\n",
    "            'num_merges': self.num_merges,\n",
    "            # Include other attributes as needed\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(bpe_data, f)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            bpe_data = json.load(f)\n",
    "        \n",
    "        self.merges = bpe_data['merges']\n",
    "        self.vocab = bpe_data['vocab']\n",
    "        self.num_merges = bpe_data['num_merges']\n",
    "\n",
    "class WordPiece:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.root = self.build_trie(vocab)\n",
    "        self.compute_failure_links(self.root)\n",
    "        print(\"Trie built successfully.\")\n",
    "\n",
    "    # Add debug prints to build_trie to confirm structure\n",
    "    def build_trie(self, vocab):\n",
    "        root = TrieNode()\n",
    "        for token in vocab:\n",
    "            node = root\n",
    "            for char in token:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.is_end = True\n",
    "            node.token = token\n",
    "        print(\"Trie Construction Completed Successfully\")\n",
    "        return root\n",
    "\n",
    "\n",
    "    def compute_failure_links(self, root):\n",
    "        queue = [root]\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "            for char, child_node in current_node.children.items():\n",
    "                failure_node = current_node.failure_link\n",
    "                while failure_node and char not in failure_node.children:\n",
    "                    failure_node = failure_node.failure_link\n",
    "                child_node.failure_link = failure_node.children[char] if failure_node else root\n",
    "                queue.append(child_node)\n",
    "\n",
    "    # Improved debug prints in tokenize method\n",
    "    def tokenize(self, text):\n",
    "        # Preprocess input text\n",
    "        text = self.preprocess_text(text)\n",
    "        node = self.root\n",
    "        tokens = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(text):\n",
    "            char = text[i]\n",
    "            if char == ' ':\n",
    "                node = self.root\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if char not in node.children:\n",
    "                if node != self.root:\n",
    "                    tokens.append(node.token)  # Add the longest token found\n",
    "                    node = self.root  # Reset to root\n",
    "                    continue\n",
    "                else:\n",
    "                    tokens.append(self.unk_token)\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "            node = node.children[char]\n",
    "            if node.is_end:\n",
    "                if i + 1 == len(text) or text[i + 1] == ' ':\n",
    "                    tokens.append(node.token)\n",
    "                    node = self.root\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        # Convert text to lowercase to ensure case insensitivity\n",
    "        text = text.lower()\n",
    "\n",
    "        # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "        # This depends on how your vocabulary handles punctuation\n",
    "        text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Trim leading and trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = [line.strip() for line in file.readlines()]\n",
    "    return texts\n",
    "\n",
    "texts = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "\n",
    "num_merges = 100\n",
    "\n",
    "# Initialize and train the SimpleSentencePiece model with BPE\n",
    "ssp = SimpleSentencePiece(model_type=\"bpe\", vocab_size=1000)\n",
    "ssp.train('\\n'.join(texts))  # Train the model on the entire dataset\n",
    "\n",
    "# Test the model on a subset or the entire dataset\n",
    "for i, text in enumerate(texts[:100]):  # Example: test on the first 100 texts\n",
    "    encoded = ssp.encode(text)\n",
    "    decoded = ssp.decode(encoded)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Decoded: {decoded}\\n\")\n",
    "\n",
    "# Save the trained model\n",
    "ssp.save_model(\"ssp_model.json\")\n",
    "\n",
    "# Create a new instance and load the model\n",
    "new_ssp = SimpleSentencePiece(model_type=\"bpe\")\n",
    "new_ssp.load_model(\"ssp_model.json\")\n",
    "\n",
    "# New Model\n",
    "for i, text in enumerate(texts[:100]):  # Example: test on the first 100 texts\n",
    "    re_encoded = ssp.encode(text)\n",
    "    re_decoded = ssp.decode(re_encoded)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Decoded: {re_decoded}\\n\")\n",
    "\n",
    "\n",
    "# Assuming ssp is your SimpleSentencePiece instance\n",
    "vocab = ssp.vocab  # This gets the vocabulary after BPE training\n",
    "new_vocab = new_ssp.vocab  # This gets the vocabulary after BPE training\n",
    "\n",
    "print(f\"vocab ; {len(vocab)} , new_vocab: {len(new_vocab)}\")\n",
    "\n",
    "def adapt_vocab_for_wordpiece(ssp_vocab):\n",
    "    adapted_vocab = {}\n",
    "    for token, id_or_freq in ssp_vocab.items():\n",
    "        # Check if a token is a continuation subword and not a standalone word\n",
    "        # Since BPE might not mark subwords in a way WordPiece expects, we adapt based on our best approximation\n",
    "        if not token.startswith(\" \") and not token.endswith(\"</w>\"):\n",
    "            adapted_token = \"##\" + token.replace(\"</w>\", \"\")  # Removing BPE's end-of-word marker and prepending \"##\"\n",
    "        else:\n",
    "            adapted_token = token.replace(\"</w>\", \"\")  # Just remove the BPE's end-of-word marker for standalone words\n",
    "\n",
    "        adapted_vocab[adapted_token] = id_or_freq\n",
    "    return adapted_vocab\n",
    "\n",
    "\n",
    "# Assuming ssp is your SimpleSentencePiece instance after BPE training\n",
    "wordpiece_vocab = adapt_vocab_for_wordpiece(ssp.vocab)\n",
    "\n",
    "# Debugging step to ensure vocabulary completeness\n",
    "def debug_vocab(adapted_vocab):\n",
    "    print(\"Sample Vocabulary Check:\")\n",
    "    # Iterate over the first 10 key-value pairs in the adapted vocabulary\n",
    "    for i, (token, id_or_freq) in enumerate(adapted_vocab.items()):\n",
    "        print(f\"{token}: {id_or_freq}\")\n",
    "        if i >= 9:  # Stop after printing 10 entries\n",
    "            break\n",
    "    # Specifically check for subtokens if your tokenizer expects them\n",
    "    subtokens = [token for token in adapted_vocab.keys() if token.startswith(\"##\")]\n",
    "    print(f\"Found {len(subtokens)} subtokens in vocabulary.\")\n",
    "\n",
    "\n",
    "# Ensure wordpiece_vocab is a list of vocabulary tokens\n",
    "debug_vocab(wordpiece_vocab)  # Call this after initializing wordpiece_vocab\n",
    "\n",
    "# Initialize WordPiece with the adapted vocabulary\n",
    "wordpiece = WordPiece(wordpiece_vocab)\n",
    "\n",
    "for i, text in enumerate(texts[:100]):\n",
    "    print(f\"Testing ': {text}\")\n",
    "    tokenized = wordpiece.tokenize(text)\n",
    "    print(f\"Tokenized: {tokenized}\\n\")\n",
    "\n",
    "\n",
    "# Load your WordPiece tokenizer\n",
    "wordpiece_tokenizer = WordPiece(wordpiece_vocab)  # Assuming wordpiece_vocab is your BPE-trained vocabulary\n",
    "\n",
    "# Modify the preprocess_dpo_data function to use your WordPiece tokenizer\n",
    "def preprocess_dpo_data(examples, tokenizer, max_length):\n",
    "    # Ensure 'question', 'chosen', and 'rejected' fields are strings\n",
    "    question = \" \".join(examples['question']) if isinstance(examples['question'], list) else examples['question']\n",
    "    chosen = \" \".join(examples['chosen']) if isinstance(examples['chosen'], list) else examples['chosen']\n",
    "    rejected = \" \".join(examples['rejected']) if isinstance(examples['rejected'], list) else examples['rejected']\n",
    "\n",
    "    # Tokenize 'question', 'chosen', and 'rejected' fields using your WordPiece tokenizer\n",
    "    tokenized_questions = tokenizer.tokenize(question)\n",
    "    tokenized_chosen = tokenizer.tokenize(chosen)\n",
    "    tokenized_rejected = tokenizer.tokenize(rejected)\n",
    "\n",
    "    # Truncate or pad the tokenized sequences to the specified max_length\n",
    "    tokenized_questions = tokenized_questions[:max_length]  # Truncate if longer than max_length\n",
    "    tokenized_questions += [0] * (max_length - len(tokenized_questions))  # Pad to max_length\n",
    "    tokenized_chosen = tokenized_chosen[:max_length]  # Truncate if longer than max_length\n",
    "    tokenized_chosen += [0] * (max_length - len(tokenized_chosen))  # Pad to max_length\n",
    "    tokenized_rejected = tokenized_rejected[:max_length]  # Truncate if longer than max_length\n",
    "    tokenized_rejected += [0] * (max_length - len(tokenized_rejected))  # Pad to max_length\n",
    "\n",
    "    # Prepare the labels: 1 for 'chosen' and 0 for 'rejected'\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(len(examples['question']))]\n",
    "\n",
    "    return {\n",
    "        'input_ids_question': tokenized_questions,\n",
    "        'input_ids_chosen': tokenized_chosen,\n",
    "        'input_ids_rejected': tokenized_rejected,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the DPO dataset from Hugging Face\n",
    "dpo_dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "max_seq_length = 512 // 3\n",
    "# Apply the preprocessing to the dataset with your WordPiece tokenizer\n",
    "dpo_dataset = dpo_dataset.map(lambda x: preprocess_dpo_data(x, wordpiece_tokenizer, max_seq_length), batched=True)\n",
    "\n",
    "\n",
    "# You can convert to PyTorch tensors after processing\n",
    "dpo_dataset.set_format(type='torch', columns=['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(dpo_dataset['train'], batch_size=2, shuffle=True)\n",
    "\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "expert_model = Expert(config)\n",
    "\n",
    "model_vocab_size = expert_model.lmt.decoder.word_embedding.num_embeddings\n",
    "print(f\"Model vocab size: {model_vocab_size}\")\n",
    "print(f\"Model embedding size: {expert_model.lmt.decoder.word_embedding.embedding_dim}\")\n",
    "print(f\"Configured max length: {config.max_length}\")\n",
    "\n",
    "optimizer = AdamW(expert_model.parameters(), lr=1e-5)\n",
    "save_path = 'D:/EXPERT_WEIGHTS/dpo_model.pth'\n",
    "\n",
    "# Train the DPO model\n",
    "label_column = 'labels'\n",
    "input_columns = ['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
    "# Assuming expert_model is an instance of Expert\n",
    "avg_loss = expert_model.train_dpo(train_loader, optimizer, config, save_path)\n",
    "\n",
    "# Save the model\n",
    "torch.save(expert_model.transformer_dpo.state_dict(), save_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "\n",
    "# Initialize Expert system\n",
    "expert_system = Expert(config)\n",
    "sequence_length = 30 \n",
    "# Load dataset\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=sequence_length)\n",
    "    \n",
    "    # Shift input_ids to create labels and truncate the last token\n",
    "    labels = [seq[1:] + [tokenizer.pad_token_id] for seq in tokenized_output['input_ids']]\n",
    "    tokenized_output['labels'] = labels\n",
    "    \n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=64, shuffle=True)\n",
    "\n",
    "# Rag data\n",
    "train_rag_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        config.rag_dataset['train'][0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        config.rag_dataset['train'][0],\n",
    "        config.rag_dataset['train'][0],\n",
    "        config.rag_dataset['train'][0],\n",
    "        # Contexts from MAMBA.pdf\n",
    "        config.rag_dataset['train'][1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        config.rag_dataset['train'][1],\n",
    "        config.rag_dataset['train'][1],\n",
    "        config.rag_dataset['train'][1],\n",
    "        # Contexts from QLORA.pdf\n",
    "        config.rag_dataset['train'][2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        config.rag_dataset['train'][2],\n",
    "        config.rag_dataset['train'][2],\n",
    "        config.rag_dataset['train'][2],\n",
    "        # Contexts from RAG.pdf\n",
    "        config.rag_dataset['train'][3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        config.rag_dataset['train'][3],\n",
    "        config.rag_dataset['train'][3],\n",
    "        config.rag_dataset['train'][3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset['train'][4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset['train'][4],\n",
    "        config.rag_dataset['train'][4],\n",
    "        config.rag_dataset['train'][4]\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Train the LMT_Rag sub-model within the Expert system\n",
    "# Train th model\n",
    "model, average_loss = expert_system.train_language_model_rag(\n",
    "    expert_system.transformer_rag,  # Assuming transformer_rag is your RAG model within Expert\n",
    "    config.rag_model_path,\n",
    "    train_loader,\n",
    "    device = config.device,\n",
    "    num_epochs=5,\n",
    "    lr=1e-5,  # Adjust learning rate as needed\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Train DPR encoders\n",
    "(context_encoder, question_encoder), average_loss = expert_system.train_dpr_encoders(\n",
    "    train_rag_data,\n",
    "    expert_system.context_encoder,  # Assuming you have these initialized within Expert\n",
    "    expert_system.question_encoder,\n",
    "    optimizer_context = AdamW(expert_system.context_encoder.parameters(), lr=1e-5),  \n",
    "    optimizer_question = AdamW(expert_system.question_encoder.parameters(), lr=1e-5),\n",
    "    epochs=5,\n",
    "    context_save_path=config.context_encoder_path,\n",
    "    question_save_path=config.question_encoder_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "\n",
    "# Initialize Expert system\n",
    "expert_system = Expert(config)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "    \n",
    "    # Convert lists to PyTorch tensors\n",
    "    input_ids = torch.tensor(tokenized_output['input_ids']).to(config.device)\n",
    "    attention_mask = torch.tensor(tokenized_output['attention_mask']).to(config.device)\n",
    "\n",
    "    # Creating labels by shifting the input_ids\n",
    "    labels = input_ids[:, :-1].clone().to(config.device)\n",
    "    labels = torch.nn.functional.pad(labels, (0, 1), value=tokenizer.pad_token_id)  # Pad labels to match sequence length\n",
    "    \n",
    "    tokenized_output['input_ids'] = input_ids\n",
    "    tokenized_output['attention_mask'] = attention_mask\n",
    "    tokenized_output['labels'] = labels\n",
    "\n",
    "    return tokenized_output\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "# DataLoader - same as your working code\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "# DataLoader - same as your working code\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "expert_system.train_mamba(train_loader, 5, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Whole Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Expert model and Data\n",
    "config = ExpertConfig()\n",
    "\n",
    "# Initialize Expert system\n",
    "expert_system = Expert(config)\n",
    "\n",
    "# Load Wikipedia dataset and preprocess\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:1%]\")  # Using 1% of the data for demonstration\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=config.max_length)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])  # Remove original text to only keep tokenized versions\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "\n",
    "main_loss_function = torch.nn.CrossEntropyLoss()\n",
    "aux_loss_weight = 0.1  # Adjust based on the significance of the auxiliary loss in your training\n",
    "\n",
    "optimizer = torch.optim.AdamW(expert_model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "# Rag data\n",
    "train_rag_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        config.rag_dataset['train'][0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        config.rag_dataset['train'][0],\n",
    "        config.rag_dataset['train'][0],\n",
    "        config.rag_dataset['train'][0],\n",
    "        # Contexts from MAMBA.pdf\n",
    "        config.rag_dataset['train'][1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        config.rag_dataset['train'][1],\n",
    "        config.rag_dataset['train'][1],\n",
    "        config.rag_dataset['train'][1],\n",
    "        # Contexts from QLORA.pdf\n",
    "        config.rag_dataset['train'][2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        config.rag_dataset['train'][2],\n",
    "        config.rag_dataset['train'][2],\n",
    "        config.rag_dataset['train'][2],\n",
    "        # Contexts from RAG.pdf\n",
    "        config.rag_dataset['train'][3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        config.rag_dataset['train'][3],\n",
    "        config.rag_dataset['train'][3],\n",
    "        config.rag_dataset['train'][3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset['train'][4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset['train'][4],\n",
    "        config.rag_dataset['train'][4],\n",
    "        config.rag_dataset['train'][4]\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trained_expert_model, average_loss = expert_model.train_expert(\n",
    "    train_loader=train_loader,\n",
    "    train_data=train_rag_data,\n",
    "    optimizer=optimizer,\n",
    "    main_loss_function=main_loss_function,\n",
    "    aux_loss_weight=aux_loss_weight,\n",
    "    device=config.device,\n",
    "    save_path=save_path,\n",
    "    accumulation_steps=4,  # Adjust based on your preference\n",
    "    num_epochs=5  # Adjust based on your training needs\n",
    ")\n",
    "\n",
    "print(f\"Training completed. Average loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Encoding and Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, config: ExpertConfig):\n",
    "        super(Expert, self).__init__()\n",
    "\n",
    "        # Initialize components for other functionalities (e.g., RAG, DPO, MAMBA)\n",
    "        self.rag = self._initialize_rag(config)\n",
    "        self.dpo = self._initialize_dpo(config)\n",
    "        self.mamba = self._initialize_mamba(config)\n",
    "        \n",
    "\n",
    "        # Initialize other necessary layers or parameters\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.layer_norm = nn.LayerNorm(config.input_dim)\n",
    "\n",
    "        # Load the custom Transformer model\n",
    "        self.custom_transformer = TransformerModel(\n",
    "            vocab=config.vocab,  # Assuming vocab is part of config\n",
    "            vocab_size=config.actual_vocab_size,\n",
    "            embedding_dim=128,\n",
    "            max_seq_len=512,\n",
    "            nhead=8,\n",
    "            dim_feedforward=2048,\n",
    "            freq_threshold=config.freq_threshold,\n",
    "            smaller_embed_dim=64\n",
    "        )\n",
    "        self.custom_transformer.load_state_dict(torch.load(config.transformer_model_path))\n",
    "        self.custom_transformer.eval()\n",
    "\n",
    "        # Load the custom tokenizer\n",
    "        with open(config.tokenizer_path, 'rb') as f:\n",
    "            self.custom_tokenizer = pickle.load(f)\n",
    "\n",
    "    def _initialize_rag(self, config):\n",
    "        # Placeholder for initializing the RAG model component\n",
    "        # This would typically involve loading a model and its weights\n",
    "        return None  # Replace with actual initialization\n",
    "\n",
    "    def _initialize_dpo(self, config):\n",
    "        # Placeholder for initializing the DPO model component\n",
    "        return None  # Replace with actual initialization\n",
    "\n",
    "    def _initialize_mamba(self, config):\n",
    "        # Placeholder for initializing the MAMBA model component\n",
    "        return None  # Replace with actual initialization\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Example forward pass using the custom transformer and tokenizer\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        tokenized_input = self.custom_tokenizer.encode(input_text)  # Assuming a method that returns token IDs\n",
    "        tokenized_input = torch.tensor(tokenized_input).unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Pass the tokenized input through the custom transformer\n",
    "        with torch.no_grad():\n",
    "            output_logits, _ = self.custom_transformer(tokenized_input)\n",
    "\n",
    "        # Further processing based on output_logits\n",
    "        # For example, applying softmax to get probabilities for classification tasks\n",
    "        return output_logits\n",
    "\n",
    "# Assuming ExpertConfig is defined elsewhere with the necessary attributes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
