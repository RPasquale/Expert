{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', text)\n",
    "\n",
    "def build_vocab(tokens, max_vocab_size=10000):\n",
    "    token_freqs = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # Ensure special tokens are included\n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4}\n",
    "    for token, _ in sorted_tokens[:max_vocab_size - len(vocab)]:\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def cosine_annealing_scheduler(optimizer, initial_lr, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return 0.5 * (1. + math.cos(math.pi * progress)) \n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def combined_loss(output, target, model, l2_reg_strength=1.0, l1_reg_strength=0.0):\n",
    "    task_loss = nn.CrossEntropyLoss()(output, target)  \n",
    "    regularization_loss = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if isinstance(param, nn.Parameter):  \n",
    "            regularization_loss += param.pow(2).sum() * l2_reg_strength  # L2\n",
    "            regularization_loss += param.abs().sum() * l1_reg_strength  # L1\n",
    "\n",
    "    return task_loss + regularization_loss\n",
    "\n",
    "# Encoding Transformer Code\n",
    "\n",
    "class AdaptiveDropoutLayer(nn.Module):\n",
    "    def __init__(self, init_dropout_rate=0.1):\n",
    "        super(AdaptiveDropoutLayer, self).__init__()\n",
    "        # Use logit transformation for stability\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(math.log(init_dropout_rate / (1 - init_dropout_rate))).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.log_alpha)\n",
    "        # Convert p from a tensor to a float\n",
    "        p_value = p.item()  # This extracts the scalar value as a Python float\n",
    "        return nn.functional.dropout(x, p=p_value, training=self.training)\n",
    "\n",
    "\n",
    "class AdaptiveWeightDecayOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, init_l2_strength=0.01):\n",
    "        super().__init__(params, {'lr': lr})\n",
    "        self.log_l2_strength = nn.Parameter(torch.tensor(math.log(init_l2_strength)).float())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = torch.exp(self.log_l2_strength)  \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay) \n",
    "                p.update(d_p, group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Create positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension (B x T x C)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Tensor of shape [Batch Size, Sequence Length, Embedding Dimension]\n",
    "        # Adjust positional encoding to match the input size and device\n",
    "        pe = self.pe[:, :x.size(1)]\n",
    "        # Assuming x is on the correct device, pe will be automatically aligned to the same device\n",
    "        return pe\n",
    "\n",
    "\n",
    "class MultiHeadLinformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, k=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k if k is not None else embed_dim // num_heads  # Projection dimension per head\n",
    "\n",
    "        self.key_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.value_projections = nn.Linear(embed_dim, self.k * num_heads)\n",
    "        self.out_projection = nn.Linear(self.k * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        # Project keys and values\n",
    "        keys = self.key_projections(query)\n",
    "        values = self.value_projections(query)\n",
    "        \n",
    "        # Reshape into [batch_size, num_heads, seq_len, k]\n",
    "        keys = keys.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        values = values.reshape(batch_size, seq_len, self.num_heads, self.k).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention (scaled dot-product attention)\n",
    "        # Scaling by the square root of the depth of the key vectors to prevent large values in the dot product\n",
    "        # which could push the softmax function into regions where it has extremely small gradients\n",
    "        keys = keys / (self.k ** 0.5)\n",
    "        attention_scores = torch.softmax(torch.matmul(keys, values.transpose(-2, -1)), dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention_scores, values)\n",
    "        \n",
    "        # Concatenate heads and project back to original embedding dimension\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, self.num_heads * self.k)\n",
    "        out = self.out_projection(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len= 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text)[:self.max_seq_len]         \n",
    "        labels = encoded_text.copy()  # Copy encoded text for labels\n",
    "\n",
    "        # Apply dynamic masking\n",
    "        masked_indices = np.random.rand(len(encoded_text)) < 0.15\n",
    "        for i in range(len(encoded_text)):\n",
    "            if masked_indices[i]:\n",
    "                encoded_text[i] = self.tokenizer.vocab[\"[MASK]\"]\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_len - len(encoded_text)\n",
    "        attention_mask = [1] * len(encoded_text) + [0] * padding_length\n",
    "        encoded_text += [self.tokenizer.vocab[\"[PAD]\"]] * padding_length\n",
    "        labels += [-100] * padding_length  # Use -100 for padding positions\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class AdaptiveEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab,  vocab_size, freq_threshold, large_embed_dim, small_embed_dim, max_seq_len):\n",
    "        super(AdaptiveEmbeddingLayer, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab_size\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.large_embed_dim = large_embed_dim\n",
    "        self.small_embed_dim = small_embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.split_vocab(vocab, freq_threshold)  \n",
    "\n",
    "        self.frequent_embeddings = nn.Embedding(num_embeddings=len(self.frequent_vocab), embedding_dim=large_embed_dim)\n",
    "        self.infrequent_embeddings = nn.Embedding(num_embeddings=len(self.infrequent_vocab), embedding_dim=small_embed_dim)\n",
    "        self.infrequent_projection = nn.Linear(small_embed_dim, large_embed_dim)\n",
    "        self.positional_embeddings = PositionalEncoding(large_embed_dim, max_seq_len)\n",
    "\n",
    "\n",
    "    def split_vocab(self, vocab, freq_threshold):\n",
    "        token_counts = [(token, count) for token, count in vocab.items()]\n",
    "        token_counts.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        split_point = next(i for i, (_, count) in enumerate(token_counts) if count < freq_threshold)\n",
    "        \n",
    "        self.frequent_vocab = dict(token_counts[:split_point])\n",
    "        self.infrequent_vocab = dict(token_counts[split_point:])\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        device = token_ids.device\n",
    "        seq_len = token_ids.size(1)\n",
    "        batch_size = token_ids.size(0)  # Obtain batch size from token_ids tensor\n",
    "\n",
    "        # Initialize embeddings tensor\n",
    "        embeddings = torch.zeros(token_ids.shape[0], seq_len, self.large_embed_dim, device=device)\n",
    "\n",
    "        # Map token_ids to indices for frequent and infrequent vocab\n",
    "        frequent_indices = torch.zeros_like(token_ids)\n",
    "        infrequent_indices = torch.zeros_like(token_ids)\n",
    "        \n",
    "        for token_id, index in self.vocab.items():\n",
    "            mask = token_ids == token_id\n",
    "            if token_id in self.frequent_vocab:\n",
    "                # Map to index in frequent_vocab\n",
    "                frequent_indices[mask] = self.frequent_vocab[token_id]\n",
    "            elif token_id in self.infrequent_vocab:\n",
    "                # Map to index in infrequent_vocab\n",
    "                infrequent_indices[mask] = self.infrequent_vocab[token_id]\n",
    "\n",
    "        # Create masks for frequent and infrequent tokens\n",
    "        frequent_mask = frequent_indices > 0\n",
    "        infrequent_mask = infrequent_indices > 0\n",
    "\n",
    "        # Embed frequent tokens\n",
    "        if frequent_mask.any():\n",
    "            frequent_embeddings = self.frequent_embeddings(frequent_indices[frequent_mask])\n",
    "            embeddings[frequent_mask] = frequent_embeddings\n",
    "\n",
    "        # Embed and project infrequent tokens\n",
    "        if infrequent_mask.any():\n",
    "            infrequent_embeddings = self.infrequent_embeddings(infrequent_indices[infrequent_mask])\n",
    "            infrequent_embeddings_projected = self.infrequent_projection(infrequent_embeddings)\n",
    "            embeddings[infrequent_mask] = infrequent_embeddings_projected\n",
    "\n",
    "        # Apply positional embeddings\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        position_embeddings = self.positional_embeddings(position_ids)  # Generate for seq_len\n",
    "\n",
    "        # Ensure positional embeddings are broadcastable to the embeddings tensor\n",
    "        # This step may not be necessary if your positional embeddings are already correctly shaped\n",
    "        if position_embeddings.size(0) != batch_size:\n",
    "            position_embeddings = position_embeddings.expand(batch_size, -1, -1)\n",
    "\n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"Positional embeddings shape: {position_embeddings.shape}\")\n",
    "        embeddings += position_embeddings\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadLinformerAttention(embed_dim=d_model, num_heads=nhead)\n",
    "        self.dropout1 = AdaptiveDropoutLayer()  # Use AdaptiveDropoutLayer\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            AdaptiveDropoutLayer(),  # Use AdaptiveDropoutLayer here as well\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.dropout2 = AdaptiveDropoutLayer()  # And here\n",
    " \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2 = self.norm1(src)\n",
    "        attn_output = self.attn(src2)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout2(self.ffnn(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Assuming input_tensor is of shape [batch_size, seq_len, d_model], take the first token's representations\n",
    "        first_token_tensor = input_tensor[:, 0]\n",
    "        pooled_output = self.linear(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab, vocab_size, embedding_dim, max_seq_len, nhead, dim_feedforward, \n",
    "                 freq_threshold, smaller_embed_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = AdaptiveEmbeddingLayer(\n",
    "            vocab=vocab,\n",
    "            vocab_size=vocab_size, \n",
    "            freq_threshold=freq_threshold,  \n",
    "            large_embed_dim=embedding_dim,       \n",
    "            small_embed_dim=smaller_embed_dim,   \n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.encoder = TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward)\n",
    "        self.pooler = Pooler(embedding_dim)  # Retain Pooler for sentence-level representation\n",
    "        # Add an output projection layer for token-level predictions\n",
    "        self.output_projection = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        encoded = self.encoder(embedded, src_mask=attention_mask)\n",
    "        # Get pooled output for sentence-level tasks\n",
    "        pooled_output = self.pooler(encoded)\n",
    "        # Project encoded output to vocabulary size for token-level predictions\n",
    "        logits = self.output_projection(encoded)\n",
    "        return logits, pooled_output\n",
    "\n",
    "# Tokenizing Code\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None  # Store token IDs for efficient lookup\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, token, token_id):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        node = self.root\n",
    "        subword_ids = []\n",
    "        for char in token:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "                if node.token_id is not None:\n",
    "                    subword_ids.append(node.token_id)\n",
    "                    break  # Assuming one token maps to one subword for simplicity\n",
    "            else:\n",
    "                break  # No further subword match found\n",
    "        if not subword_ids:  # If no subword was found\n",
    "            subword_ids.append(self.unk_token_id if hasattr(self, 'unk_token_id') else 1) \n",
    "        return subword_ids\n",
    "    \n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction (remains the same)\n",
    "        self.trie = Trie()  \n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token, self.trie.token_id)  # Assuming insertion includes token_id\n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  \n",
    "\n",
    "                # Find Failure Link:\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None: \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break \n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:  \n",
    "                    child_node.failure_link = self.trie.root \n",
    "\n",
    "        # Step 3: Failure Pop Calculation \n",
    "        for node in queue:  # Could traverse in different orders; this is one option\n",
    "            if node.failure_link is not None and node.token_id is None: \n",
    "                # Condition: Node does not represent a valid vocabulary item itself\n",
    "                for i in range(current_node.failure_pop):\n",
    "                    current_node = current_node.failure_link \n",
    "                current_node.failure_pop += node.failure_pop \n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = None  # Will store vocabulary/frequency pairs\n",
    "        self.num_merges = 10  # Default number of merge operations\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the BPE model using the provided algorithm.\n",
    "\n",
    "        Args:\n",
    "            corpus: A text corpus represented as a list of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab = self.init_vocab(corpus)\n",
    "\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            print(best)  # Track most frequent pair in each iteration\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_chars = self.preprocess_to_characters(word) \n",
    "        subwords = []\n",
    "\n",
    "        while word_chars:  # Greedy encoding example\n",
    "            for i in range(len(word_chars), 0, -1):\n",
    "                subword = ''.join(word_chars[:i]) \n",
    "                if subword in self.vocab:\n",
    "                    subwords.append(subword)\n",
    "                    word_chars = word_chars[i:]\n",
    "                    break \n",
    "\n",
    "        return subwords \n",
    "\n",
    "    def init_vocab(self, corpus):\n",
    "        \"\"\"Creates initial vocabulary of words and their frequencies.\"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.split()  # Assuming simple word splitting\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Gets frequency of character/subword pairs\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Replaces a frequent pair with a new symbol.\"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "        merged_vocab = {}\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            merged_vocab[new_word] = merged_vocab.get(new_word, 0) + freq\n",
    "        return merged_vocab\n",
    "\n",
    "class SentencePiece:\n",
    "    def __init__(self):\n",
    "        self.trie = None  # Trie structure\n",
    "        self.failure_links = None \n",
    "        self.failure_pops = None\n",
    "\n",
    "    def train(self, corpus):\n",
    "        vocabulary = self._build_vocabulary(corpus)  # Build the word list\n",
    "        self.trie, self.failure_links, self.failure_pops = self._precompute(vocabulary)\n",
    "\n",
    "    def _encode(self, word):\n",
    "        subword_ids = []\n",
    "        current_node = self.trie.root\n",
    "\n",
    "        for i, char in enumerate(word):\n",
    "            if char in current_node.children:\n",
    "                current_node = current_node.children[char]\n",
    "                if current_node.token_id:\n",
    "                    subword_ids.append(current_node.token_id)\n",
    "            else:  # No direct match. Implement failure logic\n",
    "                while True:\n",
    "                    if current_node.failure_link is not None:\n",
    "                        current_node = current_node.failure_link\n",
    "\n",
    "                        # Check for pops to skip further backtracking\n",
    "                        for _ in range(current_node.failure_pop):\n",
    "                            current_node = current_node.failure_link\n",
    "\n",
    "                        if char in current_node.children:\n",
    "                            current_node = current_node.children[char]\n",
    "                            if current_node.token_id:\n",
    "                                subword_ids.append(current_node.token_id)\n",
    "                            break  # Successful subword transition \n",
    "\n",
    "                    else:  # No failure link, we're back at root level\n",
    "                        # Handle situation based on your application\n",
    "                        # e.g., append an unknown token \n",
    "                        subword_ids.append(self.unk_token_id)  \n",
    "                        break  \n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def _build_vocabulary(self, corpus, vocab_size=10000, model_type=\"unigram\"):\n",
    "        if model_type == \"unigram\":\n",
    "            tokens = self._unigram_tokenize(corpus)  #  Hypothetical tokenize function\n",
    "            vocabulary = self._build_unigram_vocab(tokens, vocab_size)\n",
    "        elif model_type == \"bpe\":\n",
    "            vocabulary = self._build_bpe_vocab(corpus, vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Use 'unigram' or 'bpe'\")\n",
    "        return vocabulary\n",
    "\n",
    "    def _build_unigram_vocab(self, tokens, vocab_size):\n",
    "        # Count token frequencies\n",
    "        token_freqs = collections.Counter(tokens)\n",
    "        # Select the most frequent tokens up to vocab_size\n",
    "        vocab = {token: idx for idx, (token, _) in enumerate(token_freqs.most_common(vocab_size))}\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def _precompute(self, vocabulary):\n",
    "        # Step 1: Trie Construction\n",
    "        self.trie = Trie()  # Assuming you have a Trie class as well\n",
    "        for token in vocabulary:\n",
    "            self.trie.insert(token)  \n",
    "\n",
    "        # Step 2: Failure Link Calculation\n",
    "        queue = [self.trie.root]  # Start with the root node\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            # Iterate over all possible immediate children \n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)  # Explore branches \n",
    "\n",
    "                # Find failure link (similar logic to Aho-Corasick)\n",
    "                failure_link_candidate = current_node.failure_link \n",
    "                while failure_link_candidate is not None:  \n",
    "                    if char in failure_link_candidate.children:\n",
    "                        child_node.failure_link = failure_link_candidate.children[char] \n",
    "                        break\n",
    "                    failure_link_candidate = failure_link_candidate.failure_link \n",
    "                else:\n",
    "                    child_node.failure_link = self.trie.root  # Fallback to root\n",
    "\n",
    "    def _unigram_tokenize(self, corpus):\n",
    "        \"\"\"\n",
    "        Tokenizes the given corpus into unigram tokens, checking against the built vocabulary.\n",
    "        Tokens not found in the vocabulary are treated as unknowns.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): The text corpus to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list of str: A list of tokens extracted from the corpus, adjusted to fit the vocabulary.\n",
    "        \"\"\"\n",
    "        # Initial tokenization based on word boundaries and punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[\\s\\.,!?;]', corpus)\n",
    "        \n",
    "        # Adjust tokens based on the vocabulary\n",
    "        adjusted_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                # Token is in the vocabulary, keep it\n",
    "                adjusted_tokens.append(token)\n",
    "            else:\n",
    "                # Token not found in the vocabulary, treat as unknown\n",
    "                adjusted_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        return adjusted_tokens\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, actual_vocab_size):\n",
    "        self.vocab = vocab\n",
    "        self.actual_vocab_size = actual_vocab_size\n",
    "        self.unk_token_id = self.vocab.get(\"[UNK]\", 1)  # Get ID of [UNK] \n",
    "        self.max_subword_length = max(len(token) for token in vocab.keys())\n",
    "        self.pattern = re.compile(r'\\b\\w+\\b|[\\s\\.,!?;]')\n",
    "\n",
    "        # Build the Trie\n",
    "        self.trie = Trie()\n",
    "        for token, token_id in vocab.items():\n",
    "            self.trie.insert(token, token_id)\n",
    "\n",
    "    def _find_subwords(self, word):\n",
    "        # 1. Trie Lookup \n",
    "        subword_ids = self.trie.find_subwords(word)\n",
    "\n",
    "        # 2. Fallback to Original Logic \n",
    "        if len(subword_ids) == 1 and subword_ids[0] == self.unk_token_id:  \n",
    "            subwords = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                for j in range(self.max_subword_length, 0, -1):\n",
    "                    subword = word[i:i+j]\n",
    "                    if subword in self.vocab:\n",
    "                        subwords.append(self.vocab[subword])\n",
    "                        i += j\n",
    "                        break \n",
    "                else: \n",
    "                    subwords.append(self.vocab[\"[UNK]\"])\n",
    "                    i += 1\n",
    "            subword_ids = subwords  # Replace with token IDs\n",
    "\n",
    "        return subword_ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        token_ids = [self.vocab.get(\"[CLS]\", 1)]  # Use [CLS] token or [UNK] if not found\n",
    "        tokens = self.pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            token_id = self.vocab.get(token, self.vocab.get(\"[UNK]\", 1))  # Fallback to [UNK] if token is not found\n",
    "            token_ids.append(token_id)\n",
    "        token_ids.append(self.vocab.get(\"[SEP]\", 1))  # Use [SEP] token or [UNK] if not found\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "# Training Code\n",
    "\n",
    "# Load corpus and build vocab\n",
    "corpus = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "tokens = tokenize(corpus)\n",
    "vocab = build_vocab(tokens)\n",
    "actual_vocab_size = len(vocab)  # This includes [PAD], [UNK], [CLS], [SEP], [MASK]\n",
    "\n",
    "print(f\"Actual vocabulary size (including special tokens): {actual_vocab_size}\")\n",
    "\n",
    "tokenizer = Tokenizer(vocab=vocab, actual_vocab_size=actual_vocab_size)\n",
    "\n",
    "# Data Loading (Illustrative)\n",
    "train_texts = [corpus]  # Treat your whole sample as one \"document\"\n",
    "train_dataset = TextDataset(train_texts, tokenizer, max_seq_len=512)\n",
    "\n",
    "print(\"Tokenizer unk_token_id:\", tokenizer.unk_token_id) \n",
    "print(\"Tokenizer Vocabulary:\", tokenizer.vocab)\n",
    "\n",
    "\n",
    "\n",
    "device='cpu'\n",
    "freq_threshold_values = [10, 50, 100, 200, 500]  \n",
    "best_validation_accuracy = 0.0 \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for freq_threshold in freq_threshold_values:\n",
    "\n",
    "    # Model instantiation and training setup\n",
    "    model = TransformerModel(\n",
    "        vocab = vocab,\n",
    "        vocab_size=actual_vocab_size,     \n",
    "        embedding_dim=128,\n",
    "        max_seq_len=512,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        freq_threshold=freq_threshold,  # frequency threshold for splitting vocab\n",
    "        smaller_embed_dim=64\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4) \n",
    "    meta_optimizer = AdaptiveWeightDecayOptimizer(model.parameters(), lr=1e-5) \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    meta_update_freq = 5\n",
    "\n",
    "    # Training loop adjusted for the updated model architecture\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)  # Assuming labels are of shape [batch_size, sequence_length]\n",
    "\n",
    "            # Forward pass, model now returns logits and pooled_output\n",
    "            logits, pooled_output = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Correctly reshape logits to match the labels' shape\n",
    "            # Change from [1, 512, vocab_size] to [512, vocab_size] to align with labels\n",
    "            logits = logits.view(-1, logits.size(-1))  # Reshape logits for loss calculation\n",
    "            \n",
    "            labels = labels.view(-1)  # Ensure labels are a flat vector\n",
    "\n",
    "            # Calculate loss using logits for token-level predictions\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Meta-update occasionally\n",
    "            if (i + 1) % meta_update_freq == 0:\n",
    "                meta_optimizer.zero_grad()\n",
    "                # Recalculate or reuse the loss for the meta-update\n",
    "                meta_loss = combined_loss(logits.detach(), labels.detach(), model)\n",
    "                meta_loss.backward()\n",
    "                meta_optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "# Save model state\n",
    "model_path = \"D:\\\\EXPERT_WEIGHTS\\\\encoding_transformer.bin\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save tokenizer using pickle for simplicity\n",
    "import pickle\n",
    "tokenizer_path = \"D:\\\\EXPERT_WEIGHTS\\\\tokenizer.pkl\"\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "\n",
    "# Assuming TransformerModel and Tokenizer classes are defined in the scope\n",
    "\n",
    "# Load the model\n",
    "model_loaded = TransformerModel(\n",
    "    vocab=vocab,  # Ensure `vocab` is loaded or defined in the scope\n",
    "    vocab_size=actual_vocab_size,\n",
    "    embedding_dim=128,\n",
    "    max_seq_len=512,\n",
    "    nhead=8,\n",
    "    dim_feedforward=2048,\n",
    "    freq_threshold=freq_threshold,  # Define or load `freq_threshold` as appropriate\n",
    "    smaller_embed_dim=64\n",
    ")\n",
    "model_loaded.load_state_dict(torch.load(\"D:\\\\EXPERT_WEIGHTS\\\\encoding_transformer.bin\"))\n",
    "model_loaded.eval()  # Set to evaluation mode\n",
    "\n",
    "# Load the tokenizer\n",
    "with open(\"D:\\\\EXPERT_WEIGHTS\\\\tokenizer.pkl\", 'rb') as f:\n",
    "    tokenizer_loaded = pickle.load(f)\n",
    "\n",
    "\n",
    "# Example text\n",
    "text = \"Here is some text to encode\"\n",
    "\n",
    "# Tokenize the input\n",
    "encoded_input = tokenizer_loaded.encode(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "# Predict with your model\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "    output = model_loaded(encoded_input)\n",
    "\n",
    "# Process the output as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import BertModel\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Set the HF_HOME environment variable to a new cache directory on the D drive\n",
    "os.environ['HF_HOME'] = 'D:/hf_datasets_cache'\n",
    "\n",
    "class ExpertConfig:\n",
    "    def __init__(self, seq_len=512, head_dim=64, block_size=64, sparsity_factor=4,\n",
    "                 input_dim=512, num_experts=3, vocab_size=30522, embed_size=256,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0.1,\n",
    "                 max_length=512,  # Updated to match seq_len for consistency\n",
    "                 rank=16, device='cpu',\n",
    "                 mamba_model_path='D:\\\\EXPERT_WEIGHTS\\\\mamba_model_weights.pth',\n",
    "                 rag_model_path='D:\\\\EXPERT_WEIGHTS\\\\rag_model_weights.pth',\n",
    "                 context_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\context_encoder.pth',\n",
    "                 language_model_path='D:\\\\EXPERT_WEIGHTS\\\\language_model.pth',\n",
    "                 question_encoder_path='D:\\\\EXPERT_WEIGHTS\\\\question_encoder.pth',\n",
    "                 dpo_model_path='D:\\\\EXPERT_WEIGHTS\\\\dpo_model_weights.pth',\n",
    "                 model_name='bert-base-uncased', embedding_dim=768,\n",
    "                 alpha=1, quantization_bits=8, tokenizer_name='bert-base-uncased',\n",
    "                 d_model=512, d_state=2048, d_conv=3, expansion_factor=2, \n",
    "                 clip_gradient = 1.0, mamba_learning_rate = 5e-4, weight_decay = 0.1,\n",
    "                 warmup_steps = 10, total_mamba_steps = 100\n",
    "                ):\n",
    "\n",
    "        # Common hyperparameters\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        self.input_dim = input_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length  # Ensure this is properly reflected in model components\n",
    "        self.rank = rank\n",
    "\n",
    "        # Model paths and device\n",
    "        self.mamba_model_path = mamba_model_path\n",
    "        self.rag_model_path = rag_model_path\n",
    "        self.context_encoder_path = context_encoder_path\n",
    "        self.language_model_path = language_model_path\n",
    "        self.question_encoder_path = question_encoder_path\n",
    "        self.dpo_model_path = dpo_model_path\n",
    "        self.device = device\n",
    "\n",
    "        # Unique hyperparameters\n",
    "        self.num_experts = num_experts\n",
    "        self.model_name = model_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.clip_gradient = clip_gradient\n",
    "        self.mamba_learning_rate = mamba_learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_mamba_steps = total_mamba_steps\n",
    "\n",
    "        # PDFs (unchanged)\n",
    "        self.pdf_file_paths = [\n",
    "            r'C:\\Users\\robbi\\IEEMM\\DPO.pdf', \n",
    "            r'C:\\Users\\robbi\\IEEMM\\MAMBA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\QLORA.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\RAG.pdf',\n",
    "            r'C:\\Users\\robbi\\IEEMM\\SWITCH_TRANSFORMER.pdf'\n",
    "        ]\n",
    "        \n",
    "        # Preserving original dataset loading functionality\n",
    "        self.rag_dataset = Expert.TransformerRAG.create_dataset_from_pdfs(self.pdf_file_paths)\n",
    "\n",
    "    def validate(self):\n",
    "        assert self.seq_len % self.block_size == 0, \"seq_len must be divisible by block_size\"\n",
    "        assert self.max_length >= self.seq_len, \"max_length should be equal to or greater than seq_len\"\n",
    "\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    # Load model weights function\n",
    "    def load_model_weights(model, model_path):\n",
    "        #checkpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        checkpoint = torch.load(model_path, map_location=config.device)\n",
    "\n",
    "        if isinstance(checkpoint, dict):\n",
    "            # Check for 'state_dict' or 'model_state_dict' keys\n",
    "            if 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            elif 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                # If no known key is found, try loading it as a raw state dictionary\n",
    "                try:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                except RuntimeError as e:\n",
    "                    raise ValueError(f\"Error loading state dict: {e}\")\n",
    "        elif isinstance(checkpoint, nn.Module):\n",
    "            # If the checkpoint is a model object, assign it directly\n",
    "            model = checkpoint\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    ###############################\n",
    "    # Flash2_Attention\n",
    "    class FlashAttention2(nn.Module):\n",
    "        def __init__(self, sequence_length, head_dimension, block_size):\n",
    "            super(Expert.FlashAttention2, self).__init__()\n",
    "            self.block_size = block_size\n",
    "            # Ensure that sequence_length is divisible by block_size for simplicity\n",
    "            assert sequence_length % block_size == 0\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            # Partitioning of inputs\n",
    "            Q_blocks, K_blocks, V_blocks = self.partition_inputs(Q, K, V)\n",
    "\n",
    "            # Efficient computation of the attention mechanism\n",
    "            outputs = []\n",
    "            for i, Q_block in enumerate(Q_blocks):\n",
    "                output_block = self.process_block(Q_block, K_blocks, V_blocks)\n",
    "                outputs.append(output_block)\n",
    "\n",
    "            # Concatenating the processed blocks\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "            return output\n",
    "\n",
    "        def partition_inputs(self, Q, K, V):\n",
    "            # The actual partitioning scheme should be based on sequence length, head dimension, and block size\n",
    "            Q_blocks = Q.chunk(chunks=Q.size(0) // self.block_size, dim=0)\n",
    "            K_blocks = K.chunk(chunks=K.size(0) // self.block_size, dim=0)\n",
    "            V_blocks = V.chunk(chunks=V.size(0) // self.block_size, dim=0)\n",
    "            return Q_blocks, K_blocks, V_blocks\n",
    "\n",
    "        def process_block(self, Q_block, K_blocks, V_blocks):\n",
    "            # Process each block efficiently as per FLASH2's optimized method\n",
    "            # This includes computing QK^T, applying online softmax, and multiplying with V\n",
    "            output_blocks = []\n",
    "            for K_block, V_block in zip(K_blocks, V_blocks):\n",
    "                attention_scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n",
    "                attention_scores = self.online_softmax(attention_scores)\n",
    "                output_block = torch.matmul(attention_scores, V_block)\n",
    "                output_blocks.append(output_block)\n",
    "\n",
    "            # Summing up the results from each block\n",
    "            output_block_sum = sum(output_blocks)\n",
    "            return output_block_sum\n",
    "\n",
    "        def online_softmax(self, scores, chunk_size=128):\n",
    "            # Apply softmax in chunks for large sequences\n",
    "            softmaxed_scores = []\n",
    "            for i in range(0, scores.size(0), chunk_size):\n",
    "                chunk = scores[i:i + chunk_size, :]\n",
    "                softmaxed_chunk = F.softmax(chunk, dim=1)\n",
    "                softmaxed_scores.append(softmaxed_chunk)\n",
    "            return torch.cat(softmaxed_scores, dim=0)\n",
    "\n",
    "    ###############################\n",
    "    # SparseFlash2_Attention\n",
    "    class SparseFlash2Attention(nn.Module):\n",
    "        def __init__(self, seq_len, head_dim, blk_size, sparsity_factor):\n",
    "            super().__init__()\n",
    "            self.flash_attention = Expert.FlashAttention2(seq_len, head_dim, blk_size)\n",
    "            self.seq_len = seq_len\n",
    "            self.head_dim = head_dim\n",
    "            self.block_size = blk_size  # Storing block_size as an instance variable\n",
    "            self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        def generate_sparsity_mask(self):\n",
    "            mask = torch.zeros(self.seq_len, self.seq_len)\n",
    "            step = self.sparsity_factor\n",
    "            for i in range(0, self.seq_len, step):\n",
    "                mask[i:i + step, :] = 1\n",
    "            return mask.bool()\n",
    "\n",
    "        def forward(self, Q, K, V):\n",
    "            output = self.flash_attention(Q, K, V)  # output shape: [sequence_length, head_dimension]\n",
    "\n",
    "            # Reshape output to be 3D for batch matrix multiplication\n",
    "            output = output.unsqueeze(0)  # New shape: [1, sequence_length, head_dimension]\n",
    "\n",
    "            sparsity_mask = self.generate_sparsity_mask()  # shape: [sequence_length, sequence_length]\n",
    "\n",
    "            # Apply the sparsity mask to the output\n",
    "            sparsity_mask = sparsity_mask.unsqueeze(0)  # New shape: [1, sequence_length, sequence_length]\n",
    "            output = torch.bmm(sparsity_mask.float(), output.float())  # Perform batch matrix multiplication\n",
    "\n",
    "            # Reshape the output back to 2D\n",
    "            output = output.squeeze(0)  # New shape: [sequence_length, head_dimension]\n",
    "\n",
    "            return output\n",
    "\n",
    "    ###############################\n",
    "    # MAMBA\n",
    "    # RoPE\n",
    "    class RotaryPositionalEncoding(nn.Module):\n",
    "        def __init__(self, dim, max_len=5000):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "            t = torch.arange(max_len).type_as(inv_freq)\n",
    "            freqs = torch.einsum('n , d -> n d', t, inv_freq)\n",
    "            self.register_buffer('sin', freqs.sin())\n",
    "            self.register_buffer('cos', freqs.cos())\n",
    "\n",
    "        def forward(self, x):\n",
    "            n, _, device = x.shape[1], self.dim // 2, x.device\n",
    "            sin, cos = self.sin[:n].to(device), self.cos[:n].to(device)\n",
    "\n",
    "            # Apply RoPE to even and odd indices separately\n",
    "            x_even = x[..., :self.dim:2] * cos.unsqueeze(0) + torch.roll(x[..., 1:self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            x_odd = x[..., 1:self.dim:2] * cos.unsqueeze(0) - torch.roll(x[..., :self.dim:2], shifts=1, dims=-1) * sin.unsqueeze(0)\n",
    "            return torch.cat((x_even, x_odd), dim=-1)\n",
    "\n",
    "    # SWIGLU\n",
    "    class SwiGLU(nn.Module):\n",
    "        def __init__(self, dim_in, dim_out):\n",
    "            super(Expert.SwiGLU, self).__init__()\n",
    "            self.fc1 = nn.Linear(dim_in, dim_out)\n",
    "            self.fc2 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "        def forward(self, x):\n",
    "            gate = torch.sigmoid(self.fc2(x))\n",
    "            return self.fc1(x) * gate\n",
    "\n",
    "    class SimplifiedMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "            \n",
    "            self.feedforward = nn.Sequential(\n",
    "                nn.Linear(self.d_model, self.d_state),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(self.d_state, self.d_model)\n",
    "            )\n",
    "            self.input_embedding = nn.Linear(self.d_model, self.d_model)\n",
    "            self.convs = nn.Sequential(*[nn.Conv1d(self.d_model, self.d_model, kernel_size=self.d_conv, padding=(self.d_conv // 2)) for _ in range(self.num_layers)])\n",
    "            self.swiglu = Expert.SwiGLU(self.d_model, self.d_model)\n",
    "            self.output_projection = nn.Linear(self.d_model, self.d_model * self.expansion_factor)\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "\n",
    "            nn.init.orthogonal_(self.input_embedding.weight, gain)\n",
    "            nn.init.normal_(self.input_embedding.bias, mean=0, std=0.01)\n",
    "\n",
    "            nn.init.kaiming_uniform_(self.convs[-1].weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.convs[-1].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.zeros_(self.feedforward[0].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.feedforward[2].weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.feedforward[2].bias)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=nn.init.calculate_gain('linear'))\n",
    "            nn.init.zeros_(self.output_projection.bias)\n",
    "\n",
    "        def forward(self, inputs, attention_mask=None):\n",
    "            print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "            # Apply the attention mask if provided\n",
    "            if attention_mask is not None:\n",
    "                inputs = inputs * attention_mask.unsqueeze(-1)\n",
    "\n",
    "            projected_inputs = self.input_embedding(inputs)\n",
    "            print(\"projected_inputs pre-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post-reshape shape:\", projected_inputs.shape)\n",
    "\n",
    "            for conv in self.convs:\n",
    "                projected_inputs = conv(projected_inputs)\n",
    "\n",
    "            projected_inputs = projected_inputs.permute(0, 2, 1)\n",
    "            print(\"projected_inputs post convolution reshape:\", projected_inputs.shape)\n",
    "\n",
    "            projected_inputs = self.swiglu(projected_inputs)\n",
    "            print(\"projected_inputs post swiglu shape:\", projected_inputs.shape)\n",
    "\n",
    "            output = self.output_projection(projected_inputs)\n",
    "            print(\"output shape:\", output.shape)\n",
    "\n",
    "            return output\n",
    "\n",
    "    class SimplifiedLanguageModelMAMBA(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Using configuration parameters\n",
    "            self.vocab_size = config.vocab_size\n",
    "            self.num_layers = config.num_layers\n",
    "            self.d_model = config.d_model\n",
    "            self.d_state = config.d_state\n",
    "            self.d_conv = config.d_conv\n",
    "            self.expansion_factor = config.expansion_factor\n",
    "\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "            self.pos_encoder = Expert.RotaryPositionalEncoding(self.d_model)\n",
    "            self.simplified_mamba = Expert.SimplifiedMAMBA(config)\n",
    "            self.output_projection = nn.Linear(self.d_model * 2, self.vocab_size)  # Adjust if needed\n",
    "\n",
    "            self.initialize_weights()\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            gain = 1.0\n",
    "\n",
    "            nn.init.orthogonal_(self.embedding.weight, gain)\n",
    "            nn.init.xavier_uniform_(self.output_projection.weight, gain=gain)\n",
    "\n",
    "        def forward(self, input_values, attention_mask):\n",
    "            embedded = self.embedding(input_values) * math.sqrt(self.d_model)\n",
    "            embedded = self.pos_encoder(embedded)\n",
    "            simplified_mamba_output = self.simplified_mamba(embedded, attention_mask)\n",
    "            logits = self.output_projection(simplified_mamba_output)\n",
    "            return logits\n",
    "\n",
    "    ###############################\n",
    "    # Switch Router\n",
    "\n",
    "    class SwitchRouter(nn.Module):\n",
    "        CAPACITY_FACTOR = 1  # Class constant\n",
    "\n",
    "        class SwitchGate(nn.Module):\n",
    "            def __init__(self, input_dim, num_experts):\n",
    "                super(Expert.SwitchRouter.SwitchGate, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, input_dim // 2)\n",
    "                self.fc2 = nn.Linear(input_dim // 2, num_experts)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                gate_scores = F.softmax(self.fc2(x), dim=-1)\n",
    "                return gate_scores\n",
    "\n",
    "        def __init__(self, config):\n",
    "            super(Expert.SwitchRouter, self).__init__()\n",
    "            self.config = config\n",
    "            self.device = config.device\n",
    "            self.router = self.SwitchGate(config.input_dim, config.num_experts).to(config.device)\n",
    "            self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "            self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.transformer_dpo = Expert.DPO(self.lmt, config.device,config.embed_size).to(config.device)\n",
    "            self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "            self.experts = nn.ModuleList([self.transformer_rag, self.transformer_dpo, self.mamba])\n",
    "            self.input_embedding = nn.Linear(config.input_dim, config.input_dim)\n",
    "\n",
    "        def forward(self, x, attention_mask, context_texts, question_text):\n",
    "            x = x.to(self.device)\n",
    "            if x.dtype != torch.float32:\n",
    "                x = x.float()\n",
    "\n",
    "            x = self.input_embedding(x)\n",
    "            gate_scores = self.router(x)\n",
    "            expert_indices = torch.argmax(gate_scores, dim=1)\n",
    "            final_output = torch.zeros_like(x)\n",
    "\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                mask = expert_indices == i\n",
    "                if mask.any():\n",
    "                    selected_inputs = x[mask]\n",
    "                    selected_attention_mask = attention_mask[mask].to(self.device)\n",
    "                    if isinstance(expert, Expert.TransformerRAG):\n",
    "                        expert_output = expert.forward(context_texts, selected_inputs, selected_attention_mask, question_text)\n",
    "                    else:\n",
    "                        expert_output = expert(selected_inputs, selected_attention_mask)\n",
    "                    final_output[mask] = expert_output\n",
    "\n",
    "            aux_loss = self.auxiliary_loss(gate_scores)\n",
    "            return final_output, aux_loss\n",
    "\n",
    "        def auxiliary_loss(self, gate_scores):\n",
    "            expert_load = gate_scores.sum(0) / gate_scores.size(0)\n",
    "            loss_balancing = torch.std(expert_load)\n",
    "            return loss_balancing\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def route_inputs(expert_indices, gate_scores, num_experts):\n",
    "            capacity_factor_tensor = torch.tensor([Expert.SwitchRouter.CAPACITY_FACTOR], dtype=torch.float32)\n",
    "            capacities = (gate_scores.size(0) * capacity_factor_tensor / num_experts).int()\n",
    "            expert_counts = torch.zeros(num_experts, dtype=torch.int32)\n",
    "            for idx in range(len(expert_indices)):\n",
    "                selected_expert = expert_indices[idx]\n",
    "                if expert_counts[selected_expert] < capacities[0]:\n",
    "                    expert_counts[selected_expert] += 1\n",
    "                else:\n",
    "                    available_experts = (expert_counts < capacities[0]).nonzero(as_tuple=False).view(-1)\n",
    "                    if len(available_experts) > 0:\n",
    "                        alternative_expert = available_experts[0]\n",
    "                        expert_indices[idx] = alternative_expert\n",
    "                        expert_counts[alternative_expert] += 1\n",
    "                    else:\n",
    "                        print(\"No available experts to reroute. Handling overflow.\")\n",
    "            return expert_indices\n",
    "    \n",
    "    ###############################\n",
    "    # RAG\n",
    "    class CustomDPRContextEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.CustomDPRContextEncoder, self).__init__()  \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None):\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            context_embeddings = self.embedding_layer(pooled_output)\n",
    "            return context_embeddings\n",
    "\n",
    "    class DPRQuestionEncoder(nn.Module):\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(Expert.DPRQuestionEncoder, self).__init__() \n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.embedding_layer = nn.Linear(self.bert_model.config.hidden_size, embedding_dim)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, **kwargs):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            embeddings = self.embedding_layer(pooled_output)\n",
    "            return embeddings\n",
    "\n",
    "    class TransformerRAG(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super(Expert.TransformerRAG, self).__init__()\n",
    "            self.config = config\n",
    "            self.context_encoder = Expert.CustomDPRContextEncoder(config.embedding_dim).to(config.device)\n",
    "            self.language_model = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "            self.question_encoder = Expert.DPRQuestionEncoder(config.embedding_dim).to(config.device)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "\n",
    "        def forward(self, context_texts, question_input_ids, question_attention_mask, question_text):\n",
    "            if question_input_ids.max() >= self.tokenizer.vocab_size:\n",
    "                raise ValueError(\"question_input_ids contain ID(s) beyond the tokenizer's vocabulary size\")\n",
    "            \n",
    "            aggregated_context_embeddings = []\n",
    "            for context_list in context_texts:\n",
    "                if not all(isinstance(context, dict) for context in context_list):\n",
    "                    raise TypeError(\"Each item in context_texts must be a list of tokenized context dictionaries\")\n",
    "                \n",
    "                aggregated_context_embedding = torch.zeros(self.context_encoder.bert_model.config.hidden_size, device=device)\n",
    "                for context in context_list:\n",
    "                    context_input_ids = context['input_ids'].to(config.device)\n",
    "                    context_attention_mask = context['attention_mask'].to(config.device)\n",
    "                    context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                    aggregated_context_embedding += context_embedding.mean(dim=0)\n",
    "                \n",
    "                aggregated_context_embeddings.append(aggregated_context_embedding / len(context_list))\n",
    "            \n",
    "            question_input_ids = question_input_ids.to(config.device).long()\n",
    "            question_attention_mask = question_attention_mask.to(config.device).long()\n",
    "            question_embeddings = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "            \n",
    "            cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "            similarities = [cos_sim(question_embeddings, context_emb.squeeze(0)) for context_emb in aggregated_context_embeddings]\n",
    "            most_relevant_context_idx = torch.argmax(torch.tensor(similarities, device=config.device))\n",
    "            \n",
    "            combined_input = question_text + \" \" + context_texts[most_relevant_context_idx]\n",
    "            tokenized_combined_input = self.tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            tokenized_combined_input = {k: v.to(config.device) for k, v in tokenized_combined_input.items()}\n",
    "            response_logits = self.language_model(**tokenized_combined_input)\n",
    "            probabilities = F.softmax(response_logits.logits, dim=-1)\n",
    "            predicted_token_ids = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            \n",
    "            return response\n",
    "\n",
    "        @staticmethod\n",
    "        def extract_text_from_pdf(file_path):\n",
    "            text = \"\"\n",
    "            with fitz.open(file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            return text\n",
    "\n",
    "        @staticmethod\n",
    "        def split_into_chunks(text, chunk_size):\n",
    "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        @staticmethod\n",
    "        def preprocess_text(text, max_length=512):\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            chunk_size = max_length - 50\n",
    "            text_chunks = Expert.TransformerRAG.split_into_chunks(text, chunk_size)\n",
    "            processed_chunks = []\n",
    "            for chunk in text_chunks:\n",
    "                tokenized_output = tokenizer(chunk, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "                processed_chunk = {\n",
    "                    'input_ids': tokenized_output['input_ids'],\n",
    "                    'attention_mask': tokenized_output['attention_mask']\n",
    "                }\n",
    "                processed_chunks.append(processed_chunk)\n",
    "            return processed_chunks\n",
    "\n",
    "        @staticmethod\n",
    "        def create_dataset_from_pdfs(pdf_file_paths):\n",
    "            dataset = []\n",
    "            for file_path in pdf_file_paths:\n",
    "                text = Expert.TransformerRAG.extract_text_from_pdf(file_path)\n",
    "                processed_text = Expert.TransformerRAG.preprocess_text(text)\n",
    "                dataset.append(processed_text)\n",
    "            return dataset\n",
    "\n",
    "        def retrieve_contexts(self, dataset, query_embedding, top_k=5):\n",
    "            similarity_scores = []\n",
    "            for context in dataset:\n",
    "                context_input_ids = context['input_ids'].to(self.config.device)\n",
    "                context_attention_mask = context['attention_mask'].to(self.config.device)\n",
    "                # Use the class's context_encoder\n",
    "                context_embedding = self.context_encoder(context_input_ids, context_attention_mask)\n",
    "                similarity = torch.matmul(query_embedding, context_embedding.T)\n",
    "                similarity_scores.append(similarity.squeeze().item())\n",
    "            top_k_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "            top_contexts = [dataset[i] for i in top_k_indices]\n",
    "            return top_contexts\n",
    "\n",
    "        def rag_retrieve_and_generate(self, dataset, query):\n",
    "            # Tokenize the query using the class's tokenizer\n",
    "            tokenized_query = self.tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
    "            input_ids = tokenized_query['input_ids']\n",
    "            attention_mask = tokenized_query['attention_mask']\n",
    "            # Use the class's question_encoder\n",
    "            encoded_query = self.question_encoder(input_ids, attention_mask)\n",
    "            relevant_contexts = self.retrieve_contexts(dataset, encoded_query)\n",
    "            # Assuming generate_response is a method of LanguageModelTransformer that accepts tokenized contexts and generates a response\n",
    "            response = self.language_model.generate_response(relevant_contexts)\n",
    "            return response\n",
    "\n",
    "    ###############################\n",
    "    # LORA\n",
    "    class LORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1):\n",
    "            super(Expert.LORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "\n",
    "            # Original weight and bias of the linear layer\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # LORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"LORALayer Input Shape:\", x.shape)\n",
    "            \n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ self.A) @ self.B\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "            \n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            #print(\"LORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # QLORA\n",
    "    class QLORALayer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, rank, alpha=1, quantization_bits=8):\n",
    "            super(Expert.QLORALayer, self).__init__()\n",
    "            self.rank = rank\n",
    "            self.alpha = alpha\n",
    "            self.quantization_bits = quantization_bits\n",
    "\n",
    "            # Original weight and bias\n",
    "            self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "            # QLORA specific parameters\n",
    "            self.A = nn.Parameter(torch.Tensor(input_dim, rank))\n",
    "            self.B = nn.Parameter(torch.Tensor(rank, output_dim))\n",
    "\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.bias)\n",
    "            nn.init.normal_(self.A, 0, 0.02)\n",
    "            nn.init.normal_(self.B, 0, 0.02)\n",
    "\n",
    "        def quantize(self, x, num_bits):\n",
    "            # Implement a simple quantization method\n",
    "            scale = x.abs().max()\n",
    "            x_quantized = torch.round(x / scale * (2**num_bits - 1))\n",
    "            return x_quantized, scale\n",
    "\n",
    "        def forward(self, x):\n",
    "            #print(\"QLORALayer Input Shape:\", x.shape)\n",
    "            original_size = x.size()\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x_flattened = x.reshape(-1, original_size[-1])\n",
    "\n",
    "            A_quantized, scale_A = self.quantize(self.A, self.quantization_bits)\n",
    "            B_quantized, scale_B = self.quantize(self.B, self.quantization_bits)\n",
    "\n",
    "            # Compute lora_adjustment for each input in the batch\n",
    "            lora_adjustment = self.alpha * (x_flattened @ (A_quantized / scale_A)) @ (B_quantized / scale_B)\n",
    "            lora_adjustment = lora_adjustment.reshape(batch_size, seq_len, -1)\n",
    "            lora_adjustment = self.dropout(lora_adjustment)\n",
    "            #print(\"Adjusted lora_adjustment Shape:\", lora_adjustment.shape)\n",
    "            #print(\"self.weight Shape:\", self.weight.shape)\n",
    "\n",
    "            # Apply linear transformation to x_flattened\n",
    "            x_transformed = nn.functional.linear(x_flattened, self.weight, self.bias)\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "            # Add lora_adjustment to the transformed x\n",
    "            x = x_transformed + lora_adjustment\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            #print(\"QLORALayer Output Shape:\", x.shape)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def update_alpha(self, new_alpha):\n",
    "            \"\"\"\n",
    "            Update the alpha scaling factor.\n",
    "            \"\"\"\n",
    "            self.alpha = new_alpha\n",
    "\n",
    "    ###############################\n",
    "    # Language Model Transformer\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, embed_size, heads):\n",
    "            super(Expert.MultiHeadAttention, self).__init__()\n",
    "            self.embed_size = embed_size\n",
    "            self.heads = heads\n",
    "            self.head_dim = embed_size // heads\n",
    "\n",
    "            assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "            ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        def forward(self, values, keys, query, mask):\n",
    "            N = query.shape[0]\n",
    "            value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "            # Split the embedding into self.heads different pieces\n",
    "            values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "            keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "            queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            # Einsum does the matrix multiplication for query*keys for each training example\n",
    "            # with every other training example, then sum it up\n",
    "            attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "            out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "                N, query_len, self.heads * self.head_dim\n",
    "            )\n",
    "\n",
    "            out = self.fc_out(out)\n",
    "            return out\n",
    "\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, embed_size, heads, dropout, forward_expansion, rank):\n",
    "            super(Expert.TransformerBlock, self).__init__()\n",
    "            self.attention = Expert.MultiHeadAttention(embed_size, heads)\n",
    "            self.norm1 = nn.LayerNorm(embed_size)\n",
    "            self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                Expert.LORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.LORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, value, key, query, mask):\n",
    "            attention = self.attention(value, key, query, mask)\n",
    "            x = self.dropout(self.norm1(attention + query))\n",
    "            forward = self.feed_forward(x)\n",
    "            out = self.dropout(self.norm2(forward + x))\n",
    "            return out\n",
    "\n",
    "    class LanguageModelDecoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, rank):\n",
    "            super(Expert.LanguageModelDecoder, self).__init__()\n",
    "            self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "            # Adding BatchNorm layers\n",
    "            self.bn1 = nn.BatchNorm1d(embed_size)\n",
    "            self.bn2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    Expert.TransformerBlock(embed_size, heads, dropout, forward_expansion, rank)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # QLORA layers\n",
    "            self.qlora_feed_forward = nn.Sequential(\n",
    "                Expert.QLORALayer(embed_size, forward_expansion * embed_size, rank),\n",
    "                nn.ReLU(),\n",
    "                Expert.QLORALayer(forward_expansion * embed_size, embed_size, rank),\n",
    "            )\n",
    "            self.use_qlora = False  # Flag to toggle QLORA\n",
    "\n",
    "            self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, trg_mask):\n",
    "            N, seq_length = x.shape\n",
    "            if seq_length > self.position_embedding.num_embeddings:\n",
    "                raise ValueError(f\"Sequence length {seq_length} exceeds maximum allowed {self.position_embedding.num_embeddings}\")\n",
    "            positions = torch.arange(0, seq_length).expand(N, seq_length).to(config.device)\n",
    "            x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "            print(f\"Max position index: {positions.max().item()}, Position Embedding Size: {self.position_embedding.num_embeddings}\")\n",
    "\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn1(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                x = layer(x, x, x, trg_mask)\n",
    "                if self.use_qlora:\n",
    "                    x = self.qlora_feed_forward(x)\n",
    "\n",
    "            # Transpose for BatchNorm, apply batch normalization, and then transpose back\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.bn2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            out = self.fc_out(x)\n",
    "            #print(f\"shape of output of forward method of LanguageModelDecoder: {out.shape} \")\n",
    "\n",
    "            return out\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.use_qlora = use_qlora\n",
    "\n",
    "    class LanguageModelTransformer(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.vocab_size = config.vocab_size  # Use vocab_size from ExpertConfig\n",
    "            self.embed_size = config.embed_size  # Use embed_size from ExpertConfig\n",
    "            self.num_layers = config.num_layers  # Use num_layers from ExpertConfig\n",
    "            self.forward_expansion = config.forward_expansion  # Use forward_expansion from ExpertConfig\n",
    "            self.heads = config.heads  # Use heads from ExpertConfig\n",
    "            self.dropout = config.dropout  # Use dropout from ExpertConfig\n",
    "            self.max_length = config.max_length  # Use max_length from ExpertConfig\n",
    "            self.rank = config.rank  # Use rank from ExpertConfig\n",
    "            self.tokenizer_name = config.tokenizer_name  # Use tokenizer_name from ExpertConfig\n",
    "\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "            self.decoder = Expert.LanguageModelDecoder(\n",
    "                self.vocab_size,\n",
    "                self.embed_size,\n",
    "                self.num_layers,\n",
    "                self.heads,\n",
    "                self.forward_expansion,\n",
    "                self.dropout,\n",
    "                self.max_length,\n",
    "                self.rank,\n",
    "            )\n",
    "\n",
    "        def forward(self, trg, attention_mask=None):  # Remove attention_mask here since it's not used\n",
    "            print(f\"Input shape to LanguageModelTransformer: {trg.shape}\")\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            out = self.decoder(trg, trg_mask)  # Do not pass attention_mask here\n",
    "            print(f\"Language Model Transformer out shape: {out.shape}\")\n",
    "            return out\n",
    "\n",
    "        def make_trg_mask(self, trg):\n",
    "            N, trg_len = trg.shape\n",
    "            trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len).to(trg.device)\n",
    "            return trg_mask\n",
    "\n",
    "        def toggle_qlora(self, use_qlora: bool):\n",
    "            self.decoder.toggle_qlora(use_qlora)\n",
    "\n",
    "        def generate_response(self, input_ids, attention_mask):\n",
    "            logits = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_token_id = torch.argmax(probabilities, dim=-1)\n",
    "            predicted_tokens = [self.tokenizer.convert_ids_to_tokens(idx.item()) for idx in predicted_token_id]\n",
    "            response = self.tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "            return response\n",
    "\n",
    "    ###############################\n",
    "    # DPO\n",
    "    class DPO(nn.Module):\n",
    "        def __init__(self, language_model, device, embed_size):\n",
    "            super(Expert.DPO, self).__init__()\n",
    "            self.language_model = language_model\n",
    "            self.device = device\n",
    "            # Assuming embed_size is accessible and correct\n",
    "            self.projection = nn.Linear(language_model.vocab_size, embed_size)  # Project from vocab_size to embed_size\n",
    "            self.classifier = nn.Linear(embed_size, 2)  # Assuming embed_size is accessible\n",
    "\n",
    "        def forward(self, input_ids_question, input_ids_chosen=None, input_ids_rejected=None, labels=None):\n",
    "            combined_input_ids = torch.cat((input_ids_question, input_ids_chosen, input_ids_rejected), dim=1)\n",
    "\n",
    "            # Assuming combined_input_ids has shape [batch_size, sequence_length]\n",
    "            logits = self.language_model(combined_input_ids)  # Output shape: [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "            # Project logits to embedding space before pooling\n",
    "            projected_logits = self.projection(logits)  # New shape: [batch_size, sequence_length, embed_size]\n",
    "            \n",
    "            # Apply global mean pooling across the sequence length dimension\n",
    "            pooled_logits = projected_logits.mean(dim=1)  # New shape: [batch_size, embed_size]\n",
    "\n",
    "            # Pass the pooled representation through the classifier\n",
    "            predictions = self.classifier(pooled_logits)  # New shape: [batch_size, 2]\n",
    "\n",
    "            # Calculate loss if labels are provided\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(predictions, labels)\n",
    "\n",
    "            return predictions, loss\n",
    "\n",
    "\n",
    "    def __init__(self, config: ExpertConfig):\n",
    "        super().__init__()\n",
    "        # Validate configuration\n",
    "        config.validate()\n",
    "\n",
    "        # 1. SparseFlash2_attention\n",
    "        self.sparse_flash2_attention = Expert.SparseFlash2Attention(\n",
    "            config.seq_len, \n",
    "            config.head_dim, \n",
    "            config.block_size, \n",
    "            config.sparsity_factor\n",
    "        )\n",
    "\n",
    "        # Inside the Expert class definition\n",
    "        self.lmt = Expert.LanguageModelTransformer(config).to(config.device)\n",
    "\n",
    "        self.transformer_rag = Expert.TransformerRAG(config).to(config.device)\n",
    "\n",
    "        # Corrected instantiation of SimplifiedLanguageModelMAMBA\n",
    "        self.mamba = Expert.SimplifiedLanguageModelMAMBA(config).to(config.device)\n",
    "\n",
    "        # Now initialize DPO with the language model transformer\n",
    "        self.transformer_dpo = Expert.DPO(self.lmt, config.device, config.embed_size).to(config.device)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        self.layer_norm = nn.LayerNorm(config.input_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        self.switch_router = Expert.SwitchRouter(config)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        self.qlora = Expert.QLORALayer(config.input_dim, config.input_dim, config.rank)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask, context_texts, question_text):\n",
    "        # 1. SparseFlash2_attention\n",
    "        x = self.sparse_flash2_attention(x)\n",
    "\n",
    "        # 2. LayerNorm and Dropout\n",
    "        x = self.dropout(self.layer_norm(x))\n",
    "\n",
    "        # 3. Internal Switch Routing\n",
    "        x, aux_loss = self.switch_router(x, attention_mask, context_texts, question_text)\n",
    "\n",
    "        # 4. QLORA Layer\n",
    "        x = self.qlora(x)\n",
    "\n",
    "        return x, aux_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_new_alpha(current_loss, initial_loss, initial_alpha=1.0, final_alpha=0.1):\n",
    "        \"\"\"\n",
    "        Calculate a new alpha value based on the current loss.\n",
    "        \"\"\"\n",
    "        if current_loss >= initial_loss:\n",
    "            return initial_alpha  # Keep initial alpha if loss isn't decreasing\n",
    "\n",
    "        loss_ratio = current_loss / initial_loss\n",
    "        alpha_range = initial_alpha - final_alpha\n",
    "        new_alpha = final_alpha + (alpha_range * loss_ratio)\n",
    "        return new_alpha  \n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_optimizer(model, learning_rate, weight_decay, warmup_steps, total_steps):\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Linear warmup with cosine decay\n",
    "        scheduler = LambdaLR(optimizer, lambda step: min((step + 1) / warmup_steps, 0.5 * (1 + math.cos(math.pi * step / total_steps))))\n",
    "\n",
    "        return optimizer, scheduler\n",
    "    ###############################\n",
    "    # TRAINING METHODS\n",
    "\n",
    "    # DPO Training\n",
    "    def train_dpo(self, train_loader, optimizer, config, save_path):\n",
    "            self.train()  # Set the model to training mode\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                input_ids_question = batch['input_ids_question'].to(config.device)\n",
    "                input_ids_chosen = batch['input_ids_chosen'].to(config.device)\n",
    "                input_ids_rejected = batch['input_ids_rejected'].to(config.device)\n",
    "                labels = batch['labels'].to(config.device)\n",
    "                print(f\"train_dpo input_ids_question: {input_ids_question.shape}\")\n",
    "                print(f\"train_dpo input_ids_chosen: {input_ids_chosen.shape}\")\n",
    "                print(f\"train_dpo input_ids_rejected: {input_ids_rejected.shape}\")\n",
    "                print(f\"train_dpo labels: {labels.shape}\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                logit, loss = self.transformer_dpo(input_ids_question, input_ids_chosen, input_ids_rejected, labels)\n",
    "                print(f\"Logits shape: {logit.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            print(f\"Training complete. Average Loss: {average_loss}\")\n",
    "            \n",
    "            # Save the model\n",
    "            torch.save(self.transformer_dpo.state_dict(), save_path)\n",
    "\n",
    "            return average_loss\n",
    "\n",
    "    # RAG Training\n",
    "    def train_language_model_rag(self, model, save_path, train_loader, device, num_epochs=5, lr=1e-8, weight_decay=1e-4):\n",
    "        # Enable QLORA during training\n",
    "        model.decoder.toggle_qlora(True)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "\n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                targets = batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        # Toggle QLORA off after training\n",
    "        model.decoder.toggle_qlora(False)\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"Training Complete\")\n",
    "        return model, average_loss\n",
    "\n",
    "    # DPR Training\n",
    "    def train_dpr_encoders(self, train_data, context_encoder, question_encoder, optimizer_context, optimizer_question, epochs , context_save_path, question_save_path):\n",
    "        loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(train_data[\"queries\"])):\n",
    "                query = train_data[\"queries\"][i]\n",
    "                context = train_data[\"contexts\"][i]\n",
    "\n",
    "                # Ensure query is a string\n",
    "                if not isinstance(query, str):\n",
    "                    raise ValueError(\"Query must be a string.\")\n",
    "                tokenized_query = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                # Ensure context is a string\n",
    "                if isinstance(context, dict):\n",
    "                    # If context is a dictionary, extract the text content. This is a placeholder and might need adjustment.\n",
    "                    context_text = context.get(\"text\", \"\")\n",
    "                elif isinstance(context, str):\n",
    "                    context_text = context\n",
    "                else:\n",
    "                    raise ValueError(\"Context must be a string or a dictionary containing a text field.\")\n",
    "                tokenized_context = tokenizer(context_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "                question_embeddings = question_encoder(input_ids=tokenized_query['input_ids'], attention_mask=tokenized_query['attention_mask'])\n",
    "                context_embeddings = context_encoder(input_ids=tokenized_context['input_ids'], attention_mask=tokenized_context['attention_mask'])\n",
    "\n",
    "                # The labels tensor should have the same first dimension size as the input tensors\n",
    "                labels = torch.tensor([1.0] * question_embeddings.size(0), dtype=torch.float).to(question_embeddings.device)\n",
    "\n",
    "                loss = loss_function(question_embeddings, context_embeddings, labels)\n",
    "\n",
    "                optimizer_context.zero_grad()\n",
    "                optimizer_question.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_context.step()\n",
    "                optimizer_question.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_data['queries'])}\")\n",
    "        average_loss = total_loss / len(train_data['queries'])\n",
    "        torch.save(context_encoder.state_dict(), context_save_path)\n",
    "        torch.save(question_encoder.state_dict(), question_save_path)\n",
    "        return (context_encoder, question_encoder), average_loss\n",
    "       \n",
    "    # LMT Training\n",
    "    def train_language_model_transformer(self, train_loader, device, vocab_size, save_path):\n",
    "        model = self.lmt\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.98)\n",
    "        num_epochs = 5\n",
    "        \n",
    "        initial_loss = None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            model.decoder.toggle_qlora(True)\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                inputs, targets = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "                # Check for NaN in loss\n",
    "                if math.isnan(loss.item()):\n",
    "                    print(\"Encountered NaN loss, stopping training\")\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Set the initial_loss after the first batch of the first epoch\n",
    "                if initial_loss is None and batch_idx == 0:\n",
    "                    initial_loss = loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Check for NaN in total_loss\n",
    "            if math.isnan(total_loss):\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} stopped due to NaN loss\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Update alpha at the end of each epoch based on the average loss\n",
    "            new_alpha = self.calculate_new_alpha(average_loss, initial_loss)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, Expert.QLORALayer):\n",
    "                    layer.update_alpha(new_alpha)\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        return model, average_loss\n",
    "\n",
    "    # MAMBA Training\n",
    "    def train_mamba(self, train_loader, num_epochs, config):\n",
    "            # Initialize the optimizer and scheduler with MAMBA model parameters\n",
    "            optimizer, scheduler = self.setup_optimizer(self.mamba, \n",
    "                                                        config.mamba_learning_rate, \n",
    "                                                        config.weight_decay, \n",
    "                                                        config.warmup_steps, \n",
    "                                                        config.total_mamba_steps)\n",
    "\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            progress_bar = tqdm(range(num_epochs))\n",
    "\n",
    "            for epoch in progress_bar:\n",
    "                self.mamba.train()\n",
    "                total_loss = 0\n",
    "\n",
    "                for batch in train_loader:\n",
    "                    input_values, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "                    input_values = input_values.to(config.device)\n",
    "                    attention_mask = attention_mask.to(config.device)\n",
    "                    labels = labels.to(config.device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass through MAMBA model\n",
    "                    outputs = self.mamba(input_values, attention_mask)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = loss_fn(outputs.view(-1, config.vocab_size), labels.view(-1))\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Clip gradients and perform an optimization step\n",
    "                    torch.nn.utils.clip_grad_norm_(self.mamba.parameters(), config.clip_gradient)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save the trained MAMBA model\n",
    "            torch.save(self.mamba.state_dict(), config.mamba_model_path)\n",
    "            print(f\"MAMBA Training Complete. Model saved to {config.mamba_model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Full Expert Training\n",
    "    def train_expert(self, train_loader, train_data, optimizer, main_loss_function, aux_loss_weight, device,save_path, accumulation_steps=4, num_epochs=5):\n",
    "            self.train()  # Set the model to training mode\n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0\n",
    "                optimizer.zero_grad()  # Initialize gradients to zero\n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "                    inputs, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "\n",
    "                    # Calculate start and end indices for current batch in train_data\n",
    "                    start_idx = batch_idx * batch['input_ids'].size(0)\n",
    "                    end_idx = start_idx + batch['input_ids'].size(0)\n",
    "\n",
    "                    # Extract current_queries and current_contexts for the batch\n",
    "                    current_queries = train_data['queries'][start_idx:end_idx]\n",
    "                    current_contexts = train_data['contexts'][start_idx:end_idx]\n",
    "\n",
    "                    # Call to the model forward function\n",
    "                    outputs, aux_loss = self(inputs, attention_mask, current_contexts, current_queries)\n",
    "\n",
    "                    # Calculate loss and accumulate\n",
    "                    main_loss = main_loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                    loss = (main_loss + aux_loss_weight * aux_loss) / accumulation_steps\n",
    "                    loss.backward()\n",
    "\n",
    "                    if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    total_loss += loss.item() * accumulation_steps  # Scale back up\n",
    "\n",
    "                average_loss = total_loss / len(train_loader)\n",
    "                print(f'End of Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            torch.save(self.state_dict(), save_path)\n",
    "            return self, average_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Language Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Assuming the Expert class and ExpertConfig class have already been defined\n",
    "\n",
    "# Initialize configuration\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "\n",
    "# Initialize Expert system\n",
    "expert_system = Expert(config)\n",
    "\n",
    "# Load Wikipedia dataset and preprocess\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:1%]\")  # Using 1% of the data for demonstration\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=config.max_length)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])  # Remove original text to only keep tokenized versions\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(tokenized_datasets, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define save path for the trained model\n",
    "save_path = 'D:/EXPERT_WEIGHTS/lmt_expert_trained.pth'\n",
    "\n",
    "\n",
    "# Train the LMT sub-model within the Expert system\n",
    "trained_model, average_loss = expert_system.train_language_model_transformer(\n",
    "    train_loader=train_loader, \n",
    "    device=config.device, \n",
    "    vocab_size=config.vocab_size, \n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "print(f\"Training complete. Model saved to {save_path}. Average Loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Training transformer_with_dpo\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def preprocess_dpo_data(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    vocab_size = len(tokenizer.vocab)  # Make sure this matches your embedding layer's vocab size\n",
    "\n",
    "    # Define max sequence length\n",
    "    max_seq_length = 512 // 3\n",
    "\n",
    "    # Tokenize 'question', 'chosen', and 'rejected' fields\n",
    "    tokenized_questions = tokenizer(examples['question'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_chosen = tokenizer(examples['chosen'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "    tokenized_rejected = tokenizer(examples['rejected'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "\n",
    "    # Prepare the labels: 1 for 'chosen' and 0 for 'rejected'\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(len(examples['question']))]\n",
    "\n",
    "    return {\n",
    "        'input_ids_question': tokenized_questions['input_ids'],\n",
    "        'attention_mask_question': tokenized_questions['attention_mask'],\n",
    "        'input_ids_chosen': tokenized_chosen['input_ids'],\n",
    "        'attention_mask_chosen': tokenized_chosen['attention_mask'],\n",
    "        'input_ids_rejected': tokenized_rejected['input_ids'],\n",
    "        'attention_mask_rejected': tokenized_rejected['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Load the DPO dataset from Hugging Face\n",
    "dpo_dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "\n",
    "# Apply the preprocessing to the dataset\n",
    "dpo_dataset = dpo_dataset.map(preprocess_dpo_data, batched=True)\n",
    "\n",
    "\n",
    "# You can convert to PyTorch tensors after processing\n",
    "dpo_dataset.set_format(type='torch', columns=['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(dpo_dataset['train'], batch_size=2, shuffle=True)\n",
    "\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "expert_model = Expert(config)\n",
    "\n",
    "model_vocab_size = expert_model.lmt.decoder.word_embedding.num_embeddings\n",
    "print(f\"Model vocab size: {model_vocab_size}\")\n",
    "print(f\"Model embedding size: {expert_model.lmt.decoder.word_embedding.embedding_dim}\")\n",
    "print(f\"Configured max length: {config.max_length}\")\n",
    "\n",
    "optimizer = AdamW(expert_model.parameters(), lr=1e-5)\n",
    "save_path = 'D:/EXPERT_WEIGHTS/dpo_model.pth'\n",
    "\n",
    "# Train the DPO model\n",
    "label_column = 'labels'\n",
    "input_columns = ['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
    "# Assuming expert_model is an instance of Expert\n",
    "avg_loss = expert_model.train_dpo(train_loader, optimizer, config, save_path)\n",
    "\n",
    "# Save the model\n",
    "torch.save(expert_model.transformer_dpo.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [402, 1, 2, 3, 4, 5, 6]\n",
      "Original: How transferable are features in deep neural\n",
      "Decoded: how  transferable  are  features  in  deep  neural\n",
      "\n",
      "Encoded: [7, 8]\n",
      "Original: networks?\n",
      "Decoded: networks  ?\n",
      "\n",
      "Encoded: [0, 693, 11, 12, 0, 0, 11, 15, 0, 0]\n",
      "Original: Jason Yosinski,1 Jeff Clune,2 Yoshua Bengio,3 and Hod Lipson4\n",
      "Decoded: How  yosinski  ,  1  How  How  ,  2  How  How  ,  3  and  How  How\n",
      "\n",
      "Encoded: [12, 0, 23, 0, 0, 11, 0, 0]\n",
      "Original: 1 Dept. Computer Science, Cornell University\n",
      "Decoded: 1  How  .  How  How  ,  How  How\n",
      "\n",
      "Encoded: [15, 0, 23, 0, 0, 11, 0, 28, 0]\n",
      "Original: 2 Dept. Computer Science, University of Wyoming\n",
      "Decoded: 2  How  .  How  How  ,  How  of  How\n",
      "\n",
      "Encoded: [18, 0, 23, 0, 0, 30, 0, 0, 11, 0]\n",
      "Original: 3 Dept. Computer Science & Operations Research, University of Montreal\n",
      "Decoded: 3  How  .  How  How  &  How  How  ,  How  of  How\n",
      "\n",
      "Encoded: [34, 0, 23, 0, 30, 0, 0, 11, 0, 0]\n",
      "Original: 4 Dept. Mechanical & Aerospace Engineering, Cornell University\n",
      "Decoded: 4  How  .  How  &  How  How  ,  How  How\n",
      "\n",
      "Encoded: [0]\n",
      "Original: Abstract\n",
      "Decoded: How\n",
      "\n",
      "Encoded: [75, 5, 6, 7, 40, 41, 42, 43, 44, 45]\n",
      "Original: Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters\n",
      "Decoded: many  deep  neural  networks  trained  on  natural  images  exhibit  a  curious  phenomenon  in  common  :  on  the  first  layer  they  learn  features  similar  to  How  filters\n",
      "\n",
      "Encoded: [19, 59, 60, 23, 651, 51, 62, 52, 3, 63]\n",
      "Original: and color blobs. Such first-layer features appear not to be specific to a particular\n",
      "Decoded: and  color  blobs  .  such  first  -  layer  features  appear  not  to  be  specific  to  a  particular\n",
      "\n",
      "Encoded: [402, 1, 2, 3, 4, 5, 6]\n",
      "Original: How transferable are features in deep neural\n",
      "Decoded: how  transferable  are  features  in  deep  neural\n",
      "\n",
      "Encoded: [7, 8]\n",
      "Original: networks?\n",
      "Decoded: networks  ?\n",
      "\n",
      "Encoded: [0, 693, 11, 12, 0, 0, 11, 15, 0, 0]\n",
      "Original: Jason Yosinski,1 Jeff Clune,2 Yoshua Bengio,3 and Hod Lipson4\n",
      "Decoded: How  yosinski  ,  1  How  How  ,  2  How  How  ,  3  and  How  How\n",
      "\n",
      "Encoded: [12, 0, 23, 0, 0, 11, 0, 0]\n",
      "Original: 1 Dept. Computer Science, Cornell University\n",
      "Decoded: 1  How  .  How  How  ,  How  How\n",
      "\n",
      "Encoded: [15, 0, 23, 0, 0, 11, 0, 28, 0]\n",
      "Original: 2 Dept. Computer Science, University of Wyoming\n",
      "Decoded: 2  How  .  How  How  ,  How  of  How\n",
      "\n",
      "Encoded: [18, 0, 23, 0, 0, 30, 0, 0, 11, 0]\n",
      "Original: 3 Dept. Computer Science & Operations Research, University of Montreal\n",
      "Decoded: 3  How  .  How  How  &  How  How  ,  How  of  How\n",
      "\n",
      "Encoded: [34, 0, 23, 0, 30, 0, 0, 11, 0, 0]\n",
      "Original: 4 Dept. Mechanical & Aerospace Engineering, Cornell University\n",
      "Decoded: 4  How  .  How  &  How  How  ,  How  How\n",
      "\n",
      "Encoded: [0]\n",
      "Original: Abstract\n",
      "Decoded: How\n",
      "\n",
      "Encoded: [75, 5, 6, 7, 40, 41, 42, 43, 44, 45]\n",
      "Original: Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters\n",
      "Decoded: many  deep  neural  networks  trained  on  natural  images  exhibit  a  curious  phenomenon  in  common  :  on  the  first  layer  they  learn  features  similar  to  How  filters\n",
      "\n",
      "Encoded: [19, 59, 60, 23, 651, 51, 62, 52, 3, 63]\n",
      "Original: and color blobs. Such first-layer features appear not to be specific to a particular\n",
      "Decoded: and  color  blobs  .  such  first  -  layer  features  appear  not  to  be  specific  to  a  particular\n",
      "\n",
      "vocab ; 1770 , new_vocab: 1770\n",
      "Sample Vocabulary Check:\n",
      "How: 0\n",
      "transferable: 1\n",
      "are: 2\n",
      "features: 3\n",
      "in: 4\n",
      "deep: 5\n",
      "neural: 6\n",
      "networks: 7\n",
      "?: 8\n",
      "Jason: 9\n",
      "Found 0 subtokens in vocabulary.\n",
      "Trie Construction Completed Successfully\n",
      "Trie built successfully.\n",
      "Testing ': How transferable are features in deep neural\n",
      "Token IDs: [402, 1, 2, 3, 4, 5, 6]\n",
      "Tokenized: [402, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "Testing ': networks?\n",
      "Token IDs: [7, 8]\n",
      "Tokenized: [7, 8]\n",
      "\n",
      "Testing ': Jason Yosinski,1 Jeff Clune,2 Yoshua Bengio,3 and Hod Lipson4\n",
      "Token IDs: [0, 0, 0, 693, 11, 12, 0, 0, 0, 0]\n",
      "Tokenized: [0, 0, 0, 693, 11, 12, 0, 0, 0, 0, 0, 11, 15, 0, 0, 0, 0, 0, 0, 11, 18, 19, 0, 0, 0, 531, 34]\n",
      "\n",
      "Testing ': 1 Dept. Computer Science, Cornell University\n",
      "Token IDs: [12, 0, 23, 345, 0, 0, 0, 11, 0, 1707]\n",
      "Tokenized: [12, 0, 23, 345, 0, 0, 0, 11, 0, 1707, 0, 0, 0, 0, 0, 0, 249, 1193]\n",
      "\n",
      "Testing ': 2 Dept. Computer Science, University of Wyoming\n",
      "Token IDs: [15, 0, 23, 345, 0, 0, 0, 11, 0, 0]\n",
      "Tokenized: [15, 0, 23, 345, 0, 0, 0, 11, 0, 0, 0, 0, 0, 249, 1193, 28, 0, 0, 0, 660, 626]\n",
      "\n",
      "Testing ': 3 Dept. Computer Science & Operations Research, University of Montreal\n",
      "Token IDs: [18, 0, 23, 345, 0, 0, 0, 30, 0, 0]\n",
      "Tokenized: [18, 0, 23, 345, 0, 0, 0, 30, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 249, 1193, 28, 0, 0, 0, 0, 0]\n",
      "\n",
      "Testing ': 4 Dept. Mechanical & Aerospace Engineering, Cornell University\n",
      "Token IDs: [34, 0, 23, 0, 0, 0, 30, 45, 0, 0]\n",
      "Tokenized: [34, 0, 23, 0, 0, 0, 30, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 1707, 0, 0, 0, 0, 0, 0, 249, 1193]\n",
      "\n",
      "Testing ': Abstract\n",
      "Token IDs: [973, 0, 0]\n",
      "Tokenized: [973, 0, 0]\n",
      "\n",
      "Testing ': Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters\n",
      "Token IDs: [75, 5, 6, 7, 40, 41, 42, 43, 44, 45]\n",
      "Tokenized: [75, 5, 6, 7, 40, 41, 42, 43, 44, 45, 46, 47, 4, 48, 49, 41, 50, 51, 52, 53, 54, 3, 55, 56, 0, 0, 0, 58]\n",
      "\n",
      "Testing ': and color blobs. Such first-layer features appear not to be specific to a particular\n",
      "Token IDs: [19, 59, 60, 23, 651, 51, 62, 52, 3, 63]\n",
      "Tokenized: [19, 59, 60, 23, 651, 51, 62, 52, 3, 63, 64, 56, 65, 66, 56, 45, 67]\n",
      "\n",
      "Token IDs: [402, 1, 2, 3, 4, 5, 6]\n",
      "Token IDs: [7, 8]\n",
      "Token IDs: [0, 0, 0, 693, 11, 12, 0, 0, 0, 0]\n",
      "Token IDs: [12, 0, 23, 345, 0, 0, 0, 11, 0, 1707]\n",
      "Token IDs: [15, 0, 23, 345, 0, 0, 0, 11, 0, 0]\n",
      "Token IDs: [18, 0, 23, 345, 0, 0, 0, 30, 0, 0]\n",
      "Token IDs: [34, 0, 23, 0, 0, 0, 30, 45, 0, 0]\n",
      "Token IDs: [973, 0, 0]\n",
      "Token IDs: [75, 5, 6, 7, 40, 41, 42, 43, 44, 45]\n",
      "Token IDs: [19, 59, 60, 23, 651, 51, 62, 52, 3, 63]\n",
      "Token IDs: [68, 69, 70, 11, 71, 72, 4, 73, 53, 2]\n",
      "Token IDs: [3, 79, 80, 81, 82, 72, 56, 66, 83, 50]\n",
      "Token IDs: [50, 85, 11, 71, 86, 81, 87, 64, 88, 89]\n",
      "Token IDs: [94, 95, 50, 96, 97, 98, 28, 99, 4, 100]\n",
      "Token IDs: [28, 45, 5, 101, 6, 85, 19, 102, 45, 103]\n",
      "Token IDs: [116, 52, 99, 56, 117, 118, 70, 119, 50, 120]\n",
      "Token IDs: [122, 70, 11, 123, 124, 125, 11, 19, 113, 15]\n",
      "Token IDs: [139, 140, 11, 141, 41, 142, 3, 2, 143, 82]\n",
      "Token IDs: [145, 11, 69, 146, 28, 50, 85, 23, 93, 148]\n",
      "Token IDs: [73, 156, 3, 157, 82, 158, 77, 159, 65, 160]\n",
      "Token IDs: [3, 23, 45, 165, 104, 166, 107, 73, 167, 45]\n",
      "Token IDs: [3, 82, 169, 170, 171, 28, 172, 159, 173, 45]\n",
      "Token IDs: [73, 176, 157, 177, 178, 62, 179, 56, 50, 122]\n",
      "Token IDs: [12, 0, 0, 0, 0]\n",
      "Token IDs: [1370, 5, 6, 7, 44, 45, 46, 47, 49, 182]\n",
      "Token IDs: [56, 54, 51, 62, 52, 3, 73, 185, 137, 0]\n",
      "Token IDs: [58, 107, 188, 48, 73, 189, 190, 191, 41, 45]\n",
      "Token IDs: [195, 196, 197, 69, 45, 198, 199, 23, 86, 47]\n",
      "Token IDs: [76, 11, 71, 157, 168, 205, 204, 206, 207, 11]\n",
      "Token IDs: [113, 1194, 0, 0, 0, 0, 1741, 1193, 212, 213]\n",
      "Token IDs: [296, 225, 138, 226, 3, 41, 50, 51, 52, 227]\n",
      "Token IDs: [232, 19, 42, 192, 68, 11, 93, 233, 138, 51]\n",
      "Token IDs: [237, 73, 50, 3, 238, 83, 50, 84, 52, 28]\n",
      "Token IDs: [196, 68, 19, 70, 23, 203, 134, 11, 4, 45]\n",
      "Token IDs: [87, 88, 246, 40, 247, 45, 209, 210, 248, 11]\n",
      "Token IDs: [66, 56, 45, 67, 251, 23, 93, 252, 233, 50]\n",
      "Token IDs: [28, 72, 19, 66, 203, 123, 93, 250, 256, 257]\n",
      "Token IDs: [12]\n",
      "Token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 23, 264]\n",
      "Token IDs: [3, 2, 72, 19, 84, 62, 52, 3, 2, 66]\n",
      "Token IDs: [56, 66, 274, 4, 50, 85, 23, 86, 275, 276]\n",
      "Token IDs: [278, 159, 93, 95, 50, 280, 56, 123, 45, 67]\n",
      "Token IDs: [278, 290, 50, 81, 228, 282, 119, 45, 283, 52]\n",
      "Token IDs: [278, 512, 290, 86, 81, 291, 292, 49, 293, 50]\n",
      "Token IDs: [93, 2, 294, 4, 50, 295, 56, 138, 277, 296]\n",
      "Token IDs: [85, 2, 72, 11, 93, 250, 65, 299, 56, 300]\n",
      "Token IDs: [212, 213, 23, 11, 223, 305, 0, 0, 0, 11]\n",
      "Token IDs: [70, 11, 19, 272, 93, 307, 50, 308, 3, 11]\n",
      "Token IDs: [65, 40, 41, 45, 122, 68, 19, 70, 23, 86]\n",
      "Token IDs: [313, 314, 56, 315, 154, 19, 122, 77, 11, 316]\n",
      "Token IDs: [182, 50, 122, 68, 107, 318, 319, 161, 50, 154]\n",
      "Token IDs: [320, 321, 56, 322, 206, 45, 323, 122, 85, 324]\n",
      "Token IDs: [329, 330, 28, 86, 331, 56, 332, 333, 62, 28]\n",
      "Token IDs: [113, 0, 0, 0, 1168, 212, 213, 23, 11, 336]\n",
      "Token IDs: [138, 172, 28, 6, 7, 343, 344, 345, 3, 73]\n",
      "Token IDs: [347, 348, 50, 349, 28, 350, 50, 230, 351, 19]\n",
      "Token IDs: [50, 352, 302, 217, 353, 107, 56, 306, 45, 154]\n",
      "Token IDs: [50, 51, 356, 172, 28, 45, 122, 85, 23, 50]\n",
      "Token IDs: [359, 19, 40, 247, 50, 122, 70, 23, 404, 159]\n",
      "Token IDs: [50, 364, 70, 365, 50, 154, 113, 366, 114, 3]\n",
      "Token IDs: [368, 172, 159, 65, 369, 370, 11, 313, 73, 53]\n",
      "Token IDs: [50, 373, 28, 142, 69, 64, 56, 178, 62, 367]\n",
      "Token IDs: [375, 28, 50, 122, 68, 19, 50, 171, 28, 376]\n",
      "Token IDs: [377, 19, 50, 171, 28, 376, 107, 323, 11, 178]\n",
      "Token IDs: [2, 378, 369, 370, 23, 41, 50, 235, 236, 11]\n",
      "Token IDs: [377, 11, 188, 73, 325, 107, 64, 45, 379, 11]\n",
      "Token IDs: [56, 381, 121, 23, 28, 383, 11, 312, 50, 122]\n",
      "Token IDs: [302, 296, 50, 387, 388, 58, 389, 390, 65, 308]\n",
      "Token IDs: [392, 105, 82, 100, 28, 138, 110, 393, 394, 178]\n",
      "Token IDs: [50, 395, 396, 23]\n",
      "Token IDs: [4, 86, 92, 93, 397, 288, 398, 49]\n",
      "Token IDs: [12, 23, 93, 399, 45, 400, 56, 95, 50, 280]\n",
      "Token IDs: [402, 403, 3, 119, 73, 52, 302, 82, 404, 70]\n",
      "Token IDs: [28, 101, 6, 7, 41, 50, 192, 791, 68, 19]\n",
      "Token IDs: [81, 82, 72, 56, 66, 113, 34, 114, 11, 123]\n",
      "Token IDs: [15, 23, 93, 94, 411, 110, 412, 112, 73, 413]\n",
      "Token IDs: [113, 417, 114, 126, 127, 418, 56, 129, 50, 154]\n",
      "Token IDs: [419, 172, 23, 93, 411, 402, 100, 28, 138, 110]\n",
      "Token IDs: [50, 85, 23, 113, 34, 23, 12, 114]\n",
      "Token IDs: [18, 23, 93, 95, 402, 50, 121, 421, 28, 156]\n",
      "Token IDs: [50, 154, 70, 19, 122, 70, 2, 23, 113, 34]\n",
      "Token IDs: [34, 23, 41, 50, 423, 323, 192, 791, 68, 11]\n",
      "Token IDs: [426, 203, 319, 76, 113, 0, 0, 0, 0, 0]\n",
      "Token IDs: [387, 62, 52, 428, 429, 23, 40, 428, 23, 93]\n",
      "Token IDs: [315, 370, 19, 178, 62, 380, 394, 19, 424, 50]\n",
      "Token IDs: [431, 23, 165, 11, 93, 424, 73, 167, 45, 85]\n",
      "Token IDs: [28, 172, 159, 173, 45, 174, 56, 175, 121, 177]\n",
      "Token IDs: [86, 107, 433, 104, 296, 50, 434, 28, 435, 436]\n",
      "Token IDs: [177, 438, 178, 62, 179, 23, 113, 34, 23, 12]\n",
      "Token IDs: [15]\n",
      "Token IDs: [15, 96, 429, 23, 98, 836, 1198, 152, 302, 121]\n",
      "Token IDs: [93, 328, 444, 50, 46, 445, 28, 0, 0, 0]\n",
      "Token IDs: [6, 7, 40, 41, 42, 43, 23, 4, 86, 447]\n",
      "Token IDs: [28, 3, 308, 41, 70, 45, 152, 50, 297, 56]\n",
      "Token IDs: [107, 452, 56, 453, 73, 86, 454, 374, 41, 50]\n",
      "Token IDs: [28, 210, 77, 45, 19, 1181, 83, 457, 407, 28]\n",
      "Token IDs: [68, 23, 12, 138, 460, 159, 65, 196, 56, 65]\n",
      "Token IDs: [56, 456, 77, 45, 19, 1181, 11, 93, 358, 462]\n",
      "Token IDs: [404, 475, 62, 52, 101, 85, 41, 45, 19, 405]\n",
      "Token IDs: [19, 154, 1181, 11, 2, 478, 4, 50, 146, 110]\n",
      "Token IDs: [19, 306, 288, 364, 7, 23, 4, 50, 395, 484]\n",
      "Token IDs: [50, 134, 52, 196, 23, 51, 11, 93, 399, 19]\n",
      "Token IDs: [278, 45, 487, 85, 1181, 18, 1181, 49, 50, 51]\n",
      "Token IDs: [172, 113, 34, 490, 491, 114, 2, 359, 358, 19]\n",
      "Token IDs: [493, 302, 85, 23, 113, 730, 12, 11, 494, 18]\n",
      "Token IDs: [278, 45, 302, 85, 45, 18, 1181, 49, 50, 51]\n",
      "Token IDs: [172, 113, 34, 490, 491, 114, 2, 359, 358, 19]\n",
      "Token IDs: [50, 51, 18, 172, 82, 45, 85, 40, 41, 68]\n",
      "Token IDs: [28, 301, 56, 498, 45, 364, 122, 68, 1181, 23]\n",
      "Token IDs: [73, 50, 501, 62, 52, 3, 2, 72, 11, 119]\n",
      "Token IDs: [107, 500, 73, 50, 501, 62, 52, 3, 2, 66]\n",
      "Token IDs: [93, 505, 86, 310, 203, 183, 356, 4, 481, 12]\n",
      "Token IDs: [15]\n",
      "Token IDs: [19, 4, 315, 506, 113, 415, 23, 507, 23, 133]\n",
      "Token IDs: [50, 510, 110, 7, 11, 50, 143, 172, 2, 370]\n",
      "Token IDs: [7, 512, 50, 143, 172, 2, 178, 62, 380, 49]\n",
      "Token IDs: [278, 45, 487, 85, 1181, 18, 1181, 513, 49, 390]\n",
      "Token IDs: [278, 45, 302, 85, 45, 18, 1181, 513, 49, 390]\n",
      "Token IDs: [56, 456, 154, 19, 122, 76, 73, 2, 55, 56]\n",
      "Token IDs: [192, 791, 464, 56, 45, 19, 469, 56, 1181, 23]\n",
      "Token IDs: [19, 519, 11, 514, 138, 520, 464, 82, 50, 521]\n",
      "Token IDs: [0, 0, 0, 0, 525, 11, 507, 626, 1193, 0]\n",
      "Token IDs: [41, 537, 11, 45, 19, 1181, 250, 100, 538, 468]\n",
      "Token IDs: [154, 7, 40, 41, 100, 68, 250, 328, 3, 119]\n",
      "Token IDs: [28, 544, 23, 182, 545, 56, 50, 235, 68, 11]\n",
      "Token IDs: [548, 40, 41, 146, 28, 549, 550, 62, 388, 539]\n",
      "Token IDs: [182, 552, 83, 358, 553, 464, 56, 100, 11, 19]\n",
      "Token IDs: [430, 160, 161, 182, 45, 19, 1181, 2, 554, 55]\n",
      "Token IDs: [203, 0, 0, 367, 11, 4, 192, 791, 93, 2]\n",
      "Token IDs: [560, 561, 56, 456, 45, 562, 462, 28, 50, 68]\n",
      "Token IDs: [82, 100, 235, 152, 565, 49, 168, 68, 45, 466]\n",
      "Token IDs: [42, 568, 23, 50, 462, 107, 64, 569, 157, 11]\n",
      "Token IDs: [42, 571, 23, 347, 574, 28, 86, 462, 19, 50]\n",
      "Token IDs: [577, 23, 4, 34, 23, 15, 93, 250, 411, 73]\n",
      "Token IDs: [182, 50, 76, 2, 554, 55, 23]\n",
      "Token IDs: [12, 50, 192, 791, 68, 11, 152, 579, 4, 50]\n",
      "Token IDs: [113, 0, 212, 213, 23, 11, 219, 114, 516, 12]\n",
      "Token IDs: [168, 404, 28, 463, 464, 23]\n",
      "Token IDs: [15, 453, 73, 356, 485, 491, 593, 594, 595, 397]\n",
      "Token IDs: [284, 107, 600, 40, 41, 1181, 23]\n",
      "Token IDs: [18]\n",
      "Token IDs: [601]\n",
      "Token IDs: [45]\n",
      "Token IDs: [602]\n",
      "Token IDs: [45]\n",
      "Token IDs: [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs: [601]\n",
      "Token IDs: [1181]\n",
      "Token IDs: [602]\n",
      "Token IDs: [1181]\n",
      "Token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs: [0, 0, 0]\n",
      "Token IDs: [69, 69, 69]\n",
      "Token IDs: [1181, 18, 1181]\n",
      "Token IDs: [19]\n",
      "Token IDs: [1181, 18, 1181, 513]\n",
      "Token IDs: [69, 69, 69]\n",
      "Token IDs: [0, 0, 0, 0, 0, 0]\n",
      "Token IDs: [154, 45]\n",
      "Token IDs: [154, 1181]\n",
      "Token IDs: [45, 18, 1181]\n",
      "Token IDs: [19]\n",
      "Token IDs: [45, 18, 1181, 513]\n",
      "Token IDs: [730, 12, 49, 287, 28, 50, 620, 621, 19, 622]\n",
      "Token IDs: [2, 40, 162, 226, 209, 624, 41, 202, 469, 28]\n",
      "Token IDs: [469, 11, 309, 494, 49, 1181, 469, 114, 23, 50]\n",
      "Token IDs: [73, 52, 11, 168, 50, 59, 630, 123, 68, 50]\n",
      "Token IDs: [633, 634, 130, 628, 635, 627, 50, 636, 28, 50]\n",
      "Token IDs: [494, 49, 4, 50, 487, 85, 492, 11, 50, 51]\n",
      "Token IDs: [2, 366, 82, 45, 154, 85, 113, 507, 23, 626]\n",
      "Token IDs: [359, 11, 19, 272, 50, 640, 85, 107, 40, 41]\n",
      "Token IDs: [50, 51, 356, 172, 2, 137, 642, 372, 206, 113]\n",
      "Token IDs: [54, 113, 643, 178, 62, 380, 644, 487, 645, 1181]\n",
      "Token IDs: [65, 653, 182, 404, 52, 107, 370, 23, 410, 494]\n",
      "Token IDs: [107, 50, 641, 152, 50, 487, 645, 11, 655, 73]\n",
      "Token IDs: [41, 404, 68, 113, 507, 23, 626, 23, 45, 114]\n",
      "Token IDs: [645, 656, 50, 297, 56, 123, 50, 3, 41, 52]\n",
      "Token IDs: [18, 620, 682]\n",
      "Token IDs: [1377, 1194, 0, 0, 0, 0, 1741, 1193, 212, 213]\n",
      "Token IDs: [19, 311, 247, 664, 197, 28, 323, 101, 665, 23]\n",
      "Token IDs: [93, 667, 64, 56, 668, 669, 121, 11, 71, 670]\n",
      "Token IDs: [672, 23, 93, 300, 50, 673, 674, 556, 83, 0]\n",
      "Token IDs: [105, 250, 65, 678, 11, 679, 11, 19, 680, 56]\n",
      "Token IDs: [50, 206, 682, 113, 217, 683, 11, 684, 23, 114]\n",
      "Token IDs: [34, 105, 19, 0, 0, 0]\n",
      "Token IDs: [93, 697, 698, 699, 28, 689, 23, 50, 700, 701]\n",
      "Token IDs: [34]\n",
      "Token IDs: [706, 12, 15, 18, 34, 431, 269, 482]\n",
      "Token IDs: [706, 23, 707]\n",
      "Token IDs: [706, 23, 708]\n",
      "Token IDs: [706, 23, 709]\n",
      "Token IDs: [706, 23, 710]\n",
      "Token IDs: [706, 23, 711]\n",
      "Token IDs: [706, 23, 712]\n",
      "Token IDs: [706, 23, 713]\n",
      "Token IDs: [706, 23, 714]\n",
      "Token IDs: [146, 62, 12, 715, 113, 116, 107, 160, 114]\n",
      "Token IDs: [154, 1181]\n",
      "Token IDs: [487, 1181, 356, 1181]\n",
      "Token IDs: [487, 1181, 356, 1181]\n",
      "Token IDs: [513]\n",
      "Token IDs: [302, 133, 1181]\n",
      "Token IDs: [302, 133, 1181]\n",
      "Token IDs: [513]\n",
      "Token IDs: [706, 12, 15, 18, 34, 431, 269, 482]\n",
      "Token IDs: [52, 356, 119, 123, 85, 107, 718, 19, 719]\n",
      "Token IDs: [706, 23, 708]\n",
      "Token IDs: [706, 23, 709]\n",
      "Token IDs: [706, 23, 710]\n",
      "Token IDs: [706, 23, 711]\n",
      "Token IDs: [706, 23, 712]\n",
      "Token IDs: [706, 23, 713]\n",
      "Token IDs: [146, 62, 12, 715, 113, 116, 107, 160, 114]\n",
      "Token IDs: [431, 49, 302, 513, 178, 62, 179, 720, 175]\n",
      "Token IDs: [18, 49, 178, 62, 179, 722, 131, 62, 132, 723]\n",
      "Token IDs: [15, 49, 121, 724]\n",
      "Token IDs: [418, 56, 648]\n",
      "Token IDs: [131, 62, 725]\n",
      "Token IDs: [34, 49, 121]\n",
      "Token IDs: [724, 418, 56]\n",
      "Token IDs: [726]\n",
      "Token IDs: [98]\n",
      "Token IDs: [730, 15, 49, 50, 105, 82, 86, 92, 594, 727]\n",
      "Token IDs: [50, 537, 715, 287, 50, 732, 448, 203, 45, 40]\n",
      "Token IDs: [706, 627, 50, 715, 28, 154, 1181, 23, 273, 2]\n",
      "Token IDs: [163, 45, 692, 1181, 702, 23, 100, 738, 739, 740]\n",
      "Token IDs: [1181, 356, 1181, 513, 7, 11, 69, 178, 62, 380]\n",
      "Token IDs: [743, 744, 2, 50, 178, 62, 380, 133, 1181, 513]\n",
      "Token IDs: [751, 23, 144, 49, 934, 754, 50, 755, 28, 100]\n",
      "Token IDs: [758, 759, 56, 123, 760, 82, 34, 23, 12, 761]\n",
      "Token IDs: [34, 23, 12, 55, 76, 49, 163, 45, 692, 1181]\n",
      "Token IDs: [50, 105, 28, 183, 45, 692, 1181, 302, 217, 689]\n",
      "Token IDs: [765]\n",
      "Token IDs: [4, 730, 15, 23, 50, 105, 766, 75, 204, 767]\n",
      "Token IDs: [18, 7, 19, 1181, 356, 1181, 7, 2, 771, 772]\n",
      "Token IDs: [41, 467, 163, 464, 23, 56, 774, 775, 93, 776]\n",
      "Token IDs: [771, 779, 1181, 19, 133, 1181, 7, 19, 390, 233]\n",
      "Token IDs: [431]\n",
      "Token IDs: [12, 23, 50, 733, 154, 1181, 734, 411, 73, 45]\n",
      "Token IDs: [781, 45, 146, 62, 12, 715, 28, 706, 23, 782]\n",
      "Token IDs: [787, 41, 50, 463, 62, 251, 85, 23, 1169, 785]\n",
      "Token IDs: [40, 41, 202, 469, 28, 50, 470, 11, 123, 389]\n",
      "Token IDs: [387, 296, 273, 2, 202, 467, 464, 11, 188, 273]\n",
      "Token IDs: [15, 23, 50, 738, 739, 1181, 356, 1181, 736, 411]\n",
      "Token IDs: [50, 641, 152, 50, 154, 1181, 736, 23, 73, 107]\n",
      "Token IDs: [308, 0, 0, 0, 3, 19, 59, 60, 11, 798]\n",
      "Token IDs: [641, 70, 11, 284, 290, 390, 152, 403, 23, 86]\n",
      "Token IDs: [19, 269, 11, 433, 34, 19, 431, 11, 44, 803]\n",
      "Token IDs: [50, 118, 85, 805, 648, 131, 62, 132, 3, 41]\n",
      "Token IDs: [73, 807, 168, 100, 235, 4, 45, 808, 69, 648]\n",
      "Token IDs: [809, 83, 50, 638, 172, 810, 23, 828, 812, 124]\n",
      "Token IDs: [815, 11, 71, 86, 124, 202, 565, 296, 50, 172]\n",
      "Token IDs: [107, 819, 820, 56, 50, 154, 388, 11, 152, 107]\n",
      "Token IDs: [244, 245, 52, 491, 11, 273, 107, 554, 56, 823]\n",
      "Token IDs: [107, 826, 827, 203, 828, 812, 56, 424, 45, 813]\n",
      "Token IDs: [273, 107, 554, 131, 62, 725, 28, 3, 130, 172]\n",
      "Token IDs: [831, 172, 23, 56, 677, 832, 284, 87, 64, 88]\n",
      "Token IDs: [126, 127, 139, 65, 803, 4, 50, 145, 28, 45]\n",
      "Token IDs: [18, 23, 50, 745, 739, 1181, 356, 1181, 513, 736]\n",
      "Token IDs: [122, 68, 113, 123, 497, 107, 50, 641, 152, 50]\n",
      "Token IDs: [597, 23, 651, 178, 62, 179, 252, 835, 50, 121]\n",
      "Token IDs: [34, 23, 50, 738, 743, 133, 1181, 744, 411, 50]\n",
      "Token IDs: [838, 82, 45, 56, 1181, 11, 839, 500, 73, 11]\n",
      "Token IDs: [0, 0, 0, 19, 59, 840, 3, 72, 11, 71]\n",
      "Token IDs: [698, 841, 45, 842, 804, 11, 19, 172, 34, 62]\n",
      "Token IDs: [56, 50, 1181, 356, 1181, 736, 11, 93, 159, 845]\n",
      "Token IDs: [804, 82, 847, 131, 62, 725, 19, 50, 804, 82]\n",
      "Token IDs: [18, 11, 34, 11, 19, 431, 11, 50, 51, 434]\n",
      "Token IDs: [50, 98, 28, 726, 848, 50, 804, 4, 121, 23]\n",
      "Token IDs: [1750, 474, 28, 852, 368, 302, 328, 88, 426, 853]\n",
      "Token IDs: [113, 0, 0, 0, 0, 0, 0, 212, 213, 23]\n",
      "Token IDs: [56, 857, 73, 302, 82, 45, 575, 52, 107, 662]\n",
      "Token IDs: [41, 50, 122, 70, 11, 415, 23, 507, 23, 857]\n",
      "Token IDs: [183, 172, 82, 391, 23, 93, 860, 86, 107, 50]\n",
      "Token IDs: [852, 87, 88, 861, 862, 52, 83, 52, 11, 19]\n",
      "Token IDs: [328, 88, 863, 11, 864, 73, 100, 434, 848, 4]\n",
      "Token IDs: [431, 23, 50, 745, 743, 133, 1181, 513, 744, 411]\n",
      "Token IDs: [19, 272, 178, 62, 179, 301, 105, 4, 7, 73]\n",
      "Token IDs: [50, 122, 68, 23, 425, 11, 50, 871, 404, 789]\n",
      "Token IDs: [206, 324, 325, 41, 377, 122, 76, 11, 71, 86]\n",
      "Token IDs: [3, 250, 174, 175, 121, 157, 312, 50, 122, 68]\n",
      "Token IDs: [434, 875, 64, 65, 876, 56, 50, 877, 878, 206]\n",
      "Token IDs: [203, 50, 641, 877, 882, 28, 815, 19, 343, 64]\n",
      "Token IDs: [252, 11, 45, 884, 484, 107, 73, 157, 177, 879]\n",
      "Token IDs: [886, 163, 146, 172, 114, 11, 50, 420, 28, 435]\n",
      "Token IDs: [175, 121, 23, 284, 107, 104, 73, 86, 434, 176]\n",
      "Token IDs: [86, 175, 883, 227, 64, 56, 239, 662, 41, 402]\n",
      "Token IDs: [93, 892, 56, 893, 50, 309, 85, 49, 894, 895]\n",
      "Token IDs: [898, 121, 11, 168, 748, 160, 121, 152, 93, 892]\n",
      "Token IDs: [174, 899, 172, 12, 56, 482, 107, 12, 23, 269]\n",
      "Token IDs: [172, 107, 15, 23, 12, 784, 23, 34, 50, 280]\n",
      "Token IDs: [34, 93, 902, 121, 287, 288, 172, 296, 100, 903]\n",
      "Token IDs: [113, 906, 23, 431, 907, 41, 45, 626, 114, 11]\n",
      "Token IDs: [269]\n",
      "Token IDs: [0, 0, 12, 49, 121, 174, 28, 133, 1181, 513]\n",
      "Token IDs: [914, 174, 914, 174]\n",
      "Token IDs: [172, 287, 287]\n",
      "Token IDs: [778, 154, 1181, 487, 1181, 356, 1181, 513]\n",
      "Token IDs: [12, 62, 482, 12, 23, 269, 784, 12, 23, 34]\n",
      "Token IDs: [18, 62, 482, 12, 23, 491, 784, 12, 23, 34]\n",
      "Token IDs: [431, 62, 482, 15, 23, 12, 784, 12, 23, 482]\n",
      "Token IDs: [34, 23, 15, 422, 76, 49, 129, 566, 62, 567]\n",
      "Token IDs: [152, 922, 425, 11, 50, 923, 28, 368, 302, 107]\n",
      "Token IDs: [122, 77, 925, 554, 55, 23, 93, 591, 86, 926]\n",
      "Token IDs: [55, 76, 113, 50, 163, 45, 692, 1181, 702, 703]\n",
      "Token IDs: [553, 566, 62, 567, 928, 464, 56, 45, 19, 42]\n",
      "Token IDs: [929, 76, 152, 422, 152, 565, 298, 50, 192, 791]\n",
      "Token IDs: [50, 638, 62, 369, 930, 28, 730, 18, 841, 50]\n",
      "Token IDs: [19, 1181, 19, 133, 1181, 7, 113, 931, 932, 114]\n",
      "Token IDs: [110, 934, 516, 868, 7, 40, 247, 50, 122, 70]\n",
      "Token IDs: [19, 133, 1181, 114, 23, 138, 7, 430, 160, 161]\n",
      "Token IDs: [139, 65, 418, 56, 435, 202, 572, 464, 316, 28]\n",
      "Token IDs: [34, 23, 18, 163, 428]\n",
      "Token IDs: [93, 148, 392, 56, 163, 11, 940, 428, 296, 0]\n",
      "Token IDs: [7, 28, 110, 69, 698, 308, 172, 19, 41, 50]\n",
      "Token IDs: [284, 107, 42, 56, 952, 142, 69, 64, 50, 819]\n",
      "Token IDs: [287, 56, 45, 955, 85, 40, 41, 45, 956, 68]\n",
      "Token IDs: [50, 638, 62, 749, 930, 28, 730, 18, 841, 50]\n",
      "Token IDs: [51, 356, 172, 203, 958, 959, 28, 356, 23, 121]\n",
      "Token IDs: [724, 56, 293, 62, 963, 540, 203, 172, 18, 513]\n",
      "Token IDs: [101, 6, 7, 139, 64, 65, 152, 965, 152, 284]\n",
      "Token IDs: [19, 319, 68, 449, 83, 0, 0, 0, 0, 0]\n",
      "Token IDs: [849, 677, 7, 328, 968, 944, 19, 945, 946, 41]\n",
      "Token IDs: [212, 213, 23, 113, 219, 114, 969, 11, 93, 300]\n",
      "Token IDs: [975, 19, 171, 28, 172, 11, 152, 403, 152, 235]\n",
      "Token IDs: [341, 256, 404, 364, 981, 11, 71, 284, 139, 403]\n",
      "Token IDs: [983, 574, 56, 322, 662, 160, 121, 203, 163, 428]\n",
      "Token IDs: [50, 144, 930, 28, 730, 18, 841, 50, 105, 28]\n",
      "Token IDs: [177, 984, 50, 121, 28, 117, 985, 154, 773, 23]\n",
      "Token IDs: [2, 988, 899, 50, 171, 28, 172, 356, 73, 2]\n",
      "Token IDs: [154, 68, 23, 86, 966, 989, 110, 990, 991, 23]\n",
      "Token IDs: [370, 3, 993, 257, 962, 152, 356, 155, 203, 422]\n",
      "Token IDs: [113, 744, 114, 11, 168, 45, 804, 83, 50, 165]\n",
      "Token IDs: [309, 11, 156, 157, 82, 45, 158, 70, 107, 160]\n",
      "Token IDs: [86, 996, 166, 139, 997, 82, 0, 0, 0, 0]\n",
      "Token IDs: [7, 816, 325, 257, 41, 50, 319, 0, 0, 0]\n",
      "Token IDs: [1000, 11, 1001, 11, 296, 50, 121, 119, 100, 52]\n",
      "Token IDs: [52, 983, 428, 23, 252, 11, 50, 331, 73, 172]\n",
      "Token IDs: [163, 1003, 873, 73, 1004, 1005, 119, 45, 575, 52]\n",
      "Token IDs: [431]\n",
      "Token IDs: [203, 134, 11, 50, 206, 1006, 28, 50, 85, 168]\n",
      "Token IDs: [963, 62, 388, 732, 121, 23, 662, 160, 1011, 139]\n",
      "Token IDs: [482]\n",
      "Token IDs: [706, 12, 15, 18, 34, 431, 269, 482]\n",
      "Token IDs: [706, 23, 18]\n",
      "Token IDs: [706, 23, 34]\n",
      "Token IDs: [706, 23, 431]\n",
      "Token IDs: [706, 23, 269]\n",
      "Token IDs: [706, 23, 482]\n",
      "Token IDs: [146, 62, 12, 715]\n",
      "Token IDs: [566, 62, 567, 692, 42, 462]\n",
      "Token IDs: [706, 12, 15, 18, 34, 431, 269, 482]\n",
      "Token IDs: [706, 23, 706]\n",
      "Token IDs: [706, 23, 12]\n",
      "Token IDs: [706, 23, 15]\n",
      "Token IDs: [706, 23, 18]\n",
      "Token IDs: [706, 23, 34]\n",
      "Token IDs: [706, 23, 431]\n",
      "Token IDs: [706, 23, 269, 163, 11, 940, 58]\n",
      "Token IDs: [706, 12, 15, 18, 34, 431, 269, 482]\n",
      "Token IDs: [52, 356, 119, 123, 85, 107, 718, 19, 719]\n",
      "Token IDs: [639, 706, 23, 1012]\n",
      "Token IDs: [639, 706, 23, 994]\n",
      "Token IDs: [639, 706, 23, 1013]\n",
      "Token IDs: [639, 706, 23, 1014]\n",
      "Token IDs: [639, 706, 23, 1015]\n",
      "Token IDs: [639, 706, 23, 1016]\n",
      "Token IDs: [706, 23, 1017]\n",
      "Token IDs: [1635, 146, 62, 12, 715, 113, 116, 107, 160, 114]\n",
      "Token IDs: [673]\n",
      "Token IDs: [914, 133, 1181, 11, 163, 702]\n",
      "Token IDs: [914, 133, 1181, 11, 1019, 692, 356, 462]\n",
      "Token IDs: [163, 3]\n",
      "Token IDs: [730, 18, 49, 121, 414, 429, 23, 52, 23, 146]\n",
      "Token IDs: [1023, 7, 40, 56, 50, 643, 42, 644, 122, 70]\n",
      "Token IDs: [940, 428, 23, 144, 49, 50, 146, 110, 1025, 1026]\n",
      "Token IDs: [113, 743, 744, 114, 11, 183, 986, 83, 984, 117]\n",
      "Token IDs: [68, 11, 1027, 117, 163, 58, 430, 160, 83, 966]\n",
      "Token IDs: [93, 256, 133, 1028, 701, 630, 50, 297, 56, 123]\n",
      "Token IDs: [431, 767]\n",
      "Token IDs: [93, 328, 1031, 45, 1032, 203, 1033, 50, 150, 28]\n",
      "Token IDs: [45, 6, 85, 11, 123, 646, 117, 96, 69, 98]\n",
      "Token IDs: [108, 109, 83, 110, 111, 112, 49, 126, 127, 128]\n",
      "Token IDs: [50, 145, 28, 1034, 131, 62, 132, 172, 19, 50]\n",
      "Token IDs: [70, 119, 50, 120, 28, 121, 41, 50, 122, 70]\n",
      "Token IDs: [139, 140, 11, 141, 41, 142, 3, 2, 143, 82]\n",
      "Token IDs: [50, 85, 23, 93, 148, 862, 402, 50, 150, 992]\n",
      "Token IDs: [155, 11, 433, 182, 156, 116, 172, 11, 71, 1035]\n",
      "Token IDs: [158, 77, 2, 160, 161, 163, 428, 23, 165, 11]\n",
      "Token IDs: [3, 159, 381, 175, 121, 157, 177, 1036, 178, 62]\n",
      "Token IDs: [123, 389, 65, 45, 1037, 680, 1038, 203, 1039, 5]\n",
      "Token IDs: []\n",
      "Token IDs: []\n",
      "Token IDs: [1058, 7, 203, 103, 62, 1043, 217]\n",
      "Token IDs: [0, 0, 0, 0, 0]\n",
      "Token IDs: [0, 0, 0, 0, 0, 249, 1193, 28, 56, 0]\n",
      "Token IDs: [0, 0, 0, 727, 0, 0, 0]\n",
      "Token IDs: [0, 0, 0, 0]\n",
      "Token IDs: [0, 0, 0, 0, 0, 727, 23, 0, 0, 0]\n",
      "Token IDs: [0, 0, 0, 0, 0, 249, 1193, 28, 56, 0]\n",
      "Token IDs: [973, 0, 0]\n",
      "Token IDs: [93, 1057, 1058, 7, 203, 50, 379, 28, 103, 62]\n",
      "Token IDs: [45, 1059, 79, 867, 56, 364, 464, 64, 436, 4]\n",
      "Token IDs: [45, 377, 171, 28, 474, 28, 100, 364, 251, 23]\n",
      "Token IDs: [1060, 1061, 4, 123, 210, 159, 65, 697, 83, 1062]\n",
      "Token IDs: [56, 1064, 221, 28, 100, 251, 23, 1026, 56, 1066]\n",
      "Token IDs: [103, 62, 1043, 217, 11, 53, 1068, 45, 1069, 1070]\n",
      "Token IDs: [856, 62, 470, 866, 11, 19, 1073, 1074, 105, 23]\n",
      "Token IDs: [73, 542, 826, 1076, 1077, 159, 766, 1036, 1078, 287]\n",
      "Token IDs: [1067, 1079, 1080, 980, 959, 19, 1081, 62, 217, 23]\n",
      "Token IDs: [347, 1082, 1058, 7, 56, 1083, 62, 1043, 217, 19]\n",
      "Token IDs: [12, 0, 0, 0, 0]\n",
      "Token IDs: [103, 62, 1043, 210, 265, 1013, 11, 1087, 11, 520]\n",
      "Token IDs: [364, 464, 64, 436, 4, 206, 11, 575, 202, 45]\n",
      "Token IDs: [651, 152, 1090, 62, 206, 50, 1091, 41, 50, 364]\n",
      "Token IDs: [1093, 11, 284, 87, 88, 1031, 73, 1094, 328, 50]\n",
      "Token IDs: [512, 202, 45, 283, 134, 28, 100, 364, 251, 107]\n",
      "Token IDs: [110, 1066, 1067, 328, 567, 843, 1097, 4, 103, 62]\n",
      "Token IDs: [1100, 1101, 7, 11, 123, 1102, 133, 1103, 1104, 287]\n",
      "Token IDs: [589, 448, 28, 474, 113, 50, 1106, 448, 114, 56]\n",
      "Token IDs: [1101, 7, 159, 65, 1111, 152, 45, 1112, 1113, 62]\n",
      "Token IDs: [1105, 1061, 23, 0, 0, 0, 11, 86, 1091, 1117]\n",
      "Token IDs: [512, 100, 1123, 107, 1124, 56, 1125, 50, 103, 62]\n",
      "Token IDs: [736, 23, 50, 300, 28, 1122, 989, 50, 206, 379]\n",
      "Token IDs: [1129, 720, 175, 23, 0, 19, 0, 0, 0, 0]\n",
      "Token IDs: [19, 1057, 45, 1081, 62, 217, 353, 56, 103, 62]\n",
      "Token IDs: [0, 0, 0, 265, 906, 268, 56, 173, 50, 1138]\n",
      "Token IDs: [45, 591, 62, 448, 23, 497, 11, 670, 161, 206]\n",
      "Token IDs: [1141, 56, 306, 45, 1142, 1091, 203, 100, 1123, 23]\n",
      "Token IDs: [93, 1143, 50, 379, 28, 103, 62, 1043, 217, 83]\n",
      "Token IDs: [1092, 856, 11, 93, 311, 1147, 50, 1148, 73, 45]\n",
      "Token IDs: [1071, 23, 677, 353, 11, 1058, 7, 11, 107, 1002]\n",
      "Token IDs: [123, 736, 1151, 1152, 45, 283, 1064, 726, 203, 100]\n",
      "Token IDs: [93, 54, 45, 458, 62, 1154, 1155, 28, 50, 601]\n",
      "Token IDs: [291, 45, 251, 594, 727, 1064, 56, 65, 50, 914]\n",
      "Token IDs: [107, 272, 697, 203, 133, 1157, 1109, 903, 83, 936]\n",
      "Token IDs: [1158, 50, 641, 353, 56, 1159, 1083, 62, 1043, 217]\n",
      "Token IDs: [45, 547, 62, 388, 1161, 28, 50, 251, 670, 161]\n",
      "Token IDs: [54, 133, 1105, 28, 50, 1081, 62, 470, 365, 45]\n",
      "Token IDs: [1165, 1505, 311, 83, 51, 1167, 1168, 1169, 119, 0]\n",
      "Token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 23, 1171]\n",
      "Token IDs: [1175]\n",
      "Token IDs: [1176]\n",
      "Token IDs: [1177]\n",
      "Token IDs: [972]\n",
      "Token IDs: [113, 45, 114, 103, 62, 1043]\n",
      "Token IDs: [1178]\n",
      "Token IDs: [1179]\n",
      "Token IDs: [1180]\n",
      "Token IDs: [1175]\n",
      "Token IDs: [1176]\n",
      "Token IDs: [1177]\n",
      "Token IDs: [972]\n",
      "Token IDs: [113, 1181, 114, 1083, 62, 1043]\n",
      "Token IDs: [730, 12, 49, 1058, 7, 4, 50, 103, 62, 1043]\n",
      "Token IDs: [1186, 2, 238, 152, 50, 914, 28, 1157, 1106, 474]\n",
      "Token IDs: [1185, 1186, 2, 1188, 83, 1105, 251, 1081, 62, 470]\n",
      "Token IDs: [2, 1190, 1191, 45, 244, 287, 1063, 56, 251, 1185]\n",
      "Token IDs: [210, 107, 697, 11, 152, 4, 50, 103, 62, 1043]\n",
      "Token IDs: [1157, 1109, 903, 23]\n",
      "Token IDs: [4, 86, 92, 11, 93, 1201, 1058, 7, 203, 315]\n",
      "Token IDs: [1203, 1204, 56, 1101, 7, 4, 50, 404, 62, 1043]\n",
      "Token IDs: [232, 449, 4, 50, 1091, 23, 4, 67, 11, 93]\n",
      "Token IDs: [1210, 50, 300, 28, 251, 755, 152, 1185, 182, 1063]\n",
      "Token IDs: [651, 152, 1213, 507, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs: [507, 0, 0, 0, 0, 0, 0, 153, 240, 1217]\n",
      "Token IDs: [1220, 77, 11, 93, 1073, 333, 62, 28, 62, 50]\n",
      "Token IDs: [257, 1221, 161, 1066, 1081, 62, 217, 1222, 11, 1027]\n",
      "Token IDs: [19, 1083, 62, 1043, 217, 23]\n",
      "Token IDs: [15, 1058, 7]\n",
      "Token IDs: [15, 23, 12, 775]\n",
      "Token IDs: [4, 103, 62, 1043, 210, 93, 2, 575, 45, 377]\n",
      "Token IDs: [481, 113, 1225, 11, 1226, 114, 11, 23, 23, 23]\n",
      "Token IDs: [1198, 107, 50, 1198, 62, 243, 368, 629, 28, 133]\n",
      "Token IDs: [19, 1233, 1230, 481, 12, 11, 23, 23, 23, 11]\n",
      "Token IDs: [15, 23, 15, 1091]\n",
      "Token IDs: [1058, 7, 345, 133, 1019, 62, 243, 726, 1186, 1230]\n",
      "Token IDs: [251, 890, 133, 1105, 232, 1199, 49]\n",
      "Token IDs: [1198, 1241, 0, 168, 1242, 376, 1243, 23, 100, 1064]\n",
      "Token IDs: [107, 50, 914, 629, 28, 50, 1157, 1106, 736, 1244]\n",
      "Token IDs: [1186, 485]\n",
      "Token IDs: [12]\n",
      "Token IDs: [1195, 727, 1194, 1195]\n",
      "Token IDs: [972]\n",
      "Token IDs: [113, 1229, 11, 1233, 114, 1230, 727, 1194]\n",
      "Token IDs: [1199, 113, 1229, 114, 113, 12, 114]\n",
      "Token IDs: [575, 45, 153, 232, 1198, 49, 0, 1247, 0, 1241]\n",
      "Token IDs: [287, 464, 203, 45, 1109, 903, 972, 1002, 41, 45]\n",
      "Token IDs: [1061, 49]\n",
      "Token IDs: [1192, 113, 1193, 485, 1194, 1195, 972, 114, 485, 1197]\n",
      "Token IDs: []\n",
      "Token IDs: [1251, 1197, 113, 639, 1198, 113, 1199, 113, 972, 114]\n",
      "Token IDs: [217, 1253, 83, 1254, 50, 1255, 1256, 62, 1257, 113]\n",
      "Token IDs: [802, 251, 1194, 1191, 727, 626, 1198, 23, 206, 1122]\n",
      "Token IDs: [50, 206, 448, 11, 272, 1263, 45, 780, 28, 474]\n",
      "Token IDs: [780, 28, 50, 1265, 56, 1164, 152, 1109, 736, 23]\n",
      "Token IDs: [1123, 107, 556, 4, 1301, 12, 23]\n",
      "Token IDs: [15]\n",
      "Token IDs: [1301, 12, 206, 1123, 1006, 1268, 203, 1058, 7, 23]\n",
      "Token IDs: [474, 4, 50, 206, 448, 11, 1194, 107, 50, 171]\n",
      "Token IDs: [28, 464, 910, 1123, 11, 356, 727, 107, 50, 171]\n",
      "Token IDs: [474, 910, 251, 23, 163, 1722, 113, 727, 11, 356]\n",
      "Token IDs: [163, 82, 448, 727, 11, 324, 1276, 23]\n",
      "Token IDs: [601, 49, 206, 448, 1198, 485, 481, 113, 1225, 11]\n",
      "Token IDs: [780, 28, 1198, 466, 183, 1274, 113, 1229]\n",
      "Token IDs: [11, 1233, 114, 651, 73, 1233, 485, 1194, 23]\n",
      "Token IDs: [245, 49, 50, 1006, 203, 45, 358, 1280, 206, 1123]\n",
      "Token IDs: [1282, 163, 1722, 113, 481, 12, 11, 23, 23, 23]\n",
      "Token IDs: [203, 1194, 4, 481, 12, 11, 23, 23, 23, 11]\n",
      "Token IDs: [727, 1194, 1282, 163, 1722, 113, 1198, 1189]\n",
      "Token IDs: [11, 356, 727, 114, 23, 1106, 474]\n",
      "Token IDs: [0, 1282, 163, 1722, 113, 1198, 1189]\n",
      "Token IDs: [1287, 727, 1194, 11, 356, 114, 23, 1109, 474]\n",
      "Token IDs: [1186, 1282]\n",
      "Token IDs: [12]\n",
      "Token IDs: [356]\n",
      "Token IDs: [972]\n",
      "Token IDs: [113, 1229, 11, 1233, 114, 1230, 727, 1194]\n",
      "Token IDs: [1199, 113, 1229, 114, 23, 345, 1064, 82, 1106, 474]\n",
      "Token IDs: [1289, 203]\n",
      "Token IDs: [1282, 706, 23, 893, 1006]\n",
      "Token IDs: [203, 1194, 4, 481, 12, 11, 23, 23, 23, 11]\n",
      "Token IDs: [203, 113, 972, 11, 1193, 114, 4, 0, 343]\n",
      "Token IDs: [1282, 513]\n",
      "Token IDs: [12]\n",
      "Token IDs: [356, 356]\n",
      "Token IDs: [1291]\n",
      "Token IDs: [1198, 113, 1199, 113, 972, 114, 11, 1186, 114, 114]\n",
      "Token IDs: [1251]\n",
      "Token IDs: [1197, 113, 639, 1198, 113, 1199, 113, 972, 114, 11]\n",
      "Token IDs: [23, 1006]\n",
      "Token IDs: [1289, 203]\n",
      "Token IDs: [1289, 203]\n",
      "Token IDs: [15, 23, 18, 1058, 7, 152, 1303, 216, 1304]\n",
      "Token IDs: [203, 45, 67, 251, 28, 153, 1298, 11, 671, 152]\n",
      "Token IDs: [133, 1305, 522, 216, 23, 45, 1299, 1181, 0, 0]\n",
      "Token IDs: []\n",
      "Token IDs: [512, 1308, 107, 45, 1309, 11, 859, 1310, 232, 28]\n",
      "Token IDs: [1300, 1314, 1213, 507, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs: [1317]\n",
      "Token IDs: [15]\n",
      "Token IDs: [19, 0, 0, 0, 0, 566, 0, 153, 23]\n",
      "Token IDs: [1064, 1268, 159, 65, 1320, 4, 1321, 28, 1322, 1209]\n",
      "Token IDs: [910, 251, 19, 100, 1106, 903, 1323, 56, 355, 1235]\n",
      "Token IDs: [203, 1181, 0, 0, 1300, 73, 50, 1151, 1324, 1325]\n",
      "Token IDs: [736, 107, 50, 1151, 914, 23, 252, 50, 1064, 1268]\n",
      "Token IDs: [1328, 575, 50, 1106, 448, 602, 182, 45, 1181, 0]\n",
      "Token IDs: [257, 287, 11, 170, 1299, 1305, 522, 1249, 1330, 113]\n",
      "Token IDs: [232, 1333, 159, 65, 1334, 4, 1321, 28, 45, 1335]\n",
      "Token IDs: []\n",
      "Token IDs: [203, 133, 1337, 62, 1112, 1303, 1091, 168, 404, 1151]\n",
      "Token IDs: [772, 56, 1109, 251, 1340, 113, 15, 114, 168, 1199]\n",
      "Token IDs: [7, 2, 1343, 1302, 1303, 216, 1304, 168, 133, 1305]\n",
      "Token IDs: [1336, 83, 1306, 23, 50, 373, 28, 153, 1162, 1344]\n",
      "Token IDs: []\n",
      "Token IDs: [15, 23, 34, 0, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs: [45, 826, 1075, 107, 680, 4, 1350, 1351, 365, 50]\n",
      "Token IDs: [507, 0, 0, 0, 0, 0, 0, 153, 11, 272]\n",
      "Token IDs: [1091, 168, 45, 67, 1352, 265, 1172, 268, 23]\n",
      "Token IDs: [50, 51, 1353, 4, 0, 0, 0, 0, 113, 482]\n",
      "Token IDs: [1356, 23]\n",
      "Token IDs: []\n",
      "Token IDs: [93, 1357, 1358, 41, 1213, 507, 0, 0, 0, 0]\n",
      "Token IDs: [86, 311, 23, 677, 105, 1362, 73, 507, 0, 0]\n",
      "Token IDs: [56, 45, 1154, 1091, 23, 93, 1366, 86, 107, 296]\n",
      "Token IDs: [298, 50, 1105, 232, 23, 344, 11, 86, 107, 50]\n",
      "Token IDs: [1371, 1372, 300, 11, 507, 23, 626, 23, 11, 265]\n",
      "Token IDs: [15, 23, 431, 966, 56, 1101, 7]\n",
      "Token IDs: [1058, 7, 997, 82, 1101, 7, 4, 50, 103, 62]\n",
      "Token IDs: [404, 62, 1043, 1200, 23, 1101, 7, 265, 1099, 268]\n",
      "Token IDs: [1106, 448, 11, 1169, 1058, 7, 173, 45, 1154, 1059]\n",
      "Token IDs: [107, 449, 23, 4, 50, 597, 28, 404, 62, 1043]\n",
      "Token IDs: [1101, 7, 19, 1058, 7, 925, 772, 23]\n",
      "Token IDs: [45, 42, 1378, 107, 142, 284, 989, 596, 56, 300]\n",
      "Token IDs: [312, 50, 171, 28, 1185, 910, 251, 107, 1379, 19]\n",
      "Token IDs: [1383, 56, 347, 1151, 50, 1106, 736, 298, 45, 251]\n",
      "Token IDs: [212, 213, 23, 265, 1172, 268, 19, 0, 0, 0]\n",
      "Token IDs: [863, 82, 50, 628, 1138, 11, 1169, 677, 353, 107]\n",
      "Token IDs: [812, 1386, 23]\n",
      "Token IDs: [0, 0, 1191, 0, 212, 213, 23, 265, 1099, 268]\n",
      "Token IDs: [50, 1106, 19, 1109, 736, 11, 19, 162, 45, 309]\n",
      "Token IDs: [365, 1394, 66, 736, 4, 100, 1123, 23, 138, 389]\n",
      "Token IDs: [7, 11, 1001, 53, 1397, 50, 171, 28, 1242, 376]\n",
      "Token IDs: [1400, 41, 50, 1106, 448, 162, 45, 1401, 62, 1402]\n",
      "Token IDs: [1073, 50, 641, 388, 28, 121, 162, 826, 1076, 959]\n",
      "Token IDs: [15, 23, 269, 1076, 959]\n",
      "Token IDs: [153, 1060, 0, 0, 1191, 0, 212, 213, 23, 265]\n",
      "Token IDs: [1219, 153, 23, 1001, 203, 315, 1058, 19, 1101, 7]\n",
      "Token IDs: [19, 93, 1035, 73, 162, 1213, 507, 0, 0, 0]\n",
      "Token IDs: [1410, 86, 107, 1358, 418, 56, 1219, 153, 64, 937]\n",
      "Token IDs: [1365, 56, 1303, 216, 1304, 703, 4, 15, 23, 18]\n",
      "Token IDs: [1123, 1413, 45, 965, 400, 56, 1414, 1122, 11, 449]\n",
      "Token IDs: [0, 19, 0, 0, 0, 0, 0, 0, 0, 265]\n",
      "Token IDs: [50, 125, 1417, 119, 591, 62, 815, 23, 73, 107]\n",
      "Token IDs: [19, 12, 62, 1043, 217, 11, 272, 206, 1122, 389]\n",
      "Token IDs: [1001, 11, 73, 284, 159, 65, 1419, 1072, 56, 306]\n",
      "Token IDs: [119, 591, 62, 815, 23, 4, 677, 689, 11, 93]\n",
      "Token IDs: [1422, 107, 142, 56, 1416, 356, 727, 11, 69, 643]\n",
      "Token IDs: [93, 1035, 73, 284, 107, 1423, 1424, 56, 306, 19]\n",
      "Token IDs: [15, 23, 482, 1083, 62, 1043, 217]\n",
      "Token IDs: [1083, 62, 1043, 217, 1426, 82, 103, 62, 1043, 217]\n",
      "Token IDs: [206, 736, 11, 93, 2, 575, 45, 251, 1081, 62]\n",
      "Token IDs: [34]\n",
      "Token IDs: [0, 0, 12, 49, 103, 62, 1043, 210, 1427, 41]\n",
      "Token IDs: [431, 62, 400, 23, 1013, 62, 400, 23]\n",
      "Token IDs: [1091, 23, 178, 367, 12, 62, 1043, 431, 62, 1043]\n",
      "Token IDs: [1101, 7, 265, 1099, 268, 1219, 356, 1435, 23, 12]\n",
      "Token IDs: [1101, 7, 265, 1099, 268, 1219, 1193, 1438, 23, 906]\n",
      "Token IDs: [6, 1516, 265, 269, 268, 62, 356, 1435, 23, 12]\n",
      "Token IDs: [1058, 7, 113, 999, 114, 507, 0, 0, 0, 0]\n",
      "Token IDs: [4, 1446, 11, 69, 53, 389, 65, 308, 82, 507]\n",
      "Token IDs: [168, 50, 1083, 62, 1043, 597, 107, 965, 49, 93]\n",
      "Token IDs: [28, 50, 1081, 62, 470, 629, 23, 133, 1453, 28]\n",
      "Token IDs: [284, 1455, 56, 50, 103, 62, 1043, 1454, 107, 478]\n",
      "Token IDs: [903, 1456, 82, 204, 601, 1457, 11, 93, 1035, 284]\n",
      "Token IDs: [1105, 626, 56, 328, 249, 882, 11, 1001, 93, 343]\n",
      "Token IDs: [18, 689]\n",
      "Token IDs: [203, 103, 62, 1043, 217, 11, 93, 697, 689, 41]\n",
      "Token IDs: [28, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs: [689, 41, 50, 223, 1464, 28, 50, 0, 0, 0]\n",
      "Token IDs: [18, 23, 12, 0, 41, 415, 626, 0, 103, 62]\n",
      "Token IDs: [0, 41, 415, 626, 0, 265, 1087, 268, 107, 45]\n",
      "Token IDs: [474, 1477, 168, 100, 1478, 11, 512, 100, 134, 107]\n",
      "Token IDs: [93, 1158, 50, 1454, 28, 0, 0, 1191, 0, 212]\n",
      "Token IDs: [1484, 50, 1478, 464, 168, 1485, 4, 1486, 28, 1487]\n",
      "Token IDs: [1490, 1485, 203, 206, 113, 34, 11, 1491, 464, 4]\n",
      "Token IDs: [591, 23, 677, 1105, 672, 1492, 73, 449, 83, 0]\n",
      "Token IDs: [101, 1494, 23, 100, 1495, 1496, 45, 713, 62, 1497]\n",
      "Token IDs: [265, 1015, 268, 11, 45, 971, 970, 19, 45, 15]\n",
      "Token IDs: [43, 86, 672, 105, 4, 45, 713, 62, 243, 245]\n",
      "Token IDs: [1105, 315, 1106, 19, 1109, 736, 23, 183, 28, 677]\n",
      "Token IDs: [93, 449, 133, 1505, 217, 1506, 28, 1015, 639, 18]\n",
      "Token IDs: [19, 1507, 50, 217, 1506, 4, 469, 1508, 1509, 1122]\n",
      "Token IDs: [1511, 124, 449, 235, 161, 1499, 946, 23]\n",
      "Token IDs: [93, 40, 1058, 7, 162, 507, 0, 0, 0, 0]\n",
      "Token IDs: [206, 1122, 466, 711, 464, 19, 431, 1109, 736, 910]\n",
      "Token IDs: [56, 1416, 50, 206, 62, 1043, 168, 50, 591, 62]\n",
      "Token IDs: [1123, 670, 161, 1513, 23, 93, 392, 1514, 958, 1515]\n",
      "Token IDs: [265, 269, 268, 19, 315, 50, 178, 62, 380, 19]\n",
      "Token IDs: [210, 715, 203, 677, 665, 912, 287, 463, 358, 1280]\n",
      "Token IDs: [448, 23, 50, 105, 2, 478, 4, 0, 0, 12]\n",
      "Token IDs: [68, 23]\n",
      "Token IDs: [18, 23, 15, 1119, 192, 791, 103, 62, 1043, 210]\n",
      "Token IDs: [50, 1119, 192, 791, 68, 11, 631, 1100, 83, 0]\n",
      "Token IDs: [0, 0, 0, 0, 0, 0, 0, 68, 265, 1466]\n",
      "Token IDs: [1519, 1247, 1519, 1520, 365, 1521, 464, 168, 1522, 474]\n",
      "Token IDs: [1523, 83, 0, 19, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs: [203, 103, 62, 1043, 217, 23, 117, 702, 300, 45]\n",
      "Token IDs: [732, 11, 19, 1013, 591, 464, 23, 93, 1158, 117]\n",
      "Token IDs: [162, 50, 1087, 732, 464, 203, 1524, 175, 121, 202]\n",
      "Token IDs: [93, 300, 50, 641, 410, 62, 1495, 1105, 672, 152]\n",
      "Token IDs: [284, 105, 4, 45, 1526, 62, 243, 245, 1061, 418]\n",
      "Token IDs: [431]\n",
      "Token IDs: [0, 0, 15, 49, 103, 62, 1043, 210, 1427, 41]\n",
      "Token IDs: [1522, 591, 1122, 19, 2, 426, 168, 1528, 784, 1529]\n",
      "Token IDs: [431, 62, 400, 23]\n",
      "Token IDs: [1091, 23, 178, 367, 12, 62, 1043, 431, 62, 1043]\n",
      "Token IDs: [1113, 1114, 727, 1048, 1219, 356, 1374, 23, 1534, 1535]\n",
      "Token IDs: [1101, 7, 265, 1099, 268, 1048, 1219, 356, 1538, 23]\n",
      "Token IDs: [1101, 7, 1461, 0, 265, 1099, 268, 1048, 1219, 356]\n",
      "Token IDs: [1081, 62, 1140, 0, 0, 0, 265, 1132, 268, 1048]\n",
      "Token IDs: [62, 356, 1538, 23, 1548, 1535, 706, 23, 1549, 784]\n",
      "Token IDs: [1058, 7, 113, 999, 114, 507, 0, 0, 0, 0]\n",
      "Token IDs: [431, 62, 400]\n",
      "Token IDs: [1219]\n",
      "Token IDs: [431, 62, 400]\n",
      "Token IDs: [507, 0, 0, 0, 0, 23]\n",
      "Token IDs: [1013, 62, 400]\n",
      "Token IDs: [1219]\n",
      "Token IDs: [1013, 62, 400]\n",
      "Token IDs: [507, 0, 0, 0, 0, 23]\n",
      "Token IDs: [12, 62, 1043]\n",
      "Token IDs: [1013, 784]\n",
      "Token IDs: [1012, 784]\n",
      "Token IDs: [1539, 784]\n",
      "Token IDs: [590, 784]\n",
      "Token IDs: [711, 784]\n",
      "Token IDs: [1551, 784]\n",
      "Token IDs: [1552, 784]\n",
      "Token IDs: [12, 62, 1043, 715, 113, 431, 62, 400, 114]\n",
      "Token IDs: [1101, 692, 23, 1709]\n",
      "Token IDs: [431, 62, 400]\n",
      "Token IDs: [1219]\n",
      "Token IDs: [431, 62, 400]\n",
      "Token IDs: [507, 0, 0, 0, 0, 23]\n",
      "Token IDs: [1013, 62, 400]\n",
      "Token IDs: [1219]\n",
      "Token IDs: [1013, 62, 400]\n",
      "Token IDs: [507, 0, 0, 0, 0, 23]\n",
      "Token IDs: [431, 62, 1043]\n",
      "Token IDs: [1013, 784]\n",
      "Token IDs: [1012, 784]\n",
      "Token IDs: [1539, 784]\n",
      "Token IDs: [590, 784]\n",
      "Token IDs: [711, 784]\n",
      "Token IDs: [1551, 784]\n",
      "Token IDs: [1552, 784]\n",
      "Token IDs: [431, 62, 1043, 715, 113, 431, 62, 400, 114]\n",
      "Token IDs: [1101, 1709]\n",
      "Token IDs: [23, 1709]\n",
      "Token IDs: [730, 15, 49, 966, 864, 50, 434, 28, 153, 1060]\n",
      "Token IDs: [41, 431, 62, 400, 210, 715, 203, 315, 1101, 19]\n",
      "Token IDs: [50, 972, 62, 1556, 1557, 1558, 28, 50, 206, 1122]\n",
      "Token IDs: [1557, 431, 62, 400, 591, 715, 203, 50, 1235, 1043]\n",
      "Token IDs: [152, 238, 287, 1522, 591, 1122, 23, 453, 73, 1101]\n",
      "Token IDs: [779, 4, 50, 12, 62, 1043, 597, 23]\n",
      "Token IDs: [300, 50, 641, 217, 1506, 1560, 152, 4, 677, 0]\n",
      "Token IDs: [1562, 1039, 23, 93, 306, 162, 1012, 62, 400, 1122]\n",
      "Token IDs: [431, 62, 1043, 210, 23, 93, 1416, 306, 1043, 56]\n",
      "Token IDs: [1123, 23, 93, 392, 56, 50, 1515, 152, 426, 83]\n",
      "Token IDs: [826, 1113, 1114, 353, 41, 146, 28, 3, 308, 83]\n",
      "Token IDs: [206, 464, 23, 50, 235, 1515, 2, 110, 458, 62]\n",
      "Token IDs: [1388, 19, 1461, 0, 114, 19, 50, 1081, 62, 1140]\n",
      "Token IDs: [1566, 333, 62, 28, 62, 50, 62, 334, 497, 83]\n",
      "Token IDs: [93, 1569, 347, 1075, 11, 56, 1570, 50, 434, 28]\n",
      "Token IDs: [464, 910, 1123, 41, 50, 121, 28, 1058, 7, 19]\n",
      "Token IDs: [50, 1386, 678, 11, 93, 300, 677, 1571, 674, 28]\n",
      "Token IDs: [641, 1105, 672, 152, 677, 1058, 7, 23, 4, 730]\n",
      "Token IDs: [507, 0, 0, 0, 0, 0, 0, 153, 19, 431]\n",
      "Token IDs: [1014, 1109, 736, 910, 251, 910, 1123, 23, 93, 453]\n",
      "Token IDs: [19, 1410, 73, 50, 1527, 1572, 28, 1013, 62, 400]\n",
      "Token IDs: [160, 11, 296, 284, 1574, 50, 1091, 56, 397, 257]\n",
      "Token IDs: [162, 507, 0, 0, 0, 0, 0, 0, 153, 720]\n",
      "Token IDs: [257, 1578, 203, 1058, 7, 11, 4, 123, 1062, 50]\n",
      "Token IDs: [1157, 1106, 736, 107, 257, 1579, 1580, 56, 507, 0]\n",
      "Token IDs: [45, 1181, 0, 0, 1212, 23]\n",
      "Token IDs: [18, 23, 18, 0, 1083, 62, 1043, 210]\n",
      "Token IDs: [4, 1153, 56, 1581, 50, 1582, 28, 677, 353, 203]\n",
      "Token IDs: [50, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs: [1470, 1468, 1585, 23, 93, 1586, 1158, 50, 1454, 28]\n",
      "Token IDs: [269]\n",
      "Token IDs: [0, 0, 18, 49, 1083, 62, 1043, 210, 1427, 41]\n",
      "Token IDs: [1091, 192]\n",
      "Token IDs: [3]\n",
      "Token IDs: [590, 62, 400, 23]\n",
      "Token IDs: [706, 62, 1043]\n",
      "Token IDs: [213, 507, 265, 12, 268, 0, 0, 0, 1466, 23]\n",
      "Token IDs: [727, 0, 265, 15, 268, 213, 0, 0, 1539, 23]\n",
      "Token IDs: [1722, 1209, 265, 1597, 268, 213, 0, 0, 1548, 23]\n",
      "Token IDs: [727, 0, 265, 15, 268, 0, 0, 0, 0, 0]\n",
      "Token IDs: [1198, 727, 62, 727, 0, 265, 1588, 268, 0, 0]\n",
      "Token IDs: [0, 0, 0, 0, 265, 1588, 268, 0, 0, 0]\n",
      "Token IDs: [23, 1709, 113, 999, 114, 0, 0, 0, 0, 0]\n",
      "Token IDs: [117, 702, 56, 1603, 50, 464, 365, 1521, 206, 11]\n",
      "Token IDs: [243, 3, 1605, 83, 1606, 0, 0, 0, 0, 0]\n",
      "Token IDs: [369, 11, 19, 387, 749, 1607, 28, 50, 118, 19]\n",
      "Token IDs: [23, 119, 591, 815, 93, 300, 202]\n",
      "Token IDs: [50, 145, 1612, 28, 50, 118, 192, 23, 203, 251]\n",
      "Token IDs: [1615, 635, 556, 168, 50, 0, 68, 23, 138, 1616]\n",
      "Token IDs: [50, 1468, 1585, 651, 152, 117, 59, 11, 1619, 11]\n",
      "Token IDs: [93, 308, 45, 826, 1154, 1155, 41, 146, 28, 315]\n",
      "Token IDs: [1613, 62, 243, 1615, 635, 56, 173, 45, 12, 11]\n",
      "Token IDs: [1035, 284, 1458, 56, 1623, 50, 251, 1185, 113, 1157]\n",
      "Token IDs: [1377, 50, 1615, 635, 1456, 82, 45, 204, 1624, 161]\n",
      "Token IDs: [1625, 168, 590, 464, 19, 1015, 1109, 43, 910, 251]\n",
      "Token IDs: [168, 0, 119, 45, 1379, 217, 1506, 28, 1015, 639]\n",
      "Token IDs: [19, 628, 1628, 28, 1015, 639, 431]\n",
      "Token IDs: [23, 0, 0, 0, 1630, 41, 732]\n",
      "Token IDs: [1006, 124, 449, 56, 1570, 50, 953, 171, 28, 1631]\n",
      "Token IDs: [448, 23]\n",
      "Token IDs: [0, 0, 18, 841, 73, 93, 1073, 333, 62, 28]\n",
      "Token IDs: [1632, 1616, 152, 251, 1081, 62, 470, 23, 93, 392]\n",
      "Token IDs: [152, 213, 507, 265, 12, 268, 11, 727, 0, 265]\n",
      "Token IDs: [265, 1597, 268, 123, 1633, 133, 727, 0, 41, 45]\n",
      "Token IDs: [1083, 62, 1043, 210, 105, 136, 73, 677, 353, 107]\n",
      "Token IDs: [182, 50, 470, 736, 113, 43, 114, 2, 82, 45]\n",
      "Token IDs: [34, 128, 311]\n",
      "Token IDs: [50, 834, 41, 1060, 217, 107, 1638, 265, 1014, 11]\n",
      "Token IDs: [1100, 1032, 23, 1658, 1717, 727, 1075, 113, 356, 114]\n",
      "Token IDs: [668, 1194, 62, 1113, 62, 1114, 594, 727, 113, 0]\n",
      "Token IDs: [1568, 1113, 1114, 113, 0, 0, 0, 114, 210, 265]\n",
      "Token IDs: [290, 188, 162, 45, 1656, 1006, 73, 1657, 50, 945]\n",
      "Token IDs: [168, 50, 641, 776, 23, 50, 1198, 791, 62, 0]\n",
      "Token IDs: [83, 1632, 45, 6, 85, 56, 430, 50, 1105, 316]\n",
      "Token IDs: [28, 138, 11, 677, 1032, 107, 1640, 55, 56, 50]\n",
      "Token IDs: [6, 85, 56, 430, 50, 1105, 19, 93, 1655, 45]\n",
      "Token IDs: [4, 50, 1648, 1061, 11, 152, 1663, 56, 45, 1568]\n",
      "Token IDs: [19, 458, 62, 1154, 356, 107, 73, 93, 1665, 45]\n",
      "Token IDs: [238, 82, 1063, 56, 100, 251, 594, 727, 1064, 726]\n",
      "Token IDs: [1667, 726, 1668, 28, 50, 171, 28, 470, 736, 19]\n",
      "Token IDs: [640, 1106, 448, 56, 397, 1671, 23]\n",
      "Token IDs: [677, 353, 107, 148, 55, 56, 50, 1113, 251, 914]\n",
      "Token IDs: [83, 50, 914, 28, 355, 474, 23, 86, 353, 124]\n",
      "Token IDs: [45, 1059, 324, 891, 11, 1001, 284, 1676, 41, 45]\n",
      "Token IDs: [15]\n",
      "Token IDs: [3, 1678, 82, 1679, 49, 692, 692, 1680, 23, 694]\n",
      "Token IDs: [482]\n",
      "Token IDs: [50, 597, 512, 50, 1683, 464, 1456, 168, 45, 323]\n",
      "Token IDs: [1117, 6, 7, 56, 458, 62, 1685, 1686, 736, 19]\n",
      "Token IDs: [4, 1153, 56, 1677, 50, 103, 62, 1043, 1200, 23]\n",
      "Token IDs: [430, 458, 62, 1154, 210, 11, 71, 53, 343, 188]\n",
      "Token IDs: [53, 424, 138, 1185, 4, 45, 1690, 62, 1691, 1692]\n",
      "Token IDs: [430, 45, 1693, 62, 1694, 1695, 28, 117, 1154, 1105]\n",
      "Token IDs: [54, 45, 458, 62, 1154, 1105, 4, 133, 1289, 62]\n",
      "Token IDs: [458, 62, 1154, 1059, 73, 887, 202, 1698, 404, 1064]\n",
      "Token IDs: [1700, 56, 235, 153, 1298, 11, 433, 1181, 0, 0]\n",
      "Token IDs: [405, 1641, 103, 62, 1043, 217, 1032, 107, 50, 1081]\n",
      "Token IDs: [0, 0, 0, 0, 0, 0, 0, 265, 1132, 268]\n",
      "Token IDs: [4, 1343, 50, 641, 400, 23, 133, 0, 0, 0]\n",
      "Token IDs: [1123, 11, 168, 50, 121, 1703, 28, 545, 403, 41]\n",
      "Token IDs: [19, 1058, 7, 159, 148, 65, 436, 152, 1704, 28]\n",
      "Token IDs: [826, 1705, 1706, 82, 364, 206, 1122, 305, 1001, 50]\n",
      "Token IDs: [41, 2, 1379, 177, 206, 23, 50, 1461, 0, 1662]\n",
      "Token IDs: [374, 41, 50, 1106, 448, 23, 1001, 11, 4, 50]\n",
      "Token IDs: [826, 1070, 1071, 227, 56, 311, 403, 11, 324, 50]\n",
      "Token IDs: [1123, 23]\n",
      "Token IDs: [1058, 7, 2, 148, 128, 56, 50, 6, 1516, 265]\n",
      "Token IDs: [834, 11, 123, 1713, 50, 1714, 1715, 265, 1518, 11]\n",
      "Token IDs: [670, 161, 985, 736, 23, 404, 1717, 28, 50, 6]\n",
      "Token IDs: [1719, 45, 448, 28, 470, 736, 365, 45, 1718, 629]\n",
      "Token IDs: [68, 11, 1721, 45, 1722, 914, 11, 19, 1606, 45]\n",
      "Token IDs: [1725, 287, 50, 1718, 629, 23, 507, 1198, 0, 0]\n",
      "Token IDs: [50, 0, 41, 415, 626, 0, 68, 83, 1728, 100]\n",
      "Token IDs: [1002, 41, 50, 251, 1729, 1724, 1725, 287, 50, 1718]\n",
      "Token IDs: [82, 50, 1725, 1731, 83, 50, 591, 903, 23, 514]\n",
      "Token IDs: [1718, 203, 100, 251, 23, 1001, 11, 999, 107, 45]\n",
      "Token IDs: [103, 62, 1043, 210, 23]\n",
      "Token IDs: [168, 503, 56, 1083, 62, 1043, 217, 11, 50, 300]\n",
      "Token IDs: [50, 1032, 28, 265, 18, 268, 4, 73, 315, 1107]\n",
      "Token IDs: [353, 28, 265, 1588, 268, 148, 1141, 5, 1738, 1105]\n",
      "Token IDs: [0, 0, 0, 999, 11, 53, 54, 162, 133, 1740]\n",
      "Token IDs: [1666, 561, 56, 541, 1744, 446, 206, 19, 1745, 50]\n",
      "Token IDs: [431]\n",
      "Token IDs: [93, 328, 1100, 45, 826, 1032, 1121, 1058, 7, 203]\n",
      "Token IDs: [1134, 73, 93, 159, 627, 100, 251, 83, 50, 914]\n",
      "Token IDs: [83, 45, 6, 85, 23, 93, 306, 138, 7, 56]\n",
      "Token IDs: [162, 1133, 206, 23, 50, 353, 107, 1748, 1069, 19]\n",
      "Token IDs: [1067, 11, 19, 897, 333, 62, 28, 62, 50, 62]\n",
      "Token IDs: [203, 1101, 7, 113, 1750, 138, 159, 65, 1115, 56]\n",
      "Token IDs: [121, 159, 65, 240, 898, 83, 861, 1728, 50, 196]\n",
      "Token IDs: [1751, 50, 1133, 217, 1454, 23, 93, 347, 136, 402]\n",
      "Token IDs: [7, 56, 50, 1083, 62, 1043, 1205, 11, 19, 1073]\n",
      "Token IDs: [42, 1752, 203, 1753, 311, 107, 56, 1754, 1181, 0]\n",
      "Token IDs: [153, 11, 1235, 56, 251, 62, 1391, 1755, 1756, 1359]\n",
      "Token IDs: [1758, 1759, 28, 86, 11, 208, 217, 45, 1760, 910]\n",
      "Token IDs: [64, 790, 56, 170, 1740, 1762, 11, 342, 73, 50]\n",
      "Token IDs: [1571, 324, 1764, 1765, 1766, 376, 910, 251, 23, 287]\n",
      "Token IDs: [28, 1058, 7, 989, 284, 45, 1769, 353, 203, 103]\n",
      "Mean length: 15.091885441527447\n",
      "Median length: 17.0\n",
      "95th percentile length: 28.0\n",
      "99th percentile length: 40.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4a22323b9944df8c441e59e0764d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 550\u001b[0m\n\u001b[0;32m    548\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing to the dataset with your WordPiece tokenizer\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m dpo_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdpo_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_dpo_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordpiece_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# You can convert to PyTorch tensors after processing\u001b[39;00m\n\u001b[0;32m    554\u001b[0m dpo_dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\dataset_dict.py:853\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 853\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\dataset_dict.py:854\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    853\u001b[0m     {\n\u001b[1;32m--> 854\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    874\u001b[0m     }\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3091\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   3092\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3093\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3094\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3095\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3096\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3097\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3098\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3099\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:3474\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3470\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3471\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3472\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3474\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3479\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3483\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robbi\\anaconda3\\envs\\my_gpu_env_llm\\Lib\\site-packages\\datasets\\arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3352\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3353\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3355\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3356\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3357\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[35], line 550\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    548\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing to the dataset with your WordPiece tokenizer\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m dpo_dataset \u001b[38;5;241m=\u001b[39m dpo_dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpreprocess_dpo_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordpiece_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m)\u001b[49m, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# You can convert to PyTorch tensors after processing\u001b[39;00m\n\u001b[0;32m    554\u001b[0m dpo_dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_rejected\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[35], line 473\u001b[0m, in \u001b[0;36mpreprocess_dpo_data\u001b[1;34m(examples, tokenizer, max_length)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_dpo_data\u001b[39m(examples, tokenizer, max_length):\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;66;03m# Tokenize fields\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m     tokenized_questions \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m     tokenized_chosen \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    475\u001b[0m     tokenized_rejected \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[35], line 264\u001b[0m, in \u001b[0;36mWordPiece.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# Preprocess input text\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot\n\u001b[0;32m    266\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Will store token IDs instead of tokens\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[35], line 341\u001b[0m, in \u001b[0;36mWordPiece.preprocess_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;66;03m# Convert text to lowercase to ensure case insensitivity\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;66;03m# Optionally, handle punctuation by adding spaces around it for better tokenization\u001b[39;00m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;66;03m# This depends on how your vocabulary handles punctuation\u001b[39;00m\n\u001b[0;32m    345\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([.,!?()])\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Test on larger text\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.token_id = None\n",
    "        self.frequency = 0\n",
    "        self.failure_link = None\n",
    "        self.is_end = False  # Add is_end attribute to mark the end of a word\n",
    "        self.token = None  # Add token attribute to store the token associated with the node\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self, unk_token_id=0):\n",
    "        self.root = TrieNode()\n",
    "        self.unk_token_id = unk_token_id\n",
    "\n",
    "    def insert(self, token, token_id, frequency):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.token_id = token_id\n",
    "        node.frequency = frequency\n",
    "\n",
    "    def find_subwords(self, token):\n",
    "        \"\"\"Finds the most probable subwords based on frequency.\"\"\"\n",
    "        node = self.root\n",
    "        best_subwords = []\n",
    "\n",
    "        def dfs(current_node, subword='', collected_subwords=[]):\n",
    "            if current_node.token_id is not None:\n",
    "                # Update to correctly calculate total_frequency based on the structure of collected_subwords\n",
    "                total_frequency = sum(n.frequency for _, _, n in collected_subwords) + current_node.frequency\n",
    "                probability = current_node.frequency / total_frequency if total_frequency else 0\n",
    "                collected_subwords.append((subword, probability, current_node))\n",
    "\n",
    "            for char, next_node in current_node.children.items():\n",
    "                dfs(next_node, subword + char, list(collected_subwords))  # Create a copy of the list to avoid shared state\n",
    "\n",
    "        dfs(node)\n",
    "        best_subwords = sorted(best_subwords, key=lambda x: x[1], reverse=True)\n",
    "        return [subword for subword, _, _ in best_subwords][:5] or [self.unk_token_id]\n",
    "\n",
    "\n",
    "    def compute_failure_links(self):\n",
    "        root = self.root\n",
    "        root.failure_link = root  # Root's failure link points to itself\n",
    "        queue = [root]\n",
    "\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            for char, child_node in current_node.children.items():\n",
    "                queue.append(child_node)\n",
    "\n",
    "                # Follow failure link to find the longest suffix for the child_node\n",
    "                failure_candidate = current_node.failure_link\n",
    "                while failure_candidate != root and char not in failure_candidate.children:\n",
    "                    failure_candidate = failure_candidate.failure_link\n",
    "                child_node.failure_link = failure_candidate.children.get(char, root)\n",
    "\n",
    "\n",
    "class SimpleSentencePiece:\n",
    "    def __init__(self, model_type=\"bpe\", vocab_size=30522):\n",
    "        self.vocab = {}\n",
    "        self.id_to_subword = {}\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.unk_token_id = 0\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model = None if model_type == \"bpe\" else None\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def train(self, text):\n",
    "        if self.model_type == \"bpe\":\n",
    "            self.model = BPE(num_merges=self.vocab_size, unk_token_id=self.unk_token_id)\n",
    "            self.model.train(text)\n",
    "            self.vocab = self.model.vocab\n",
    "            self.id_to_subword = {i: word for word, i in self.vocab.items()}\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported yet.\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.preprocess_text(text)  # Preprocess text before encoding\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        encoded = self.model.encode(text)\n",
    "        print(f\"Encoded: {encoded[:10]}\")  # Print first 10 encoded tokens\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if not self.id_to_subword:\n",
    "            raise ValueError(\"Vocabulary is empty. Ensure the model is trained first.\")\n",
    "        text = \" \".join([self.id_to_subword.get(id_, self.unk_token) for id_ in ids])\n",
    "        text = text.replace(\" </w>\", \"\").replace(\"</w>\", \" \").strip()\n",
    "        return text\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Convert text to lowercase to ensure case insensitivity\n",
    "        text = text.lower()\n",
    "        # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "        text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Trim leading and trailing spaces\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        model_data = {\n",
    "            'vocab': self.vocab,\n",
    "            'id_to_subword': self.id_to_subword,\n",
    "            'model_type': self.model_type,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            # Potentially include other relevant attributes\n",
    "        }\n",
    "        # Save the high-level tokenizer settings\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(model_data, f)\n",
    "        \n",
    "        # Now save the BPE model specifically\n",
    "        if self.model_type == \"bpe\" and self.model:\n",
    "            self.model.save_model(filepath + \"_bpe\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        self.vocab = model_data['vocab']\n",
    "        self.id_to_subword = model_data['id_to_subword']\n",
    "        self.model_type = model_data['model_type']\n",
    "        self.vocab_size = model_data['vocab_size']\n",
    "        \n",
    "        # Assuming model_type is still \"bpe\", we now load the BPE model\n",
    "        if self.model_type == \"bpe\":\n",
    "            self.model = BPE(self.vocab_size, self.unk_token_id)\n",
    "            self.model.load_model(filepath + \"_bpe\")\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, num_merges=100, unk_token_id=0):  # Accept unk_token_id parameter\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        self.num_merges = num_merges\n",
    "        self.unk_token_id = unk_token_id  # Store the unknown token ID\n",
    "\n",
    "    def train(self, text):\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "        vocab = collections.Counter(words)\n",
    "        vocab = {word + '</w>': count for word, count in vocab.items()}\n",
    "        \n",
    "        for _ in range(self.num_merges):  # Use the num_merges from the instance variable\n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best, vocab)\n",
    "            self.merges.append(best)\n",
    "\n",
    "        self.vocab = {word: i for i, word in enumerate(vocab.keys())}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stats(vocab):\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[symbols[i], symbols[i+1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_vocab(pair, vocab):\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in vocab:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = vocab[word]\n",
    "        return v_out\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text into subwords using learned BPE merges.\"\"\"\n",
    "        encoded_tokens = []\n",
    "        for word in re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE):\n",
    "            word += '</w>'\n",
    "            subwords = [word]  # Start with the entire word as one subword\n",
    "            for merge in self.merges:\n",
    "                new_subwords = []\n",
    "                for subword in subwords:\n",
    "                    # If the merge is in subword, split it; otherwise, keep it as is\n",
    "                    if ' '.join(merge) in subword:\n",
    "                        new_subwords.extend(subword.replace(' '.join(merge), ''.join(merge)).split(' '))\n",
    "                    else:\n",
    "                        new_subwords.append(subword)\n",
    "                subwords = new_subwords\n",
    "            encoded_tokens.extend(subwords)\n",
    "        return [self.vocab.get(token, self.unk_token_id) for token in encoded_tokens]\n",
    "    \n",
    "        # New method to save trained model\n",
    "    def save_model(self, filepath):\n",
    "        bpe_data = {\n",
    "            'merges': self.merges,\n",
    "            'vocab': self.vocab,\n",
    "            'num_merges': self.num_merges,\n",
    "            # Include other attributes as needed\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(bpe_data, f)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            bpe_data = json.load(f)\n",
    "        \n",
    "        self.merges = bpe_data['merges']\n",
    "        self.vocab = bpe_data['vocab']\n",
    "        self.num_merges = bpe_data['num_merges']\n",
    "\n",
    "\n",
    "\n",
    "class WordPiece:\n",
    "    def __init__(self, vocab, unk_token_id=0, unk_token=\"[UNK]\"):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token_id = unk_token_id\n",
    "        self.unk_token = unk_token  # Define the unknown token\n",
    "        self.root = self.build_trie(vocab)\n",
    "        self.compute_failure_links(self.root)\n",
    "        print(\"Trie built successfully.\")\n",
    "\n",
    "    # Add debug prints to build_trie to confirm structure\n",
    "    def build_trie(self, vocab):\n",
    "        root = TrieNode()\n",
    "        for token in vocab:\n",
    "            node = root\n",
    "            for char in token:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.is_end = True\n",
    "            node.token = token\n",
    "        print(\"Trie Construction Completed Successfully\")\n",
    "        return root\n",
    "\n",
    "\n",
    "    def compute_failure_links(self, root):\n",
    "        queue = [root]\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "            for char, child_node in current_node.children.items():\n",
    "                failure_node = current_node.failure_link\n",
    "                while failure_node and char not in failure_node.children:\n",
    "                    failure_node = failure_node.failure_link\n",
    "                child_node.failure_link = failure_node.children[char] if failure_node else root\n",
    "                queue.append(child_node)\n",
    "\n",
    "    # Improved debug prints in tokenize method\n",
    "                \n",
    "    def tokenize(self, text):\n",
    "        # Preprocess input text\n",
    "        text = self.preprocess_text(text)\n",
    "        node = self.root\n",
    "        token_ids = []  # Will store token IDs instead of tokens\n",
    "        i = 0\n",
    "\n",
    "        while i < len(text):\n",
    "            char = text[i]\n",
    "            if char == ' ':\n",
    "                node = self.root\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if char not in node.children:\n",
    "                if node != self.root and node.token is not None:\n",
    "                    # Convert found token to its ID\n",
    "                    token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                    token_ids.append(token_id)\n",
    "                    node = self.root  # Reset to root\n",
    "                    continue\n",
    "                else:\n",
    "                    # Append unknown token ID\n",
    "                    token_ids.append(self.unk_token_id)\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "            node = node.children[char]\n",
    "            if node.is_end:\n",
    "                if i + 1 == len(text) or text[i + 1] == ' ':\n",
    "                    # Convert found token to its ID\n",
    "                    token_id = self.vocab.get(node.token, self.unk_token_id)\n",
    "                    token_ids.append(token_id)\n",
    "                    node = self.root\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        print(f\"Token IDs: {token_ids[:10]}\")\n",
    "        return token_ids\n",
    "    '''\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        # Preprocess input text\n",
    "        text = self.preprocess_text(text)\n",
    "        node = self.root\n",
    "        tokens = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(text):\n",
    "            char = text[i]\n",
    "            if char == ' ':\n",
    "                node = self.root\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if char not in node.children:\n",
    "                if node != self.root:\n",
    "                    tokens.append(node.token)  # Add the longest token found\n",
    "                    node = self.root  # Reset to root\n",
    "                    continue\n",
    "                else:\n",
    "                    tokens.append(self.unk_token)\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "            node = node.children[char]\n",
    "            if node.is_end:\n",
    "                if i + 1 == len(text) or text[i + 1] == ' ':\n",
    "                    tokens.append(node.token)\n",
    "                    node = self.root\n",
    "\n",
    "            i += 1\n",
    "        tokens = [token if token is not None else self.unk_token_id for token in tokens]\n",
    "\n",
    "        print(f\"Tokenized: {tokens[:10]}\")\n",
    "        return tokens\n",
    "    '''\n",
    "    def preprocess_text(self, text):\n",
    "        # Convert text to lowercase to ensure case insensitivity\n",
    "        text = text.lower()\n",
    "\n",
    "        # Optionally, handle punctuation by adding spaces around it for better tokenization\n",
    "        # This depends on how your vocabulary handles punctuation\n",
    "        text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Trim leading and trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = [line.strip() for line in file.readlines()]\n",
    "    return texts\n",
    "\n",
    "#texts = load_corpus(\"D:\\\\EXPERT_WEIGHTS\\\\sample.txt\")\n",
    "texts = load_corpus(\"C:/Users/robbi/Expert/sample.txt\")\n",
    "num_merges = 100\n",
    "\n",
    "# Initialize and train the SimpleSentencePiece model with BPE\n",
    "ssp = SimpleSentencePiece(model_type=\"bpe\", vocab_size=30522)\n",
    "ssp.train('\\n'.join(texts))  # Train the model on the entire dataset\n",
    "\n",
    "# Test the model on a subset or the entire dataset\n",
    "for i, text in enumerate(texts[:10]):  # Example: test on the first 100 texts\n",
    "    encoded = ssp.encode(text)\n",
    "    decoded = ssp.decode(encoded)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Decoded: {decoded}\\n\")\n",
    "\n",
    "# Save the trained model\n",
    "ssp.save_model(\"ssp_model.json\")\n",
    "\n",
    "# Create a new instance and load the model\n",
    "new_ssp = SimpleSentencePiece(model_type=\"bpe\")\n",
    "new_ssp.load_model(\"ssp_model.json\")\n",
    "\n",
    "# New Model\n",
    "for i, text in enumerate(texts[:10]):  # Example: test on the first 100 texts\n",
    "    re_encoded = ssp.encode(text)\n",
    "    re_decoded = ssp.decode(re_encoded)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Decoded: {re_decoded}\\n\")\n",
    "\n",
    "\n",
    "# Assuming ssp is your SimpleSentencePiece instance\n",
    "vocab = ssp.vocab  # This gets the vocabulary after BPE training\n",
    "new_vocab = new_ssp.vocab  # This gets the vocabulary after BPE training\n",
    "\n",
    "print(f\"vocab ; {len(vocab)} , new_vocab: {len(new_vocab)}\")\n",
    "\n",
    "def adapt_vocab_for_wordpiece(ssp_vocab):\n",
    "    adapted_vocab = {}\n",
    "    for token, id_or_freq in ssp_vocab.items():\n",
    "        # Check if a token is a continuation subword and not a standalone word\n",
    "        # Since BPE might not mark subwords in a way WordPiece expects, we adapt based on our best approximation\n",
    "        if not token.startswith(\" \") and not token.endswith(\"</w>\"):\n",
    "            adapted_token = \"##\" + token.replace(\"</w>\", \"\")  # Removing BPE's end-of-word marker and prepending \"##\"\n",
    "        else:\n",
    "            adapted_token = token.replace(\"</w>\", \"\")  # Just remove the BPE's end-of-word marker for standalone words\n",
    "\n",
    "        adapted_vocab[adapted_token] = id_or_freq\n",
    "    return adapted_vocab\n",
    "\n",
    "\n",
    "# Assuming ssp is your SimpleSentencePiece instance after BPE training\n",
    "wordpiece_vocab = adapt_vocab_for_wordpiece(ssp.vocab)\n",
    "\n",
    "# Debugging step to ensure vocabulary completeness\n",
    "def debug_vocab(adapted_vocab):\n",
    "    print(\"Sample Vocabulary Check:\")\n",
    "    # Iterate over the first 10 key-value pairs in the adapted vocabulary\n",
    "    for i, (token, id_or_freq) in enumerate(adapted_vocab.items()):\n",
    "        print(f\"{token}: {id_or_freq}\")\n",
    "        if i >= 9:  # Stop after printing 10 entries\n",
    "            break\n",
    "    # Specifically check for subtokens if your tokenizer expects them\n",
    "    subtokens = [token for token in adapted_vocab.keys() if token.startswith(\"##\")]\n",
    "    print(f\"Found {len(subtokens)} subtokens in vocabulary.\")\n",
    "\n",
    "\n",
    "# Ensure wordpiece_vocab is a list of vocabulary tokens\n",
    "debug_vocab(wordpiece_vocab)  # Call this after initializing wordpiece_vocab\n",
    "\n",
    "\n",
    "# Load your WordPiece tokenizer\n",
    "# wordpiece_tokenizer = WordPiece(wordpiece_vocab)  # Assuming wordpiece_vocab is your BPE-trained vocabulary\n",
    "\n",
    "# Initialize WordPiece with the adapted vocabulary\n",
    "wordpiece_tokenizer = WordPiece(wordpiece_vocab, unk_token_id=0, unk_token=\"[UNK]\")\n",
    "\n",
    "for i, text in enumerate(texts[:10]):\n",
    "    print(f\"Testing ': {text}\")\n",
    "    tokenized = wordpiece_tokenizer.tokenize(text)\n",
    "    print(f\"Tokenized: {tokenized}\\n\")\n",
    "\n",
    "# Token Sequence Length Testing\n",
    "'''\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_sequence_lengths(texts, tokenizer):\n",
    "    lengths = [len(tokenizer.tokenize(text)) for text in texts]\n",
    "    return lengths\n",
    "\n",
    "# Assuming `texts` is a list of all sequences in your dataset\n",
    "# and `wordpiece_tokenizer` is your tokenizer instance\n",
    "sequence_lengths = calculate_sequence_lengths(texts, wordpiece_tokenizer)\n",
    "\n",
    "# Analyze length distribution\n",
    "length_counter = Counter(sequence_lengths)\n",
    "print(\"Mean length:\", np.mean(sequence_lengths))\n",
    "print(\"Median length:\", np.median(sequence_lengths))\n",
    "print(\"95th percentile length:\", np.percentile(sequence_lengths, 95))\n",
    "print(\"99th percentile length:\", np.percentile(sequence_lengths, 99))\n",
    "\n",
    "# Visualize the distribution of sequence lengths\n",
    "plt.hist(sequence_lengths, bins=50, alpha=0.75)\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "# Modify the preprocess_dpo_data function to use your WordPiece tokenizer\n",
    "    \n",
    "def preprocess_dpo_data(examples, tokenizer, max_length):\n",
    "    # Tokenize fields\n",
    "    tokenized_questions = tokenizer.tokenize(examples['question'])\n",
    "    tokenized_chosen = tokenizer.tokenize(examples['chosen'])\n",
    "    tokenized_rejected = tokenizer.tokenize(examples['rejected'])\n",
    "\n",
    "    # Pad tokenized sequences to ensure they all have the same length\n",
    "    desired_length = 1000  # or any other length suitable for your model\n",
    "    tokenized_questions = tokenized_questions[:desired_length] + [0] * (desired_length - len(tokenized_questions))\n",
    "    tokenized_chosen = tokenized_chosen[:desired_length] + [0] * (desired_length - len(tokenized_chosen))\n",
    "    tokenized_rejected = tokenized_rejected[:desired_length] + [0] * (desired_length - len(tokenized_rejected))\n",
    "\n",
    "    # Generate labels (ensure labels are correctly sized if they are used here)\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(desired_length)]  # Adjust as needed for your application\n",
    "    \n",
    "    print(f\"Type of tokenized questions: {type(tokenized_questions)}, Length: {len(tokenized_questions)}\")\n",
    "    print(f\"Type of tokenized chosen: {type(tokenized_chosen)}, Length: {len(tokenized_chosen)}\")\n",
    "    print(f\"Type of tokenized rejected: {type(tokenized_rejected)}, Length: {len(tokenized_rejected)}\")\n",
    "    print(f\"First 5 tokens of tokenized questions: {tokenized_questions[:5]}\")\n",
    "    print(f\"tokenized questions length: {len(tokenized_questions)}\")\n",
    "    print(f\"tokenized chosen length: {len(tokenized_chosen)}\")\n",
    "    print(f\"tokenized_rejected length: {len(tokenized_rejected)}\")\n",
    "    print(f\"Labels Length: {len(labels)}\")\n",
    "\n",
    "    return {\n",
    "        'input_ids_question': tokenized_questions,\n",
    "        'input_ids_chosen': tokenized_chosen,\n",
    "        'input_ids_rejected': tokenized_rejected,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "'''\n",
    "def preprocess_dpo_data(examples, tokenizer, max_length):\n",
    "    # Ensure 'question', 'chosen', and 'rejected' fields are strings\n",
    "    question = \" \".join(examples['question']) if isinstance(examples['question'], list) else examples['question']\n",
    "    chosen = \" \".join(examples['chosen']) if isinstance(examples['chosen'], list) else examples['chosen']\n",
    "    rejected = \" \".join(examples['rejected']) if isinstance(examples['rejected'], list) else examples['rejected']\n",
    "\n",
    "    # Tokenize 'question', 'chosen', and 'rejected' fields using your WordPiece tokenizer\n",
    "    tokenized_questions = tokenizer.tokenize(question)\n",
    "    tokenized_chosen = tokenizer.tokenize(chosen)\n",
    "    tokenized_rejected = tokenizer.tokenize(rejected)\n",
    "\n",
    "    # Truncate or pad the tokenized sequences to the specified max_length\n",
    "    tokenized_questions = tokenized_questions[:max_length]  # Truncate if longer than max_length\n",
    "    tokenized_questions += [0] * (max_length - len(tokenized_questions))  # Pad to max_length\n",
    "    tokenized_chosen = tokenized_chosen[:max_length]  # Truncate if longer than max_length\n",
    "    tokenized_chosen += [0] * (max_length - len(tokenized_chosen))  # Pad to max_length\n",
    "    tokenized_rejected = tokenized_rejected[:max_length]  # Truncate if longer than max_length\n",
    "    tokenized_rejected += [0] * (max_length - len(tokenized_rejected))  # Pad to max_length\n",
    "\n",
    "    # Ensure that the length of the labels matches the length of the tokenized sequences\n",
    "    labels_length = len(tokenized_questions)  # Use the length of the tokenized sequences as the labels length\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(labels_length)]  # Generate labels with the correct length\n",
    "\n",
    "    # Pad input_ids_question to the desired length of 1000\n",
    "    tokenized_questions += [0] * (1000 - len(tokenized_questions))\n",
    "    # Debugging: Print types and lengths of tokenized sequences\n",
    "    print(f\"Type of tokenized questions: {type(tokenized_questions)}, Length: {len(tokenized_questions)}\")\n",
    "    print(f\"Type of tokenized chosen: {type(tokenized_chosen)}, Length: {len(tokenized_chosen)}\")\n",
    "    print(f\"Type of tokenized rejected: {type(tokenized_rejected)}, Length: {len(tokenized_rejected)}\")\n",
    "    print(f\"First 5 tokens of tokenized questions: {tokenized_questions[:5]}\")\n",
    "    print(f\"tokenized questions length: {len(tokenized_questions)}\")\n",
    "    print(f\"tokenized chosen length: {len(tokenized_chosen)}\")\n",
    "    print(f\"tokenized_rejected length: {len(tokenized_rejected)}\")\n",
    "    print(f\"Labels Length: {len(labels)}\")\n",
    "\n",
    "    return {\n",
    "        'input_ids_question': tokenized_questions,\n",
    "        'input_ids_chosen': tokenized_chosen,\n",
    "        'input_ids_rejected': tokenized_rejected,\n",
    "        'labels': labels\n",
    "    }\n",
    "'''\n",
    "\n",
    "# Load the DPO dataset from Hugging Face\n",
    "dpo_dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "max_seq_length = 512 // 3\n",
    "# Apply the preprocessing to the dataset with your WordPiece tokenizer\n",
    "dpo_dataset = dpo_dataset.map(lambda x: preprocess_dpo_data(x, wordpiece_tokenizer, max_seq_length), batched=True)\n",
    "\n",
    "\n",
    "# You can convert to PyTorch tensors after processing\n",
    "dpo_dataset.set_format(type='torch', columns=['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(dpo_dataset['train'], batch_size=2, shuffle=True)\n",
    "\n",
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "expert_model = Expert(config)\n",
    "\n",
    "model_vocab_size = expert_model.lmt.decoder.word_embedding.num_embeddings\n",
    "print(f\"Model vocab size: {model_vocab_size}\")\n",
    "print(f\"Model embedding size: {expert_model.lmt.decoder.word_embedding.embedding_dim}\")\n",
    "print(f\"Configured max length: {config.max_length}\")\n",
    "\n",
    "optimizer = AdamW(expert_model.parameters(), lr=1e-5)\n",
    "#save_path = 'D:/EXPERT_WEIGHTS/dpo_model.pth'\n",
    "\n",
    "# Train the DPO model\n",
    "label_column = 'labels'\n",
    "input_columns = ['input_ids_question', 'attention_mask_question', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
    "# Assuming expert_model is an instance of Expert\n",
    "avg_loss = expert_model.train_dpo(train_loader, optimizer, config, save_path)\n",
    "\n",
    "# Save the model\n",
    "#torch.save(expert_model.transformer_dpo.state_dict(), save_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "\n",
    "# Initialize Expert system\n",
    "expert_system = Expert(config)\n",
    "sequence_length = 30 \n",
    "# Load dataset\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=sequence_length)\n",
    "    \n",
    "    # Shift input_ids to create labels and truncate the last token\n",
    "    labels = [seq[1:] + [tokenizer.pad_token_id] for seq in tokenized_output['input_ids']]\n",
    "    tokenized_output['labels'] = labels\n",
    "    \n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=64, shuffle=True)\n",
    "\n",
    "# Rag data\n",
    "train_rag_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        config.rag_dataset['train'][0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        config.rag_dataset['train'][0],\n",
    "        config.rag_dataset['train'][0],\n",
    "        config.rag_dataset['train'][0],\n",
    "        # Contexts from MAMBA.pdf\n",
    "        config.rag_dataset['train'][1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        config.rag_dataset['train'][1],\n",
    "        config.rag_dataset['train'][1],\n",
    "        config.rag_dataset['train'][1],\n",
    "        # Contexts from QLORA.pdf\n",
    "        config.rag_dataset['train'][2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        config.rag_dataset['train'][2],\n",
    "        config.rag_dataset['train'][2],\n",
    "        config.rag_dataset['train'][2],\n",
    "        # Contexts from RAG.pdf\n",
    "        config.rag_dataset['train'][3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        config.rag_dataset['train'][3],\n",
    "        config.rag_dataset['train'][3],\n",
    "        config.rag_dataset['train'][3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset['train'][4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset['train'][4],\n",
    "        config.rag_dataset['train'][4],\n",
    "        config.rag_dataset['train'][4]\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Train the LMT_Rag sub-model within the Expert system\n",
    "# Train th model\n",
    "model, average_loss = expert_system.train_language_model_rag(\n",
    "    expert_system.transformer_rag,  # Assuming transformer_rag is your RAG model within Expert\n",
    "    config.rag_model_path,\n",
    "    train_loader,\n",
    "    device = config.device,\n",
    "    num_epochs=5,\n",
    "    lr=1e-5,  # Adjust learning rate as needed\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Train DPR encoders\n",
    "(context_encoder, question_encoder), average_loss = expert_system.train_dpr_encoders(\n",
    "    train_rag_data,\n",
    "    expert_system.context_encoder,  # Assuming you have these initialized within Expert\n",
    "    expert_system.question_encoder,\n",
    "    optimizer_context = AdamW(expert_system.context_encoder.parameters(), lr=1e-5),  \n",
    "    optimizer_question = AdamW(expert_system.question_encoder.parameters(), lr=1e-5),\n",
    "    epochs=5,\n",
    "    context_save_path=config.context_encoder_path,\n",
    "    question_save_path=config.question_encoder_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Expert model and optimizer\n",
    "config = ExpertConfig()\n",
    "\n",
    "# Initialize Expert system\n",
    "expert_system = Expert(config)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset('wikipedia', '20220301.simple')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "    \n",
    "    # Convert lists to PyTorch tensors\n",
    "    input_ids = torch.tensor(tokenized_output['input_ids']).to(config.device)\n",
    "    attention_mask = torch.tensor(tokenized_output['attention_mask']).to(config.device)\n",
    "\n",
    "    # Creating labels by shifting the input_ids\n",
    "    labels = input_ids[:, :-1].clone().to(config.device)\n",
    "    labels = torch.nn.functional.pad(labels, (0, 1), value=tokenizer.pad_token_id)  # Pad labels to match sequence length\n",
    "    \n",
    "    tokenized_output['input_ids'] = input_ids\n",
    "    tokenized_output['attention_mask'] = attention_mask\n",
    "    tokenized_output['labels'] = labels\n",
    "\n",
    "    return tokenized_output\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "# DataLoader - same as your working code\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "# DataLoader - same as your working code\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "expert_system.train_mamba(train_loader, 5, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Whole Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Expert model and Data\n",
    "config = ExpertConfig()\n",
    "\n",
    "# Initialize Expert system\n",
    "expert_system = Expert(config)\n",
    "\n",
    "# Load Wikipedia dataset and preprocess\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:1%]\")  # Using 1% of the data for demonstration\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained(config.tokenizer_name)\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=config.max_length)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])  # Remove original text to only keep tokenized versions\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "\n",
    "main_loss_function = torch.nn.CrossEntropyLoss()\n",
    "aux_loss_weight = 0.1  # Adjust based on the significance of the auxiliary loss in your training\n",
    "\n",
    "optimizer = torch.optim.AdamW(expert_model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "# Rag data\n",
    "train_rag_data = {\n",
    "    \"queries\": [\n",
    "        # Queries for DPO.pdf\n",
    "        \"What is Direct Preference Optimization (DPO)?\",\n",
    "        \"How does Direct Preference Optimization work?\",\n",
    "        \"How can I implement Direct Preference Optimization in my organization?\",\n",
    "        \"Why does Direct Preference Optimization improve the efficiency of language modelling?\",\n",
    "        # Queries for MAMBA.pdf\n",
    "        \"What is MAMBA?\",\n",
    "        \"How does MAMBA function?\",\n",
    "        \"How can I build a system based on MAMBA technology?\",\n",
    "        \"Why does MAMBA enhance the performance of its application area?\",\n",
    "        # Queries for QLORA.pdf\n",
    "        \"What is QLORA?\",\n",
    "        \"How does QLORA operate?\",\n",
    "        \"How can I develop a project using QLORA?\",\n",
    "        \"Why does QLORA improve the capabilities of its relevant field?\",\n",
    "        # Queries for RAG.pdf\n",
    "        \"What is Retrieval Augmented Generation (RAG)?\",\n",
    "        \"How does Retrieval Augmented Generation work?\",\n",
    "        \"How can I build a Retrieval Augmented Generation model?\",\n",
    "        \"Why does Retrieval Augmented Generation enhance language model performance?\",\n",
    "        # Queries for SWITCH_TRANSFORMER.pdf\n",
    "        \"What is the Switch Transformer model?\",\n",
    "        \"How does the Switch Transformer model operate?\",\n",
    "        \"How can I construct a Switch Transformer model?\",\n",
    "        \"Why does the Switch Transformer model improve language processing tasks?\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        # Contexts from DPO.pdf\n",
    "        config.rag_dataset['train'][0],  # Assuming dataset[0] is the processed content of DPO.pdf\n",
    "        config.rag_dataset['train'][0],\n",
    "        config.rag_dataset['train'][0],\n",
    "        config.rag_dataset['train'][0],\n",
    "        # Contexts from MAMBA.pdf\n",
    "        config.rag_dataset['train'][1],  # Assuming dataset[1] is the processed content of MAMBA.pdf\n",
    "        config.rag_dataset['train'][1],\n",
    "        config.rag_dataset['train'][1],\n",
    "        config.rag_dataset['train'][1],\n",
    "        # Contexts from QLORA.pdf\n",
    "        config.rag_dataset['train'][2],  # Assuming dataset[2] is the processed content of QLORA.pdf\n",
    "        config.rag_dataset['train'][2],\n",
    "        config.rag_dataset['train'][2],\n",
    "        config.rag_dataset['train'][2],\n",
    "        # Contexts from RAG.pdf\n",
    "        config.rag_dataset['train'][3],  # Assuming dataset[3] is the processed content of RAG.pdf\n",
    "        config.rag_dataset['train'][3],\n",
    "        config.rag_dataset['train'][3],\n",
    "        config.rag_dataset['train'][3],\n",
    "        # Contexts from SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset['train'][4],  # Assuming dataset[4] is the processed content of SWITCH_TRANSFORMER.pdf\n",
    "        config.rag_dataset['train'][4],\n",
    "        config.rag_dataset['train'][4],\n",
    "        config.rag_dataset['train'][4]\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trained_expert_model, average_loss = expert_model.train_expert(\n",
    "    train_loader=train_loader,\n",
    "    train_data=train_rag_data,\n",
    "    optimizer=optimizer,\n",
    "    main_loss_function=main_loss_function,\n",
    "    aux_loss_weight=aux_loss_weight,\n",
    "    device=config.device,\n",
    "    save_path=save_path,\n",
    "    accumulation_steps=4,  # Adjust based on your preference\n",
    "    num_epochs=5  # Adjust based on your training needs\n",
    ")\n",
    "\n",
    "print(f\"Training completed. Average loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Encoding and Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, config: ExpertConfig):\n",
    "        super(Expert, self).__init__()\n",
    "\n",
    "        # Initialize components for other functionalities (e.g., RAG, DPO, MAMBA)\n",
    "        self.rag = self._initialize_rag(config)\n",
    "        self.dpo = self._initialize_dpo(config)\n",
    "        self.mamba = self._initialize_mamba(config)\n",
    "        \n",
    "\n",
    "        # Initialize other necessary layers or parameters\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.layer_norm = nn.LayerNorm(config.input_dim)\n",
    "\n",
    "        # Load the custom Transformer model\n",
    "        self.custom_transformer = TransformerModel(\n",
    "            vocab=config.vocab,  # Assuming vocab is part of config\n",
    "            vocab_size=config.actual_vocab_size,\n",
    "            embedding_dim=128,\n",
    "            max_seq_len=512,\n",
    "            nhead=8,\n",
    "            dim_feedforward=2048,\n",
    "            freq_threshold=config.freq_threshold,\n",
    "            smaller_embed_dim=64\n",
    "        )\n",
    "        self.custom_transformer.load_state_dict(torch.load(config.transformer_model_path))\n",
    "        self.custom_transformer.eval()\n",
    "\n",
    "        # Load the custom tokenizer\n",
    "        with open(config.tokenizer_path, 'rb') as f:\n",
    "            self.custom_tokenizer = pickle.load(f)\n",
    "\n",
    "    def _initialize_rag(self, config):\n",
    "        # Placeholder for initializing the RAG model component\n",
    "        # This would typically involve loading a model and its weights\n",
    "        return None  # Replace with actual initialization\n",
    "\n",
    "    def _initialize_dpo(self, config):\n",
    "        # Placeholder for initializing the DPO model component\n",
    "        return None  # Replace with actual initialization\n",
    "\n",
    "    def _initialize_mamba(self, config):\n",
    "        # Placeholder for initializing the MAMBA model component\n",
    "        return None  # Replace with actual initialization\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Example forward pass using the custom transformer and tokenizer\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        tokenized_input = self.custom_tokenizer.encode(input_text)  # Assuming a method that returns token IDs\n",
    "        tokenized_input = torch.tensor(tokenized_input).unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Pass the tokenized input through the custom transformer\n",
    "        with torch.no_grad():\n",
    "            output_logits, _ = self.custom_transformer(tokenized_input)\n",
    "\n",
    "        # Further processing based on output_logits\n",
    "        # For example, applying softmax to get probabilities for classification tasks\n",
    "        return output_logits\n",
    "\n",
    "# Assuming ExpertConfig is defined elsewhere with the necessary attributes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_gpu_env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
